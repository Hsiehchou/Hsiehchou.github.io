<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>第六章 Python机器学习</title>
      <link href="/2020/06/07/di-liu-zhang-python-ji-qi-xue-xi/"/>
      <url>/2020/06/07/di-liu-zhang-python-ji-qi-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="第六章-Python-机器学习"><a href="#第六章-Python-机器学习" class="headerlink" title="第六章 Python 机器学习"></a>第六章 Python 机器学习</h2><h3 id="1、机器学习基础"><a href="#1、机器学习基础" class="headerlink" title="1、机器学习基础"></a>1、机器学习基础</h3><h4 id="1-1-机器学习分类"><a href="#1-1-机器学习分类" class="headerlink" title="1.1 机器学习分类"></a>1.1 机器学习分类</h4><h5 id="1-1-1-机器学习定义"><a href="#1-1-1-机器学习定义" class="headerlink" title="1.1.1 机器学习定义"></a>1.1.1 机器学习定义</h5><ol><li>机器学习(Machine Learning, ML)是一门综合性非常强的多领域<code>交叉学科</code>，涉及线性代数、概率论、统计学、算法复杂度理论等多门学科。</li><li>机器学习根据已知<code>数据</code>来不断<code>学习和积累经验</code>，然后<code>总结出规律并尝试预测未知数据的属性</code>。机器学习可利用数据或经验等不断<code>改善自身的性能</code>。</li><li>机器学习是目前<code>弱人工智能</code>的核心，其应用十分广泛，如计算机视觉、自然语言处理、生物特征识别、搜索引擎、垃圾邮件过滤、推荐系统、广告投放、信用评价、欺诈检测、股票交易和医疗诊断等应用。</li></ol><h5 id="1-1-2-机器学习分类"><a href="#1-1-2-机器学习分类" class="headerlink" title="1.1.2 机器学习分类"></a>1.1.2 机器学习分类</h5><ol><li><p>可以定义为：机器学习是从<code>数据</code>中自动分析获得<code>模型</code>，并利用模型对未知数据进行<code>预测</code>。可分为：</p></li><li><p>监督学习（supervised learning）<br>1）监督学习：主要特点是要在训练模型时提供给学习系统<code>训练样本以及样本对应的类别标签</code>，因此又称为有导师学习。例：学生从老师那里获取知识、信息，老师提供对错指示、告知最终答案的学习过程。<br>2）典型的监督学习方法：决策树、支持向量机（SVM）、监督式神经网络等分类算法和线性回归等回归算法。<br>3）监督学习目标：利用一组带有<code>标签</code>的数据，学习从<code>输入到输出的映射</code>，然后将这种映射关系<code>应用到未知数据</code>上，达到分类（输出是<code>离散</code>的）或回归（输出是<code>连续</code>的）的目的。<br>4）监督学习（supervised learning）<br>•    目标值：类别——分类问题。<br>•    目标值：连续型的数据——回归问题。<br>5）示例<br>➢    预测明天的气温是多少度？<br>➢    预测明天天气是晴、阴或雨？<br>➢    人的年龄预测？<br>➢    人脸识别？</p></li><li><p>无监督学习（unsupervised learning）<br>1）无监督学习：主要特点是训练时<code>只提供</code>给学习系统<code>训练样本</code>，而没有样本对应的类别标签信息。例：没有老师的情况下，学生从书本或网络自学的过程。<br>2）无监督学习中，训练数据包含一组输入向量而<code>没有相应的目标值</code>。这类算法的目标可能是发现原始数据中<code>相似样本的组合（称作聚类）</code>，或者确定<code>数据的分布（称作密度估计）</code>，或者把数据从高维空间投影到低维空间（称作<code>降维</code>）以便进行可视化。<br>3）典型的无监督学习方法：聚类学习、自组织神经网络学习。</p></li><li><p>半监督学习（semi-supervised learning）<br>1）半监督学习方式下，训练数据有<code>部分被标识</code>，<code>部分没有被标识</code>，这种模型首先需要学习数据的内在结构，以便合理的组织数据来进行预测。算法上，包括一些对常用监督式学习算法的延伸，这些算法首先试图<code>对未标识数据进行建模</code>，在此基础上<code>再对标识的数据进行预测</code>。<br>2）例：给学生很多未分类的书本与少量的清单，清单上说明哪些书属于同一类别，要求对其他所有书本进行分类。</p></li><li><p>强化学习（reinforcement learning，增强学习）<br>1）强化学习：主要特点是通过试错来发现最优行为策略而不是带有标签的样本学习。<br>2）主要包含四个元素，agent，环境状态，行动，奖励, 强化学习的目标就是获得最多的累计奖励。<br>3）例：小孩学走路、下棋 （包括下围棋和象棋）、机器人、自动驾驶等。</p></li></ol><h4 id="1-2-监督学习-分类"><a href="#1-2-监督学习-分类" class="headerlink" title="1.2 监督学习-分类"></a>1.2 监督学习-分类</h4><h5 id="1-2-1-分类学习"><a href="#1-2-1-分类学习" class="headerlink" title="1.2.1 分类学习"></a>1.2.1 分类学习</h5><ol><li><p>输入：一组有<code>标签</code>的训练数据(也称观察和评估)，标签表明了这些数据（观察）的所署类别。</p></li><li><p>输出：分类模型根据这些训练数据，训练自己的<code>模型参数</code>，学习出一个适合这组数据的<code>分类器</code>，当有新数据（非训练数据）需要进行类别判断，就可以将这组新数据作为输入送给学习好的分类器进行判断。</p></li><li><p>数据集<br>1）训练集(training set):顾名思义用来训练模型的<code>已标注数据</code>，用来建立模型，发现规律。<br>2）测试集(testing set):也是<code>已标注数据</code>，通常做法是将<code>标注隐藏</code>，输送给训练好的模型，通过结果与真实标注进行对比，<code>评估模型</code>的学习能力。</p></li><li><p>训练集/测试集划分方法<br>根据已有标注数据，随机选出一部分数据（70%）作为训练数据，余下的作为测试数据，此外还有交叉验证法等用来评估分类模型。</p></li><li><p>分类学习评价标准<br>1）精确率：针对预测结果而言，也称查全率，是所有样本中<code>被识别为A类</code>的样本数量与<code>实际属于A类</code>的样本数量的比值。以二分类为例，它表示的是<code>预测为正</code>的样本中有多少是<code>真正的正</code>样本。那么就有两种可能：一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)。精确率P=TP/(TP+FP)<br>2）召回率：针对原样本而言，它表示的是样本中的正例有多少被预测正确。那也有两种可能：一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(TN)。召回率P=TP/(TP+TN)</p></li><li><p>分类学习评价标准——例：<br>•    假设我们手上有60个正样本，40个负样本，我们要找出所有的正样本。<br>•    TP: 将正类预测为正类数40；TN: 将正类预测为负类数20；FP: 将负类预测为正类数10；FN: 将负类预测为负类数30。<br>•    准确率（accuracy）=预测对的/所有=(TP+FN)/(TP+FN+FP+TN)=(40+30)/100=70%<br>•    精确率（precision）=TP/(TP+FP)=40/(40+10)=80%<br>•    召回率（recall）=TP/(TP+TN)=40/(40+20)=66.7%</p></li><li><p>分类学习应用<br>•    金融：贷款是否批准进行评估。<br>•    医疗诊断：判断一个肿瘤是恶性还是良性。<br>•    欺诈检测：判断一笔银行的交易是否涉嫌欺诈。<br>•    网页分类：判断网页的所属类别，财经或者是娱乐？<br>•    垃圾邮件分类：判定邮件是否为垃圾邮件。</p></li></ol><h5 id="1-2-2-分类学习常用算法"><a href="#1-2-2-分类学习常用算法" class="headerlink" title="1.2.2 分类学习常用算法"></a>1.2.2 分类学习常用算法</h5><ol><li><p>K近邻分类器(KNN)<br>KNN：通过计算待分类数据点，与已有数据集中的所有数据点的距离。取距离最小的前K个点，根据“少数服从多数“的原则，将这个数据点划分为出现次数最多的那个类别。<br><img src="/medias/1591458262946.png" alt="K近邻分类器(KNN)"></p></li><li><p>决策树<br>1）决策树是一种树形结构的分类器，通过顺序询问分类点的属性决定分类点最终的类别。<br>2）通常根据特征的信息增益或其他指标，构建一颗决策树。<br>3）在分类时，只需要按照决策树中的结点依次进行判断，即可得到样本所属类别。<br><img src="/medias/1591458315138.png" alt="判定是否去相亲的决策树"></p></li><li><p>朴素贝叶斯<br>1）朴素贝叶斯分类器是一个以贝叶斯定理为基础的多分类的分类器。<br>2）对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。<br>$$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$</p></li></ol><h4 id="1-3-监督学习-回归分析"><a href="#1-3-监督学习-回归分析" class="headerlink" title="1.3 监督学习-回归分析"></a>1.3 监督学习-回归分析</h4><h5 id="1-3-1-回归分析"><a href="#1-3-1-回归分析" class="headerlink" title="1.3.1 回归分析"></a>1.3.1 回归分析</h5><ol><li><p>回归：统计学分析数据的方法，目的在于了解两个或多个<code>变数间是否相关</code>、研究其<code>相关方向与强度</code>，并建立<code>数学模型</code>以便<code>观察特定变数</code>来<code>预测</code>研究者感兴趣的<code>变数</code>。</p></li><li><p>回归分析可以帮助人们了解在<code>自变量变化时因变量的变化量</code>。一般来说，通过回归分析我们可以由给出的自变量估计因变量的条件期望。</p></li><li><p>回归分析应用：回归方法适合对一些带有时序信息的数据进行预测或者趋势拟合，常用在金融及其他涉及时间序列分析的领域：<br>1）股票趋势预测<br>2）交通流量预测<br>3）房价预测</p></li></ol><h5 id="1-3-2-回归分析方法"><a href="#1-3-2-回归分析方法" class="headerlink" title="1.3.2 回归分析方法"></a>1.3.2 回归分析方法</h5><ol><li>线性回归<br>线性回归 (Linear Regression) 是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，其使用形如 y=wx+b 的<code>线性模型</code>拟合数据输入和输出之间的映射关系。</li></ol><p><img src="/medias/1591458735099.png" alt="线性回归"></p><ol start="2"><li><p>线性回归——实际用途<br>1）利用数据拟合<code>模型进行预测</code>：如果目标是预测或者映射，线性回归可以用来对观测数据集的 y 和 X 的值拟合出一个预测模型。当完成这样一个模型以后，对于一个新增的 X 值，在没有给定与它相配对的 y 的情况下，可以用这个拟合过的模型预测出一个 y 值。<br>2）相关性分析<code>去除冗余</code>：给定一个变量 y 和一些变量 $X_1-X_n$，这些变量有可能与 y 相关，线性回归分析可以用来量化 y 与 $X_i$  之间相关性的强度，评估出与 y 不相关的 $X_i$ ，并识别出哪些 $X_i$ 的子集包含了关于 y 的冗余信息。</p></li><li><p>房价与房屋尺寸关系的线性拟合<br>1）背景：我们可以根据已知的房屋成交价和房屋的尺寸进行线性回归，继而可以对已知房屋尺寸，而未知房屋成交价格的实例进行成交价格的预测。<br>2）目标：对房屋成交信息建立回归方程，并依据回归方程对房屋价格进行预测。</p><p><img src="/medias/1591458884615.png" alt="房价与房屋尺寸关系的线性拟合图"></p></li><li><p>非线性拟合：多项式回归<br>1）多项式回归(Polynomial Regression)是研究一个因变量与一个或多个自变量间多项式的回归分析方法。<br>2）如果自变量只有一个时，称为一元多项式回归；如果自变量有多个时，称为多元多项式回归。</p></li><li><p>什么时候用多项式回归<br>1）在一元回归分析中，如果依变量 y 与自变量 x 的关系为<code>非线性的</code>，但是又找不到适当的函数曲线来拟合，则可以采用一元多项式回归。<br>2）多项式回归的最大优点就是可以<code>通过增加x的高次项对实测点进行逼近</code>，直至满意为止。<br>3）事实上，多项式回归可以处理相当一类非线性问题，它在回归分析中占有重要的地位，因为任一函数都可以分段用多项式来逼近。</p></li></ol><h3 id="2、Scikit-learn-库"><a href="#2、Scikit-learn-库" class="headerlink" title="2、Scikit-learn 库"></a>2、Scikit-learn 库</h3><h4 id="2-1-Sklearn-库概述"><a href="#2-1-Sklearn-库概述" class="headerlink" title="2.1 Sklearn 库概述"></a>2.1 Sklearn 库概述</h4><h5 id="2-1-1-Scikit-learn-库概述"><a href="#2-1-1-Scikit-learn-库概述" class="headerlink" title="2.1.1 Scikit-learn 库概述"></a>2.1.1 Scikit-learn 库概述</h5><ol><li>Scikit-learn 项目最早由数据科学家 David Cournapeau 在2007年发起，使用需要 NumPy 和 SciPy 等其他库的支持，是 Python 中专门针对机器学习应用而发展起来的一款开源扩展库。</li><li>和其他众多的开源项目一样，Scikit-learn 目前主要由社区成员自发进行维护。</li><li>Scikit-learn 相比其他开源项目显得更为保守，主要体现在：一是Scikit-learn 从来不做除机器学习领域之外的其他扩展，二是 Scikit-learn 从来不采用未经广泛验证的算法。</li><li><a href="http://scikit-learn.org/stable/index.html" target="_blank" rel="noopener">http://scikit-learn.org/stable/index.html</a></li></ol><p><img src="/medias/1591459169266.png" alt="Scikit-learn 库"></p><h5 id="2-1-2-Scikit-learn-库数据集"><a href="#2-1-2-Scikit-learn-库数据集" class="headerlink" title="2.1.2 Scikit-learn 库数据集"></a>2.1.2 Scikit-learn 库数据集</h5><p>在机器学习中，经常需要使用各种各样的数据集，Scikit-learn 库提供了一些常用的数据集：</p><table><thead><tr><th align="left">序号</th><th align="left">数据集名称</th><th align="left">调研方式</th><th align="left">数据描述</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">鸢尾花数据集</td><td align="left">Load_iris()</td><td align="left">用于多分类任务的数据集</td></tr><tr><td align="left">2</td><td align="left">波士顿房价数据集</td><td align="left">Load_boston()</td><td align="left">用于回归任务的经典数据集</td></tr><tr><td align="left">3</td><td align="left">糖尿病数据集</td><td align="left">Load_diabetes()</td><td align="left">用于回归任务的经典数据集</td></tr><tr><td align="left">4</td><td align="left">手写数字数据集</td><td align="left">Load_digits()</td><td align="left">用于多分类任务的数据集</td></tr><tr><td align="left">5</td><td align="left">乳腺癌数据集</td><td align="left">Load_breast_cancer()</td><td align="left">经典的用于二分类任务的数据集</td></tr><tr><td align="left">6</td><td align="left">体能训练数据集</td><td align="left">Load_linnerud()</td><td align="left">经典的用于多变量回归任务的数据集</td></tr></tbody></table><h5 id="2-1-3-Scikit-learn-库功能"><a href="#2-1-3-Scikit-learn-库功能" class="headerlink" title="2.1.3 Scikit-learn 库功能"></a>2.1.3 Scikit-learn 库功能</h5><ol><li><p>分类<br>1）分类是指识别给定对象的所属类别，属于监督学习的范畴，最常见的应用场景包括垃圾邮件检测和图像识别等。<br>2）目前 Scikit-learn 已经实现的算法包括：支持向量机（SVM），最近邻，逻辑回归，随机森林，决策树以及多层感知器（MLP）神经网络等。</p></li><li><p>回归<br>1）回归是指预测与给定对象相关联的连续值属性，最常见的应用场景包括预测药物反应和预测股票价格等。<br>2）目前 Scikit-learn 已经实现的算法包括：支持向量回归(SVR)，脊回归，Lasso 回归，弹性网络（Elastic Net），最小角回归（LARS），贝叶斯回归，以及各种不同的鲁棒回归算法等。<br>3）回归算法几乎涵盖了所有开发者的需求范围，而且更重要的是，Scikit-learn 还针对每种算法都提供了简单明了的用例参考。</p></li><li><p>聚类<br>1）聚类是指自动识别具有相似属性的给定对象，并将其分组为集合，属于无监督学习的范畴。<br>2）最常见的应用场景包括顾客细分和试验结果分组。目前 Scikit-learn 已经实现的算法包括：K-均值聚类，谱聚类，均值偏移，分层聚类，DBSCAN 聚类等。</p></li><li><p>数据降维<br>数据降维是指使用主成分分析（PCA）、非负矩阵分解（NMF）或特征选择等降维技术来减少要考虑的随机变量的个数，其主要应用场景包括可视化处理和效率提升。</p></li><li><p>模型选择<br>模型选择是指对于给定参数和模型的比较、验证和选择，其主要目的是通过参数调整来提升精度。目前 Scikitlearn 实现的模块包括：格点搜索，交叉验证和各种针对预测误差评估的度量函数。</p></li><li><p>数据预处理<br>1）数据预处理是指数据的特征提取和归一化，是机器学习过程中的第一个也是最重要的一个环节。<br>2）归一化是指将输入数据转换为具有零均值和单位权方差的新变量，但因为大多数时候都做不到精确等于零，因此会设置一个可接受的范围，一般都要求落在 0-1 之间。<br>3）特征提取是指将文本或图像数据转换为可用于机器学习的数字变量。</p></li></ol><p><img src="/medias/1591459683330.png" alt="Scikit-learn 库功能"></p><h4 id="2-2-Sklearn-库分类算法"><a href="#2-2-Sklearn-库分类算法" class="headerlink" title="2.2 Sklearn 库分类算法"></a>2.2 Sklearn 库分类算法</h4><h5 id="2-2-1-Scikit-learn-库分类算法"><a href="#2-2-1-Scikit-learn-库分类算法" class="headerlink" title="2.2.1 Scikit-learn 库分类算法"></a>2.2.1 Scikit-learn 库分类算法</h5><ol><li>K近邻分类器(KNN)<br>KNN：通过计算待分类数据点，与已有数据集中的所有数据点的距离。取距离最小的前 K 个点，根据“少数服从多数“的原则，将这个数据点划分为出现次数最多的那个类别。</li></ol><p><strong>K近邻算法</strong></p><p> <img src="/medias/1591459969667.png" alt="K近邻分类器(KNN)"></p><ol start="2"><li>Sklearn 中的 k 邻分类器<br>在 sklearn 库中，可以使用 sklearn.neighbors.KNeighborsClassifier 创建一个 K 近邻分类器，主要参数有：<br>1）n_neighbors：用于指定分类器中K的大小(默认值为5)。<br>2）Weights：设置选中的K个点对分类结果影响的权重（默认值为平均权重”uniform”，可以选择“distance”代表越近的点权重越高，或者传入自己编写的以距离为参数的权重计算函数）。<br>3）algorithm：设置用于计算临近点的方法 （ 选项中有ball_tree、kd_tree 和 brute，分别代表不同的寻找邻居的优化算法，默认值为 auto，根据训练数据自动选择）。<br> <strong>k值的选择</strong><br>①    如果 k 值较小，就相当于用较小邻域中的训练实例进行预测，极端情况下 k=1，测试实例只和最接近的一个样本有关，训练误差很小(0)，但是如果这个样本恰好是噪声，预测就会出错。也就是说，如果k值过小，容易产生过拟合，误差过大。<br>②    如果 k 值较大，就相当于用很大邻域中的训练实例进行预测，相当于和估计数据不相近的样本也参与了，造成模型偏差过大。极端情况是 k=n，测试实例的结果是训练数据集中实例最多的类，这样会产生欠拟合。<br>③    在应用中，一般选择较小 k 并且 k 是奇数。通常采用交叉验证的方法来选取合适的 k 值，或者 k 一般低于训练样本数的平方根。<br> <strong>Sklearn 中的 k 邻分类器——示例</strong></li></ol><pre><code>from sklearn.neighbors import KNeighborsClassifier# 创建一组数据X和它对应的标签yX=[[0],[1],[2],[3],[4],[5]]y=[0,0,0,1,1,1]# 使用最近的3个邻居作为分类的依据，得到分类器neigh = KNeighborsClassifier(n_neighbors=3)# 将训练数据 X 和标签 y 送入分类器进行学习neigh.fit(X, y)# 调用 predict() 函数，对未知分类样本 [1.1]分类，可以直接并将需要分类的数据构造为数组形式作为参数传入，得到分类标签作为返回值print(neigh.predict([[1.4]]))print(neigh.predict([[2.4]]))print(neigh.predict([[2.5]]))print(neigh.predict([[2.6]]))</code></pre><p><strong>运行结果</strong><br>[0]<br>[0]<br>[0]<br>[1]</p><ol start="3"><li><p>决策树<br>1）决策树是一种树形结构的分类器，通过顺序询问分类点的属性决定分类点最终的类别。<br>2）通常根据特征的信息增益或其他指标，构建一颗决策树。<br>3）在分类时，只需要按照决策树中的结点依次进行判断，即可得到样本所属类别。</p><p><img src="/medias/1591460354838.png" alt="判定是否去相亲的决策树"><br><strong>Sklearn中的决策树</strong><br>在 sklearn 库中 ， 可以使用sklearn.tree.DecisionTreeClassifier 创建一个决策树用于分类，其主要参数有：<br>1）criterion ：用于选择属性的准则，可以传入 “gini” 代表基尼系数，或者 “entropy”  代表信息增益。<br>2）max_features ：表示在决策树结点进行分裂时，从多少个特征中选择最优特征。可以设定固定数目、百分比或其他标准，默认值是所有特征个数。</p></li></ol><p><strong>Sklearn中的决策树算法示例</strong></p><pre><code>from sklearn import datasets # 导入方法类iris = datasets.load_iris() # 加载 iris 数据集iris_feature = iris.data # 特征数据iris_target = iris.target # 分类数据from sklearn.model_selection import train_test_splitfeature_train, feature_test, target_train, target_test = train_test_split(iris_feature, iris_target, test_size=0.33, random_state=42)from sklearn.tree import DecisionTreeClassifierdt_model = DecisionTreeClassifier() # 所以参数均置为默认状态dt_model.fit(feature_train,target_train) # 使用训练集训练模型predict_results = dt_model.predict(feature_test) # 使用模型对测试集进行预测print(&#39;predict_results:&#39;, predict_results)print(&#39;target_test:&#39;, target_test)</code></pre><p><strong>运行结果</strong><br>predict_results: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0 0 0 2 1 1 0 0 1 1 2 1 2]<br>target_test: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0 0 0 2 1 1 0 0 1 2 2 1 2]</p><ol start="4"><li>Sklearn 中的朴素贝叶斯<br>•    在 sklearn 库中，实现了三个朴素贝叶斯分类器，如下表所示</li></ol><table><thead><tr><th align="left">分类器</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">naive_bayes.GussianNB</td><td align="left">高斯朴素贝叶斯</td></tr><tr><td align="left">naive_bayes.MultinomialNB</td><td align="left">针对多项式模型的斯朴素贝叶斯分类器</td></tr><tr><td align="left">naive_bayes.BernoulliNB</td><td align="left">针对多元伯努利模型的斯朴素贝叶斯分类器</td></tr><tr><td align="left">•    区别在于假设某一特征的所有属于某个类别的观测值符合特定分布，如，分类问题的特征包括人的身高，身高符合高斯分布，这类问题适合高斯朴素贝叶斯</td><td align="left"></td></tr></tbody></table><p><strong>Sklearn中的朴素贝叶斯算法示例</strong></p><pre><code>import numpy as npfrom sklearn.naive_bayes import GaussianNBX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])Y = np.array([1, 1, 1, 2, 2, 2])#使用默认参数，创建一个高斯朴素贝叶斯分类器，并将该分类器赋给变量clf = GaussianNB(priors=None)clf.fit(X, Y)print(clf.predict([[-0.8, -1]]))</code></pre><p><strong>运行结果</strong><br>[1]</p><h4 id="2-3-Sklearn-库回归算法"><a href="#2-3-Sklearn-库回归算法" class="headerlink" title="2.3 Sklearn 库回归算法"></a>2.3 Sklearn 库回归算法</h4><h5 id="2-3-1-Scikit-learn-库回归算法"><a href="#2-3-1-Scikit-learn-库回归算法" class="headerlink" title="2.3.1 Scikit-learn 库回归算法"></a>2.3.1 Scikit-learn 库回归算法</h5><ol><li><p>Sklearn 中的回归算法<br>•    线性回归函数包括：普通线性回归函数（LinearRegression），岭回归（Ridge）， Lasso回归（Lasso）。<br>•    非线性回归函数，如多项式回归（Polynomial Features）通过<br>sklearn.preprocessing 子模块进行调用</p></li><li><p>Sklearn 中的线性回归：房价与房屋尺寸线性拟合<br>•    技术路线：sklearn.linear_model.LinearRegression<br>•    调用 sklearn.linear_model.LinearRegression() 所需参数：<br>1）fit_intercept : 布尔型参数，表示是否计算该模型截距。可选参数；<br>2）normalize : 布尔型参数，若为 True，则X在回归前进行归一化。可选参数。默认值为 False；<br>3）copy_X : 布尔型参数，若为True，则X将被复制；否则将被覆盖。 可选参数。默认值为 True；<br>4）n_jobs : 整型参数，表示用于计算的作业数量；若为 -1，则用所有的 CPU。可选参数。默认值为 1；</p></li></ol><h5 id="2-3-2-Sklearn中的线性回归：房价与房屋尺寸线性拟合"><a href="#2-3-2-Sklearn中的线性回归：房价与房屋尺寸线性拟合" class="headerlink" title="2.3.2 Sklearn中的线性回归：房价与房屋尺寸线性拟合"></a>2.3.2 Sklearn中的线性回归：房价与房屋尺寸线性拟合</h5><pre><code>import matplotlib.pyplot as pltimport numpy as npfrom sklearn import linear_model# 读取数据集datasets_X = []#房屋面积datasets_Y = []#房屋价格fr = open(&#39;prices.txt&#39;,&#39;r&#39;) lines = fr.readlines()for line in lines:    items = line.strip().split(&#39;,&#39;)    datasets_X.append(int(items[0]))#注意加上类型转换    datasets_Y.append(int(items[1]))#将 datasets_X 转换为二维数组，以符合 linear.fit 函数的参数要求datasets_X = np.array(datasets_X).reshape([-1,1])datasets_Y = np.array(datasets_Y)#以数据datasets_X的最大值和最小值为范围，建立等差数列，方便后续画图。 minX = min(datasets_X)maxX = max(datasets_X)X = np.arange(minX,maxX).reshape([-1,1])linear = linear_model.LinearRegression() linear.fit(datasets_X, datasets_Y)# 图像中显示plt.scatter(datasets_X, datasets_Y, color = &#39;red&#39;,label=&#39;origin data&#39;)plt.plot(X, linear.predict(X), color = &#39;blue&#39;,label=&#39;linear regression prediction&#39;) plt.legend()  #使label生效plt.xlabel(&#39;Area&#39;)plt.ylabel(&#39;Price&#39;)plt.show()</code></pre><h3 id="3、线性回归预测实战"><a href="#3、线性回归预测实战" class="headerlink" title="3、线性回归预测实战"></a>3、线性回归预测实战</h3><h4 id="3-1-线性回归原理"><a href="#3-1-线性回归原理" class="headerlink" title="3.1 线性回归原理"></a>3.1 线性回归原理</h4><h5 id="3-1-1-“回归”的由来"><a href="#3-1-1-“回归”的由来" class="headerlink" title="3.1.1 “回归”的由来"></a>3.1.1 “回归”的由来</h5><ol><li><p>“回归” 是由高尔顿在研究人类遗传问题时提出来的。</p></li><li><p>为研究父代与子代身高的关系，高尔顿搜集了 1078 对父亲及其儿子的身高数据并进行了深入分析，发现了回归效应:</p></li><li><p>当父亲高于平均身高时，他们的儿子身高比他更高的概率要小于比他更矮的概率；父亲矮于平均身高时，他们的儿子身高比他更矮的概率要小于比他更高的概率。</p></li><li><p>即这两种身高父亲的儿子的身高，有向他们父辈的平均身高回归的趋势，解释为：大自然约束使人类身高分布相对稳定而非两极分化，就是所谓的“回归效应”。</p></li></ol><h5 id="3-1-2-回归分析概念"><a href="#3-1-2-回归分析概念" class="headerlink" title="3.1.2 回归分析概念"></a>3.1.2 回归分析概念</h5><ol><li>回归分析法：指将具有相关关系的两个变量之间的数量关系进行测定，通过建立一个数学表达式进行统计估计和预测的统计研究方法。</li><li>自变量：一般把作为估测依据的变量叫做自变量</li><li>因变量： 待估测的变量</li><li>回归方程：反映自变量和因变量之间联系的数学表达式</li><li>回归模型：某一类回归方程的总称</li></ol><h5 id="3-1-3-回归分析步骤"><a href="#3-1-3-回归分析步骤" class="headerlink" title="3.1.3 回归分析步骤"></a>3.1.3 回归分析步骤</h5><p><img src="/medias/1591461605573.png" alt="回归分析步骤"></p><h4 id="3-2-线性回归模型原理"><a href="#3-2-线性回归模型原理" class="headerlink" title="3.2 线性回归模型原理"></a>3.2 线性回归模型原理</h4><p><img src="/medias/1591461655296.png" alt="线性回归模型"><br><img src="/medias/1591461668594.png" alt="线性回归模型"><br><img src="/medias/1591461678967.png" alt="线性回归模型"></p><h4 id="3-3-预测儿童身高实战"><a href="#3-3-预测儿童身高实战" class="headerlink" title="3.3 预测儿童身高实战"></a>3.3 预测儿童身高实战</h4><h5 id="3-3-1-线性回归预测实战"><a href="#3-3-1-线性回归预测实战" class="headerlink" title="3.3.1 线性回归预测实战"></a>3.3.1 线性回归预测实战</h5><ol><li><p>线性回归预测儿童身高<br>•    假定一个人的身高只受年龄、性别、父母身高、祖父母身高和外祖父母身高这几个因素的影响，并假定大致符合线性关系。<br>•    在其他条件不变的情况下，随着年龄的增长，会越来越高；<br>•    同样，对于其他条件都相同的儿童，其父母身高较高的话，儿童也会略高一些。<br>•    假定到18岁后身高会长期保持固定而不再变化（不考虑年龄太大之后会稍微变矮一点的情况）。<br>•    根据给定<code>训练数据和对应标签</code>，线性拟合出儿童身高模型，预测测试数据儿童身高。</p></li><li><p>线性回归预测儿童身高<br>•    训练数据：每行表示一个样本，包含信息为：儿童年龄，性别（0女1田）、父亲、母亲、祖父、祖母、外祖父、和外祖母的身高。<br>[[1, 0, 180, 165, 175, 165, 170, 165],<br>[3, 0, 180, 165, 175, 165, 173, 165],<br>[4, 0, 180, 165, 175, 165, 170, 165],<br>[6, 0, 180, 165, 175, 165, 170, 165],<br>[8, 1, 180, 165, 175, 167, 170, 165],<br>[10, 0, 180, 166, 175, 165, 170, 165],<br>[11, 0, 180, 165, 175, 165, 170, 165],<br>[12, 0, 180, 165, 175, 165, 170, 165],<br>[13, 1, 180, 165, 175, 165, 170, 165],<br>[14, 0, 180, 165, 175, 165, 170, 165],<br>[17, 0, 170, 165, 175, 165, 170, 165]]<br>• 对应标签数据：儿童身高<br>[60, 90, 100, 110, 130, 140, 150, 164, 160, 163, 168]</p></li><li><p>线性回归预测儿童身高<br>1）导入扩展库</p></li></ol><pre><code>import copyimport numpy as npfrom sklearn import linear_model</code></pre><p>2）训练数据准备</p><pre><code>x = np.array([[1, 0, 180, 165, 175, 165, 170, 165],[3, 0, 180, 165, 175, 165, 173, 165],[4, 0, 180, 165, 175, 165, 170, 165],[6, 0, 180, 165, 175, 165, 170, 165],[8, 1, 180, 165, 175, 167, 170, 165],[10, 0, 180, 166, 175, 165, 170, 165],[11, 0, 180, 165, 175, 165, 170, 165],[12, 0, 180, 165, 175, 165, 170, 165],[13, 1, 180, 165, 175, 165, 170, 165],[14, 0, 180, 165, 175, 165, 170, 165],[17, 0, 170, 165, 175, 165, 170, 165]])y = np.array([60, 90, 100, 110, 130, 140, 150, 164, 160, 163, 168])</code></pre><p>3）创建线性回归模型，得出拟合直线</p><pre><code># 创建线性回归模型lr = linear_model.LinearRegression() # 根据已知数据拟合最佳直线lr.fit(x, y)</code></pre><p>4）待预测数据准备</p><pre><code>xs = np.array([[10, 0, 180, 165, 175, 165, 170, 165],[17, 1, 173, 153, 175, 161, 170, 161],[34, 0, 170, 165, 170, 165, 170, 165]])</code></pre><p>5）预测并输出结果</p><pre><code>for item in xs:    # 为不改变原始数据，进行深复制，并假设超过18岁以后就不再长高了    # 对于18岁以后的年龄，返回18岁时的身高    item1 = copy.deepcopy(item)    if item1[0] &gt; 18:        item1[0] = 18    print(item, &#39;:&#39;, lr.predict(item1.reshape(1,-1)))</code></pre><p><strong>运行结果</strong><br>[ 10   0 180 165 175 165 170 165] : [140.56153846]<br>[ 17   1 173 153 175 161 170 161] : [158.41]<br>[ 34   0 170 165 170 165 170 165] : [176.03076923]</p><h3 id="4、贝叶斯分类实战"><a href="#4、贝叶斯分类实战" class="headerlink" title="4、贝叶斯分类实战"></a>4、贝叶斯分类实战</h3><h4 id="4-1-分类基本概念"><a href="#4-1-分类基本概念" class="headerlink" title="4.1 分类基本概念"></a>4.1 分类基本概念</h4><h5 id="4-1-1-什么是分类"><a href="#4-1-1-什么是分类" class="headerlink" title="4.1.1 什么是分类"></a>4.1.1 什么是分类</h5><p><strong>超市物品分类</strong></p><p><img src="/medias/1591462398944.png" alt="超市物品分类"></p><p><strong>垃圾分类</strong></p><p><img src="/medias/1591462403300.png" alt="垃圾分类"></p><p><strong>生活信息分类</strong></p><p><img src="/medias/1591462408292.png" alt="生活信息分类"></p><h5 id="4-1-2-分类在数据挖掘中的定义"><a href="#4-1-2-分类在数据挖掘中的定义" class="headerlink" title="4.1.2 分类在数据挖掘中的定义"></a>4.1.2 分类在数据挖掘中的定义</h5><p>•    分类就是把一些新的数据项<code>映射到给定类别</code>的中的某一个类别。<br>•    分类属于<code>有监督学习</code>(supervised learning)，与之相对于的是无监督学习<br>(unsupervised learning)，比如聚类。<br>•    分类与聚类的最大区别在于，分类数据中的一部分的<code>类别是已知</code>的，而聚类数据的<code>类别未知</code>。</p><h5 id="4-1-3-分类问题"><a href="#4-1-3-分类问题" class="headerlink" title="4.1.3 分类问题"></a>4.1.3 分类问题</h5><p><strong>动物分类问题</strong><br>根据现有的知识，我们得到了一些关于哺乳动物和鸟类的信息，我们能否对新发现的物种，比如动物A，动物B进行分类？</p><table><thead><tr><th align="left">动物种类</th><th align="left">体型</th><th align="left">翅膀数量</th><th align="left">脚的只数</th><th align="left">是否产蛋</th><th align="left">是否有毛</th><th align="left">类别</th></tr></thead><tbody><tr><td align="left">狗</td><td align="left">中</td><td align="left">0</td><td align="left">4</td><td align="left">否</td><td align="left">是</td><td align="left">哺乳动物</td></tr><tr><td align="left">猪</td><td align="left">大</td><td align="left">0</td><td align="left">4</td><td align="left">否</td><td align="left">是</td><td align="left">哺乳动物</td></tr><tr><td align="left">牛</td><td align="left">大</td><td align="left">0</td><td align="left">4</td><td align="left">否</td><td align="left">是</td><td align="left">哺乳动物</td></tr><tr><td align="left">麻雀</td><td align="left">小</td><td align="left">2</td><td align="left">2</td><td align="left">是</td><td align="left">是</td><td align="left">鸟类</td></tr><tr><td align="left">天鹅</td><td align="left">中</td><td align="left">2</td><td align="left">2</td><td align="left">是</td><td align="left">是</td><td align="left">鸟类</td></tr><tr><td align="left">大雁</td><td align="left">中</td><td align="left">2</td><td align="left">2</td><td align="left">是</td><td align="left">是</td><td align="left">鸟类</td></tr><tr><td align="left">动物A</td><td align="left">大</td><td align="left">0</td><td align="left">2</td><td align="left">是</td><td align="left">无</td><td align="left">?</td></tr><tr><td align="left">动物B</td><td align="left">中</td><td align="left">2</td><td align="left">2</td><td align="left">否</td><td align="left">是</td><td align="left">?</td></tr></tbody></table><h5 id="4-1-4-分类的流程"><a href="#4-1-4-分类的流程" class="headerlink" title="4.1.4 分类的流程"></a>4.1.4 分类的流程</h5><ol><li>步骤一：将样本转化为等维的数据特征（<code>特征提取</code>）。所有样本必须具有相同数量的特征。兼顾特征的全面性和独立性。</li><li>步骤二：选择与类别相关的特征（<code>特征选择</code>）。比如，绿色代表与类别非常相关，黑色代表部分相关，浅蓝色代表完全无关。</li><li>步骤三：建立分类模型或分类器。</li><li>分类器通常可以看作一个函数，它把特征映射到类的空间上。</li><li>$$f(x_{i1},  x_{i2}, x_{i3}, …,x_{in})→y_i$$</li></ol><table><thead><tr><th align="left">动物种类</th><th align="left">体型</th><th align="left">翅膀数量</th><th align="left">脚的只数</th><th align="left">是否产蛋</th><th align="left">是否有毛</th><th align="left">类别</th></tr></thead><tbody><tr><td align="left">狗</td><td align="left">中</td><td align="left">0</td><td align="left">4</td><td align="left">否</td><td align="left">是</td><td align="left">哺乳动物</td></tr><tr><td align="left">猪</td><td align="left">大</td><td align="left">0</td><td align="left">4</td><td align="left">否</td><td align="left">是</td><td align="left">哺乳动物</td></tr><tr><td align="left">牛</td><td align="left">大</td><td align="left">0</td><td align="left">4</td><td align="left">否</td><td align="left">是</td><td align="left">哺乳动物</td></tr><tr><td align="left">麻雀</td><td align="left">小</td><td align="left">2</td><td align="left">2</td><td align="left">是</td><td align="left">是</td><td align="left">鸟类</td></tr><tr><td align="left">天鹅</td><td align="left">中</td><td align="left">2</td><td align="left">2</td><td align="left">是</td><td align="left">是</td><td align="left">鸟类</td></tr><tr><td align="left">大雁</td><td align="left">中</td><td align="left">2</td><td align="left">2</td><td align="left">是</td><td align="left">是</td><td align="left">鸟类</td></tr></tbody></table><h5 id="4-1-5-分类的方法"><a href="#4-1-5-分类的方法" class="headerlink" title="4.1.5 分类的方法"></a>4.1.5 分类的方法</h5><p>1）常用分类算法主要包括相似函数，关联规则分类算法，K近邻分类算法，决策树分类算法，贝叶斯分类算法和基于模糊逻辑，遗传算法，粗糙集和神经网络的分类算法。<br>2）分类算法有很多种，都有各自的优缺点和应用范围，本课程我们以贝叶斯分类算法为例。</p><h4 id="4-2-贝叶斯分类概述"><a href="#4-2-贝叶斯分类概述" class="headerlink" title="4.2 贝叶斯分类概述"></a>4.2 贝叶斯分类概述</h4><ol><li><p>背景<br>1）贝叶斯分类基于<code>贝叶斯</code>定理，贝叶斯定理是由 18 世纪概率论和决策论的研究者 Thomas Bayes 发明的，故用其名字命名为贝叶斯定理。<br>2）通过对分类算法的比较研究发现，朴素贝叶斯分类法可以与决策树和经过挑选的神经网络分类器相媲美。用于大型数据库，贝叶斯分类法也具有很高准确率和速度。</p></li><li><p>先验概率与后验概率<br>1）先验概率：由<code>以往</code>的数据分析得到的概率；<br>2）后验概率：<code>得到信息之后</code>重新加以修正的概率。</p></li><li><p>贝叶斯理论<br>1）简单的说，<code>贝叶斯定理是基于假设的先验概率</code>、给定假设下观察到不同数据的概率，提供了一种计算后验概率的方法。<br>2）在人工智能领域，贝叶斯方法是一种非常具有代表性的<code>不确定性知识表示和推理方法</code>。</p></li><li><p>贝叶斯定理<br>1）P(A)  是 A 的<code>先验概率或边缘概率</code>。之所以称为“先验”是因为它不考虑任何B方面的因素。<br>2）P(A|B) 是已知 B 发生后 A 的<code>条件概率</code>，也由于得自 B 的取值而被称作 A 的<code>后验概率</code>。<br>3）P(B|A ) 是已知 A 发生后 B 的<code>条件概率</code>，也由于得自 A 的取值而被称作B的<code>后验概率</code>。<br>4）P(B)是B的<code>先验概率或边缘概率</code>，也作标准化常量（normalized constant）。<br>$$P(A|B)=P(A)\frac{P(B|A)}{P(B)}$$<br>贝叶斯公式提供了从先验概率 P(A)、P(B) 和 P(B|A) 计算后验概率 P(A|B) 的方法。</p></li><li><p>朴素贝叶斯<br>假设待分类项的各个属性相互独立的情况下构造的分类算法就是朴素贝叶斯算法。</p></li></ol><p><strong>基本思想</strong><br>给定的待分类项 X{a1,a2，…,an}，求解在此项出现条件下各个类别 $y_i$ 出现的概率，哪个 P($y_i$|X) 最大，就把此待分类项归属哪个类别。</p><ol start="6"><li>四种贝叶斯分类器<br>1）Naïve Bayes：朴素贝叶斯，假定各特征变量 x 是相互独立的。<br>2）TAN：对朴素贝叶斯进行了扩展，允许各特征变量所对应的节点构成一棵树。<br>3）BAN：对 TAN 扩展，允许各特征变量所对应的节点间关系构成一个图，而不止是树。<br>4）GBN：一种无约束的贝叶斯网络分类器。</li></ol><p><img src="/medias/1591463598040.png" alt="四种贝叶斯分类器"></p><ol start="7"><li><p>贝叶斯分类流程<br>1）准备阶段：主要是依据具体情况<code>确定特征属性</code>，并且对特征属性进行适当划分。然后就是对一部分待分类项进行人工划分，以确定训练样本。<code>输入是所有的待分类项，输出是特征属性和训练样本</code>。<br>2）分类器训练阶段：计算每个类别在训练样本中出现频率以及每个特征属性划分对每个类别的条件概率估计。<code>输入是特征属性和训练样本，输出是分类器</code>。<br>3）应用阶段：使用分类器对待分类项进行分类，其<code>输入是分类器和待分类项，输出是待分类项与类别的映射关系</code>。</p><p><img src="/medias/1591463645577.png" alt="贝叶斯分类流程"></p></li></ol><h4 id="4-3-垃圾邮件分类实战"><a href="#4-3-垃圾邮件分类实战" class="headerlink" title="4.3 垃圾邮件分类实战"></a>4.3 垃圾邮件分类实战</h4><p>训练数据：<br>垃圾邮件（0.txt至150.txt），其中 0-127 为垃圾邮件，128 至 150 为正常邮件，151 至 156 为测试邮件。</p><p><img src="/medias/1591463717009.png" alt="垃圾邮件分类"></p><h5 id="4-3-1-问题分析"><a href="#4-3-1-问题分析" class="headerlink" title="4.3.1 问题分析"></a>4.3.1 问题分析</h5><p>①    读取全部训练集，<code>删除其中的干扰字符</code>，例如【】*。、，等等，然后<code>分词</code>，再删除<code>长度为 1</code>的单个字，这样的单个字对于文本分类没有贡献，剩下的词汇认为是有效词汇。</p><p>②    统计全部训练集中每个<code>有效词汇的出现次数</code>，截取出现次数最多的<code>前N个</code>。</p><p>③    根据第 1 步预处理后的垃圾邮件和非垃圾邮件内容生成特征向量，统计第 2 步中得到的N个词语分别在该邮件中的出现频率。</p><p>④    根据第 3 步中得到特征向量和已知邮件分类创建并训练朴素贝叶斯模型。</p><p>⑤    读取测试邮件，参考第 1 步，对邮件文本进行预处理，提取特征向量。</p><p>⑥    使用第 4 步中训练好的模型，根据第 5 步提取的特征向量对邮件进行分类。</p><h5 id="4-3-2-垃圾邮件分类实战"><a href="#4-3-2-垃圾邮件分类实战" class="headerlink" title="4.3.2 垃圾邮件分类实战"></a>4.3.2 垃圾邮件分类实战</h5><ol><li>导入扩展库</li></ol><pre><code>from re import subfrom collections import Counter from itertools import chainfrom numpy import arrayfrom jieba import cutfrom sklearn.naive_bayes import MultinomialNB</code></pre><ol start="2"><li>获取文件中所有词</li></ol><pre><code>def getWordsFromFile(txtFile):    words = []    with open(txtFile, encoding=&#39;utf8&#39;) as fp:        for line in fp:            line = line.strip()            line = sub(r&#39;[.【】0-9、—。，！~\*]&#39;, &#39;&#39;, line)             line = cut(line)            line = filter(lambda word: len(word)&gt;1, line)            words.extend(line)    return words</code></pre><ol start="3"><li>获得全部训练集中出现次数最多的词</li></ol><pre><code>allWords = []def getTopNWords(topN):    txtFiles = [str(i)+&#39;.txt&#39; for i in range(151)]    for txtFile in txtFiles:        allWords.append(getWordsFromFile(txtFile))     freq = Counter(chain(*allWords))    return [w[0] for w in freq.most_common(topN)]topWords = getTopNWords(600)</code></pre><ol start="4"><li>获取特征向量、创建模型训练</li></ol><pre><code>vectors = []for words in allWords:    temp = list(map(lambda x: words.count(x), topWords))    vectors.append(temp) vectors = array(vectors) labels = array([1]*127 + [0]*24)model = MultinomialNB() model.fit(vectors, labels)</code></pre><ol start="5"><li>预测未知邮件，进行分类，看是否为垃圾邮件</li></ol><pre><code>def predict(txtFile):    words = getWordsFromFile(txtFile)    currentVector = array(tuple(map(lambda x: words.count(x),topWords)))    result = model.predict(currentVector.reshape(1, -1))[0]     return &#39;垃圾邮件&#39; if result==1 else &#39;正常邮件&#39;for mail in (&#39;%d.txt&#39;%i for i in range(151, 156)):    print(mail, predict(mail), sep=&#39;:&#39;)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第五章 Python数据可视化</title>
      <link href="/2020/06/06/di-wu-zhang-python-shu-ju-ke-shi-hua/"/>
      <url>/2020/06/06/di-wu-zhang-python-shu-ju-ke-shi-hua/</url>
      
        <content type="html"><![CDATA[<h2 id="第五章-Python-数据可视化"><a href="#第五章-Python-数据可视化" class="headerlink" title="第五章 Python 数据可视化"></a>第五章 Python 数据可视化</h2><h3 id="1、Matplotlib-基础"><a href="#1、Matplotlib-基础" class="headerlink" title="1、Matplotlib 基础"></a>1、Matplotlib 基础</h3><h4 id="1-1-Matplotlib-库介绍"><a href="#1-1-Matplotlib-库介绍" class="headerlink" title="1.1 Matplotlib 库介绍"></a>1.1 Matplotlib 库介绍</h4><ol><li><p>Matplotlib 是 Python 最著名的绘图库，它提供了一整套和 Matlab 相似的命令 API，十分适合交互式地进行制图。而且也可以方便地将它作为绘图控件，嵌入 GUI 应用程序中。</p></li><li><p>Matplotlib 库由各种可视化类构成，内部结构复杂。</p></li><li><p>Matplotlib.pyplot 是绘制各类可视化图形的命令字库，相当于快捷方式。</p></li><li><p>Matplotlib 文档相当完备，并且 Gallery 页面中有上百幅缩略图，打开之后都有源代码。因此如果你需要绘制某种类型的图，只需要在这个页面中浏览、复制、 粘贴一下，基本上通过修改数据和设置都能搞定。</p></li></ol><h5 id="1-1-1-Matplotlib-库效果"><a href="#1-1-1-Matplotlib-库效果" class="headerlink" title="1.1.1 Matplotlib 库效果"></a>1.1.1 Matplotlib 库效果</h5><p><a href="https://matplotlib.org/gallery/index.html" target="_blank" rel="noopener">https://matplotlib.org/gallery/index.html</a></p><p><img src="/medias/1591439857009.png" alt="Matplotlib 库"></p><h5 id="1-1-2-Matplotlib-库-Gallery-示例"><a href="#1-1-2-Matplotlib-库-Gallery-示例" class="headerlink" title="1.1.2 Matplotlib 库 Gallery 示例"></a>1.1.2 Matplotlib 库 Gallery 示例</h5><pre><code>import numpy as npimport matplotlib.pyplot as pltN = 5menMeans = (20, 35, 30, 35, 27)womenMeans = (25, 32, 34, 20, 25)menStd = (2, 3, 4, 1, 2)womenStd = (3, 5, 2, 3, 3)ind = np.arange(N)    # the x locations for the groups width = 0.35       # the width of the barsp1 = plt.bar(ind, menMeans, width, yerr=menStd) p2 = plt.bar(ind, womenMeans, width,bottom=menMeans, yerr=womenStd)plt.ylabel(&#39;Scores&#39;)plt.title(&#39;Scores by group and gender&#39;)plt.xticks(ind, (&#39;G1&#39;, &#39;G2&#39;, &#39;G3&#39;, &#39;G4&#39;, &#39;G5&#39;))plt.yticks(np.arange(0, 81, 10))plt.legend((p1[0], p2[0]), (&#39;Men&#39;, &#39;Women&#39;))plt.show()</code></pre><p><img src="/medias/1591439822693.png" alt="Gallery 示例"><br>或者新版本代码</p><pre><code>import numpy as npimport matplotlib.pyplot as pltlabels = [&#39;G1&#39;, &#39;G2&#39;, &#39;G3&#39;, &#39;G4&#39;, &#39;G5&#39;]men_means = [20, 35, 30, 35, 27]women_means = [25, 32, 34, 20, 25]men_std = [2, 3, 4, 1, 2]women_std = [3, 5, 2, 3, 3]width = 0.35 # the width of the bars: can also be len(x) sequencefig, ax = plt.subplots()ax.bar(labels, men_means, width, yerr=men_std, label=&#39;Men&#39;)ax.bar(labels, women_means, width, yerr=women_std, bottom=men_means,       label=&#39;Women&#39;)ax.set_ylabel(&#39;Scores&#39;)ax.set_title(&#39;Scores by group and gender&#39;)ax.legend()plt.show()</code></pre><p><img src="/medias/1591439831012.png" alt="Gallery 示例"></p><h4 id="1-2-Matplotlib-快速绘图"><a href="#1-2-Matplotlib-快速绘图" class="headerlink" title="1.2 Matplotlib 快速绘图"></a>1.2 Matplotlib 快速绘图</h4><h5 id="1-2-1-快速绘图"><a href="#1-2-1-快速绘图" class="headerlink" title="1.2.1 快速绘图"></a>1.2.1 快速绘图</h5><ol><li>matplotlib 中的快速绘图的函数库可以通过如下语句载入：</li></ol><p><strong>import matplotlib.pyplot as plt</strong></p><ol start="2"><li>接下来调用 figure 创建一个绘图对象，并且使它成为当前的绘图对象：</li></ol><p><strong>plt.figure(figsize=(8,4))</strong></p><ol start="3"><li><p>通过 figsize 参数可以指定绘图对象的宽度和高度，单位为英寸； dpi 参数指定绘图对象的分辨率，即每英寸多少个像素，缺省值为 80 。</p></li><li><p>也可以不创建绘图对象直接调用接下来的 plot 函数直接绘图，matplotlib 会自动创建一个绘图对象。</p></li><li><p>在使用 Jupyter Notebook 环境绘图时，需要先运行 Jupyter Notebook 的魔术命令 <code>%matplotlib inline</code>。这条命令的作用是将 Matplotlib 绘制的图形嵌入在当前页面中。而在桌面环境中绘图时，不需要添加此命令，而是在全部绘图代码之后追加<code>plt.show()</code>。</p></li></ol><p><strong>%matplotlib inline</strong></p><ol start="6"><li><p>如果需要同时绘制多幅图表的话，可以是给 figure 传递一个整数参数指定图标的序号，如果所指定序号的绘图对象已经存在的话，将不创建新的对象，而只是让它成为当前绘图对象。</p></li><li><p>plot 函数的调用方式很灵活，使用关键字参数指定各种属性：<br>➢    label: 给所绘制的曲线一个名字 ， 此名字在图示 (legend) 中显示 。 只要在字符串前后添加 “$” 符号 ， matplotlib 就会使用其内嵌的latex 引擎绘制的数学公式；<br>➢    color: 指定曲线的颜色；<br>➢    linewidth: 指定曲线的宽度；<br>➢    参数 <code>b --</code>指定曲线的颜色和线型</p></li><li><p>可通过一系列函数设置绘图对象的各个属性：<br>➢    xlabel/ylabel：设置 X 轴/Y轴 的文字<br>➢    title：设置图表的标题<br>➢    ylim：设置Y轴的范围<br>➢    legend：图例图示<br>➢    plt.show ()：显示出创建的所有绘图对象</p></li></ol><pre><code>plt.xlabel(&quot;Time(s)&quot;) plt.ylabel(&quot;Volt&quot;) plt.title(&quot;PyPlot First Example&quot;) plt.ylim(-1.2,1.2) plt.legend() </code></pre><ol start="9"><li>可以调用 plt. savefig ()  将当前的 Figure 对象保存成图像文件 ， 图像格式由图像文件的扩展名决定</li></ol><p><strong>plt.savefig(“test.png”,dpi=120)</strong></p><h5 id="1-2-2-快速绘图示例"><a href="#1-2-2-快速绘图示例" class="headerlink" title="1.2.2 快速绘图示例"></a>1.2.2 快速绘图示例</h5><pre><code>import numpy as npimport matplotlib.pyplot as plt x = np.linspace(0, 10, 500)y = np.sin(x)z = np.cos(x**2) plt.figure(figsize=(8,6))plt.plot(x,y,label=&quot;$sin(x)$&quot;,color=&quot;red&quot;,linewidth=2) plt.plot(x,z,&quot;b--&quot;,label=&quot;$cos(x^2)$&quot;)plt.xlabel(&quot;Time(s)&quot;) plt.ylabel(&quot;Volt&quot;) plt.title(&quot;PyPlot First Example&quot;) plt.ylim(-1.2,1.2) plt.legend() plt.show()#plt.savefig(&quot;test.png&quot;,dpi=120)</code></pre><p><img src="/medias/1591440532131.png" alt="快速绘图示例"></p><h4 id="1-3-Matplotlib-库使用初探"><a href="#1-3-Matplotlib-库使用初探" class="headerlink" title="1.3 Matplotlib 库使用初探"></a>1.3 Matplotlib 库使用初探</h4><ol><li>pyplot 的基础图标函数如下</li></ol><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">plt.plot(x,y,fmt,…)</td><td align="left">绘制一个坐标图</td></tr><tr><td align="left">plt.boxplot(data, notch, position)</td><td align="left">绘制一个箱形图</td></tr><tr><td align="left">plt.bar(left, height, width, bottom)</td><td align="left">绘制一个柱状图</td></tr><tr><td align="left">plt.barh(width, bottom, left, height)</td><td align="left">绘制一个横向条形图</td></tr><tr><td align="left">plt.polar(theta, r)</td><td align="left">绘制极坐标图</td></tr><tr><td align="left">plt.pie(data, explode)</td><td align="left">绘制饼状图</td></tr><tr><td align="left">plt.psd(x, NFFT=256, pad_to, Fs)</td><td align="left">绘制功率谱密度图</td></tr><tr><td align="left">plt.cohere(x, y, NFFT=256, Fs)</td><td align="left">绘制X-Y的相关性函数</td></tr><tr><td align="left">plt.scatter(x, y)</td><td align="left">绘制散点图，其中，x和y长度相同</td></tr><tr><td align="left">plt.step(x, y, where)</td><td align="left">绘制步阶图</td></tr><tr><td align="left">plt.hist(x, bins, normed)</td><td align="left">绘制直方图</td></tr><tr><td align="left">plt.contour(X, Y, Z, N)</td><td align="left">绘制等值图</td></tr><tr><td align="left">plt.vlines()</td><td align="left">绘制垂直图</td></tr><tr><td align="left">plt.stem(x, y, linefmt, markerfmt)</td><td align="left">绘制柴火图</td></tr><tr><td align="left">plt.plot_date()</td><td align="left">绘制数据日期</td></tr></tbody></table><ol start="2"><li><p>pyplot 的绘图区域<br>1）可以用子图来将图样（plot）放在均匀的坐标网格中。<br>2）plt.subplot(nrows, ncols, plot_number)。<br>3）用 subplot 函数的时候，需要指明网格的行列数量，以及你希望将图样放在哪一个网格区域中。</p></li><li><p>pyplot的绘图区域示例</p></li></ol><pre><code>import matplotlib.pyplot as pltplt.subplot(2,1,1)plt.xticks([]), plt.yticks([])plt.text(0.5,0.5, &#39;subplot(2,1,1)&#39;,ha=&#39;center&#39;,va=&#39;center&#39;,size=24,alpha=.5)plt.subplot(2,1,2)plt.xticks([]), plt.yticks([])plt.text(0.5,0.5, &#39;subplot(2,1,2)&#39;,ha=&#39;center&#39;,va=&#39;center&#39;,size=24,alpha=.5)# plt.savefig(&#39;./medias/figures/subplot-horizontal.png&#39;, dpi=64) plt.show()</code></pre><p><img src="/medias/1591440988040.png" alt="pyplot的绘图区域示例"></p><pre><code>import matplotlib.pyplot as pltplt.subplot(1,2,1)plt.xticks([]), plt.yticks([])plt.text(0.5,0.5, &#39;subplot(1,2,1)&#39;,ha=&#39;center&#39;,va=&#39;center&#39;,size=24,alpha=.5)plt.subplot(1,2,2)plt.xticks([]), plt.yticks([])plt.text(0.5,0.5, &#39;subplot(1,2,2)&#39;,ha=&#39;center&#39;,va=&#39;center&#39;,size=24,alpha=.5)# plt.savefig(&#39;./medias/figures/subplot-horizontal.png&#39;, dpi=64) plt.show()</code></pre><p><img src="/medias/1591441076189.png" alt="pyplot的绘图区域示例"></p><pre><code>import matplotlib.pyplot as pltplt.subplot(2,2,1)plt.xticks([]), plt.yticks([])plt.text(0.5,0.5, &#39;subplot(2,2,1)&#39;,ha=&#39;center&#39;,va=&#39;center&#39;,size=24,alpha=.5)plt.subplot(2,2,2)plt.xticks([]), plt.yticks([])plt.text(0.5,0.5, &#39;subplot(2,2,2)&#39;,ha=&#39;center&#39;,va=&#39;center&#39;,size=24,alpha=.5)plt.subplot(2,2,3)plt.xticks([]), plt.yticks([])plt.text(0.5,0.5, &#39;subplot(2,2,3)&#39;,ha=&#39;center&#39;,va=&#39;center&#39;,size=24,alpha=.5)plt.subplot(2,2,4)plt.xticks([]), plt.yticks([])plt.text(0.5,0.5, &#39;subplot(2,2,4)&#39;,ha=&#39;center&#39;,va=&#39;center&#39;,size=24,alpha=.5)# plt.savefig(&#39;./medias/figures/subplot-horizontal.png&#39;, dpi=64) plt.show()</code></pre><p><img src="/medias/1591441165908.png" alt="pyplot的绘图区域示例"></p><h3 id="2、折线图、散点图实战"><a href="#2、折线图、散点图实战" class="headerlink" title="2、折线图、散点图实战"></a>2、折线图、散点图实战</h3><h4 id="2-1-折线图实战"><a href="#2-1-折线图实战" class="headerlink" title="2.1 折线图实战"></a>2.1 折线图实战</h4><h5 id="2-1-1-折线图"><a href="#2-1-1-折线图" class="headerlink" title="2.1.1 折线图"></a>2.1.1 折线图</h5><ol><li>折线图通常用来表示数据随时间或有序类别变化的趋势。</li><li>最简单的折线图示例。</li></ol><pre><code>import matplotlib.pyplot as pltdata=[1,2,3,4,5,4,2,4,6,7] # 随意创建的数据plt.plot(data) # 引用matplotlib库中的pyplot模块绘图plt.show()</code></pre><p><img src="/medias/1591442728046.png" alt="折线图"></p><ol start="3"><li><p>折线图通常用来表示数据随时间或有序类别变化的趋势。</p></li><li><p>plot() 函数的第一个参数表示横坐标数据</p></li><li><p>第二个参数表示纵坐标数据</p></li><li><p>第三个参数表示颜色、线型和标记样式</p></li><li><p>颜色常用的值有（r/g/b/c/m/y/k/w）</p></li><li><p>线型常用的值有（-/–/:/-.）</p></li><li><p>标记样式常用的值有（/medias/,/o/v/^/s/*/D/d/x/&lt;/&gt;/h/H/1/2/3/4/_/|）</p></li><li><p>绘制多条曲线、曲线颜色、线型、标记等参数设置</p></li></ol><pre><code>import matplotlib.pyplot as pltimport matplotlib.font_manager as fmyy=[1,2,3,4,5,3,1,2,7,8] #随便创建的数据xx=[3,5,4,1,9,3,2,5,6,3]zz=[2,2,4,7,4,8,2,4,5,6]plt.plot(yy, color=&#39;r&#39;, linewidth=5, linestyle=&#39;:&#39;, label=&#39;Data 1&#39;) plt.plot(xx, color=&#39;g&#39;, linewidth=2, linestyle=&#39;--&#39;, label=&#39;Data 2&#39;) plt.plot(zz,color=&#39;b&#39;, linewidth=0.5, linestyle=&#39;-&#39;, label=&#39;Data 3&#39;) plt.legend(loc=2) plt.xlabel(&#39;X轴名称&#39;, fontproperties=&#39;simhei&#39;)plt.ylabel(&#39;Y轴名称&#39;, fontproperties=&#39;simhei&#39;)plt.title(&#39;折线图美化示例&#39;, fontproperties=&#39;simhei&#39;)plt.ylim(0,10)</code></pre><p><img src="/medias/1591443248270.png" alt="绘制多条曲线、曲线颜色、线型、标记等参数"></p><h5 id="2-1-2-折线图实战一"><a href="#2-1-2-折线图实战一" class="headerlink" title="2.1.2 折线图实战一"></a>2.1.2 折线图实战一</h5><p>已知王府井某小吃店 2018 年每个月份的营业额如下表所示。请使用matplotlib 扩展库编写 Python 程序绘制折线图对该小吃店全年营业额进行可视化，并使用红色点划线连接每个月份的数据，并在每个月份的数据处使用三角形标记。</p><table><thead><tr><th align="left">月份</th><th align="left">1</th><th align="left">2</th><th align="left">3</th><th align="left">4</th><th align="left">5</th><th align="left">6</th><th align="left">7</th><th align="left">8</th><th align="left">9</th><th align="left">10</th><th align="left">11</th><th align="left">12</th></tr></thead><tbody><tr><td align="left">营业额（万元）</td><td align="left">5.2</td><td align="left">7.7</td><td align="left">5.8</td><td align="left">5.7</td><td align="left">7.3</td><td align="left">9.2</td><td align="left">18.7</td><td align="left">14.6</td><td align="left">20.5</td><td align="left">17.0</td><td align="left">9.8</td><td align="left">6.9</td></tr></tbody></table><p><strong>小吃店营业额折线图</strong></p><pre><code>import matplotlib.pyplot as pltmonth = list(range(1,13))money = [5.2, 2.7, 5.8, 5.7, 7.3, 9.2,18.7, 15.6, 20.5, 18.0, 7.8, 6.9]plt.plot(month, money, &#39;b-.^&#39;)plt.xlabel(&#39;月份&#39;, fontproperties=&#39;simhei&#39;, fontsize=14)plt.ylabel(&#39;营业额（万元）&#39;, fontproperties=&#39;simhei&#39;, fontsize=14)plt.title(&#39;小吃店2018年营业额变化趋势图&#39;, fontproperties=&#39;simhei&#39;, fontsize=18)plt.tight_layout()   # 紧缩四周空白，扩大绘图区域可用面积plt.show()</code></pre><p><img src="/medias/1591443587731.png" alt="小吃店营业额折线图"></p><h4 id="2-2-散点图实战"><a href="#2-2-散点图实战" class="headerlink" title="2.2 散点图实战"></a>2.2 散点图实战</h4><h5 id="2-2-1-散点图方法"><a href="#2-2-1-散点图方法" class="headerlink" title="2.2.1 散点图方法"></a>2.2.1 散点图方法</h5><ol><li>在 matplotlib 中使用函数  matplotlib.pyplot.scatter  绘制散点图。</li></ol><pre><code>matplotlib.pyplot.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None,  vmax=None, alpha=None, linewidths=None, verts=None, edgecolors=None, hold=None, data=None, **kwargs)</code></pre><ol start="2"><li>常用参数有：x,y 组成了散点的坐标；s 为散点的面积；c 为散点的颜色（默认为蓝色 ‘b’）；marker 为散点的标记；alpha 为散点的透明度（0 与 1 之间的数，0 为完全透明， 1 为完全不透明）; linewidths 为散点边缘的线宽；如果 marker 为 None，则使用 verts 的值构建散点标记；edgecolors 为散点边缘颜色。</li></ol><h5 id="2-2-2-绘制普通散点图"><a href="#2-2-2-绘制普通散点图" class="headerlink" title="2.2.2 绘制普通散点图"></a>2.2.2 绘制普通散点图</h5><pre><code>import matplotlib import matplotlib.pyplot as plt import numpy as np# 10个点N = 10x = np.random.rand(N)y = np.random.rand(N)plt.scatter(x, y)plt.show()</code></pre><p><img src="/medias/1591443956266.png" alt="普通散点图"></p><h5 id="2-2-3-更改散点大小"><a href="#2-2-3-更改散点大小" class="headerlink" title="2.2.3 更改散点大小"></a>2.2.3 更改散点大小</h5><pre><code>import matplotlib import matplotlib.pyplot as plt import numpy as np# 10个点N = 10x = np.random.rand(N) y = np.random.rand(N)# 每个点随机大小s = (30*np.random.rand(N))**2 plt.scatter(x, y, s=s)plt.show()</code></pre><p><img src="/medias/1591444012764.png" alt="更改散点大小"></p><h5 id="2-2-4-更改散点颜色、透明度"><a href="#2-2-4-更改散点颜色、透明度" class="headerlink" title="2.2.4 更改散点颜色、透明度"></a>2.2.4 更改散点颜色、透明度</h5><pre><code>import matplotlib import matplotlib.pyplot as plt import numpy as np# 10个点N = 10x = np.random.rand(N)y = np.random.rand(N)# 每个点随机大小s = (30*np.random.rand(N))**2# 随机颜色c = np.random.rand(N)plt.scatter(x, y, s=s, c=c, alpha=0.5) plt.show()</code></pre><p><img src="/medias/1591444069805.png" alt="更改散点颜色、透明度"></p><h5 id="2-2-5-更改散点形状"><a href="#2-2-5-更改散点形状" class="headerlink" title="2.2.5 更改散点形状"></a>2.2.5 更改散点形状</h5><pre><code>import matplotlib.pyplot as pltimport numpy as np# 10个点N = 10x = np.random.rand(N)y = np.random.rand(N)s = (30*np.random.rand(N))**2c = np.random.rand(N)plt.scatter(x, y, s=s, c=c, marker=&#39;^&#39;, alpha=0.5)plt.show()</code></pre><p><img src="/medias/1591444111716.png" alt="更改散点形状"></p><h5 id="2-2-6-一张图绘制两组数据的散点图"><a href="#2-2-6-一张图绘制两组数据的散点图" class="headerlink" title="2.2.6 一张图绘制两组数据的散点图"></a>2.2.6 一张图绘制两组数据的散点图</h5><pre><code>import matplotlib.pyplot as plt import numpy as np# 10个点N = 10x1 = np.random.rand(N) y1 = np.random.rand(N) x2 = np.random.rand(N) y2 = np.random.rand(N)plt.scatter(x1, y1, marker=&#39;o&#39;) plt.scatter(x2, y2, marker=&#39;^&#39;) plt.show()</code></pre><p><img src="/medias/1591444187747.png" alt="一张图绘制两组数据的散点图"></p><h5 id="2-2-7-为散点图增加图例"><a href="#2-2-7-为散点图增加图例" class="headerlink" title="2.2.7 为散点图增加图例"></a>2.2.7 为散点图增加图例</h5><pre><code>import matplotlib.pyplot as plt import numpy as np# 10个点N = 10x1 = np.random.rand(N) y1 = np.random.rand(N) x2 = np.random.rand(N) y2 = np.random.rand(N)plt.scatter(x1, y1, marker=&#39;o&#39;, label=&quot;circle&quot;) plt.scatter(x2, y2, marker=&#39;^&#39;, label=&quot;triangle&quot;)plt.legend(loc=&#39;best&#39;)plt.show()</code></pre><p><img src="/medias/1591444271870.png" alt="为散点图增加图例"></p><h3 id="3、柱状图、饼状图实战"><a href="#3、柱状图、饼状图实战" class="headerlink" title="3、柱状图、饼状图实战"></a>3、柱状图、饼状图实战</h3><h4 id="3-1-柱状图实战"><a href="#3-1-柱状图实战" class="headerlink" title="3.1 柱状图实战"></a>3.1 柱状图实战</h4><h5 id="3-1-1-柱状图"><a href="#3-1-1-柱状图" class="headerlink" title="3.1.1 柱状图"></a>3.1.1 柱状图</h5><ol><li><p>使用 Matplotlib 提供的 bar() 函数来绘制柱状图。</p></li><li><p>与前面介绍的 plot() 函数类似，程序每次调用 bar() 函数时都会生成一组柱状图， 如果希望生成多组柱状图，则可通过多次调用 bar() 函数来实现。</p></li><li><p>只要将 bar() 函数理解透彻，我们就能绘制各种类型的柱状图。</p></li></ol><h5 id="3-1-2-柱状图-绘图方法-bar"><a href="#3-1-2-柱状图-绘图方法-bar" class="headerlink" title="3.1.2 柱状图 绘图方法 bar()"></a>3.1.2 柱状图 绘图方法 bar()</h5><pre><code>bar(x, height, width=0.8, *, align=&#39;center&#39;, **kwargs) </code></pre><ol><li><p>主要参数：<br>1）x：包含所有柱子的下标的列表。<br>2）height：y 轴的数值序列，也是柱状图的高度，一般就是我们需要展示的数据。<br>3）width：为柱状图的宽度，一般这是为 0.8 即可。<br>4）align：柱子对齐方式，有两个可选值：center 和 edge。center 表示每根柱子是根据下标来对齐,  edge 则表示每根柱子全部以下标为起点，然后显示到下标的右边。如果不指定该参数，默认值是 center。</p></li><li><p>可选参数：<br>1）color：每根柱子呈现的颜色，可指定一个固定值或者一个列表。<br>2）edgecolor：每根柱子边框的颜色。<br>3）linewidth：每根柱子的边框宽度。如果没有设置该参数，默认无边框。<br>4）tick_label：每根柱子上显示的标签，默认无标签。<br>5）xerr：每根柱子顶部在横轴方向的线段长度。<br>6）yerr：每根柱子顶端在纵轴方向的线段长度。<br>7）ecolor：设置 xerr 和 yerr 的线段的颜色，可以指定一个固定值或者一个列表。</p></li></ol><h5 id="3-1-3-柱状图实战：利用-pandas-快速绘制"><a href="#3-1-3-柱状图实战：利用-pandas-快速绘制" class="headerlink" title="3.1.3 柱状图实战：利用 pandas 快速绘制"></a>3.1.3 柱状图实战：利用 pandas 快速绘制</h5><p>某商场2018年各部门每个月的业绩如下表所示（单位：万元）。编写程序绘制柱状图可视化各部门的业绩，可以借助于 pandas 的DataFrame 结构快速绘制图形，并要求坐标轴、标题和图例以中文形式显示。</p><table><thead><tr><th align="left">月份</th><th align="left">1</th><th align="left">2</th><th align="left">3</th><th align="left">4</th><th align="left">5</th><th align="left">6</th><th align="left">7</th><th align="left">8</th><th align="left">9</th><th align="left">10</th><th align="left">11</th><th align="left">12</th></tr></thead><tbody><tr><td align="left">男装</td><td align="left">51</td><td align="left">32</td><td align="left">58</td><td align="left">57</td><td align="left">30</td><td align="left">46</td><td align="left">38</td><td align="left">38</td><td align="left">40</td><td align="left">53</td><td align="left">58</td><td align="left">50</td></tr><tr><td align="left">女装</td><td align="left">70</td><td align="left">30</td><td align="left">48</td><td align="left">73</td><td align="left">82</td><td align="left">80</td><td align="left">43</td><td align="left">25</td><td align="left">30</td><td align="left">49</td><td align="left">79</td><td align="left">60</td></tr><tr><td align="left">餐饮</td><td align="left">60</td><td align="left">40</td><td align="left">46</td><td align="left">50</td><td align="left">57</td><td align="left">76</td><td align="left">70</td><td align="left">33</td><td align="left">70</td><td align="left">61</td><td align="left">49</td><td align="left">45</td></tr><tr><td align="left">化妆品</td><td align="left">110</td><td align="left">75</td><td align="left">130</td><td align="left">80</td><td align="left">83</td><td align="left">95</td><td align="left">87</td><td align="left">89</td><td align="left">96</td><td align="left">88</td><td align="left">86</td><td align="left">89</td></tr><tr><td align="left">金银首饰</td><td align="left">143</td><td align="left">100</td><td align="left">89</td><td align="left">90</td><td align="left">78</td><td align="left">129</td><td align="left">100</td><td align="left">97</td><td align="left">108</td><td align="left">152</td><td align="left">96</td><td align="left">87</td></tr></tbody></table><pre><code>import pandas as pdimport matplotlib.pyplot as pltimport matplotlib.font_manager as fmdata = pd.DataFrame({&#39;月份&#39;: [1,2,3,4,5,6,7,8,9,10,11,12],&#39;男装&#39;: [51,32,58,57,30,46,38,38,40,53,58,50], &#39;女装&#39;: [70,30,48,73,82,80,43,25,30,49,79,60], &#39;餐饮&#39;: [60,40,46,50,57,76,70,33,70,61,49,45],&#39;化妆品&#39;: [110,75,130,80,83,95,87,89,96,88,86,89],&#39;金银首饰&#39;: [143,100,89,90,78,129,100,97,108,152,96,87]})data.plot(x=&#39;月份&#39;, kind=&#39;bar&#39;)#绘制柱状图，指定月份数据作为x轴plt.xlabel(&#39;月份&#39;, fontproperties=&#39;simhei&#39;) # 设置x、y轴标签和字体plt.ylabel(&#39;营业额（万元）&#39;, fontproperties=&#39;simhei&#39;)myfont = fm.FontProperties(fname=r&#39;STKAITI.ttf&#39;) plt.legend(prop=myfont)plt.show()</code></pre><p><img src="/medias/Figure_1.png" alt="各部门每个月的业绩柱状图"></p><h5 id="3-1-4-柱状图实战：使用-matplotlib-绘制"><a href="#3-1-4-柱状图实战：使用-matplotlib-绘制" class="headerlink" title="3.1.4 柱状图实战：使用 matplotlib 绘制"></a>3.1.4 柱状图实战：使用 matplotlib 绘制</h5><ol><li>简单柱状图</li></ol><pre><code>import matplotlib.pyplot as plt  num_list = [1.5,0.6,7.8,6]  plt.bar(range(len(num_list)), num_list)  plt.show() </code></pre><p><img src="/medias/1591445577900.png" alt="简单柱状图"></p><p><strong>设置标签</strong></p><pre><code>import matplotlib.pyplot as plt  name_list = [&#39;Monday&#39;,&#39;Tuesday&#39;,&#39;Friday&#39;,&#39;Sunday&#39;]num_list = [1.5,0.6,7.8,6]  plt.bar(range(len(num_list)), num_list, color=&#39;rgb&#39;, tick_label=name_list)  plt.show() </code></pre><p><img src="/medias/1591445668808.png" alt="柱状图设置标签"></p><ol start="2"><li>条形柱状图</li></ol><p><strong>设置标签</strong></p><pre><code>import matplotlib.pyplot as plt  name_list = [&#39;Monday&#39;,&#39;Tuesday&#39;,&#39;Friday&#39;,&#39;Sunday&#39;]  num_list = [1.5,0.6,7.8,6] plt.barh(range(len(num_list)), num_list,color=&#39;rgb&#39;,tick_label=name_list)  plt.show() </code></pre><p><img src="/medias/1591446055626.png" alt="条形柱状图设置标签"></p><h5 id="3-1-5-柱状图实战：使用-matplotlib-绘制"><a href="#3-1-5-柱状图实战：使用-matplotlib-绘制" class="headerlink" title="3.1.5 柱状图实战：使用 matplotlib 绘制"></a>3.1.5 柱状图实战：使用 matplotlib 绘制</h5><ol><li>堆叠柱状图</li></ol><pre><code>import matplotlib.pyplot as pltname_list = [&#39;Monday&#39;,&#39;Tuesday&#39;,&#39;Friday&#39;,&#39;Sunday&#39;]  num_list = [1.5,0.6,7.8,6]  num_list1 = [1,2,3,1]  plt.bar(range(len(num_list)), num_list, label=&#39;boy&#39;, fc = &#39;y&#39;)  plt.bar(range(len(num_list)), num_list1, bottom=num_list, label=&#39;girl&#39;, tick_label = name_list, fc = &#39;r&#39;)  plt.legend()  plt.show() </code></pre><p><img src="/medias/1591446154977.png" alt="堆叠柱状图"></p><ol start="2"><li>并列柱状图</li></ol><pre><code>import matplotlib.pyplot as pltname_list = [&#39;Monday&#39;,&#39;Tuesday&#39;,&#39;Friday&#39;,&#39;Sunday&#39;]num_list = [1.5,0.6,7.8,6]  num_list1 = [1,2,3,1]  x =list(range(len(num_list)))  total_width, n = 0.8, 2  width = total_width / n  plt.bar(x, num_list, width=width, label=&#39;boy&#39;,fc = &#39;y&#39;)  for i in range(len(x)):      x[i] = x[i] + width  plt.bar(x, num_list1, width=width, label=&#39;girl&#39;, tick_label = name_list, fc = &#39;r&#39;) plt.legend()  plt.show() </code></pre><p><img src="/medias/1591446298354.png" alt="并列柱状图"></p><h4 id="3-2-饼状图实战"><a href="#3-2-饼状图实战" class="headerlink" title="3.2 饼状图实战"></a>3.2 饼状图实战</h4><h5 id="3-2-1-饼状图"><a href="#3-2-1-饼状图" class="headerlink" title="3.2.1 饼状图"></a>3.2.1 饼状图</h5><ol><li><p>概念：<br>1）饼状图显示一个系列中各项的大小与各项总和的比例。<br>2）饼状图可自动根据数据的百分比画饼。</p></li><li><p>绘制饼状图的基本语法<br>1）创建数组 x 的饼图，每个楔形的面积由 x/sum(x) 决定；<br>2）若 sum(x)&lt;1，则 x 数组不会被标准化，x 值即为楔形区域面积占比。注意，该种情况会出现 1-sum(x) 的空楔形。<br>3）若 sum(x)&gt;1，则由 x[i]/sum(x) 算出每一个楔形占比，饼图 360° 区域均被填充。<br><img src="/medias/1591446413658.png" alt="饼状图"></p></li></ol><h5 id="3-2-2-饼状图绘图方法-pie"><a href="#3-2-2-饼状图绘图方法-pie" class="headerlink" title="3.2.2 饼状图绘图方法 pie()"></a>3.2.2 饼状图绘图方法 pie()</h5><pre><code>pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, wedgeprops=None, textprops=None, center=(0,0),frame=False, rotatelabels=False, hold=None, data=None)</code></pre><p><strong>参数详解</strong><br>1）x：(创建饼状图的数据，每一块) 的比例，如果 sum(x) &gt;1 会使用 sum(x) 归一化。</p><p>2）explode：(每一块)离开中心距离，一个 list 或数组。</p><p>3）labels：list, optional, default:None；为每个楔形添加标签。</p><p>4）color：array-like, optional, default:None；若无，则用 currently active cycle 中的颜色添加。</p><p>5）autopct：控制饼图内百分比设置，可以使用 format 字符串或者format function：可以是整数(‘%d%%’)、浮点数(%1.3f%%’)、字符串(‘%s%%’)、函数。</p><p>6）label distance：float, optional, default:1.1；label 标记的绘制位置，相对于半径的比例，默认值为 1.1，如 &lt;1 则绘制在饼图内侧。</p><p>7）pctdistance：float, optional, default:0.6；类似于 labeldistance，指定 autopct 的位置刻度，默认值为0.6。</p><p>8）shadow：bool, optional, default:False；为饼状图画阴影(True)。</p><p>9）startangle : float, optional, default: None；起始绘制角度，默认图是从 x 轴正方向逆时针画起，如设定=90则从 y 轴正方向画起。</p><p>10）radius : float, optional, default: None；饼图的半径，若为None时，则默认为 1。</p><p>11）counterclock : bool, optional, default: True；指定分数方向，逆时针 (True) 或顺时针。</p><p>12）wedgeprops : dict, optional, default: None；描述楔形边界线宽度值，参数形式’’wedgeprops = {‘linewidth’: 3}’’楔形边界线宽度为 3。</p><p>13）textprops : dict, optional, default: None；传递给文本对象的字典参数。</p><p>14）center :  list of float, optional, default: (0, 0)；图标的中心为，默认(0,0)，也可以是两个标量的序列(sequence  of 2 scalars)。</p><h5 id="3-2-3-饼状图实战"><a href="#3-2-3-饼状图实战" class="headerlink" title="3.2.3 饼状图实战"></a>3.2.3 饼状图实战</h5><ol><li>简单饼状图</li></ol><pre><code>import matplotlib.pyplot as plt#用来正常显示中文标签plt.rcParams[&#39;font.sans-serif&#39;]=[&#39;SimHei&#39;]labels = &#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;sizes = [10,20,30,40]plt.pie(sizes,labels=labels)plt.title(&quot;饼状图实战&quot;)plt.text(1,-1.2,&#39;By hsiehchou&#39;)plt.show()</code></pre><p><img src="/medias/1591447406454.png" alt="简单饼状图"></p><ol start="2"><li>explode : 一块饼图离开中心距离，默认值为（0,0），就是不离开中心。</li></ol><pre><code>import matplotlib.pyplot as pltplt.rcParams[&#39;font.sans-serif&#39;]=[&#39;SimHei&#39;] #用来正常显示中文标签labels = &#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;sizes = [10,20,30,40]explode = (0,0,0.1,0)  #将第三块分离出来plt.pie(sizes,labels=labels,explode=explode) #增加explode参数plt.title(&quot;饼状图实战&quot;)plt.text(1,-1.2,&#39;By hsiehchou&#39;)plt.show()</code></pre><p><img src="/medias/1591447385887.png" alt="一块饼图离开中心距离"></p><ol start="3"><li>colors：数组，可选参数，默认为：None；用来标注每块饼图的 matplotlib 颜色。</li></ol><pre><code>import matplotlib.pyplot as pltplt.rcParams[&#39;font.sans-serif&#39;]=[&#39;SimHei&#39;]labels = &#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;sizes = [10,20,30,40]explode = (0,0,0.1,0)colors = [&#39;r&#39;,&#39;g&#39;,&#39;y&#39;,&#39;b&#39;] #自定义颜色列表plt.pie(sizes,labels=labels,explode=explode,colors=colors)plt.title(&quot;饼状图实战&quot;)plt.text(1,-1.2,&#39;By hsiehchou&#39;)plt.show()</code></pre><p><img src="/medias/1591447360661.png" alt="标注每块饼图的 matplotlib 颜色"></p><ol start="4"><li>autopct：控制饼图内百分比设置,可以使用 format 字符串或者format function。<br>‘%1.1f’：指小数点后保留一位有效数值。</li></ol><pre><code>import matplotlib.pyplot as pltplt.rcParams[&#39;font.sans-serif&#39;]=[&#39;SimHei&#39;]labels = &#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;sizes = [10,20,30,40] explode = (0,0,0.1,0)colors = [&#39;r&#39;,&#39;g&#39;,&#39;y&#39;,&#39;b&#39;] #自定义颜色列表#plt.pie(sizes,labels=labels,explode=explode,colors=colors, autopct=&#39;%1.1f&#39;) #保留一位小数plt.pie(sizes,labels=labels,explode=explode,colors=colors, autopct=&#39;%1.2f%%&#39;)  #保留两位小数，增加百分号plt.title(&quot;饼状图实战&quot;)plt.text(1,-1.2,&#39;By hsiehchou&#39;)plt.show()</code></pre><p><img src="/medias/1591447572772.png" alt="饼图内百分比设置"></p><ol start="5"><li>x：每一块饼图的比例，为必填项，如果 sum(x)&gt;1，会将多出的部分进行均分；如为必填项，如果 sum(x)&lt;1，1-sum(x) 的部分空出；</li></ol><pre><code>import matplotlib.pyplot as pltplt.rcParams[&#39;font.sans-serif&#39;]=[&#39;SimHei&#39;]labels = &#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;# sizes = [0.1,0.2,0.3,0.4]  # sum(x) == 1# sizes = [0.1,0.2,0.3,0.9]  # sum(x) &gt; 1sizes = [0.1,0.2,0.3,0.2]  # sum(x) &lt; 1explode = (0,0,0.1,0)colors = [&#39;r&#39;,&#39;g&#39;,&#39;y&#39;,&#39;b&#39;] #自定义颜色列表plt.pie(sizes,labels=labels,explode=explode,colors=colors, autopct=&#39;%1.1f&#39;)  #保留一位小数plt.title(&quot;饼状图实战&quot;)plt.text(1,-1.2,&#39;By hsiehchou&#39;)plt.show()</code></pre><p><img src="/medias/1591447644747.png" alt="每一块饼图的比例"></p><ol start="6"><li>添加图例，plt.legend( )</li></ol><pre><code>import matplotlib.pyplot as pltplt.rcParams[&#39;font.sans-serif&#39;]=[&#39;SimHei&#39;]labels = &#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;sizes = [0.1,0.2,0.3,0.4]  # sum(x) == 1explode = (0,0,0.1,0)plt.pie(sizes,labels=labels,explode=explode, autopct=&#39;%1.1f&#39;)plt.legend(loc=&quot;upper right&quot;, fontsize=10, bbox_to_anchor=(1.1,1.05), borderaxespad=0.3) # bbox_to_anchor=[0.5, 0.5]   # 外边距 上边 右边# borderaxespad = 0.3图例的内边距plt.title(&quot;饼状图实战&quot;)plt.text(1,-1.2,&#39;By hsiehchou&#39;)plt.show()</code></pre><p><img src="/medias/1591447788057.png" alt="添加图例"></p><h5 id="3-2-4-饼状图实战二：学生成绩分布饼状图"><a href="#3-2-4-饼状图实战二：学生成绩分布饼状图" class="headerlink" title="3.2.4 饼状图实战二：学生成绩分布饼状图"></a>3.2.4 饼状图实战二：学生成绩分布饼状图</h5><p>已知某高校某班级的操作系统、高等数学、英语和 Python 课程考试成绩如下表所示，要求利用 matplotlib 库绘制饼状图显示该班级每门课的成绩中优（85分以上）、及格（60-84分）、不及格（60分以下）的学生占比。</p><table><thead><tr><th align="left">课程/学生</th><th align="left">1</th><th align="left">2</th><th align="left">3</th><th align="left">4</th><th align="left">5</th><th align="left">6</th><th align="left">7</th><th align="left">8</th><th align="left">9</th><th align="left">10</th><th align="left">11</th><th align="left">12</th><th align="left">13</th><th align="left">14</th><th align="left">15</th><th align="left">16</th></tr></thead><tbody><tr><td align="left">操作系统</td><td align="left">89</td><td align="left">70</td><td align="left">49</td><td align="left">87</td><td align="left">92</td><td align="left">84</td><td align="left">73</td><td align="left">71</td><td align="left">78</td><td align="left">81</td><td align="left">90</td><td align="left">37</td><td align="left">77</td><td align="left">82</td><td align="left">75</td><td align="left">90</td></tr><tr><td align="left">高等数学</td><td align="left">70</td><td align="left">74</td><td align="left">80</td><td align="left">60</td><td align="left">50</td><td align="left">87</td><td align="left">68</td><td align="left">77</td><td align="left">95</td><td align="left">80</td><td align="left">79</td><td align="left">74</td><td align="left">69</td><td align="left">64</td><td align="left">82</td><td align="left">81</td></tr><tr><td align="left">英语</td><td align="left">83</td><td align="left">87</td><td align="left">69</td><td align="left">55</td><td align="left">80</td><td align="left">89</td><td align="left">96</td><td align="left">81</td><td align="left">83</td><td align="left">90</td><td align="left">54</td><td align="left">70</td><td align="left">79</td><td align="left">66</td><td align="left">85</td><td align="left">82</td></tr><tr><td align="left">Python</td><td align="left">90</td><td align="left">60</td><td align="left">82</td><td align="left">79</td><td align="left">88</td><td align="left">92</td><td align="left">85</td><td align="left">87</td><td align="left">89</td><td align="left">71</td><td align="left">45</td><td align="left">50</td><td align="left">80</td><td align="left">81</td><td align="left">87</td><td align="left">93</td></tr></tbody></table><pre><code>from itertools import groupbyimport matplotlib.pyplot as pltplt.rcParams[&#39;font.sans-serif&#39;] = [&#39;simhei&#39;]# 每门课程的成绩用字典存储scores = {&#39;操作系统&#39;:[89,70,49,87,92,84,73,62,78,81,90,65,77,82,81,79,80],&#39;高等数学&#39;:[70,74,80,60,50,87,68,77,95,80,79,74,69,64,82,81], &#39;英语&#39;:[83,87,69,55,80,89,96,81,83,90,54,70,79,66,85,82],&#39;Python编程&#39;:[90,60,82,79,88,92,85,87,89,71,45,50,80,81,87,93]}# 自定义分组函数，在下面的groupby()函数中使用def splitScore(score):    if score&gt;=85:        return &#39;优&#39;    elif score&gt;=70:        return &#39;良&#39;    elif score&gt;=60:        return &#39;及格&#39;     else:        return &#39;不及格&#39;# 统计每门课程中优、及格、不及格的人数# ratios的格式为{&#39;课程名称&#39;:{&#39;优&#39;:3, &#39;及格&#39;:5, &#39;不及格&#39;:1},...} ratios = dict()for subject, subjectScore in scores.items():    ratios[subject] = {}    # groupby()函数需要对原始分数进行排序才能正确分类    for category, num in groupby(sorted(subjectScore), splitScore):        ratios[subject][category] = len(tuple(num))# 创建4个子图fig, axs = plt.subplots(2,2) axs.shape = 1,4# 依次在4个子图中绘制每门课程的饼状图for index, subjectData in enumerate(ratios.items()):    # 选择子图    plt.sca(axs[0][index])    subjectName, subjectRatio = subjectData    plt.pie(list(subjectRatio.values()), # 每个扇形对应的书        labels=list(subjectRatio.keys()),  # 每个扇形的标签        autopct=&#39;%1.1f%%&#39;)    # 百分比显示格式    plt.xlabel(subjectName)plt.gca().set_aspect(&#39;equal&#39;) plt.text(1,-1.2,&#39;By hsiehchou&#39;)plt.show()</code></pre><p><img src="/medias/1591448519409.png" alt="学生成绩分布饼状图"></p><h3 id="4、雷达图、三维实战"><a href="#4、雷达图、三维实战" class="headerlink" title="4、雷达图、三维实战"></a>4、雷达图、三维实战</h3><h4 id="4-1-雷达图实战"><a href="#4-1-雷达图实战" class="headerlink" title="4.1 雷达图实战"></a>4.1 雷达图实战</h4><h5 id="4-1-1-雷达图"><a href="#4-1-1-雷达图" class="headerlink" title="4.1.1 雷达图"></a>4.1.1 雷达图</h5><ol><li><p>雷达图（Radar Chart），又蜘蛛网图（Spider Chart），可以很好刻画出某些指标的横向或纵向的对比关系。</p></li><li><p>雷达图常用于对多项指标的全面分析。比如：HR 想要比较两个应聘者的综合素质，用雷达图分别画出来，就可以进行直观的比较。</p></li><li><p>python 中用 matplotlib 模块绘制雷达图需要用到极坐标系。<br><img src="/medias/1591448640196.png" alt="雷达图"></p></li></ol><h5 id="4-1-2-雷达图之极坐标系"><a href="#4-1-2-雷达图之极坐标系" class="headerlink" title="4.1.2 雷达图之极坐标系"></a>4.1.2 雷达图之极坐标系</h5><ol><li><p>在平面内取一个定点 O，叫<code>极点</code>，引一条射线 Ox，叫做<code>极轴</code>，再选定一个长度单位和角度的正方向（通常取逆时针方向）。对于平面内任何一点 M，用 ρ 表示线段 OM 的长度（有时也用 r 表示），θ 表示从 Ox 到 OM 的角度，ρ 叫做点 M 的<code>极径</code>，θ 叫做点 M 的<code>极角</code>，有序数对 (ρ,θ) 就叫点 M 的<code>极坐标</code>，这样建立的坐标系叫做<code>极坐标系</code>。</p></li><li><p>通常情况下，M 的极径坐标单位为 1（长度单位），极角坐标单位为 °。<br><img src="/medias/1591448745576.png" alt="雷达图之极坐标系"></p></li></ol><h5 id="4-1-3-雷达图之-polar-函数"><a href="#4-1-3-雷达图之-polar-函数" class="headerlink" title="4.1.3 雷达图之 polar 函数"></a>4.1.3 雷达图之 polar 函数</h5><pre><code>polar(theta, r, **kwargs)</code></pre><p><strong>1. 主要参数：</strong><br>➢    theta：指极角θ。<br>➢    r：指极径。</p><pre><code>import numpy as npimport matplotlib.pyplot as pltplt.polar(0.25*np.pi, 20, &#39;ro&#39;, lw=2) plt.ylim(0,50)    #设置极轴的上下限plt.show()</code></pre><p><img src="/medias/1591448807308.png" alt="雷达图之 polar 函数"></p><p><strong>注：</strong><br>0.25*np.pi=45°：极角<br>20：极径<br>’ro’：绘极坐标形状为红色圆点<br>lw=2：极坐标图形宽度为 2</p><p><strong>2. 如果绘制多个极角和极轴</strong></p><pre><code>import numpy as npimport matplotlib.pyplot as plttheta = np.array([0.25,0.5,0.75,1,1.25,1.5,1.75,2]) r = [75,60,50,70,50,85,45,70]plt.polar(theta*np.pi, r, &#39;ro&#39;, lw=2)plt.ylim(0,100)plt.show()</code></pre><p><img src="/medias/1591448900309.png" alt="绘制多个极角和极轴"></p><p><strong>注：</strong><br>theta：定义了一个 ndarray 数组存储多个数据<br>r：定义了一个数组存放极轴的长度，也叫极径</p><ol start="3"><li><p>则在图中绘制出多个点:（0.25<em>π，75）,（0.5</em>π， 60）,（0.75<em>π，50）,（1.0</em>π，70）等。</p></li><li><p>绘制完极坐标点后，把每个点用线连起来，就是雷达图了。</p></li><li><p>只需要把图形绘制样式修改为 ‘ro-’ 即可.</p></li></ol><pre><code>import numpy as npimport matplotlib.pyplot as plttheta = np.array([0.25,0.5,0.75,1,1.25,1.5,1.75,2]) r = [75,60,50,70,50,85,45,70]plt.polar(theta*np.pi, r, &#39;ro-&#39;, lw=2)plt.ylim(0,100)plt.show()</code></pre><p><img src="/medias/1591449048767.png" alt="极坐标点之间的连线"></p><p><strong>注：</strong><br>’ro-’：其中-表示极坐标点之间的连线</p><ol start="6"><li>闭合曲线：多构造一个极坐标点，和第一个点重叠。</li></ol><pre><code>import numpy as npimport matplotlib.pyplot as plttheta = np.array([0.25,0.5,0.75,1,1.25,1.5,1.75,2,0.25]) r = [75,60,50,70,50,85,45,70,75]plt.polar(theta*np.pi, r, &#39;ro-&#39;, lw=2)plt.ylim(0,100)plt.show()</code></pre><p><img src="/medias/1591449141208.png" alt="闭合曲线"></p><p><strong>注：</strong><br>最后一个极坐标点与第一个参数相同：(0.25, 75)</p><ol start="7"><li>fill() 函数填充雷达图。</li></ol><pre><code>import numpy as npimport matplotlib.pyplot as plttheta =  np.array([0.25,0.5,0.75,1,1.25,1.5,1.75,2,0.25])r = [75,60,50,70,50,85,45,70,75]plt.polar(theta*np.pi, r, &#39;ro-&#39;, lw=2)plt.fill(theta*np.pi, r, facecolor=&#39;r&#39;, alpha=0.25) #填充plt.ylim(0,100)plt.show()</code></pre><p><img src="/medias/1591449248638.png" alt="填充雷达图"></p><h5 id="4-1-4-实战：学生课程成绩雷达图"><a href="#4-1-4-实战：学生课程成绩雷达图" class="headerlink" title="4.1.4 实战：学生课程成绩雷达图"></a>4.1.4 实战：学生课程成绩雷达图</h5><p>一般来说，大学的学位证只能证明学生达到该学习阶段的最低要求，并不能体现学生的综合能力以及擅长的学科与领域，因此用人单位在招聘时往往还需要借助于成绩单进行综合考察。如果把每个学生的专业核心课成绩绘制成雷达图印在学位证书上，就可让用人单位非常直观地了解学生综合能力。编写程序，根据某学生的部分专业核心课程和成绩清单绘制雷达图。</p><table><thead><tr><th align="left">科目</th><th align="left">C++</th><th align="left">Python</th><th align="left">高等数学</th><th align="left">大学英语</th><th align="left">软件工程</th><th align="left">组成原理</th><th align="left">操作系统</th><th align="left">网络工程</th></tr></thead><tbody><tr><td align="left">成绩</td><td align="left">82</td><td align="left">95</td><td align="left">72</td><td align="left">85</td><td align="left">45</td><td align="left">58</td><td align="left">65</td><td align="left">86</td></tr></tbody></table><pre><code>import numpy as npimport matplotlib.pyplot as plt# 某学生的课程与成绩courses = [&#39;C++&#39;, &#39;Python&#39;, &#39;高等数学&#39;, &#39;大学英语&#39;, &#39;软件工程&#39;, &#39;组成原理&#39;, &#39;操作系统&#39;, &#39;网络工程&#39;] scores = [82, 95, 72, 85, 45, 58, 65, 86]dataLength = len(scores)     # 数据长度# angles 数组把圆周等分为 dataLength 份angles = np.linspace(0,    # 数组第一个数据                        2*np.pi,  # 数组最后一个数据                dataLength,  # 数组中数据数量                endpoint=False)  # 不包含终点scores.append(scores[0])angles = np.append(angles, angles[0]) # 闭合# 绘制雷达图plt.polar(angles,  # 设置角度        scores,    # 设置各角度上的数据         &#39;rv--&#39;,    # 设置颜色、线型和端点符号         linewidth=2)  # 设置线宽# 设置角度网格标签plt.thetagrids(angles*180/np.pi, courses, fontproperties=&#39;simhei&#39;) # 填充雷达图内部plt.fill(angles, scores, facecolor=&#39;r&#39;, alpha=0.5)plt.show()</code></pre><p><img src="/medias/1591449650714.png" alt="学生课程成绩雷达图"></p><h4 id="4-2-三维图实战"><a href="#4-2-三维图实战" class="headerlink" title="4.2 三维图实战"></a>4.2 三维图实战</h4><h5 id="4-2-1-三维图概述"><a href="#4-2-1-三维图概述" class="headerlink" title="4.2.1 三维图概述"></a>4.2.1 三维图概述</h5><ol><li><p>matplotlib 支持一些基础的三维图表绘制，比如曲面图散点图和柱状图，需要使用 mpl_toolkits 模块。</p></li><li><p>如果要绘制三维图形，首先需要使用下面的语句导入相应的对象：<br>➢    <strong>from mpl_toolkits.mplot3d import Axes3D</strong></p></li><li><p>然后使用下面的两种方式之一声明要创建三维子图：<br>➢    <strong>ax = fig.gca(projection=’3d’)</strong><br>➢    <strong>ax = plt.subplot(111, projection=’3d’)</strong></p></li><li><p>接下来就可以使用 ax 的 plot() 方法绘制三维曲线、plot_surface() 方法绘制三维曲面、 scatter() 方法绘制三维散点图或 bar3d() 方法绘制三维柱状图了。</p></li></ol><h5 id="4-2-2-三维曲面绘制方法-p3d-Axes3D-plot-surface"><a href="#4-2-2-三维曲面绘制方法-p3d-Axes3D-plot-surface" class="headerlink" title="4.2.2 三维曲面绘制方法:  p3d.Axes3D. plot_surface()"></a>4.2.2 三维曲面绘制方法:  p3d.Axes3D. plot_surface()</h5><ol><li>在绘制三维图形时，至少需要指定x、y、z三个坐标轴的数据，然后再根据不同的图形类型指定额外的参数设置图形的属性。</li></ol><pre><code>plot_surface(X, Y, Z, *args, **kwargs)</code></pre><ol start="2"><li>常用参数：<br>➢    rstride 和 cstride 分别控制 x 和 y 两个方向的步长，这决定了曲面上每个面片的大小；<br>➢    color 用来指定面片的颜色；<br>➢    cmap 用来指定面片的颜色映射表。</li></ol><h5 id="4-2-3-三维散点图绘制方法：p3d-Axes3D-scatter"><a href="#4-2-3-三维散点图绘制方法：p3d-Axes3D-scatter" class="headerlink" title="4.2.3 三维散点图绘制方法：p3d.Axes3D.scatter()"></a>4.2.3 三维散点图绘制方法：p3d.Axes3D.scatter()</h5><pre><code>p3d.Axes3D.scatter (xs, ys, zs=0, zdir=&#39;z&#39;, s=20, c=None, depthshade=True, *args,  **kwargs)</code></pre><p><strong>常用参数</strong><br>➢    xs、ys、zs 分别用来指定散点符号的 x、y、z 坐标，如果同时为标量则指定一个三点符号的坐标，如果同时为等长数组则指定一系列散点符号的坐标。<br>➢    s 用来指定散点符号的大小，可以是标量或与 xs 等长的数组。</p><h5 id="4-2-4-三维柱状图绘制方法：p3d-Axes3D-bar3d"><a href="#4-2-4-三维柱状图绘制方法：p3d-Axes3D-bar3d" class="headerlink" title="4.2.4 三维柱状图绘制方法：p3d.Axes3D. bar3d()"></a>4.2.4 三维柱状图绘制方法：p3d.Axes3D. bar3d()</h5><pre><code>p3d.Axes3D. bar3d(x, y, z, dx, dy, dz, color=None, zsort=&#39;average&#39;, *args, **kwargs)</code></pre><p><strong>常用参数</strong><br>➢    x、y、z 分别用来指定每个柱底面的坐标，如果这三个参数都是标量则指定一个柱的底面坐标，如果是三个等长的数组则指定多个柱的底面坐标。<br>➢    dx、dy、dz 分别用来指定柱在三个坐标轴上的跨度，即 x 方向的宽度、y 方向的厚度和 z 方向的高度。<br>➢    color 用来指定柱的表面颜色。</p><h5 id="4-2-5-三维曲线图实战"><a href="#4-2-5-三维曲线图实战" class="headerlink" title="4.2.5 三维曲线图实战"></a>4.2.5 三维曲线图实战</h5><ol><li>生成测试数据 x、y、z，然后绘制三维曲线，并设置图例的字体和字号。</li></ol><pre><code>import numpy as npimport matplotlib as mplfrom mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as pltimport matplotlib.font_manager as fm# 绘制三维图形fig = plt.figure()ax = fig.gca(projection=&#39;3d&#39;)# 生成测试数据theta = np.linspace(-4 * np.pi, 4 * np.pi, 100) z = np.linspace(-4, 4, 100)*0.3r = z**4 + 1x = r * np.sin(theta)y = r * np.cos(theta)# 绘制三维曲线，设置标签ax.plot(x, y, z, &#39;bv-&#39;, label=&#39;参数曲线&#39;)# 设置图例字体、字号，显示图例font = fm.FontProperties(fname=r&#39;C:\Users\hsiehchou\.spyder-py3\STKAITI.ttf&#39;)mpl.rcParams[&#39;legend.fontsize&#39;] = 10ax.legend(prop=font)plt.show()</code></pre><p><img src="/medias/1591450502805.png" alt="三维曲线"></p><ol start="2"><li>生成测试数据 x、y、z，然后绘制三维曲线，并设置图例的字体和字号。</li></ol><pre><code>import numpy as npimport matplotlib.pyplot as pltimport mpl_toolkits.mplot3d# 生成测试数据，在x和y方向分布生成-2到2之间的20个数# 步长使用虚数，虚部表示点的个数，并且包含endx, y = np.mgrid[-2:2:20j, -2:2:20j]z = 50 * np.sin(x+y*2)# 创建三维图形ax = plt.subplot(111, projection=&#39;3d&#39;)# 绘制三维曲面ax.plot_surface(x,y,z, rstride=3, cstride=2, cmap=plt.cm.coolwarm)# 设置坐标轴标签ax.set_xlabel(&#39;X&#39;)ax.set_ylabel(&#39;Y&#39;)ax.set_zlabel(&#39;Z&#39;)# 设置图形标题ax.set_title(&#39;三维曲面&#39;, fontproperties=&#39;simhei&#39;, fontsize=24)plt.show()</code></pre><p><img src="/medias/1591450640439.png" alt="三维曲面"></p><ol start="3"><li>生成测试数据，绘制三维柱状图，设置每个柱的颜色随机，且宽度和厚度都为 1。 </li></ol><pre><code>import numpy as npimport matplotlib.pyplot as pltimport mpl_toolkits.mplot3dx = np.random.randint(0, 40, 10)y = np.random.randint(0, 40, 10)z = 80*abs(np.sin(x+y))ax = plt.subplot(projection=&#39;3d&#39;)for xx, yy, zz in zip(x, y, z):    color = np.random.random(3)    ax.bar3d(xx, yy, 0, dx=1, dy=1, dz=zz, color=color)ax.set_xlabel(&#39;X&#39;) ax.set_ylabel(&#39;Y&#39;) ax.set_zlabel(&#39;Z&#39;) plt.show()</code></pre><p><img src="/medias/1591450739894.png" alt="三维柱状图"></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第四章 Python数据处理</title>
      <link href="/2020/06/04/di-si-zhang-python-shu-ju-chu-li/"/>
      <url>/2020/06/04/di-si-zhang-python-shu-ju-chu-li/</url>
      
        <content type="html"><![CDATA[<h2 id="第四章-Python-数据处理"><a href="#第四章-Python-数据处理" class="headerlink" title="第四章 Python 数据处理"></a>第四章 Python 数据处理</h2><h3 id="1、numpy-数组操作"><a href="#1、numpy-数组操作" class="headerlink" title="1、numpy 数组操作"></a>1、numpy 数组操作</h3><h4 id="1-1-numpy-库概述"><a href="#1-1-numpy-库概述" class="headerlink" title="1.1 numpy 库概述"></a>1.1 numpy 库概述</h4><h5 id="1-1-1-引言"><a href="#1-1-1-引言" class="headerlink" title="1.1.1 引言"></a>1.1.1 引言</h5><ol><li>Python 中用列表(list)保存一组值，可用来当作数组使用，由于列表的元素可以是任何对象，因此列表中所保存的是对象的指针。为保存一个简单的[1,2,3]，需要有 3 个指针和三个整数对象。对于数值运算来说这种结构显然<strong>比较浪费内存和CPU计算时间</strong>。</li><li>此外 Python 还提供了一个 array 模块，array 对象和列表不同，它直接保存数值，和 C 语言的一维数组比较类似。但是由于它<strong>不支持多维</strong>，也没有各种运算函数，因此也不适合做数值运算。</li><li>numpy 的诞生弥补了这些不足，numpy 提供 <strong>ndarray（N-dimensional array object</strong>）对象：ndarray 是存储单一数据类型的多维数组。</li></ol><h5 id="1-1-2-numpy（Numerical-Python的简称）"><a href="#1-1-2-numpy（Numerical-Python的简称）" class="headerlink" title="1.1.2 numpy（Numerical Python的简称）"></a>1.1.2 numpy（Numerical Python的简称）</h5><p>是高性能科学计算和数据分析的基础包，支持维度数组与矩阵运算。包括：</p><ol><li>一个强大的 N 维数组对象 ndarray，具有矢量算术运算和复杂广播能力的快速且节省空间的多维数组。</li><li>用于对整组数据进行快速运算的标准数学函数（无需编写循环）。</li><li>用于读写磁盘数据的工具以及用于操作内存映射文件的工具。</li><li>线性代数、随机数生成以及傅里叶变换等功能。</li><li>用于集成由 C、C++、Fortran 等语言编写的代码的工具。</li></ol><h5 id="1-1-3-numpy-库提供了大量的库函数和操作"><a href="#1-1-3-numpy-库提供了大量的库函数和操作" class="headerlink" title="1.1.3 numpy 库提供了大量的库函数和操作"></a>1.1.3 numpy 库提供了大量的库函数和操作</h5><p>可以帮助程序员轻松地进行数值计算。这类数值计算广泛用于以下任务：</p><ol><li>机器学习模型：在编写机器学习算法时，需要对矩阵进行各种数值计算。例如矩阵乘法、加法等。使用 numpy 库可进行<strong>简单</strong>(在编写代码方面)和<strong>快速</strong>(在速度方面)计算。numpy 数组用于存储训练数据和机器学习模型的参数。</li><li>图像处理和计算机图形学：计算机中的图像表示为多维数字数组，numpy 提供了一些优秀的库函数来快速处理图像。例如，镜像图像、按特定角度旋转图像等。</li><li>数学任务：numpy 可进行数值积分、微分、内插、外推等操作。numpy 库形成了一种基于 Python 的 MATLAB 的快速替代。</li></ol><h4 id="1-2-numpy-库安装"><a href="#1-2-numpy-库安装" class="headerlink" title="1.2 numpy 库安装"></a>1.2 numpy 库安装</h4><h5 id="1-2-1-使用已有的发行版本"><a href="#1-2-1-使用已有的发行版本" class="headerlink" title="1.2.1 使用已有的发行版本"></a>1.2.1 使用已有的发行版本</h5><ol><li>对于许多用户，尤其是 Windows 用户，最简单的方法是下载Python  发行版，包含所有的关键包（包括 numpy，SciPy，matplotlib 以及 Python 核心自带的其它包）：</li><li><strong>Anaconda</strong>: 免费 Python 发行版，用于进行大规模数据处理、预测分析，和科学计算，致力于简化包的管理和部署。支持 Linux, Windows 和 Mac 系统。（推荐）</li><li>Enthought Canopy: 提供了免费和商业发行版。持 Linux, Windows 和 Mac 系统。</li><li>WinPython: 免费的 Python 发行版，包含科学计算包与 Spyder IDE。支持 Windows。</li><li>Pyzo: 基于 Anaconda 的免费发行版本及 IEP 的交互开发环境，超轻量级。 支持Linux, Windows 和 Mac 系统。</li></ol><h5 id="1-2-2-使用-pip-安装"><a href="#1-2-2-使用-pip-安装" class="headerlink" title="1.2.2 使用 pip 安装"></a>1.2.2 使用 pip 安装</h5><ol><li>安装 numpy 最简单的方法就是使用 pip 工具：<pre><code>python -m pip install --user numpy  </code></pre></li><li><code>--user</code> 选项可以设置只安装在当前的用户下，而不是写入到系统目录。</li></ol><h4 id="1-3-ndarray-概述"><a href="#1-3-ndarray-概述" class="headerlink" title="1.3 ndarray 概述"></a>1.3 ndarray 概述</h4><ol><li>N 维数组对象 ndarray 是用于存放<strong>同类型元素</strong>的多维数组。</li><li>ndarray 中的每个元素在内存中都有<strong>相同存储大小</strong>的区域。</li><li>ndarray 中的每个元素是数据类型对象的对象 （ 称为 dtype）。</li><li>与 Python 中的其他容器对象一样，可以通过对数组进行索引或切片。</li><li>可通过 ndarray 的方法和属性来访问和修改 ndarray 的内容 。</li></ol><h4 id="1-4-ndarray-之创建数组"><a href="#1-4-ndarray-之创建数组" class="headerlink" title="1.4 ndarray 之创建数组"></a>1.4 ndarray 之创建数组</h4><h5 id="1-4-1-创建-ndarray：创建数组最简单的办法就是使用-array-函数。"><a href="#1-4-1-创建-ndarray：创建数组最简单的办法就是使用-array-函数。" class="headerlink" title="1.4.1 创建 ndarray：创建数组最简单的办法就是使用 array 函数。"></a>1.4.1 创建 ndarray：创建数组最简单的办法就是使用 array 函数。</h5><p>它接受一切序列型的对象，然后产生一个含有传入数据的 numpy 数组。其中，嵌套序列（比如由一组等长列表组成的列表）将会被转换为一个多维数组。</p><pre><code>numpy.array(object, dtype = None, copy = True, order = None, subok = False, ndmin = 0)</code></pre><table><thead><tr><th align="left">名称</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">object</td><td align="left">数组或嵌套的数列</td></tr><tr><td align="left">dtype</td><td align="left">数组元素的数据类型，可选</td></tr><tr><td align="left">copy</td><td align="left">对象是否需要复制，可选</td></tr><tr><td align="left">order</td><td align="left">创建数组的样式，C 为行方向，F 为列方向，A 为任意方向（默认）</td></tr><tr><td align="left">subok</td><td align="left">默认返回一个与基类类型一致的数组</td></tr><tr><td align="left">ndmin</td><td align="left">指定生成数组的最小维度</td></tr></tbody></table><pre><code>&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; a = [1, 2, 3, 4]   # 创建简单的列表&gt;&gt;&gt; b = np.array(a)    # 将列表转换为数组&gt;&gt;&gt; barray([1, 2, 3, 4])&gt;&gt;&gt; c = np.array([[1,  2],  [3,  4]]) &gt;&gt;&gt;c[[1, 2] [3, 4]] </code></pre><h5 id="1-4-2-除了np-array之外，还有一些函数也可以新建数组："><a href="#1-4-2-除了np-array之外，还有一些函数也可以新建数组：" class="headerlink" title="1.4.2 除了np.array之外，还有一些函数也可以新建数组："></a>1.4.2 除了np.array之外，还有一些函数也可以新建数组：</h5><ol><li>zeros 和 ones 分别可以创建指定长度或者形状的全 0 或全 1 数组</li><li>empty 可以创建一个没有任何具体值的数组</li></ol><pre><code>&gt;&gt;&gt; np.zeros(3)  # 全 0 一维数组array([ 0.,  0.,  0.])&gt;&gt;&gt; np.ones(3)   # 全 1 一维数组array([ 1.,  1.,  1.])&gt;&gt;&gt; np.zeros((3,3))   # 全 0 二维数组，3 行 3 列array([[ 0.,  0.,  0.],    [ 0.,  0.,  0.],    [ 0.,  0.,  0.]])&gt;&gt;&gt; np.zeros((3,1))   # 全 0 二维数组，3 行 1 列array([[ 0.],    [ 0.],    [ 0.]])&gt;&gt;&gt; np.zeros((1,3))  # 全 0 二维数组，1 行 3 列array([[ 0.,  0.,  0.]]) &gt;&gt;&gt; np.ones((3,3))   # 全 1 二维数组，3 行 3 列array([[ 1.,  1.,  1.],    [ 1.,  1.,  1.],    [ 1.,  1.,  1.]])&gt;&gt;&gt; np.ones((1,3))     # 全 1 二维数组，1 行 3 列array([[ 1.,  1.,  1.]]) &gt;&gt;&gt; np.identity(3)     # 单位矩阵，3 行 3 列array([[ 1.,  0.,  0.],    [ 0.,  1.,  0.],    [ 0.,  0.,  1.]])</code></pre><h5 id="1-4-3-创建随机数组"><a href="#1-4-3-创建随机数组" class="headerlink" title="1.4.3 创建随机数组"></a>1.4.3 创建随机数组</h5><ol><li><p><strong>均匀分布</strong><br>1) np.random.rand(10, 10) 创建指定形状(示例为10行10列)的数组(范围在0至1之间)<br>2) np.random.uniform(0, 100)  创建指定范围内的一个数<br>3) np.random.randint(0, 100)  创建指定范围内的一个整数</p></li><li><p><strong>正态分布</strong><br>1) np.random.normal(1.75, 0.1, (2, 3))     给定均值/标准差/维度的正态分布</p></li></ol><pre><code>&gt;&gt;&gt; np.random.randint(0, 50, 5) # 随机数组，5个0到50之间的数字array([13, 47, 31, 26,  9])&gt;&gt;&gt; np.random.randint(0, 50, (3,5)) # 3行5列，共15个随机整数，都介于[0,50]array([[44, 34, 35, 28, 18],    [24, 24, 26,  4, 21],    [30, 40,  1, 24, 17]])&gt;&gt;&gt; np.random.rand(10)  # 10个介于[0,1)的随机数array([ 0.58193552,  0.11106142,  0.13848858,  0.61148304,  0.72031503,0.12807841,  0.49999167,  0.24124012,  0.15236595,  0.54568207])&gt;&gt;&gt; np.random.standard_normal(5) # 从标准正态分布中随机采样5个数字array([2.82669067, 0.9773194, -0.72595951, -0.11343254, 0.74813065])</code></pre><h5 id="1-4-4-查看数组属性的用法"><a href="#1-4-4-查看数组属性的用法" class="headerlink" title="1.4.4 查看数组属性的用法"></a>1.4.4 查看数组属性的用法</h5><table><thead><tr><th align="left">用法</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">b.size</td><td align="left">数组元素个数</td></tr><tr><td align="left">b.shape</td><td align="left">数组形状</td></tr><tr><td align="left">b.ndim</td><td align="left">数组维度</td></tr><tr><td align="left">b.dtype</td><td align="left">数组元素类型</td></tr><tr><td align="left">b.Itemsize</td><td align="left">数组元素字节大小</td></tr></tbody></table><h5 id="1-4-5-查看数组属性的用法"><a href="#1-4-5-查看数组属性的用法" class="headerlink" title="1.4.5 查看数组属性的用法"></a>1.4.5 查看数组属性的用法</h5><pre><code>import numpy as npx = np.array([(1,2,3),(4,5,6)]) print(x)print(x.size)print(x.ndim)print(x.shape)print(x.itemsize)print(x.dtype,&#39;\n&#39;)y = x.reshape(3,2)print(y,&#39;\n&#39;)print(y.shape)</code></pre><h4 id="1-5-数组和标量之间的运算"><a href="#1-5-数组和标量之间的运算" class="headerlink" title="1.5 数组和标量之间的运算"></a>1.5 数组和标量之间的运算</h4><p>数组很重要，因为它可以使我们不用编写循环即可对数据执行批量运算。这通常叫做<strong>矢量化</strong>（vectorization）。大小相等的数组之间的任何算术运算都会将运算应用到元素级。同样，数组与标量的算术运算也会将那个标量值传播到各个元素。</p><pre><code>&gt;&gt;&gt; arr = np.array([[1., 2., 3.], [4., 5., 6.]]) &gt;&gt;&gt; arrarray([[1., 2., 3.],&gt;&gt;&gt; 1 / arr array([[1., 0.5, 0.33333333],       [0.25, 0.2, 0.16666667]])&gt;&gt;&gt; arr - arr array([[0., 0., 0.],    [0., 0., 0.]])&gt;&gt;&gt; arr * arr array([[ 1.,  4.,  9.],        [16., 25., 36.]])&gt;&gt;&gt; arr ** 0.5array([[1., 1.41421356, 1.73205081],        [2., 2.23606798, 2.44948974]])</code></pre><h4 id="1-6-基本的索引和切片"><a href="#1-6-基本的索引和切片" class="headerlink" title="1.6 基本的索引和切片"></a>1.6 基本的索引和切片</h4><ol><li>选取数据子集或的单个元素的方式有很多。</li><li>一维数组很简单，从表面上看，它们跟 Python 列表的功能差不多。</li><li>一维数组跟列表最重要的区别在于，数组切片是原始数组的视图。这意味着数据不会被复制，数组视图上的任何修改都会直接反映到原始数组上。</li><li>将一个标量值赋值给一个切片时，该值会自动传播到整个选区。</li></ol><pre><code>&gt;&gt;&gt; arr = np.arange(10)&gt;&gt;&gt; arrarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) &gt;&gt;&gt; arr[5]5&gt;&gt;&gt; arr[5:8]array([5, 6, 7])&gt;&gt;&gt; arr[5:8] = 12&gt;&gt;&gt; arrarray([ 0,  1,  2,  3,  4, 12, 12, 12,  8,  9]) &gt;&gt;&gt; arr_slice = arr[5:8]&gt;&gt;&gt; arr_slice[1] = 12345&gt;&gt;&gt; arrarray([ 0, 1,  2,  3,  4, 12, 12345, 12,  8,  9]) &gt;&gt;&gt; arr_slice[:] = 64&gt;&gt;&gt; arrarray([ 0,  1,  2,  3,  4, 64, 64, 64,  8,  9])</code></pre><ol start="5"><li>在二维数组中，各索引位置上的元素不再是标量而是一维数组。</li><li>可以对各个元素进行递归访问，但是这样有点麻烦。</li><li>还有一种方式是传入一个以逗号隔开的索引列表来选取单个元素。</li><li>在多维数组中，如果省略了后面的索引，则返回对象会是一个维度低一点的 ndarray。</li></ol><pre><code>&gt;&gt;&gt; arr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])&gt;&gt;&gt; arr3darray([[[ 1,  2,  3],    [ 4,  5,  6]],    [[ 7,  8,  9],    [10, 11, 12]]])&gt;&gt;&gt; arr3d[0] array([[1, 2, 3],    [4, 5, 6]])&gt;&gt;&gt; arr3d[0][1]     array([4, 5, 6])</code></pre><h4 id="1-7-数学和统计方法"><a href="#1-7-数学和统计方法" class="headerlink" title="1.7 数学和统计方法"></a>1.7 数学和统计方法</h4><ol><li><strong>基本数组统计方法</strong><br>可以通过数组上的一组数学函数对整个数组或某个轴向的数据进行统计计算。</li></ol><table><thead><tr><th align="left">方法</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">sum</td><td align="left">对数组中全部或某轴向的元素求和。零长度的数组的sum 为 0</td></tr><tr><td align="left">mean</td><td align="left">算术平均数。零长度的数组的 mean 为 NaN。</td></tr><tr><td align="left">std, var</td><td align="left">分别为标准差和方差，自由度可调（默认为n）</td></tr><tr><td align="left">min, max</td><td align="left">最大值和最小值</td></tr><tr><td align="left">argmin, argmax</td><td align="left">分别为最大和最小元素的索引</td></tr><tr><td align="left">cumsum</td><td align="left">所有元素的累加</td></tr><tr><td align="left">cumprod</td><td align="left">所有元素的累积</td></tr></tbody></table><ol start="2"><li><strong>sum、mean 以及标准差 std</strong> 等聚合计算既可以当做数组的实例方法调用，也可以当做顶级 numpy 函数使用。</li></ol><pre><code>&gt;&gt;&gt; arr = np.random.randn(5, 4) # 5行4列正态分布的数据&gt;&gt;&gt; arr.mean()-0.022341797127577216 &gt;&gt;&gt; np.mean(arr)-0.022341797127577216 &gt;&gt;&gt; arr.sum()-0.44683594255154435</code></pre><ol start="3"><li><strong>mean 和 sum</strong> 这类的函数可以接受一个 axis 参数（用于计算该轴向上的统计值）。</li></ol><pre><code>&gt;&gt;&gt; arr.mean(axis=1)array([-0.11320162, -0.032351  , -0.24522299,  0.13275031,  0.14631631]) &gt;&gt;&gt; arr.sum(0)array([-1.71093252,  3.4431099 , -1.78081725, -0.39819607])</code></pre><pre><code># 数学和统计方法import numpy as nparr = np.random.randn(5, 4) # 正态分布的数据print(arr, &#39;\n&#39;)print(arr.mean(), &#39;\n&#39;)print(np.mean(arr), &#39;\n&#39;)print(arr.sum(), &#39;\n&#39;)print(arr.mean(axis = 0), &#39;\n&#39;) # 按列print(arr.mean(axis = 1), &#39;\n&#39;) # 按行print(arr.sum(0), &#39;\n&#39;)print(arr.sum(1), &#39;\n&#39;)b = np.array(arr[0])print(arr[0])print(b.mean())</code></pre><p><strong>运行结果</strong></p><pre><code>[[-0.80010107  1.07708638 -1.35612627 -0.0968496 ] [ 1.19282733 -0.76803973  0.96541953 -0.92755397] [ 0.1632381   0.20535389 -0.40766701 -1.59165831] [ 0.82877811  2.36588387  1.93749325 -0.59130029] [ 2.00014605 -2.02632033  0.98467712 -0.12523931]] 0.15150238691579443 0.15150238691579443 3.0300477383158886 [ 0.6769777   0.17079282  0.42475933 -0.6665203 ] [-0.29399764  0.11566329 -0.40768333  1.13521373  0.20831588] [ 3.38488852  0.85396408  2.12379663 -3.33260149] [-1.17599056  0.46265316 -1.63073333  4.54085494  0.83326354] [-0.80010107  1.07708638 -1.35612627 -0.0968496 ]-0.29399764052791716</code></pre><ol start="4"><li><p><strong>cumsum</strong><br>按照所给定的轴参数返回元素的梯形累计和，axis=0，按照行累加。axis=1，按照列累加。</p></li><li><p><strong>cumprod</strong><br>按照所给定的轴参数返回元素的梯形累计乘积，axis=0，按照行累积。 axis=1，按照列累积。</p></li></ol><pre><code>&gt;&gt;&gt; arr = np.array([[0, 1 ,2], [3, 4, 5], [6, 7, 8]]) &gt;&gt;&gt; arr.cumsum(0)array([[ 0,  1,  2],[ 3,  5,  7],[ 9, 12, 15]], dtype=int32)&gt;&gt;&gt; arr.cumprod(1)array([[  0,   0,   0],    [  3,  12,  60],    [  6,  42, 336]], dtype=int32)</code></pre><h3 id="2、numpy-矩阵操作"><a href="#2、numpy-矩阵操作" class="headerlink" title="2、numpy 矩阵操作"></a>2、numpy 矩阵操作</h3><h4 id="2-1-numpy-矩阵库-Matrix"><a href="#2-1-numpy-矩阵库-Matrix" class="headerlink" title="2.1 numpy 矩阵库(Matrix)"></a>2.1 numpy 矩阵库(Matrix)</h4><ol><li><p>NumPy 中包含了一个矩阵库 numpy.matlib，该模块中的函数返回的是一个矩阵，而不是 ndarray 对象。</p></li><li><p>一个𝑚x𝑛的矩阵是一个由 𝑚行（row）𝑛 列（column）元素排列成的矩形阵列。</p></li><li><p>矩阵里的元素可以是数字、符号或数学式。以下是一个由6个数字元素构成的2行3列的矩阵：$\begin{bmatrix}1 &amp; 9 &amp; -13\20 &amp; 5 &amp; -6 \end{bmatrix} $</p></li><li><p>NumPy 和 Matlab 不一样 ， 对于多维数组的运算 ， 缺省情况下并不使用矩阵运算 ，如果你希望对数组进行矩阵运算的话 ， 可以调用ndarry对象相应的函数</p></li></ol><h4 id="2-2-numpy矩阵生成"><a href="#2-2-numpy矩阵生成" class="headerlink" title="2.2 numpy矩阵生成"></a>2.2 numpy矩阵生成</h4><ol><li><strong>常规方式生成</strong></li></ol><pre><code>import numpy as npx = np.matrix([[1,2,3], [4,5,6]])y = np.matrix([1,2,3,4,5,6])# x[0,0]返回行下标和列下标都为0的元素# 注意，对于矩阵x来说，x[0,0]和x[0][0]的含义不一样print(x, y, x[0,0],x[0][0],sep=&#39;\n\n&#39;)</code></pre><p><strong>运行结果</strong></p><pre><code>[[1 2 3] [4 5 6]][[1 2 3 4 5 6]]1[[1 2 3]]</code></pre><ol start="2"><li><strong>matlib.empty()</strong></li></ol><p><strong>numpy.matlib.empty(shape, dtype, order)</strong></p><p>➢    <strong>shape</strong>: 定义新矩阵形状的整数或整数元组<br>➢    <strong>Dtype</strong>: 可选，数据类型<br>➢    <strong>order</strong>: C（行序优先） 或者 F（列序优先）</p><pre><code>import numpy.matlib import numpy as npprint (np.matlib.empty((1,2))) # 填充为随机数据</code></pre><p><strong>运行结果</strong><br>[[-6.04134793e-125  1.10846539e-274]]</p><ol start="3"><li><p><strong>numpy.matlib.zeros()</strong></p></li><li><p><strong>numpy.matlib.ones()</strong><br>➢    numpy.matlib.zeros() 函数创建一个以 0 填充的矩阵。<br>➢    numpy.matlib.ones() 函数创建一个以 1 填充的矩阵。</p></li></ol><pre><code>import numpy.matlib import numpy as np print (np.matlib.zeros((2,2))) print (np.matlib.ones((2,2)))</code></pre><p><strong>运行结果</strong><br>[[0. 0.]<br> [0. 0.]]<br>[[1. 1.]<br> [1. 1.]]</p><ol start="5"><li><strong>numpy.matlib.eye()</strong><br>numpy.matlib.eye() 函数返回一个矩阵，对角线元素为 1，其他位置为零。</li></ol><p><strong>numpy.matlib.eye(n, M, k, dtype)</strong><br>➢    <strong>n</strong>: 返回矩阵的行数<br>➢    <strong>M</strong>: 返回矩阵的列数，默认为 n<br>➢    <strong>k</strong>: 对角线的索引<br>➢    <strong>dtype</strong>: 数据类型</p><pre><code>import numpy.matlib import numpy as np print (np.matlib.eye(n =  3, M =  4, k =  0, dtype =  int))</code></pre><p><strong>运行结果</strong><br>[[1 0 0 0]<br> [0 1 0 0]<br> [0 0 1 0]]</p><ol start="6"><li><strong>numpy.matlib.identity():返回给定大小的单位矩阵</strong><br>单位矩阵是个方阵，从左上角到右下角的对角线（称为主对角线）上的元素均为 1，除此以外全都为 0。</li></ol><p>$$I_1=[1]，I_2=\begin{bmatrix}1 &amp; 0\0 &amp;1\end{bmatrix}，I_3=\begin{bmatrix}1 &amp; 0 &amp; 0\0 &amp; 1 &amp; 0\0 &amp; 0 &amp; 1\end{bmatrix}，…，$$</p><p>$$I_n=\begin{bmatrix}1 &amp; 0 &amp; … &amp; 0\ 0 &amp; 1 &amp; … &amp; 0\. &amp; . &amp; . &amp; .\. &amp; . &amp; . &amp; .\ . &amp; . &amp; . &amp; . \0 &amp; 0 &amp; … &amp; 1  \end{bmatrix}$$</p><pre><code>import numpy.matlib import numpy as np # 大小为4，类型为整型print (np.matlib.identity(4, dtype = int))</code></pre><p><strong>运行结果</strong><br>[[1 0 0 0]<br> [0 1 0 0]<br> [0 0 1 0]<br> [0 0 0 1]]</p><ol start="7"><li><strong>numpy.matlib.rand()</strong><br>numpy.matlib.rand()   函数创建一个给定大小的矩阵，数据是随机填充的。</li></ol><pre><code>import numpy.matlib import numpy as np print (np.matlib.rand(4,4))</code></pre><p><strong>运行结果</strong><br>[[0.32525881 0.19282243 0.63614484 0.19132335]<br> [0.89759542 0.2546368  0.129491   0.87191412]<br> [0.71666597 0.48077184 0.80972151 0.30165613]<br> [0.34678005 0.39324507 0.8171954  0.53549778]]</p><h4 id="2-3-numpy矩阵常用操作"><a href="#2-3-numpy矩阵常用操作" class="headerlink" title="2.3 numpy矩阵常用操作"></a>2.3 numpy矩阵常用操作</h4><h5 id="2-3-1-矩阵与二维数组相互转换"><a href="#2-3-1-矩阵与二维数组相互转换" class="headerlink" title="2.3.1 矩阵与二维数组相互转换"></a>2.3.1 <strong>矩阵与二维数组相互转换</strong></h5><p>矩阵总是二维的，而 ndarray 是一个 n 维数组。 两个对象都是可互换的。</p><pre><code>import numpy.matlib import numpy as np  i = np.matrix(&#39;1,2;3,4&#39;) print(i)j = np.asarray(i) print(j)k = np.asmatrix (j)  print (k)</code></pre><p><strong>运行结果</strong><br>[[1 2]<br> [3 4]]<br>[[1 2]<br> [3 4]]<br>[[1 2]<br> [3 4]]</p><h5 id="2-3-2-矩阵转置"><a href="#2-3-2-矩阵转置" class="headerlink" title="2.3.2 矩阵转置"></a>2.3.2 矩阵转置</h5><pre><code>import numpy as npx = np.matrix([[1,2,3], [4,5,6]]) y = np.matrix([1,2,3,4,5,6]) print(x.T, y.T, sep=&#39;\n\n&#39;)</code></pre><p><strong>运行结果</strong><br>[[1 4]<br> [2 5]<br> [3 6]]</p><p>[[1]<br> [2]<br> [3]<br> [4]<br> [5]<br> [6]]</p><h5 id="2-3-3-查看矩阵特征"><a href="#2-3-3-查看矩阵特征" class="headerlink" title="2.3.3 查看矩阵特征"></a>2.3.3 查看矩阵特征</h5><pre><code>import numpy as npx = np.matrix([[1,2,3], [4,5,6]]) print(x.mean(), end=&#39;\n====\n&#39;) # 所有元素平均值print(x.mean(axis=0), end=&#39;\n====\n&#39;)   # 纵向平均值print(x.mean(axis=0).shape, end=&#39;\n====\n&#39;)print(x.mean(axis=1), end=&#39;\n====\n&#39;)  # 横向平均值print(x.sum(), end=&#39;\n====\n&#39;)  # 所有元素之和   print(x.max(axis=1), end=&#39;\n====\n&#39;)  # 横向最大值print(x.argmax(axis=1), end=&#39;\n====\n&#39;) # 横向最大值的下标print(x.diagonal(), end=&#39;\n====\n&#39;)  # 对角线元素</code></pre><p><strong>运行结果</strong></p><pre><code>3.5====[[2.5 3.5 4.5]]====(1, 3)====[[2.] [5.]]====21====[[3] [6]]====[[2] [2]]====[[1 5]]====</code></pre><h5 id="2-3-4-矩阵乘法"><a href="#2-3-4-矩阵乘法" class="headerlink" title="2.3.4 矩阵乘法"></a>2.3.4 矩阵乘法</h5><p>$A=\left(a_{ij}\right)^{m,p}<em>{i,j=1}，B=\left(b</em>{ij}\right)^{p,n}<em>{i,j=1}，C=\left(c</em>{ij}\right)^{m,n}<em>{i,j=1}，c</em>{ij}=\sum\limits_{k=1}^p{a_{ik}b_{kj}}$</p><pre><code>import numpy as npx = np.matrix([[1,2,3], [4,5,6]]) y = np.matrix([[1,2], [3,4], [5,6]]) print(x*y)</code></pre><p><strong>运行结果</strong><br>[[22 28]<br> [49 64]]</p><h4 id="2-4-矩阵运算"><a href="#2-4-矩阵运算" class="headerlink" title="2.4 矩阵运算"></a>2.4 矩阵运算</h4><ol><li><p>numpy.linalg 中有一组标准的矩阵分解运算以及诸如求逆和行列式之类的东西</p></li><li><p>它们跟 MATLAB 和 R 等语言所使用的是相同的行业标准级Fortran库</p></li><li><p>常用的 numpy.linalg 函数：</p></li></ol><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">diag</td><td align="left">以一维数组的形式返回方阵的对角线（或非对角线）元素，或将一维数组转换为方阵（非对角线元素为0）</td></tr><tr><td align="left">dot</td><td align="left">矩阵乘法</td></tr><tr><td align="left">trace</td><td align="left">计算对角线元素的和</td></tr><tr><td align="left">det</td><td align="left">计算矩阵行列式</td></tr><tr><td align="left">eig</td><td align="left">计算方阵的特征值和特征向量</td></tr><tr><td align="left">inv</td><td align="left">计算方阵的逆</td></tr><tr><td align="left">svd</td><td align="left">计算奇异值分解（SVD）</td></tr><tr><td align="left">solve</td><td align="left">解线性方程组Ax=b，其中A为一个方阵</td></tr><tr><td align="left">lstsq</td><td align="left">计算Ax=b的最小二乘解</td></tr></tbody></table><h5 id="2-4-1-numpy-dot-——两个数组的点积，即元素对应相乘"><a href="#2-4-1-numpy-dot-——两个数组的点积，即元素对应相乘" class="headerlink" title="2.4.1 numpy.dot()——两个数组的点积，即元素对应相乘"></a>2.4.1 numpy.dot()——两个数组的点积，即元素对应相乘</h5><ol><li><p>对于两个一维的数组，计算的是这两个数组对应下标元素的乘积和(数学上称之为内积)。</p></li><li><p>对于二维数组，计算的是两个数组的矩阵乘积。</p></li><li><p>对于多维数组，它的通用计算公式如下，即结果数组中的每个元素都是：数组 a 的最后一维上的所有元素与数组 b 的倒数第二位上的所有元素的乘积和：</p></li></ol><p><strong>dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])</strong></p><p><strong>numpy.dot(a, b, out=None)</strong><br>➢    <strong>a</strong> : ndarray 数组<br>➢    <strong>b</strong> : ndarray 数组<br>➢    <strong>out</strong> : ndarray, 可选，用来保存 dot() 的计算结果</p><h5 id="2-4-2-numpy-dot-示例"><a href="#2-4-2-numpy-dot-示例" class="headerlink" title="2.4.2 numpy.dot()示例"></a>2.4.2 numpy.dot()示例</h5><p><strong>计算式为：[[1<em>11+2</em>13, 1<em>12+2</em>14],[3<em>11+4</em>13, 3<em>12+4</em>14]]</strong></p><pre><code>import numpy.matlib import numpy as npa = np.array([[1,2],[3,4]])b = np.array([[11,12],[13,14]]) print(np.dot(a,b))</code></pre><p><strong>运行结果</strong><br>[[37 40]<br> [85 92]]</p><h5 id="2-4-3-numpy-vdot-——返回两个向量的点积"><a href="#2-4-3-numpy-vdot-——返回两个向量的点积" class="headerlink" title="2.4.3 numpy.vdot()——返回两个向量的点积"></a>2.4.3 numpy.vdot()——返回两个向量的点积</h5><p>如果第一个参数是复数，那么它的共轭复数会用于计算。如果参数是多维数组，它会被展开。</p><p><strong>计算式为：1<em>11 + 2</em>12 + 3<em>13 + 4</em>14 = 130</strong> </p><pre><code>import numpy as np a = np.array([[1,2],[3,4]]) b = np.array([[11,12],[13,14]]) # vdot 将数组展开计算内积print (np.vdot(a,b))</code></pre><p><strong>运行结果</strong><br>130</p><h5 id="2-4-4-numpy-linalg-inv-——计算逆矩阵"><a href="#2-4-4-numpy-linalg-inv-——计算逆矩阵" class="headerlink" title="2.4.4 numpy.linalg.inv()——计算逆矩阵"></a>2.4.4 numpy.linalg.inv()——计算逆矩阵</h5><p>逆矩阵（inverse matrix）：设 A 是数域上的一个 n 阶矩阵，若在相同数域上存在另一个 n 阶矩阵 B，使得： AB=BA=E ，则我们称 B 是 A 的逆矩阵，而 A 则被称为可逆矩阵。注：E 为单位矩阵。</p><pre><code>import numpy as np x = np.array([[1,2],[3,4]]) y = np.linalg.inv(x) print (x)print (y)print (np.dot(x,y))</code></pre><p><strong>运行结果</strong><br>[[1 2]<br> [3 4]]<br>[[-2.   1. ]<br> [ 1.5 -0.5]]<br>[[1.00000000e+00 1.11022302e-16]<br> [0.00000000e+00 1.00000000e+00]]</p><h5 id="2-4-5-numpy-linalg-solve-——求矩阵形式的线性方程的解"><a href="#2-4-5-numpy-linalg-solve-——求矩阵形式的线性方程的解" class="headerlink" title="2.4.5 numpy.linalg.solve()——求矩阵形式的线性方程的解"></a>2.4.5 numpy.linalg.solve()——求矩阵形式的线性方程的解</h5><p>线性方程组<br>$$\begin{cases}<br>        a_{11}x_1&amp;+&amp;a_{12}x_2&amp;+&amp;\cdots&amp;+a_{1n}x_n&amp;=&amp;b_1\<br>        a_{21}x_1&amp;+&amp;a_{22}x_2&amp;+&amp;\cdots&amp;+a_{2n}x_n&amp;=&amp;b_2&amp;\<br>        &amp;&amp;&amp;&amp;\vdots\<br>        a_{n1}x_1&amp;+&amp;a_{n2}x_2&amp;+&amp;\cdots&amp;+a_{nn}x_n&amp;=&amp;b_n&amp;<br>    \end{cases}$$</p><p>可以写作矩阵相乘的形式 ax=b。<br>其中，a 为 n×n 的矩阵，x 和 b 为 m×1 的矩阵。</p><h5 id="2-4-6-numpy-linalg-solve-示例"><a href="#2-4-6-numpy-linalg-solve-示例" class="headerlink" title="2.4.6 numpy.linalg.solve()示例"></a>2.4.6 numpy.linalg.solve()示例</h5><p>$$\begin{cases}<br>        x+y+z=6\<br>        2y+5z=-4\<br>        2x+5y-z=27<br>    \end{cases} $$ </p><p>转换成 $\begin{bmatrix}1 &amp; 1 &amp; 1\0 &amp; 2 &amp; 5\2 &amp; 5 &amp; -1\end{bmatrix}\begin{bmatrix}x\y\z \end{bmatrix}=\begin{bmatrix}6\-4\27 \end{bmatrix}$</p><pre><code>import numpy as npa = np.array([[1,1,1], [0,2,5],[2,5,-1]])   # 系数矩阵b = np.array([6,-4,27])  # 系数矩阵x = np.linalg.solve(a, b)  # 求解print(x)print(np.dot(a, x))    # 验证</code></pre><p><strong>运行结果</strong><br>[ 5.  3. -2.]<br>[ 6. -4. 27.]</p><h3 id="3、pandas-数据结构"><a href="#3、pandas-数据结构" class="headerlink" title="3、pandas 数据结构"></a>3、pandas 数据结构</h3><h4 id="3-1-引言"><a href="#3-1-引言" class="headerlink" title="3.1 引言"></a>3.1 引言</h4><ol><li><p>Pandas 是<strong>基于 NumPy</strong> 的一种工具 ， 该工具是为了<strong>解决数据分析任务</strong>而创建的。</p></li><li><p>Pandas 纳入了大量库和一些<strong>标准的数据模型</strong> ，提供了高效地操作大型数据集所需的工具。</p></li><li><p>Pandas 提供了大量能使我们<strong>快速便捷地处理数据的函数和方法</strong>。</p></li><li><p>Pandas 是 Python 的一个数据分析包 ，最初于 2008 年 4 月开发 ，2009 年底开源 ，目前由 PyData 开发团队继续开发和维护。</p></li></ol><p>5.Pandas 最初被作为金融数据分析工具而开发出来，也为时间序列分析提供了很好的支持。</p><h4 id="3-2-pandas库介绍"><a href="#3-2-pandas库介绍" class="headerlink" title="3.2 pandas库介绍"></a>3.2 pandas库介绍</h4><ol><li><p><strong>pandas</strong> 是 python 第三方库，提供高性能易用数据类型和分析工具。</p></li><li><p><strong>pandas</strong> 基于 numpy 实现，常与 numpy 和 matplotlib 一同使用。</p></li><li><p><strong>pandas</strong> 中有两大核心数据结构：<strong>Series</strong> （一维数据） 和 <strong>DataFrame</strong>（多特征数据，既有行索引，又有列索引）。</p></li></ol><p><img src="/medias/1591420106630.png" alt="Series"></p><p><img src="/medias/1591420115214.png" alt="DataFrame"></p><ol start="4"><li><p><strong>Series</strong><br>1）一维数组，与 Numpy 中的一维 array 类似 。<br>2）Series、numpy 中的一维 array 与 Python 基本的数据结构 List 也很相近， 其区别是： List 中的元素可以是不同的数据类型，而 array 和 Series 中则只允许存储相同的数据类型。<br>3）Series可以更有效的使用内存 ， 提高运算效率。</p></li><li><p><strong>Time-Series</strong>：以时间为索引的Series。</p></li><li><p><strong>DataFrame</strong>：带标签且大小可变的二维表格型数据结构，可以将 DataFrame 理解为 Series 的容器。</p></li><li><p><strong>Panel</strong>：三维的数组，可以理解为DataFrame的容器。</p></li></ol><h4 id="3-3-Series"><a href="#3-3-Series" class="headerlink" title="3.3 Series"></a>3.3 Series</h4><ol><li><p>Series 是一种类似于一维数组的对象，它由一维数组（各种 numpy 数据类型）以及一组与之相关的数据标签（即索引）组成。</p></li><li><p>Series 创建函数：</p></li></ol><p><strong>pandas.Series( data, index, dtype, copy)</strong></p><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>data</code></td><td align="left">数据采取各种形式，如：<code>adarray</code>，<code>list</code>，<code>constants</code></td></tr><tr><td align="left"><code>index</code></td><td align="left">索引值必须是唯一的和散列的，与数据的长度相同。默认 <code>np.arange(n)</code> 如果没有索引被传递。</td></tr><tr><td align="left"><code>dtype</code></td><td align="left"><code>dtype</code> 用于数据类型，如果没有，将推断数据类型</td></tr><tr><td align="left"><code>copy</code></td><td align="left">复制数据，默认为 <code>false</code></td></tr></tbody></table><ol start="3"><li>Series 的创建：<br>1）使用 Python 数组创建<br>2）使用 numpy 数组创建<br>3）使用 python 字典创建</li></ol><p><strong>注意</strong><br>与字典不同的是：<strong>Series允许索引重复</strong></p><pre><code>import pandas as pdimport numpy as nppd.Series([11, 12], index=[&quot;北京&quot;, &quot;上海&quot;])</code></pre><p><strong>运行结果</strong><br>北京    11<br>上海    12<br>dtype: int64</p><pre><code>pd.Series(np.arange(3,6))</code></pre><p><strong>运行结果</strong><br>0    3<br>1    4<br>2    5<br>dtype: int32</p><pre><code>pd.Series({&quot;北京&quot;: 11, &quot;上海&quot;: 12, &quot;深圳&quot;: 14})</code></pre><p><strong>运行结果</strong><br>北京    11<br>上海    12<br>深圳    14<br>dtype: int64</p><ol start="4"><li><p>Series 的字符串表现形式为：<strong>索引在左边，值在右边</strong>。</p></li><li><p>如果没有为数据指定索引，则自动创建一个 0 到 N-1（N 为数据的长度）的整数型索引。</p></li><li><p>可以通过 Series 的 values 和 index 属性获取其数组表示形式和索引对象。</p></li></ol><pre><code>&gt;&gt;&gt; obj = pd.Series([4, 7, -5, 3]) &gt;&gt;&gt; obj.valuesarray([ 4,  7, -5,  3], dtype=int64) &gt;&gt;&gt; obj.indexRangeIndex(start=0, stop=4, step=1)&gt;&gt;&gt; obj[2]-5&gt;&gt;&gt; obj[1] = 8 &gt;&gt;&gt; obj[[0, 1, 3]]0    41    83    3dtype: int64</code></pre><ol start="7"><li><p>通常希望所创建的 Series 带有一个可以对各个数据点进行标记的索引。</p></li><li><p>与普通 NumPy 数组相比，可以通过索引的方式选取 Series 中的单个或一组值。</p></li></ol><pre><code>&gt;&gt;&gt; obj2=pd.Series([4,2,-5,3],index=[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;])&gt;&gt;&gt; obj2a    4b    2c   -5d    3dtype: int64&gt;&gt;&gt; obj2[&#39;a&#39;]4&gt;&gt;&gt; obj2[&#39;d&#39;] = 6 &gt;&gt;&gt; obj2a    4b    2c   -5d    6dtype: int64&gt;&gt;&gt; obj2[[&#39;a&#39;,&#39;b&#39;,&#39;d&#39;]]a    4b    2d    6dtype: int64</code></pre><ol start="9"><li>Series 中很重要的一个功能是：它会在算术运算中自动对齐不同索引的数据。</li></ol><pre><code>&gt;&gt;&gt; obj2 = pd.Series({&quot;Ohio&quot;: 35000, &quot;Oregon&quot;: 16000, &quot;Texas&quot;: 71000, &quot;Utah&quot;: 5000})&gt;&gt;&gt; obj3 = pd.Series({&quot;California&quot;: np.nan, &quot;Ohio&quot;: 35000, &quot;Oregon&quot;: 16000, &quot;Texas&quot;: 71000})&gt;&gt;&gt; obj2 + obj3California         NaNOhio           70000.0Oregon         32000.0Texas         142000.0Utah               NaNdtype: float64</code></pre><ol start="10"><li><p>Series 对象本身及其索引都有一个 name 属性</p></li><li><p>Series 的索引可以通过赋值的方式就地修改</p></li></ol><pre><code>&gt;&gt;&gt; obj3.name= &#39;population&#39; &gt;&gt;&gt; obj3.index.name = &#39;state&#39; &gt;&gt;&gt; obj3stateCalifornia        NaNOhio          35000.0Oregon        16000.0Texas         71000.0Name: population, dtype: float64&gt;&gt;&gt; obj = pd.Series([4, 7, -5, 3])&gt;&gt;&gt; obj.index = [&#39;Bob&#39;, &#39;Steve&#39;, &#39;Jeff&#39;, &#39;Ryan&#39;] &gt;&gt;&gt; objBob      4Steve    7Jeff    -5Ryan     3dtype: int64</code></pre><h4 id="3-4-DataFrame"><a href="#3-4-DataFrame" class="headerlink" title="3.4 DataFrame"></a>3.4 DataFrame</h4><h5 id="3-4-1-DataFrame介绍"><a href="#3-4-1-DataFrame介绍" class="headerlink" title="3.4.1 DataFrame介绍"></a>3.4.1 DataFrame介绍</h5><ol><li>DataFrame 是一个表格型的数据结构，它含有一组有序的列，每列可以是不同的值类型（数值、字符串、布尔值等）。</li><li>DataFrame 既有行索引也有列索引，它可以被看做由 Series 组成的字典（共用同一个索引）。</li><li>跟其他类似的数据结构相比（如 R 语言的 data.frame），DataFrame 中面向行和面向列的操作基本上是平衡的。</li><li>DataFrame 中的数据是以一个或多个二维块存放的（而不是列表、字典或别的一维数据结构）。</li></ol><h5 id="3-4-2-DataFrame特点"><a href="#3-4-2-DataFrame特点" class="headerlink" title="3.4.2 DataFrame特点"></a>3.4.2 DataFrame特点</h5><ol><li>潜在的列是不同的类型</li><li>大小可变</li><li>标记轴( ( 行和列) )</li><li>可以对行和列执行算术运算</li></ol><h5 id="3-4-3-DataFrame示例"><a href="#3-4-3-DataFrame示例" class="headerlink" title="3.4.3 DataFrame示例"></a>3.4.3 DataFrame示例</h5><p>学生数据<br><img src="/medias/1591421808705.png" alt="学生数据"></p><h5 id="3-4-4-DataFrame构造函数"><a href="#3-4-4-DataFrame构造函数" class="headerlink" title="3.4.4 DataFrame构造函数"></a>3.4.4 DataFrame构造函数</h5><p><strong>pandas.DataFrame( data, index, columns, dtype, copy)</strong></p><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>data</code></td><td align="left">数据采取各种形式，如：<code>ndarray</code>, <code>series</code>, <code>map</code>, <code>lists</code>, <code>dict</code>, <code>constant</code> 和另一个 <code>DataFrame</code></td></tr><tr><td align="left"><code>index</code></td><td align="left">对于行标签，要用于结果帧的索引是可选缺省值 <code>np.arrange(n)</code>，如果没有传递索引值</td></tr><tr><td align="left"><code>columns</code></td><td align="left">对于列标签，可选的默认语法是—— <code>np.arange(n)</code>。这只有在没有索引传递的情况下才是这样</td></tr><tr><td align="left"><code>dtype</code></td><td align="left">每列的数据类型</td></tr><tr><td align="left"><code>copy</code></td><td align="left">如果默认值为 <code>False</code>，则此命令（或任何它）用于复制数据</td></tr></tbody></table><h5 id="3-4-5-DataFrame"><a href="#3-4-5-DataFrame" class="headerlink" title="3.4.5 DataFrame"></a>3.4.5 DataFrame</h5><ol><li>创建一个空的 DataFrame：函数不指定参数返回空 DataFrame</li></ol><pre><code># 创建一个空的DataFrame import pandas as pddf = pd.DataFrame() print(df)</code></pre><p><strong>运行结果</strong><br>Empty DataFrame<br>Columns: []<br>Index: []</p><ol start="2"><li>从列表创建 DataFrame</li></ol><pre><code># 从单个列表创建DataFramedata = [1,2,3,4]df = pd.DataFrame(data) print(df)</code></pre><p><strong>运行结果</strong></p><pre><code>   00  11  22  33  4</code></pre><pre><code># 从嵌套列表创建DataFrame、并指定数据类型data = [[&#39;Alex&#39;,10],[&#39;Bob&#39;,12],[&#39;Clarke&#39;,13]]df = pd.DataFrame(data,columns=[&#39;Name&#39;,&#39;Age&#39;],dtype=float) print(df)</code></pre><p><strong>运行结果</strong></p><pre><code>     Name   Age0    Alex  10.01     Bob  12.02  Clarke  13.0</code></pre><ol start="3"><li><p>由等长列表或 numpy 数组组成的字典创建 DataFrame。</p></li><li><p>DataFrame 结果会自动加上索引（跟 Series 一样），且全部会被有序排列。</p></li></ol><pre><code>&gt;&gt;&gt; data = {&#39;state&#39;: [&#39;Ohio&#39;, &#39;Ohio&#39;, &#39;Ohio&#39;, &#39;Nevada&#39;, &#39;Nevada&#39;], &#39;year&#39;: [2000 , 2001, 2002, 2001, 2002], &#39;pop&#39;: [1.5, 1.7, 3.6, 2.4, 2.9]}&gt;&gt;&gt; frame = pd.DataFrame(data)&gt;&gt;&gt; frame</code></pre><p><strong>运行结果</strong></p><pre><code>state    year    pop0    Ohio    2000    1.51    Ohio    2001    1.72    Ohio    2002    3.63    Nevada    2001    2.44    Nevada    2002    2.9</code></pre><ol start="5"><li><p>如果指定了列顺序，则 DataFrame 的列就会按照指定顺序进行排列。</p></li><li><p>跟原 Series 一样，如果传入的列在数据中找不到，就会产生 NAN 值。</p></li></ol><pre><code>&gt;&gt;&gt; pd.DataFrame(data, columns=[&#39;year&#39;, &#39;state&#39;, &#39;pop&#39;])</code></pre><p><strong>运行结果</strong></p><pre><code>    year    state    pop0    2000    Ohio    1.51    2001    Ohio    1.72    2002    Ohio    3.63    2001    Nevada    2.44    2002    Nevada    2.9</code></pre><pre><code>&gt;&gt;&gt; frame2 = pd.DataFrame(data, columns=[&#39;year&#39;, &#39;state&#39;, &#39;pop&#39;, &#39;debt&#39;], index=[&#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;four&#39;, &#39;five&#39;])&gt;&gt;&gt; frame2</code></pre><p><strong>运行结果</strong></p><pre><code>        year    state    pop      debtone        2000    Ohio    1.5      NaNtwo        2001    Ohio    1.7      NaNthree    2002    Ohio    3.6      NaNfour    2001    Nevada    2.4      NaNfive    2002    Nevada    2.9      NaN</code></pre><pre><code>&gt;&gt;&gt; frame2.columns</code></pre><p><strong>运行结果</strong><br>Index([‘year’, ‘state’, ‘pop’, ‘debt’], dtype=’object’)</p><ol start="7"><li><p>通过类似字典标记的方式或属性的方式，可以将 DataFrame 的列获取为一个 Series。</p></li><li><p>返回的 Series 拥有原 DataFrame 相同的索引，且其 name 属性也已经被相应地设置好了。</p></li></ol><pre><code>&gt;&gt;&gt; frame2[&#39;state&#39;]</code></pre><p><strong>运行结果</strong></p><pre><code>one        Ohiotwo        Ohiothree      Ohiofour     Nevadafive     NevadaName: state, dtype: object</code></pre><pre><code>&gt;&gt;&gt; frame2[&#39;year&#39;]</code></pre><p><strong>运行结果</strong></p><pre><code>one      2000two      2001three    2002four     2001five     2002Name: year, dtype: int64</code></pre><ol start="9"><li><p>列可以通过赋值的方式进行修改。</p></li><li><p>例如，给那个空的“delt”列赋上一个标量值或一组值。</p></li></ol><pre><code>&gt;&gt;&gt; frame2[&#39;debt&#39;] = 16.5 &gt;&gt;&gt; frame2</code></pre><p><strong>运行结果</strong></p><pre><code>        year    state    pop     debtone        2000    Ohio    1.5     16.5two        2001    Ohio    1.7     16.5three    2002    Ohio    3.6  16.5four    2001    Nevada    2.4  16.5five    2002    Nevada    2.9  16.5</code></pre><pre><code>&gt;&gt;&gt; frame2[&#39;debt&#39;] = np.arange(5.) &gt;&gt;&gt; frame2</code></pre><p><strong>运行结果</strong></p><pre><code>        year    state    pop  debtone        2000    Ohio    1.5  0.0two        2001    Ohio    1.7  1.0three    2002    Ohio    3.6  2.0four    2001    Nevada    2.4  3.0five    2002    Nevada    2.9     4.0</code></pre><ol start="11"><li><p>将列表或数组赋值给某个列时，其长度必须跟DataFrame的长度相匹配。</p></li><li><p>如果赋值的是一个 Series，就会精确匹配 DataFrame 的索引，所有空位都将被填上缺失值。</p></li></ol><pre><code>&gt;&gt;&gt; val = pd.Series([-1.2, -1.5, -1.7], index=[&#39;two&#39;, &#39;four&#39;, &#39;five&#39;]) &gt;&gt;&gt; frame2[&#39;debt&#39;] = val&gt;&gt;&gt; frame2</code></pre><p><strong>运行结果</strong></p><pre><code>        year    state    pop     debtone        2000    Ohio    1.5     NaNtwo        2001    Ohio    1.7     -1.2three    2002    Ohio    3.6     NaNfour    2001    Nevada    2.4     -1.5five    2002    Nevada    2.9     -1.7</code></pre><ol start="13"><li><p>为不存在的列赋值会创建出一个新列</p></li><li><p>关键字 del 用于删除列</p></li></ol><pre><code>&gt;&gt;&gt; frame2[&#39;eastern&#39;] = frame2.state == &#39;Ohio&#39; &gt;&gt;&gt; frame2</code></pre><p><strong>运行结果</strong></p><pre><code>        year    state    pop      debt    easternone     2000    Ohio    1.5      NaN    Truetwo        2001    Ohio    1.7     -1.2     Truethree    2002    Ohio    3.6      NaN    Truefour    2001    Nevada    2.4     -1.5     Falsefive    2002    Nevada    2.9     -1.7     False</code></pre><pre><code>&gt;&gt;&gt; del frame2[&#39;eastern&#39;]&gt;&gt;&gt; frame2.columns</code></pre><p><strong>运行结果</strong><br>Index([‘year’, ‘state’, ‘pop’, ‘debt’], dtype=’object’)</p><ol start="15"><li><p>将嵌套字典（也就是字典的字典）传给 DataFrame，它就会被解释为：外层字典的键作为列，内层键则作为行索引。</p></li><li><p>也可以对上述结果进行转置。</p></li></ol><pre><code>&gt;&gt;&gt; pop = {&#39;Nevada&#39;: {2001: 2.4, 2002: 2.9}, &#39;Ohio&#39;: {2000: 1.5, 2001: 1.7, 2002: 3.6}}&gt;&gt;&gt; frame3 = pd.DataFrame(pop)&gt;&gt;&gt; frame3</code></pre><p><strong>运行结果</strong></p><pre><code>        Nevada    Ohio2001     2.4    1.72002     2.9    3.62000     NaN    1.5</code></pre><pre><code>&gt;&gt;&gt; frame3.T</code></pre><p><strong>运行结果</strong></p><pre><code>        2001    2002    2000Nevada    2.4        2.9        NaNOhio    1.7        3.6        1.5</code></pre><ol start="17"><li>如果设置了 DataFrame 的 index 和 columns 的 name 属性，则这些信息也会被显示出来。</li></ol><pre><code>&gt;&gt;&gt; frame3.index.name = &#39;year&#39; &gt;&gt;&gt; frame3.columns.name = &#39;state&#39; &gt;&gt;&gt; frame3</code></pre><p><strong>运行结果</strong></p><pre><code>state    Nevada    Ohioyear        2001    2.4        1.72002    2.9        3.62000    NaN        1.5</code></pre><ol start="18"><li><p>跟 Series 一样，values 属性也会以二维 ndarray 的形式返回 DataFrame 中的数据。</p></li><li><p>如果 DataFrame 各列的数据类型不同，则数组的数据类型就会选用能兼容所有列的数据类型。</p></li></ol><pre><code>&gt;&gt;&gt; frame3.values</code></pre><p><strong>运行结果</strong></p><pre><code>array([[2.4, 1.7],       [2.9, 3.6],       [nan, 1.5]])</code></pre><pre><code>&gt;&gt;&gt; frame2.values</code></pre><p><strong>运行结果</strong></p><pre><code>array([[2000, &#39;Ohio&#39;, 1.5, nan],       [2001, &#39;Ohio&#39;, 1.7, -1.2],       [2002, &#39;Ohio&#39;, 3.6, nan],       [2001, &#39;Nevada&#39;, 2.4, -1.5],       [2002, &#39;Nevada&#39;, 2.9, -1.7]], dtype=object)</code></pre><h3 id="4、pandas-常用方法"><a href="#4、pandas-常用方法" class="headerlink" title="4、pandas 常用方法"></a>4、pandas 常用方法</h3><h4 id="4-1-数据读取与写入"><a href="#4-1-数据读取与写入" class="headerlink" title="4.1 数据读取与写入"></a>4.1 数据读取与写入</h4><ol><li><p>Pandas 支持常用的文本格式数据(csv、json、html、剪贴板)、二进制数据(excel、hdf5 格式、Feather 格式、Parquet 格式、Msgpack、 Stata、SAS、pkl)、SQL 数据(SQL、谷歌 BigQuery 云数据)等。</p></li><li><p>一般情况下 ， 读取文件的方法以 pd.read_ 开头， 而写入文件的方法以 pd.to_ 开头。</p></li><li><p>示例1</p></li></ol><pre><code># 示例1，读入剪贴板数据，写入csv，转换为json、html、excel等格式import pandas as pdimport numpy as npfrom pandas import Series, DataFrame# 粘贴板内容#    A    B    C# x    1    4    p# y    2    5    q# z    3    6    r# 从粘贴板读取数据df1 = pd.read_clipboard()# 把数据放入到粘贴板中，数据可以直接粘贴到 excel 文件中df1.to_clipboard()# 读写 csv 文件，可以取消 indexdf1.to_csv(&#39;df1.csv&#39;)df1.to_csv(&#39;df1_noIndex.csv&#39;, index = False)# 转化为 json 格式df1.to_json(&#39;df1.json&#39;)# 转化为 html 格式df1.to_html(&#39;df1.html&#39;)# 转换为 excel 格式df1.to_excel(&#39;df1.xlsx&#39;)</code></pre><table><thead><tr><th align="left">数据类型</th><th align="left">描述符</th><th align="left">读方法</th><th align="left">写方法</th></tr></thead><tbody><tr><td align="left">text</td><td align="left">CSV</td><td align="left"><code>read_csv</code></td><td align="left"><code>to_csv</code></td></tr><tr><td align="left">text</td><td align="left">JSON</td><td align="left"><code>read_json</code></td><td align="left"><code>to_json</code></td></tr><tr><td align="left">text</td><td align="left">HTML</td><td align="left"><code>read_csv</code></td><td align="left"><code>to_csv</code></td></tr><tr><td align="left">text</td><td align="left">剪切板</td><td align="left"><code>read_clipboard</code></td><td align="left"><code>to_clipboard</code></td></tr><tr><td align="left">二进制</td><td align="left">Excel</td><td align="left"><code>read_excel</code></td><td align="left"><code>to_excel</code></td></tr><tr><td align="left">二进制</td><td align="left">HDF5</td><td align="left"><code>read_hdf</code></td><td align="left"><code>to_hdf</code></td></tr><tr><td align="left">二进制</td><td align="left">PKL</td><td align="left"><code>read_pickle</code></td><td align="left"><code>to_pickle</code></td></tr><tr><td align="left">SQL</td><td align="left">SQL</td><td align="left"><code>read_sql</code></td><td align="left"><code>to_sql</code></td></tr></tbody></table><p><strong>数据读取函数read_csv示例</strong></p><ol><li><p>read_csv（）</p></li><li><p>自定义索引：可以指定 csv 文件中的一列来使用 index_col 定制索引</p></li><li><p>dtype：数据类型转换</p></li><li><p>skiprows：跳过指定的行数</p></li><li><p>示例2</p></li></ol><pre><code># 示例 2-1 read_csvdf=pd.read_csv(&#39;temp.csv&#39;) print(df)</code></pre><p><strong>运行结果</strong></p><pre><code>   S.No    Name  Agw       City  Salary0     1     Tom   28    Toronto   200001     2     Lee   32   HongKong    30002     3  Steven   43   Bay Area    83003     4     Ram   38  Hyderabad    3900</code></pre><pre><code># 示例 2-2  自定义索引：可以指定 csv 文件中的一列来使用 index_col 指定索引df = pd.read_csv(&quot;temp.csv&quot;, index_col=[&#39;S.No&#39;]) print(df)</code></pre><p><strong>运行结果</strong></p><pre><code>        Name  Agw       City  SalaryS.No                                1        Tom   28    Toronto   200002        Lee   32   HongKong    30003     Steven   43   Bay Area    83004        Ram   38  Hyderabad    3900</code></pre><pre><code># 示例 2-3 转换器 dtypedf = pd.read_csv(&quot;temp.csv&quot;, dtype={&#39;Salary&#39;: np.float64}) print(df) print(df.dtypes)</code></pre><p><strong>运行结果</strong></p><pre><code>   S.No    Name  Agw       City   Salary0     1     Tom   28    Toronto  20000.01     2     Lee   32   HongKong   3000.02     3  Steven   43   Bay Area   8300.03     4     Ram   38  Hyderabad   3900.0S.No        int64Name       objectAgw         int64City       objectSalary    float64dtype: object</code></pre><pre><code># 示例 2-4  使用 names参数指定标题名称df=pd.read_csv(&quot;temp.csv&quot;, names=[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;,&#39;d&#39;,&#39;e&#39;]) print(df)</code></pre><p><strong>运行结果</strong></p><pre><code>      a       b    c          d       e0  S.No    Name  Agw       City  Salary1     1     Tom   28    Toronto   200002     2     Lee   32   HongKong    30003     3  Steven   43   Bay Area    83004     4     Ram   38  Hyderabad    3900</code></pre><p>观察可以看到，标题名称附加了自定义名称，但文件中的标题还没有被消除。现在使用header参数来删除它。如果标题不是第一行，则将行号传递给标题，这将跳过前面的行。</p><pre><code># 示例 2-5消除文件中的标题df=pd.read_csv(&quot;temp.csv&quot;, names=[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;,&#39;d&#39;,&#39;e&#39;], header=0) print(df)</code></pre><p><strong>运行结果</strong></p><pre><code>   a       b   c          d      e0  1     Tom  28    Toronto  200001  2     Lee  32   HongKong   30002  3  Steven  43   Bay Area   83003  4     Ram  38  Hyderabad   3900</code></pre><pre><code># 示例 2-6 skiprows跳过指定的行数df=pd.read_csv(&quot;temp.csv&quot;, skiprows=2) print (df)</code></pre><p><strong>运行结果</strong></p><pre><code>   2     Lee  32   HongKong  30000  3  Steven  43   Bay Area  83001  4     Ram  38  Hyderabad  3900</code></pre><h4 id="4-2-描述性统计方法"><a href="#4-2-描述性统计方法" class="headerlink" title="4.2 描述性统计方法"></a>4.2 描述性统计方法</h4><ol><li><p>Pandas 提供了几个统计和描述性方法，方便你从宏观的角度去了解数据集，例如count() 用于统计非空数据的数量。</p></li><li><p>除了统计类的方法，Pandas 还 提供了很多计算类的方法，比如 sum() 用于计算数值数据的总和；mean() 用 于 计 算 数 值 数 据 的 平 均 值 ；median() 用于计算数值数据的算术中值。</p></li></ol><table><thead><tr><th align="left">函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">count()</td><td align="left">非空观测数量</td></tr><tr><td align="left">sum()</td><td align="left">所有值之和</td></tr><tr><td align="left">mean()</td><td align="left">所有值的平均值</td></tr><tr><td align="left">median()</td><td align="left">所有值的中位数</td></tr><tr><td align="left">mode()</td><td align="left">值的模值</td></tr><tr><td align="left">std()</td><td align="left">值的标准偏差</td></tr><tr><td align="left">min()</td><td align="left">所有值中的最小值</td></tr><tr><td align="left">max()</td><td align="left">所有值中的最大值</td></tr><tr><td align="left">abs()</td><td align="left">绝对值</td></tr><tr><td align="left">prod()</td><td align="left">数组元素的乘积</td></tr><tr><td align="left">cumsum()</td><td align="left">累计总和</td></tr><tr><td align="left">cumprod()</td><td align="left">累计乘积</td></tr></tbody></table><p><strong>描述性统计方法示例</strong><br><strong>sum()</strong>：求和；<br><strong>mean()</strong> ：求中值；<br><strong>std()</strong>：求标准差；<br><strong>describe()</strong>：描述性统计信息摘要。 </p><pre><code># 示例3 描述统计方法import pandas as pdimport numpy as np# Create a Dictionary of seriesd = {&#39;Name&#39;:pd.Series([&#39;Tom&#39;,&#39;James&#39;,&#39;Ricky&#39;,&#39;Vin&#39;,&#39;Steve&#39;,&#39;Minsu&#39;,&#39;Jack&#39;,&#39;Lee&#39;,&#39;David&#39;,&#39;Gasper&#39;,&#39;Betina&#39;,&#39;Andres&#39;]),&#39;Age&#39;:pd.Series([25,26,25,23,30,29,23,34,40,30,51,46]),&#39;Rating&#39;:pd.Series([4.23,3.24,3.98,2.56,3.20,4.6,3.8,3.78,2.98,4.80,4.10,3.65])}#Create a DataFramedf = pd.DataFrame(d)print(df, &#39;\n&#39;)print(&quot;\n列求和&quot;)print(df.sum(), &#39;\n&#39;)  # 列求和， 默认axis=0.print(&quot;\n行求和&quot;)print(df.sum(1), &#39;\n&#39;)   # 行求和，axis=1print(&quot;\n求均值&quot;)print(df.mean(), &#39;\n&#39;)   # 求均值print(&quot;\n标准差&quot;)print(df.std(), &#39;\n&#39;)    # 标准差# include参数是用于传递关于什么列需要考虑用于总结的必要信息的参数，获取值列表，默认情况下是“数字值”。# object - 汇总字符串列；# number - 汇总数字列；# all - 将所有列汇总在一起（不应将其作为列表值传递）。print(&quot;\n统计信息摘要&quot;)print(df.describe(include=[&#39;number&#39;]))    # 统计信息摘要</code></pre><p><strong>运行结果</strong></p><pre><code>      Name   Age  Rating0      Tom   25    4.231    James   26    3.242    Ricky   25    3.983      Vin   23    2.564    Steve   30    3.205    Minsu   29    4.606     Jack   23    3.807      Lee   34    3.788    David   40    2.989   Gasper   30    4.8010  Betina   51    4.1011  Andres   46    3.65 列求和Name      TomJamesRickyVinSteveMinsuJackLeeDavidGasperBe...Age                                                     382Rating                                                44.92dtype: object 行求和0     29.231     29.242     28.983     25.564     33.205     33.606     26.807     37.788     42.989     34.8010    55.1011    49.65dtype: float64 求均值Age       31.833333Rating     3.743333dtype: float64 标准差Age       9.232682Rating    0.661628dtype: float64 统计信息摘要             Age     Ratingcount  12.000000  12.000000mean   31.833333   3.743333std     9.232682   0.661628min    23.000000   2.56000025%    25.000000   3.23000050%    29.500000   3.79000075%    35.500000   4.132500max    51.000000   4.800000</code></pre><h4 id="4-3-迭代与遍历"><a href="#4-3-迭代与遍历" class="headerlink" title="4.3 迭代与遍历"></a>4.3 迭代与遍历</h4><ol><li><p>pandas 对象之间的基本迭代的行为取决于类型。当迭代一个系列时，它被视为数组式，基本迭代产生这些值。其他数据结构，如：DataFrame，遵循类似惯例迭代对象的键。</p></li><li><p>简而言之，基本迭代(对于 i 在对象中)产生：<br>➢    Series - 值<br>➢    DataFrame - 列标签</p></li><li><p>要遍历数据帧(DataFrame)中的行，可以使用以下函数：<br>➢    iteritems() - 迭代(key，value)对<br>➢    iterrows() - 将行迭代为(索引，系列)对<br>➢    itertuples() - 以 namedtuples 的形式迭代行</p></li></ol><h5 id="4-3-1-迭代-DataFrame"><a href="#4-3-1-迭代-DataFrame" class="headerlink" title="4.3.1 迭代 DataFrame"></a>4.3.1 迭代 DataFrame</h5><pre><code># 迭代 DataFrameimport pandas as pdimport numpy as npN=5df = pd.DataFrame({    &#39;D&#39;: pd.date_range(start=&#39;2019-01-01&#39;,periods=N,freq=&#39;M&#39;),     &#39;x&#39;: np.linspace(0,stop=N-1,num=N),    &#39;y&#39;: np.random.rand(N),    &#39;z&#39;: np.random.choice([&#39;Low&#39;,&#39;Medium&#39;,&#39;High&#39;],N).tolist(),})print(df,&#39;\n&#39;)for col in df:    print(col)</code></pre><p><strong>运行结果</strong></p><pre><code>           D    x         y       z0 2019-01-31  0.0  0.088484    High1 2019-02-28  1.0  0.763299    High2 2019-03-31  2.0  0.918397    High3 2019-04-30  3.0  0.785687  Medium4 2019-05-31  4.0  0.821885     Low Dxyz</code></pre><h5 id="4-3-2-遍历Dataframe-示例"><a href="#4-3-2-遍历Dataframe-示例" class="headerlink" title="4.3.2 遍历Dataframe 示例"></a>4.3.2 遍历Dataframe 示例</h5><ol><li>iteritems()：将索引和值作为键和列值迭代为 Series 对象。</li><li>iterrows()：返回迭代器，产生每个索引值以及包含每行数据的序列。</li><li>itertuples()：方法将为 DataFrame 中的每一行返回一个产生一个命名元组的迭代器。元组的第一个元素将是行的相应索引值，而剩余的值是行值。</li></ol><pre><code># 遍历 iteritems()将每个列作为名称，将索引和值作为键和列值迭代为Series对象df = pd.DataFrame(np.random.randn(4,3), columns=[&#39;col1&#39;,&#39;col2&#39;,&#39;col3&#39;])print(df, &#39;\n&#39;)print(&quot;\niteritems---------&quot;)for key,value in df.iteritems():      print (key,value)# iterrows()返回迭代数器，产生每个索引值以及包含每行数据的序列print(&quot;\niterrows---------&quot;)for row_index,row in df.iterrows():        print (row_index,row)# itertuples()方法将为 DataFrame 中的每一行返回一个产生一个命名元组的迭代器。元组的第一个元素将是行的相应索引值，而剩余的值是行值print(&quot;\nitertuples---------&quot;)for row in df.itertuples():        print (row)</code></pre><p><strong>运行结果</strong></p><pre><code>       col1      col2      col30  0.769239 -0.321667  0.2299701 -0.414440 -0.686365 -1.8045742 -0.625321 -0.410545 -0.2668783  0.942061 -0.429502 -0.191403 iteritems---------col1 0    0.7692391   -0.4144402   -0.6253213    0.942061Name: col1, dtype: float64col2 0   -0.3216671   -0.6863652   -0.4105453   -0.429502Name: col2, dtype: float64col3 0    0.2299701   -1.8045742   -0.2668783   -0.191403Name: col3, dtype: float64iterrows---------0 col1    0.769239col2   -0.321667col3    0.229970Name: 0, dtype: float641 col1   -0.414440col2   -0.686365col3   -1.804574Name: 1, dtype: float642 col1   -0.625321col2   -0.410545col3   -0.266878Name: 2, dtype: float643 col1    0.942061col2   -0.429502col3   -0.191403Name: 3, dtype: float64itertuples---------Pandas(Index=0, col1=0.7692393725888382, col2=-0.3216667049027107, col3=0.22996991034224118)Pandas(Index=1, col1=-0.41444004324475786, col2=-0.6863646340873751, col3=-1.8045739966582113)Pandas(Index=2, col1=-0.625321480205094, col2=-0.4105445003436983, col3=-0.2668784127256191)Pandas(Index=3, col1=0.9420608482646933, col2=-0.4295015650195585, col3=-0.191402644416027)​</code></pre><h4 id="4-4-排序"><a href="#4-4-排序" class="headerlink" title="4.4 排序"></a>4.4 排序</h4><ol><li><p>按索引排序：使用 sort_index() 方法，通过传递 axis 参数和排序顺序，可以对 DataFrame 进行排序。 默认情况下，按照升序对行标签进行排序。</p></li><li><p>按数值排序：sort_values() 是按值排序的方法。它接受一个 by 参数，它将使用要与其排序值的 DataFrame 的列名称。</p></li><li><p>排序顺序：通过将布尔值传递给升序参数 ascending，可以控制排序顺序。</p></li><li><p>按行或列排序：通过设置 axis 参数为 0 或 1 ，为 0 时逐行排序，为 1 时逐列排序，默认为 0。</p></li></ol><p><strong>排序示例</strong></p><pre><code>print(&quot;\n待排序------&quot;)unsorted_df = pd.DataFrame(np.random.rand(10,2),index=[1,4,6,2,3,5,9,8,0,7],columns = [&#39;A&#39;,&#39;B&#39;])print(unsorted_df,&#39;\n&#39;)print(&quot;\n按索引排序------&quot;)sorted_df=unsorted_df.sort_index(ascending = True)  #按索引排序print (sorted_df,&#39;\n&#39;)print(&quot;\n按&#39;B&#39;列的值进行排序------&quot;)sorted_df = unsorted_df.sort_values(by=&#39;B&#39;) # 按&#39;B&#39;列的值进行排序print (sorted_df,&#39;\n&#39;)</code></pre><p><strong>运行结果</strong></p><pre><code>待排序------          A         B1  0.301330  0.3036824  0.955318  0.0358096  0.254245  0.7982252  0.408592  0.4705273  0.885707  0.1312585  0.135931  0.8597959  0.934681  0.2183918  0.992487  0.4160870  0.221823  0.4683667  0.414039  0.531079 按索引排序------          A         B0  0.221823  0.4683661  0.301330  0.3036822  0.408592  0.4705273  0.885707  0.1312584  0.955318  0.0358095  0.135931  0.8597956  0.254245  0.7982257  0.414039  0.5310798  0.992487  0.4160879  0.934681  0.218391 按&#39;B&#39;列的值进行排序------          A         B4  0.955318  0.0358093  0.885707  0.1312589  0.934681  0.2183911  0.301330  0.3036828  0.992487  0.4160870  0.221823  0.4683662  0.408592  0.4705277  0.414039  0.5310796  0.254245  0.7982255  0.135931  0.859795 </code></pre><h4 id="4-5-缺失值处理"><a href="#4-5-缺失值处理" class="headerlink" title="4.5 缺失值处理"></a>4.5 缺失值处理</h4><ol><li><p>缺失值主要是指数据丢失的现象，也就是数据集中的某一块数据不存在。</p></li><li><p>除了原始数据集就已经存在缺失值以外，当我们用到索引对齐（reindex()，选择等）方法时，也容易人为导致缺失值的产生。</p></li><li><p><strong>缺失值处理</strong>包括：<br>➢ 缺失值标记<br>➢ 缺失值填充<br>➢ 缺失值插值</p></li><li><p>Pandas 为了更方便地<strong>检测缺失值</strong>，将不同类型数据的缺失均采用 NaN 标记。这里的 NaN 代表 Not a Number ，它仅仅是作为一个标记。</p></li><li><p>Pandas 中用于<strong>标记缺失值主要用到两个方法</strong>，分别是：isnull () 和 notnull ()，顾名思义就是「是缺失值」和「不是缺失值」。默认会返回布尔值用于判断。</p></li></ol><pre><code># 使用reindex()人为生成缺失值df = pd.DataFrame(np.random.rand(4, 3), index=[&#39;a&#39;, &#39;c&#39;, &#39;e&#39;, &#39;f&#39;],columns=[&#39;one&#39;, &#39;two&#39;, &#39;three&#39;])print(&#39;原始：\n&#39;, df,&#39;\n&#39;)df = df.reindex([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;])print(&#39;reindex后：\n&#39;, df,&#39;\n&#39;)print(df[&#39;one&#39;].isnull(),&#39;\n&#39;)  # 缺失值标记print (df[&#39;one&#39;].notnull(),&#39;\n&#39;)</code></pre><p><strong>运行结果</strong></p><pre><code>原始：         one       two     threea  0.051237  0.164315  0.632927c  0.459356  0.435970  0.615046e  0.420776  0.668942  0.680803f  0.086104  0.409684  0.162472 reindex后：         one       two     threea  0.051237  0.164315  0.632927b       NaN       NaN       NaNc  0.459356  0.435970  0.615046d       NaN       NaN       NaNe  0.420776  0.668942  0.680803f  0.086104  0.409684  0.162472g       NaN       NaN       NaN a    Falseb     Truec    Falsed     Truee    Falsef    Falseg     TrueName: one, dtype: bool a     Trueb    Falsec     Trued    Falsee     Truef     Trueg    FalseName: one, dtype: bool </code></pre><ol start="6"><li>Pandas 提供了各种方法来<strong>清除缺失的值</strong>。fillna() 函数可以通过几种方法用非空数据“填充” NaN 值。<br>1）用标量值替换 NaN</li></ol><p><strong>print (df.fillna(0))</strong></p><pre><code># 用标量填充print(&quot;NaN replaced with &#39;0&#39;：&quot;)print(df.fillna(0))</code></pre><p><strong>运行结果</strong></p><pre><code>NaN replaced with &#39;0&#39;：        one       two     threea  0.051237  0.164315  0.632927b  0.000000  0.000000  0.000000c  0.459356  0.435970  0.615046d  0.000000  0.000000  0.000000e  0.420776  0.668942  0.680803f  0.086104  0.409684  0.162472g  0.000000  0.000000  0.000000</code></pre><p>2）向前填充：pad/fill<br><strong>print(df.fillna(method=’pad’))</strong></p><pre><code># 向前填充print(df.fillna(method=&#39;pad&#39;))</code></pre><p><strong>运行结果</strong></p><pre><code>        one       two     threea  0.051237  0.164315  0.632927b  0.051237  0.164315  0.632927c  0.459356  0.435970  0.615046d  0.459356  0.435970  0.615046e  0.420776  0.668942  0.680803f  0.086104  0.409684  0.162472g  0.086104  0.409684  0.162472</code></pre><p>3）向后填充：bfill/backfill<br><strong>print(df.fillna(method=’backfill’))</strong></p><pre><code># 向后填充print (df.fillna(method=&#39;backfill&#39;))</code></pre><p><strong>运行结果</strong></p><pre><code>        one       two     threea  0.051237  0.164315  0.632927b  0.459356  0.435970  0.615046c  0.459356  0.435970  0.615046d  0.420776  0.668942  0.680803e  0.420776  0.668942  0.680803f  0.086104  0.409684  0.162472g       NaN       NaN       NaN</code></pre><ol start="7"><li><strong>丢弃缺少的值</strong>：如果只想排除缺少的值，则使用 dropna 函数和 axis 参数。默认情况下，axis=0，如果行内的任何值是 NaN，那么整个行被删除。<br>1）丢弃含 NaN 值的行</li></ol><p><strong>print(df.dropna())</strong><br>2）丢弃含 NaN 值的列<br><strong>print (df.dropna(axis=1))</strong></p><pre><code>df = pd.DataFrame(np.random.rand(4, 3), index=[&#39;a&#39;, &#39;c&#39;, &#39;e&#39;, &#39;f&#39;], columns=[&#39;one&#39;, &#39;two&#39;, &#39;three&#39;])df = df.reindex([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;])print(df,&#39;\n&#39;)print(df.dropna()) # 丢弃含 NaN 值的行print (df.dropna(axis=1)) # 丢弃含 NaN 值的列</code></pre><p><strong>运行结果</strong></p><pre><code>        one       two     threea  0.430961  0.718700  0.023831b       NaN       NaN       NaNc  0.560938  0.816387  0.409205d       NaN       NaN       NaNe  0.945503  0.838175  0.707113f  0.987987  0.275285  0.532072g       NaN       NaN       NaN         one       two     threea  0.430961  0.718700  0.023831c  0.560938  0.816387  0.409205e  0.945503  0.838175  0.707113f  0.987987  0.275285  0.532072Empty DataFrameColumns: []Index: [a, b, c, d, e, f, g]</code></pre><ol start="8"><li><strong>替换缺失（或通用）值</strong>：用一些具体的值取代一个通用的值或缺失值。用标量替换 NaN 和使用 fillna() 函数等效。</li></ol><pre><code># 替换通用数据或者缺失值df = pd.DataFrame({&#39;one&#39;:[10,20,30,40,50,2000],&#39;two&#39;:[1000,0,30,40,50,60]}) print(df,&#39;\n&#39;)print(df.replace({1000:10,2000:60}))</code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第三章 Python编程进阶</title>
      <link href="/2020/06/02/di-san-zhang-python-bian-cheng-jin-jie/"/>
      <url>/2020/06/02/di-san-zhang-python-bian-cheng-jin-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="第三章-Python-编程进阶"><a href="#第三章-Python-编程进阶" class="headerlink" title="第三章 Python 编程进阶"></a>第三章 Python 编程进阶</h2><h3 id="1、输入输出与文件操作"><a href="#1、输入输出与文件操作" class="headerlink" title="1、输入输出与文件操作"></a>1、输入输出与文件操作</h3><ol><li>输入输出<br>基本输入、输出</li><li>文件操作<br>文件编码、基本操作</li></ol><h4 id="1-1-输入"><a href="#1-1-输入" class="headerlink" title="1.1 输入"></a>1.1 输入</h4><ol><li>Python 提供了 input() 内置函数从标准输入读入一行文本。</li><li>input() 函数也可以接收一个 Python 表达式作为输入，并将运算结果返回。</li><li>input() 函数的返回值永远是字符串，当我们需要返回 int 型时需要使用int(input()) 。</li></ol><p><strong>注：</strong>eval() 函数用来执行一个字符串表达式，并返回表达式的值。</p><pre><code>str = input(&quot;请输入：&quot;)print(&quot;你输入的内容是：&quot;, str)sum = input(&quot;请输入算术表达式：&quot;)sum = eval(sum)print(sum)</code></pre><p><strong>结果：</strong><br>请输入：Python编程学习<br>你输入的内容是： Python编程学习<br>请输入算术表达式：2+5<br>7</p><h4 id="1-2-输出"><a href="#1-2-输出" class="headerlink" title="1.2 输出"></a>1.2 输出</h4><ol><li>用 print() 在括号中加上字符串，就可以向屏幕上输出指定的文字。比如：输出“hello world”，用代码实现如下：<br>print(“hello world”)</li><li>print() 函数也可以接受多个字符串，用逗号“,”隔开，就可以连成一串输出：<br>print(‘The quick brown fox’, ‘jumps over’, ‘the lazy dog!’)</li><li>print() 会依次打印每个字符串，遇到逗号”,”会输出一个空格，因此，输出的字符串是这样拼起来的：</li></ol><pre><code>print(&#39;The quick brown fox&#39;, &#39;jumps over&#39;, &#39;the lazy dog!&#39;)</code></pre><p><strong>结果：</strong><br>The quick brown fox jumps over the lazy dog!</p><ol start="4"><li><p>print()也可以打印整数，或者计算结果：<br>print(300)   # 输出300<br>print(100+200)  # 输出300</p></li><li><p>print()函数也可以接受多个字符串，用逗号“,”隔开，就可以连成一串输出：</p></li></ol><pre><code>print(&#39;100+200=&#39;, 100+200)</code></pre><p><strong>结果：</strong><br>100+200= 300</p><p><strong>注意：</strong>对于100 + 200，Python解释器自动计算出结果300，但是，’100 + 200 =’是字符串而非数学公式，Python把它视为字符串。</p><h4 id="1-3-格式化输出"><a href="#1-3-格式化输出" class="headerlink" title="1.3 格式化输出"></a>1.3 格式化输出</h4><h5 id="1-3-1-整数的输出"><a href="#1-3-1-整数的输出" class="headerlink" title="1.3.1  整数的输出"></a>1.3.1  整数的输出</h5><p>1）%o —- oct 八进制<br>2）%d —- dec 十进制<br>3）%x —- hex 十六进制</p><pre><code>print(&#39;%o&#39; % 20)print(&#39;%d&#39; % 20)print(&#39;%x&#39; % 20)</code></pre><p><strong>结果：</strong><br>24<br>20<br>14</p><h5 id="1-3-2-浮点数的输出"><a href="#1-3-2-浮点数的输出" class="headerlink" title="1.3.2  浮点数的输出"></a>1.3.2  浮点数的输出</h5><p>1）<strong>格式化输出</strong></p><pre><code>print(&#39;%f&#39; % 1.11)     # 默认保留6位小数print(&#39;%.1f&#39; % 1.11)   # 取1位小数print(&#39;%e&#39; % 1.11)     # 默认6位小数，用科学计数法print(&#39;%.3e&#39; % 1.11)   # 取3位小数，用科学计数法print(&#39;%g&#39; % 1111.1111)    # 默认6位有效数字print(&#39;%.7g&#39; % 1111.1111)  # 取7位有效数字print(&#39;%.2g&#39; % 1111.1111)  # 取2位有效数字，自动转换为科学计数法1.1e+03</code></pre><p><strong>结果：</strong><br>1.110000<br>1.1<br>1.110000e+00<br>1.110e+00<br>1111.11<br>1111.111<br>1.1e+03</p><p>2）<strong>内置 round() 函数</strong><br>round(number[, ndigits])<br><strong>参数：</strong><br><code>number</code> —- 这是一个数字表达式。<br><code>ndigits</code> —- 表示从小数点到最后四舍五入的位数。默认值为 0。<br><strong>返回值</strong>：该方法返回 x 的小数点舍入为 n 位数后的值。<br><br></p><ol><li>round()函数只有一个参数，不指定位数时，返回一个整数，而且是最靠近的整数，类似于四舍五入。</li><li>当指定取舍的小数点位数的时候，一般情况也是使用四舍五入的规则。</li><li>但是碰到.5的情况时，如果要取舍的位数前的小数是奇数，则直接舍弃，如果是偶数则向上取舍。</li></ol><pre><code>round(1.1125)         # 四舍五入，不指定位数，取整round(1.1135,3)     # 取3位小数，由于3为奇数，则向下“舍”round(1.1125,3)     # 取3位小数，由于2为偶数，则向上“入”round(2.675, 2)     # 取2们有效小数</code></pre><p><strong>结果：</strong><br>1<br>1.113<br>1.113<br>2.67</p><p><strong>注：</strong>round(2.675, 2) 的结果，结果应该是2.68的，但它偏偏是2.67，为什么？这跟浮点数的精度有关。在机器中浮点数不一定能精确表达，换算成一串1和 0 后可能是无限位数的，机器已经做出了截断处理。因此在机器中保存的2.675这个数字就比实际数字要小那么一点点。这一点点就导致了它离2.67要更近一点点，所以保留两位小数时就近似到了2.67。</p><h5 id="1-3-3-字符串输出"><a href="#1-3-3-字符串输出" class="headerlink" title="1.3.3 字符串输出"></a>1.3.3 字符串输出</h5><ol><li>%s</li><li>%10s —— 右对齐，占位符10位</li><li>%.2s —— 截取2位字符串</li></ol><pre><code>print(&#39;%s&#39; % &#39;hello world&#39;) # 字符串输出print(&#39;%20s&#39; % &#39;hello world&#39;) # 右对齐，取20位，不够则补位print(&#39;%-20s&#39; % &#39;hello world&#39;) # 左对齐，取20位，不够则补位print(&#39;%.2s&#39; % &#39;hello world&#39;) # 取2位print(&#39;%10.2s&#39; % &#39;hello world&#39;) # 右对齐，取2位print(&#39;%-10.2s&#39; % &#39;hello world&#39;) # 左对齐，取2位</code></pre><p><strong>结果：</strong></p><pre><code>hello world         hello worldhello world         he        hehe       </code></pre><h3 id="2、文件"><a href="#2、文件" class="headerlink" title="2、文件"></a>2、文件</h3><h4 id="2-1-文件编码"><a href="#2-1-文件编码" class="headerlink" title="2.1 文件编码"></a>2.1 文件编码</h4><h5 id="2-1-1-ASCII-码"><a href="#2-1-1-ASCII-码" class="headerlink" title="2.1.1 ASCII 码"></a>2.1.1 ASCII 码</h5><ol><li>ASCII(American Standard Code for Information Interchange)，是一种<strong>单字节</strong>的编码。</li><li>ASCII 码使用指定的 7 位或 8 位二进制数组合来表示 128 或 256 种可能的字符。</li><li>英语中，用 128 个符号编码便可以表示全部，但是用来表示其他语言，128 个符号是不够。欧洲国家就决定，利用字节中闲置的最高位编入新的符号。欧洲国家编码体系，可以表示最多 256 个符号。</li><li>汉字多达 10 万左右，一个字节只能表示 256 种符号，就必须使用多个字节表达一个符号。比如，简体中文常见的编码方式是 GB2312，使用两个字节表示一个汉字。</li></ol><h5 id="2-1-2-Unicode"><a href="#2-1-2-Unicode" class="headerlink" title="2.1.2 Unicode"></a>2.1.2 Unicode</h5><ol><li>Unicode 码扩展自 ASCII 字元集</li><li>Unicode 是计算机科学领域里的一项业界标准，包括字符集、编码方案等。</li><li>Unicode 通常用<strong>两个字节</strong>表示一个字符，原有的英文编码从单字节变成双字节，只需要把高字节全部填为 0 就可以。</li><li>跨语言、跨平台进行文本转换和处理。</li><li>对每种语言中字符设定统一且唯一的二进制编码。</li><li>每个英文字母前都必然有二到三个字节是 0，这对于存储来说是极大的浪费。</li></ol><h5 id="2-1-3-UTF-8（8-bit-Unicode-Transformation-Format）"><a href="#2-1-3-UTF-8（8-bit-Unicode-Transformation-Format）" class="headerlink" title="2.1.3 UTF-8（8-bit Unicode Transformation Format）"></a>2.1.3 UTF-8（8-bit Unicode Transformation Format）</h5><ol><li>可变长度的Unicode的实现方式。</li><li>使用 1~4 个字节表示一个符号，根据不同的符号而变化字节长度。</li></ol><h4 id="2-2-文件类型"><a href="#2-2-文件类型" class="headerlink" title="2.2 文件类型"></a>2.2 文件类型</h4><h5 id="2-2-1-文本文件"><a href="#2-2-1-文本文件" class="headerlink" title="2.2.1 文本文件"></a>2.2.1 文本文件</h5><p>以 ASCII 码方式存储的文件</p><h5 id="2-2-2-二进制文件"><a href="#2-2-2-二进制文件" class="headerlink" title="2.2.2 二进制文件"></a>2.2.2 二进制文件</h5><p>直接由比特 0 和比特 1 组成，没有统一字符编码</p><h4 id="2-3-文件操作"><a href="#2-3-文件操作" class="headerlink" title="2.3 文件操作"></a>2.3 文件操作</h4><h5 id="2-3-1-打开文件"><a href="#2-3-1-打开文件" class="headerlink" title="2.3.1 打开文件"></a>2.3.1 打开文件</h5><p>建立磁盘上的文件与程序中的对象相关联</p><ol><li>Python 通过解释器内置的 open() 函数打开一个文件，并实现该文件与一个程序变量的关联，open() 函数格式如下：<pre><code>&lt;变量名&gt;=open(&lt;文件名&gt;,&lt;打开模式&gt;)</code></pre></li><li>open() 函数有两个参数：文件名和打开模式。</li><li>文件名可以是文件的实际名字，也可以是包含完整路径的名字。</li><li>文件打开模式：只读，写入，追加等。这个参数是非强制的，默认文件访问模式为只读 (r)。</li><li>open() 函数提供 7 种基本的打开模式</li></ol><table><thead><tr><th align="center">模式</th><th align="left">说明</th></tr></thead><tbody><tr><td align="center">r</td><td align="left">读模式（默认模式，可省略），如果文件不存在，抛出异常</td></tr><tr><td align="center">w</td><td align="left">写模式，如果文件已存在，先清空原有内容；如果文件不存在，创建新文件</td></tr><tr><td align="center">x</td><td align="left">写模式，创建新文件，如果文件已存在则抛出异常</td></tr><tr><td align="center">a</td><td align="left">追加模式，不覆盖文件中原有内容</td></tr><tr><td align="center">b</td><td align="left">二进制模式（可与 r、w、x 或 a 模式组合使用）</td></tr><tr><td align="center">t</td><td align="left">文本模式（默认模式，可省略）</td></tr><tr><td align="center">+</td><td align="left">读、写模式（可与其他模式组合使用）</td></tr></tbody></table><ol start="6"><li>读写模式</li></ol><table><thead><tr><th align="center">模式</th><th align="center">r</th><th align="center">r+</th><th align="center">w</th><th align="center">w+</th><th align="center">a</th><th align="center">a+</th></tr></thead><tbody><tr><td align="center">读</td><td align="center">+</td><td align="center">+</td><td align="center"></td><td align="center">+</td><td align="center"></td><td align="center">+</td></tr><tr><td align="center">写</td><td align="center"></td><td align="center">+</td><td align="center">+</td><td align="center">+</td><td align="center">+</td><td align="center">+</td></tr><tr><td align="center">创建</td><td align="center"></td><td align="center"></td><td align="center">+</td><td align="center">+</td><td align="center">+</td><td align="center">+</td></tr><tr><td align="center">覆盖</td><td align="center"></td><td align="center"></td><td align="center">+</td><td align="center">+</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">指针在开始</td><td align="center">+</td><td align="center">+</td><td align="center">+</td><td align="center">+</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">指针在结尾</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">+</td><td align="center">+</td></tr></tbody></table><h5 id="2-3-2-文件操作"><a href="#2-3-2-文件操作" class="headerlink" title="2.3.2 文件操作"></a>2.3.2 文件操作</h5><ol><li>读取</li></ol><p><strong>文件读取</strong><br>根据打开方式不同可以对文件进行相应的读写操作，Python提供4个常用的文件内容读取方法。</p><table><thead><tr><th align="left">方法</th><th align="left">含义</th></tr></thead><tbody><tr><td align="left"><code>&lt;file&gt;.readall()</code></td><td align="left">读入整个文件内容，返回一个字符串或字节流</td></tr><tr><td align="left"><code>&lt;file&gt;.read(size = -1)</code></td><td align="left">从文件中读入整个文件内容，如果给出参数，读入前size长度的字符串或字节流</td></tr><tr><td align="left"><code>&lt;file&gt;.readline(size = -1)</code></td><td align="left">从文件中读入一行内容，如果给出参数，读入前  size 长度的字符串或字节流</td></tr><tr><td align="left"><code>&lt;file&gt;.readlines(hint = -1)</code></td><td align="left">从文件中读入所有行，以每行为元素形成一个列表，如果给出参数，读入 hint 行</td></tr></tbody></table><ol start="2"><li>写入</li></ol><p><strong>文件写入</strong><br>Python 提供 3 个与文件内容写入有关的方法</p><table><thead><tr><th align="left">方法</th><th align="left">含义</th></tr></thead><tbody><tr><td align="left"><code>&lt;file&gt;. write(s)</code></td><td align="left">向文件写入一个字符串或字节流</td></tr><tr><td align="left"><code>&lt;file&gt;. writelines(lines)</code></td><td align="left">将一个元素为字符串的列表写入文件</td></tr><tr><td align="left"><code>&lt;file&gt;. seek(offset)</code></td><td align="left">0：文件开头；1：当前位置；2：文件结尾</td></tr></tbody></table><ol start="3"><li><p>定位</p></li><li><p>其他：追加、计算等</p></li><li><p>上下文管理语句with<br>在实际开发中，读写文件应优先考虑使用上下文管理语句with。关键字with可以自动管理资源，不论因为什么原因跳出with块，总能保证文件被正确关闭。除了用于文件操作，with关键字还可以用于数据库连接、网络连接或类似场合。用于文件内容读写时，with语句的语法形式如下：</p></li></ol><pre><code>with open(filename, mode, encoding) as fp:# 这里写通过文件对象 fp 读写文件内容的语句块</code></pre><ol start="6"><li>文件操作示例<br>编写程序将电话簿 teleAddressBook.txt 和电子邮件 emailAddressBook.txt 合并为一个完整的 addressBook.txt</li></ol><p><strong>teleAddressBook.txt</strong> </p><table><thead><tr><th align="left">姓名</th><th align="left">电话号码</th></tr></thead><tbody><tr><td align="left">张伟</td><td align="left">1534098751</td></tr><tr><td align="left">王伟</td><td align="left">1520878752</td></tr><tr><td align="left">王芳</td><td align="left">1520658753</td></tr><tr><td align="left">李娜</td><td align="left">1523098754</td></tr><tr><td align="left">刘伟</td><td align="left">1596098755</td></tr></tbody></table><p><strong>emailAddressBook.txt</strong></p><table><thead><tr><th align="left">姓名</th><th align="left">邮箱</th></tr></thead><tbody><tr><td align="left">张伟</td><td align="left"><a href="mailto:1534098751@qq.com">1534098751@qq.com</a></td></tr><tr><td align="left">王伟</td><td align="left"><a href="mailto:1520878752@qq.com">1520878752@qq.com</a></td></tr><tr><td align="left">王芳</td><td align="left"><a href="mailto:1520658753@qq.com">1520658753@qq.com</a></td></tr><tr><td align="left">李伟</td><td align="left"><a href="mailto:1520098754@qq.com">1520098754@qq.com</a></td></tr></tbody></table><p><strong>addressBook.txt</strong></p><table><thead><tr><th align="left">姓名</th><th align="left">电话</th><th align="left">邮箱</th></tr></thead><tbody><tr><td align="left">张伟</td><td align="left">1534098751</td><td align="left"><a href="mailto:1534098751@qq.com">1534098751@qq.com</a></td></tr><tr><td align="left">王伟</td><td align="left">1520878752</td><td align="left"><a href="mailto:1520878752@qq.com">1520878752@qq.com</a></td></tr><tr><td align="left">王芳</td><td align="left">1520658753</td><td align="left"><a href="mailto:1520658753@qq.com">1520658753@qq.com</a></td></tr><tr><td align="left">李娜</td><td align="left">1523098754</td><td align="left"></td></tr><tr><td align="left">刘伟</td><td align="left">1596098755</td><td align="left"></td></tr><tr><td align="left">李伟</td><td align="left"></td><td align="left"><a href="mailto:1520098754@qq.com">1520098754@qq.com</a></td></tr></tbody></table><ol start="7"><li>文件操作示例代码</li></ol><pre><code>with open(&#39;teleAddressBook.txt&#39;, &#39;rt&#39;) as ftele1:    with open(&#39;emailAddressBook.txt&#39;, &#39;rt&#39;) as ftele2:        ftele1.readline() # 跳过第一行        ftele2.readline()        lines1=ftele1.readlines()        lines2=ftele2.readlines()        # 建立空表用于存储姓名，电话，email        list1_name=[]        list1_tele=[]        list2_name=[]        list2_email=[]        for line in lines1: # 获取第一个文本中的姓名和电话信息            elements=line.split()            list1_name.append(elements[0])            list1_tele.append(elements[1])        for line in lines2: # 获取第二个文本中的姓名和邮件信息            elements=line.split()            list2_name.append(elements[0])            list2_email.append(elements[1])        # 生成新的数据        lines=[]        lines.append(&#39;姓名\t电话\t\t邮箱\n&#39;)        # 遍历列表1        for i in range(len(list1_name)):            s=&#39;&#39;            # 查看列表1中的名字是否在列表2中            if list1_name[i] in list2_name:                j=list2_name.index(list1_name[i])                s=&#39;\t&#39;.join([list1_name[i], list1_tele[i], list2_email[j]])                s+=&#39;\n&#39;            else:                s=&#39;\t&#39;.join([list1_name[i], list1_tele[i], str(&#39;   ----   &#39;)])                s+=&#39;\n&#39;            lines.append(s)        for i in range(len(list2_name)):            s=&#39;&#39;            if list2_name[i] not in list1_name:                s=&#39;\t&#39;.join([list2_name[i], str(&#39;   ----   &#39;), list2_email[i]])                s+=&#39;\n&#39;                lines.append(s)        # 写入文件        ftele3=open(&#39;addressbook&#39;, &#39;w&#39;)        ftele3.writelines(lines)# 关闭文件ftele3.close()ftele1.close()ftele2.close()print(&#39;新的通讯录已合成&#39;)</code></pre><h5 id="2-3-3-关闭文件"><a href="#2-3-3-关闭文件" class="headerlink" title="2.3.3 关闭文件"></a>2.3.3 关闭文件</h5><ol><li>切断文件与程序的联系</li><li>写入磁盘，并释放文件缓冲区</li></ol><h3 id="3、Excel-文件操作实例"><a href="#3、Excel-文件操作实例" class="headerlink" title="3、Excel 文件操作实例"></a>3、Excel 文件操作实例</h3><h4 id="3-1-Excel文件格式说明"><a href="#3-1-Excel文件格式说明" class="headerlink" title="3.1 Excel文件格式说明"></a>3.1 Excel文件格式说明</h4><ol><li>一个 Excel 电子表格文档称为一个工作簿（workbook），一个工作簿保存在扩展名为 .xlsx 的文件中。</li><li>每个工作簿可以包含多个表（Sheet）。</li><li>用户当前查看的表（或关闭 Excel 前最后查看的表），称为活动表。</li><li>每个表都有一些列（地址是从 A 开始的字母）和一些行（地址是从1开始的数字）。</li><li>在特定行和列的方格称为单元格（cell）。</li><li>每个单元格都包含一个数字或文本值。</li></ol><h4 id="3-2-openpyxl-库"><a href="#3-2-openpyxl-库" class="headerlink" title="3.2 openpyxl 库"></a>3.2 openpyxl 库</h4><h5 id="3-2-1-openpyxl-库特点"><a href="#3-2-1-openpyxl-库特点" class="headerlink" title="3.2.1 openpyxl 库特点"></a>3.2.1 openpyxl 库特点</h5><ol><li>用于读取/写入 Excel 2010 xlsx/xlsm/xltx/xltm 文件的 Python 库。</li><li>使用 getter/setter 模式。你可以随时读取某个单元格的内容，并根据其内容进行相应的修改，openpyxl 会帮你记住每个单元格的状态。</li><li>虽然它支持修改已有文件，但由于其所支持的功能有限，读入文件时会忽略掉它所不支持的内容，再写入时，这些内容就丢失了。</li></ol><h5 id="3-2-2-openpyxl-库缺点"><a href="#3-2-2-openpyxl-库缺点" class="headerlink" title="3.2.2 openpyxl 库缺点"></a>3.2.2 openpyxl 库缺点</h5><ol><li>不支持 07 版本之前的 XLS 格式。</li><li>不支持 Excel 中的公式。</li></ol><h5 id="3-2-3-openpyxl-库常用操作"><a href="#3-2-3-openpyxl-库常用操作" class="headerlink" title="3.2.3 openpyxl 库常用操作"></a>3.2.3 openpyxl 库常用操作</h5><ol><li>安装 openpyxl 库</li></ol><p><strong>conda install openpyxl</strong></p><ol start="2"><li>导入 openpyxl 库</li></ol><p><strong>import openpyxl</strong></p><ol start="3"><li>用 openpyxl 库打开 Excel 文档</li></ol><p><strong>wb = openpyxl.load_workbook()</strong></p><ol start="4"><li>从工作簿中取得工作表</li></ol><p><strong>get_sheet_names()</strong></p><ol start="5"><li><p>从表中取得行和列<br>可以将 Worksheet 对象切片，取得 Excel 表格中一行、一列或一个矩形区域中的所有 Cell 对象，然后可以循环遍历这个切片中的所有单元格。</p></li><li><p>新建工作簿：新建工作簿不需要在系统中创建新文件，在内存中操作即可。</p></li></ol><pre><code>from openpyxl import Workbook# 工作簿实例化wb = Workbook()</code></pre><ol start="7"><li>从工作簿激活工作表</li></ol><pre><code># 激活获取工作表，此方法调用 _active_sheet_index方法，默认索引为0，即第一个工作表ws = wb.active</code></pre><ol start="8"><li>也可用 create_sheet() 方法创建工作表。 </li></ol><pre><code># 默认在最后添加工作表ws1 = wb.create_sheet()# 在指定位置添加工作表，位置0，即第一个ws2 = wb.create_sheet(0)</code></pre><ol start="9"><li>可以通过工作表名称获取工作表，以下两种方式效果相同。</li></ol><pre><code>ws3 = wb[&quot;New Title&quot;]ws4 = wb.get_sheet_by_name(&quot;New Title&quot;)</code></pre><ol start="10"><li>通过 get_sheet_names() 获取工作簿所有工作表的名称，返回值为 list 类型。</li></ol><pre><code>&gt;&gt;&gt; print(wb.get_sheet_names())[&#39;Sheet2&#39;, &#39;New Title&#39;, &#39;Sheet1&#39;]</code></pre><ol start="11"><li>获取单元格</li></ol><pre><code>c = ws[&#39;A4&#39;] # 利用坐标获取单元格，若单元格不存在，则被创建。ws[&#39;A4&#39;] = 4 # 对单元格的值直接赋值。# 也可通过以下两种方式获取单元格，效果相同。c = ws.cell(&#39;A4&#39;)# 指定行和列获取，从 1 开始，不从 0 开始。d = ws.cell(row = 4, column = 1)</code></pre><ol start="12"><li>利用切片方式获取指定区域的单元格</li></ol><pre><code>cell_range = ws[&#39;A1&#39;:&#39;C2&#39;]</code></pre><ol start="13"><li>获取单元格后，写数据</li></ol><pre><code>c = ws.cell(&#39;A4’)c.value = &#39;hello, world&#39;</code></pre><ol start="14"><li>对实例化的工作簿调用 openpyxl.workbook.Workbook.save() 进行保存</li></ol><pre><code>wb.save(&#39;sample.xlsx&#39;)</code></pre><h4 id="3-3-Excel-文件-人口普查数据-处理实例"><a href="#3-3-Excel-文件-人口普查数据-处理实例" class="headerlink" title="3.3 Excel 文件(人口普查数据)处理实例"></a>3.3 Excel 文件(人口普查数据)处理实例</h4><h5 id="3-3-1-问题描述"><a href="#3-3-1-问题描述" class="headerlink" title="3.3.1 问题描述"></a>3.3.1 问题描述</h5><p>处理2010年美国人口普查数据文件 censuspopdata.xlsx，Excle 文件中只有一张表，名为 ‘Population by Census Tract’。每一行都保存了一个普查区的数据。列分别是普查区的编号(A)，州的简称(B)，县的名称(C)，普查区的人口(D)。</p><h5 id="3-3-2-题目要求"><a href="#3-3-2-题目要求" class="headerlink" title="3.3.2 题目要求"></a>3.3.2 题目要求</h5><ol><li>编写程序，从人口普查 Excel 文件中读取数据，并计算出每个县的人口统计值。</li><li>题目要求以字典嵌套字典方式存储统计信息，最外层字典的键是州，州里的键是县，县里的键是 pop 和人口普查区数量。<br>dict = { ‘州’:{ ‘县1’: {‘pop’:9000, ‘人口普查区’：2}， }， }</li></ol><h5 id="3-3-3-求解步骤"><a href="#3-3-3-求解步骤" class="headerlink" title="3.3.3 求解步骤"></a>3.3.3 求解步骤</h5><ol><li>从 Excel 电子表格中读取数据。</li><li>计算每个县中普查区的数目。</li><li>计算每个县的总人口。</li><li>打印结果。</li></ol><h5 id="3-3-4-待处理-Excel-文件"><a href="#3-3-4-待处理-Excel-文件" class="headerlink" title="3.3.4  待处理 Excel 文件"></a>3.3.4  待处理 Excel 文件</h5><table><thead><tr><th align="center">普查区编号</th><th align="center">州(state)</th><th align="center">乡/县(country)</th><th align="center">POP2010</th></tr></thead><tbody><tr><td align="center">01001020100</td><td align="center">AL</td><td align="center">Autauge</td><td align="center">1912</td></tr><tr><td align="center">01001020200</td><td align="center">AL</td><td align="center">Autauge</td><td align="center">2170</td></tr><tr><td align="center">01001020300</td><td align="center">AL</td><td align="center">Autauge</td><td align="center">3373</td></tr><tr><td align="center">01001020400</td><td align="center">AL</td><td align="center">Autauge</td><td align="center">4386</td></tr><tr><td align="center">01001020500</td><td align="center">AL</td><td align="center">Autauge</td><td align="center">10766</td></tr><tr><td align="center">01001020600</td><td align="center">AL</td><td align="center">Autauge</td><td align="center">3668</td></tr><tr><td align="center">01001020700</td><td align="center">AL</td><td align="center">Autauge</td><td align="center">2891</td></tr></tbody></table><h5 id="3-3-5-实例代码"><a href="#3-3-5-实例代码" class="headerlink" title="3.3.5 实例代码"></a>3.3.5 实例代码</h5><pre><code>import openpyxl, pprintprint(&#39;Opening workbook...&#39;)wb = openpyxl.load_workbook(&#39;censuspopdata.xlsx&#39;)sheet = wb.get_sheet_by_name(&#39;Population by Census Tract&#39;) # 获取人口统计Sheet工作表countryData = {} # 定义统计结果保存字典print(&#39;Reading rows...&#39;)for row in range(2, sheet.max_row+1):# 从 Excel 文件第二行开始读取数据，至最后一行。前闭后开，不包括 max_row+1 行    state = sheet[&#39;B&#39; + str(row)].value # 读取州名    country = sheet[&#39;C&#39; + str(row)].value # 读取县名    pop = sheet[&#39;D&#39; + str(row)].value # 读取人口数量值    countryData.setdefault(state, {})    countryData[state].setdefault(country, {&#39;tracts&#39;: 0, &#39;pop&#39;: 0})    countryData[state][country][&#39;tracts&#39;] += 1 # 该县的人口普查次数+1    countryData[state][country][&#39;pop&#39;] += int(pop) # 该县所有普查次区人口数量相加print(&#39;Writing results...&#39;)resultFile = open(&#39;census2010.py&#39;, &#39;w&#39;)resultFile.write(&#39;allData = &#39; + pprint.pformat(countryData))# pprint 生成一个字符串，格式化好的、有效的python代码resultFile.close()print(&#39;Done.&#39;)</code></pre><p><strong>运行结果</strong></p><pre><code>allData = {&#39;AK&#39;:{&#39;Aleutians East&#39;:{&#39;pop&#39;: 3141, &#39;tracts&#39;: 1}, &#39;Aleutians West&#39;:{&#39;pop&#39;: 5561, &#39;tracts&#39;: 2}, &#39;Anchorage&#39;:{&#39;pop&#39;: 291826, &#39;tracts&#39;: 55},&#39;Bethel&#39;:{&#39;pop&#39;: 17013, &#39;tracts&#39;: 3},&#39;Bristol Bay&#39;:{&#39;pop&#39;: 997, &#39;tracts&#39;: 1},&#39;Denali&#39;:{&#39;pop&#39;: 1826, &#39;tracts&#39;: 1},&#39;Dillingham&#39;:{&#39;pop&#39;: 4847, &#39;tracts&#39;: 2},&#39;Fairbanks North Star&#39;:{&#39;pop&#39;: 97581, &#39;tracts&#39;: 19},&#39;Haines&#39;:{&#39;pop&#39;: 2508, &#39;tracts&#39;: 1},&#39;Hoonah-Angoon&#39;:{&#39;pop&#39;: 2150, &#39;tracts&#39;: 2},&#39;Juneau&#39;:{&#39;pop&#39;: 31275, &#39;tracts&#39;: 6},&#39;Kenai Peninsula&#39;:{&#39;pop&#39;: 55400, &#39;tracts&#39;: 13},&#39;Ketchikan Gateway&#39;:{&#39;pop&#39;: 13477, &#39;tracts&#39;: 4},&#39;Kodiak Island&#39;:{&#39;pop&#39;: 13592, &#39;tracts&#39;: 5},&#39;Lake and Peninsula&#39;:{&#39;pop&#39;: 1631, &#39;tracts&#39;: 1},&#39;Matanuska-Susitna&#39;:{&#39;pop&#39;: 88995, &#39;tracts&#39;: 24},&#39;Nome&#39;:{&#39;pop&#39;: 9492, &#39;tracts&#39;: 2},&#39;North Slope&#39;:{&#39;pop&#39;: 9430, &#39;tracts&#39;: 3},</code></pre><h3 id="4、自然语言处理实战"><a href="#4、自然语言处理实战" class="headerlink" title="4、自然语言处理实战"></a>4、自然语言处理实战</h3><h4 id="4-1-中文分词介绍"><a href="#4-1-中文分词介绍" class="headerlink" title="4.1 中文分词介绍"></a>4.1 中文分词介绍</h4><h5 id="4-1-1-中文分词特点"><a href="#4-1-1-中文分词特点" class="headerlink" title="4.1.1 中文分词特点"></a>4.1.1 中文分词特点</h5><ol><li>词是最小的能够独立活动的有意义的语言成分。</li><li>汉语是以字为单位，不像西方语言，词与词之间没有空格之类的标志指示词的边界。</li><li>分词问题为中文文本处理的基础性工作，分词的好坏对后续中文信息处理其关键作用。</li></ol><h5 id="4-1-2-中文分词难点"><a href="#4-1-2-中文分词难点" class="headerlink" title="4.1.2 中文分词难点"></a>4.1.2 中文分词难点</h5><ol><li><p>分词规范，词的定义还不明确 (《统计自然语言处理》宗成庆)</p></li><li><p>歧义切分问题，交集型切分问题，多义组合型切分歧义等<br>结婚的和尚未结婚的 =&gt;<br>结婚／的／和／尚未／结婚／的<br>结婚／的／和尚／未／结婚／的</p></li><li><p>未登录词问题有两种解释：<br>一是已有的词表中没有收录的词。<br>二是已有的训练语料中未曾出现过的词，一些网络新词，自造词一般都属于这些词。</p></li></ol><h5 id="4-1-3-中文分词方法"><a href="#4-1-3-中文分词方法" class="headerlink" title="4.1.3 中文分词方法"></a>4.1.3 中文分词方法</h5><ol><li>基于字典、词库匹配的分词方法(基于规则)<br>将待分的字符串与一个充分大的机器词典中的词条进行匹配。常用的有：正向最大匹配，逆向最大匹配，最少切分法。实际应用中，将机械分词作为初分手段，利用语言信息提高切分准确率。</li><li>基于词频度统计的分词方法（基于统计）<br>相邻的字同时出现的次数越多，越有可能构成一个词语，对语料中的字组频度进行统计，基于词的频度统计的分词方法是一种全切分方法。jieba是基于统计的分词方法。</li><li>基于知识理解的分词方法<br>该方法主要基于句法、语法分析，并结合语义分析，通过对上下文内容所提供信息的分析对词进行定界。由于汉语语言知识的笼统、复杂性，目前还处在试验阶段。</li></ol><h4 id="4-2-jieba-扩展库"><a href="#4-2-jieba-扩展库" class="headerlink" title="4.2 jieba 扩展库"></a>4.2 jieba 扩展库</h4><h5 id="4-2-1-jieba-是-Python-中一个重要的第三方中文分词函数库"><a href="#4-2-1-jieba-是-Python-中一个重要的第三方中文分词函数库" class="headerlink" title="4.2.1 jieba 是 Python 中一个重要的第三方中文分词函数库"></a>4.2.1 jieba 是 Python 中一个重要的第三方中文分词函数库</h5><ol><li>jieba分词，<strong>完全开源</strong>，有集成的python库，简单易用。</li><li>安装：<br>1）全自动安装： easy_install jieba 或者 pip install jieba / pip3 install jieba<br>2）半自动安装：先下载 <a href="https://pypi.python.org/pypi/jieba/" target="_blank" rel="noopener">https://pypi.python.org/pypi/jieba/</a> ，解压后运行 python setup.py install<br>3）手动安装：将 jieba 目录放置于当前目录或者 site-packages 目录<br>4）通过 import jieba 来引用</li></ol><h5 id="4-2-2-jieba-实现原理"><a href="#4-2-2-jieba-实现原理" class="headerlink" title="4.2.2 jieba 实现原理"></a>4.2.2 jieba 实现原理</h5><ol><li>基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)。<br>例如：句子“抗日战争”生成的 DAG 中{0:[0,1,3]} 这样一个简单的 DAG，就是表示 0 位置开始，在 0,1,3 位置都是词，就是说 0<del>0，0</del>1，0~3 即 “抗”，“抗日”，“抗日战争”这三个词在dict.txt中是词。</li><li>采用了动态规划查找最大概率路径，找出基于词频的最大切分组合。<br>根据动态规划查找最大概率路径的基本思路就是对句子从右往左反向计算最大概率，…依次类推， 最后得到最大概率路径，得到最大概率的切分组合。</li><li>对于未登录词，采用了基于汉字成词能力的隐马尔科夫模型（HMM）模型，使用了 Viterbi 算法（一种最优路径算法）。</li></ol><h5 id="4-2-3-jieba-是-Python-中一个重要的第三方中文分词函数库"><a href="#4-2-3-jieba-是-Python-中一个重要的第三方中文分词函数库" class="headerlink" title="4.2.3 jieba 是 Python 中一个重要的第三方中文分词函数库"></a>4.2.3 jieba 是 Python 中一个重要的第三方中文分词函数库</h5><ol><li>第三方库，需要安装</li></ol><p><strong>pip install jieba</strong><br>2. 安装成功提示<br>Successfully installed jieba-0.39<br>3. 也可离线下载安装<br>个人推荐离线安装，因为网速，anaconda安装容易失败，我试了很多次了。<br>下载地址：<a href="https://pypi.org/project/jieba/#files）" target="_blank" rel="noopener">https://pypi.org/project/jieba/#files）</a></p><h5 id="4-2-4-jieba-支持三种分词模式"><a href="#4-2-4-jieba-支持三种分词模式" class="headerlink" title="4.2.4 jieba 支持三种分词模式"></a>4.2.4 jieba 支持三种分词模式</h5><ol><li>精确模式：试图将句子最精确地切开，适合文本分析。</li><li>全模式：把句子中所有的可以成词的词语都扫描出来，速度非常快，但是不能解决歧义。</li><li>搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。</li><li><strong>支持繁体分词</strong></li><li><strong>支持自定义词典</strong></li><li><strong>MIT 授权协议（开源软件授权协议）</strong></li></ol><h5 id="4-2-5-jieba-常用函数"><a href="#4-2-5-jieba-常用函数" class="headerlink" title="4.2.5 jieba 常用函数"></a>4.2.5 jieba 常用函数</h5><ol><li>jieba.cut ：<br>方法接受三个输入参数:<br>1）需要分词的字符串。<br>2）cut_all 参数用来控制是否采用全模式。<br>3）HMM 参数用来控制是否使用 HMM 模型。</li><li>jieba.cut_for_search ：<br>方法接受两个参数：<br>1）需要分词的字符串。<br>2）是否使用 HMM 模型。<br>该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细。</li><li>jieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode)，或者<strong>用 jieba.lcut 以及 jieba.lcut_for_search 直接返回 list</strong></li></ol><h4 id="4-3-jieba-扩展库示例"><a href="#4-3-jieba-扩展库示例" class="headerlink" title="4.3 jieba 扩展库示例"></a>4.3 jieba 扩展库示例</h4><h5 id="4-3-1-jieba-三种分词模式的区别"><a href="#4-3-1-jieba-三种分词模式的区别" class="headerlink" title="4.3.1 jieba 三种分词模式的区别"></a>4.3.1 jieba 三种分词模式的区别</h5><pre><code>import jieba&#39;&#39;&#39;cut 方法有两个参数1）第一个参数是我们想分词的字符串2）第二个参数cut_all是用来控制是否采用全局模式&#39;&#39;&#39;# 全模式word_list = jieba.cut(&quot;今天天气真好。亲爱的，我们去远足吧！&quot;, cut_all=True)print(&quot;全模式：&quot;,&quot;|&quot;.join(word_list))# 精确模式，默认就是精确模式word_list = jieba.cut(&quot;今天天气真好。亲爱的，我们去远足吧！&quot;, cut_all=False)print(&quot;精确模式：&quot;,&quot;|&quot;.join(word_list))# 搜索引擎模式word_list = jieba.cut_for_search(&quot;今天天气真好。亲爱的，我们去远足吧！&quot;)print(&quot;搜索引擎模式：&quot;,&quot;|&quot;.join(word_list))</code></pre><p><strong>结果：</strong><br>全模式： 今天|今天天气|天天|天气|真好|。|亲爱|的|，|我们|去|远足|吧|！<br>精确模式： 今天天气|真|好|。|亲爱|的|，|我们|去|远足|吧|！<br>搜索引擎模式： 今天|天天|天气|今天天气|真|好|。|亲爱|的|，|我们|去|远足|吧|！</p><h5 id="4-3-2-TF-IDF-释义"><a href="#4-3-2-TF-IDF-释义" class="headerlink" title="4.3.2 TF-IDF 释义"></a>4.3.2 TF-IDF 释义</h5><ol><li>TF-IDF（term frequency–inverse document frequency）是一种用于<strong>信息检索与数据挖掘</strong>的常用加权技术。</li><li>TF意思是词频，指关键词在文中出现的次数除以全文总字数。</li><li>IDF意思是逆文本频率指数，反映关键词的普遍程度——当一个词越普遍（即有大量文档包含这个词）时，其IDF值越低；反之，则IDF值越高。</li></ol><p>TF-IDF=TF×IDF<br>$词频(TF)=\frac{某个词在文章中的出现次数}{文章的总词数}\times逆文档频率(IDF)=log(\frac{语料库的文档数}{包含该词的文档数+1})$</p><h5 id="4-3-3-可以看出"><a href="#4-3-3-可以看出" class="headerlink" title="4.3.3 可以看出"></a>4.3.3 可以看出</h5><ol><li>当一个词在文档频率越高并且新鲜度高（即普遍度低），其TF-IDF值越高。</li><li>TF-IDF兼顾词频与新鲜度，过滤一些常见词，保留能提供更多信息的重要词。</li></ol><h5 id="4-3-4-基于TF-IDF算法的关键词抽取"><a href="#4-3-4-基于TF-IDF算法的关键词抽取" class="headerlink" title="4.3.4 基于TF-IDF算法的关键词抽取"></a>4.3.4 基于TF-IDF算法的关键词抽取</h5><pre><code>import jieba.analysejieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())</code></pre><ol><li>sentence：待提取的文本。</li><li>topK：返回几个 TF/IDF 权重最大的关键词，默认值为 20。</li><li>withWeight：是否一并返回关键词权重值，默认值为 False。</li><li>allowPOS：词性过滤，为空表示不过滤，若提供则仅返回符合词性要求的关键词。</li></ol><h5 id="4-3-5-基于TF-IDF算法的关键词抽取示例"><a href="#4-3-5-基于TF-IDF算法的关键词抽取示例" class="headerlink" title="4.3.5 基于TF-IDF算法的关键词抽取示例"></a>4.3.5 基于TF-IDF算法的关键词抽取示例</h5><pre><code>import jieba.analysef = open(&quot;十九大报告节选.txt&quot;, &quot;rb&quot;)sentence = f.read()keywords = jieba.analyse.extract_tags(sentence, topK=15, withWeight=True, allowPOS=(&#39;n&#39;,&#39;nr&#39;,&#39;ns&#39;))for item in keywords:    print(item[0],item[1])</code></pre><p><strong>allowPOS词性过滤：n表示名词、nr表示人名、ns表示地名</strong></p><h5 id="4-3-6-基于TF-IDF算法的关键词抽取示例运行结果"><a href="#4-3-6-基于TF-IDF算法的关键词抽取示例运行结果" class="headerlink" title="4.3.6 基于TF-IDF算法的关键词抽取示例运行结果"></a>4.3.6 基于TF-IDF算法的关键词抽取示例运行结果</h5><p><strong>十九大报告节选.txt 在目录C:\Users\用户名.spyder-py3下</strong></p><p><strong>十九大报告节选.txt</strong>（原始文档）<br>中国共产党第十九次全国代表大会，是在全面建成小康社会决胜阶段、中国特色社会主义进入新时代的关键时期召开的一次十分重要的大会。</p><p>大会的主题是：不忘初心，牢记使命，高举中国特色社会主义伟大旗帜，决胜全面建成小康社会，夺取新时代中国特色社会主义伟大胜利，为实现中华民族伟大复兴的中国梦不懈奋斗。</p><p>不忘初心，方得始终。中国共产党人的初心和使命，就是为中国人民谋幸福，为中华民族谋复兴。这个初心和使命是激励中国共产党人不断前进的根本动力。全党同志一定要永远与人民同呼吸、共命运、心连心，永远把人民对美好生活的向往作为奋斗目标，以永不懈怠的精神状态和一往无前的奋斗姿态，继续朝着实现中华民族伟大复兴的宏伟目标奋勇前进。</p><p>当前，国内外形势正在发生深刻复杂变化，我国发展仍处于重要战略机遇期，前景十分光明，挑战也十分严峻。全党同志一定要登高望远、居安思危，勇于变革、勇于创新，永不僵化、永不停滞，团结带领全国各族人民决胜全面建成小康社会，奋力夺取新时代中国特色社会主义伟大胜利。</p><p><strong>运行结果</strong><br>初心 1.056602437712<br>小康社会 0.6097804820220001<br>特色 0.5239102226336<br>全党同志 0.512082614532<br>社会主义 0.50983069314<br>中国 0.3632784823992<br>全面 0.33744171159299996<br>时代 0.3303798174336<br>人民 0.3125617860198<br>大会 0.2465050141716<br>共命运 0.228315420044<br>机遇期 0.222561778594<br>伟大旗帜 0.214452476432<br>方得 0.1954708653386<br>牢记 0.18611115603679998</p><h4 id="4-4-统计《三国演义》小说词频前20的词语"><a href="#4-4-统计《三国演义》小说词频前20的词语" class="headerlink" title="4.4 统计《三国演义》小说词频前20的词语"></a>4.4 统计《三国演义》小说词频前20的词语</h4><p><strong>源代码</strong></p><pre><code>import jiebaf = open(&quot;.\三国演义.txt&quot;, &quot;rb&quot;)txt = f.read()words = jieba.lcut(txt)counts = {}for word in words:    if len(word) == 1: # 排除单个字的分词结果        continue    else:        counts[word] = counts.get(word, 0) + 1items = list(counts.items())items.sort(key = lambda items: items[1], reverse = True)for i in range(20):    word, count = items[i]    print(&quot;{0:&lt;10}{1:&gt;5}&quot;.format(word, count))</code></pre><p><strong>运行结果</strong><br>曹操          940<br>孔明          828<br>将军          762<br>却说          648<br>玄德          568<br>关公          508<br>丞相          489<br>二人          465<br>不可          436<br>荆州          420<br>玄德曰         385<br>孔明曰         385<br>不能          383<br>如此          376<br>张飞          350<br>商议          344<br>如何          337<br>主公          328<br>军士          311</p><h4 id="4-5-统计《三国演义》分词后，词频数前50中，三国人物出现次数"><a href="#4-5-统计《三国演义》分词后，词频数前50中，三国人物出现次数" class="headerlink" title="4.5 统计《三国演义》分词后，词频数前50中，三国人物出现次数"></a>4.5 统计《三国演义》分词后，词频数前50中，三国人物出现次数</h4><p><strong>源代码</strong></p><pre><code>import jiebaf = open(&quot;.\三国演义.txt&quot;, &quot;rb&quot;)txt = f.read()words = jieba.lcut(txt)counts = {}for word in words:    if len(word) == 1: # 排除单个字的分词结果        continue    elif word == &quot;诸葛亮&quot; or word == &quot;孔明曰&quot;:        rword = &quot;孔明&quot;    elif word == &quot;关公&quot; or word == &quot;云长&quot;:        rword = &quot;关羽&quot;    elif word == &quot;玄德&quot; or word == &quot;玄德曰&quot;:        rword = &quot;刘备&quot;        elif word == &quot;孟德&quot; or word == &quot;丞相&quot;:        rword = &quot;曹操&quot;        else:        rword = word        counts[word] = counts.get(word, 0) + 1personlist = [&quot;孔明&quot;,&quot;曹操&quot;,&quot;张飞&quot;,&quot;刘备&quot;,&quot;关羽&quot;,&quot;孙权&quot;,&quot;吕布&quot;,&quot;鲁肃&quot;,&quot;周瑜&quot;,&quot;赵云&quot;,&quot;马超&quot;,&quot;姜维&quot;,&quot;魏延&quot;,&quot;庞统&quot;,&quot;董卓&quot;,&quot;袁绍&quot;,&quot;孟获&quot;,&quot;陆逊&quot;,&quot;孙尚香&quot;,&quot;孙坚&quot;,&quot;孙策&quot;,&quot;司马懿&quot;,&quot;曹丕&quot;,&quot;张辽&quot;]        items = list(counts.items())items.sort(key = lambda items: items[1], reverse = True)for i in range(50):    word, count = items[i]    if personlist.count(word) &gt; 0:        print(&quot;{0:&lt;10}{1:&gt;5}&quot;.format(word, count))</code></pre><p><strong>运行结果</strong><br>曹操          940<br>孔明          828<br>张飞          350<br>吕布          302<br>刘备          278<br>孙权          266<br>赵云          257<br>司马懿         222<br>周瑜          218<br>袁绍          191<br>马超          185<br>魏延          177</p><h3 id="5、图像处理实战"><a href="#5、图像处理实战" class="headerlink" title="5、图像处理实战"></a>5、图像处理实战</h3><h4 id="5-1-Pillow-扩展库"><a href="#5-1-Pillow-扩展库" class="headerlink" title="5.1 Pillow 扩展库"></a>5.1 Pillow 扩展库</h4><h5 id="5-1-1-Pillow扩展库概述"><a href="#5-1-1-Pillow扩展库概述" class="headerlink" title="5.1.1 Pillow扩展库概述"></a>5.1.1 Pillow扩展库概述</h5><ol><li>PIL（Python Imaging Library）是Python常用的图像处理库，功能非常强大，API 简单易用。</li><li>但 PIL 仅支持到 Python 2.7，加上年久失修。</li><li>一群志愿者在 PIL 的基础上创建了PIL的兼容版本，名字叫 Pillow，支持最新 Python 3.x，并加入许多新特性。</li><li>Pillow 提供了广泛的文件格式支持，强大的图像处理能力，主要包括图像储存、图像显示、格式转换以及基本的图像处理操作等。</li></ol><h5 id="5-1-2-Pillow-扩展库主要功能"><a href="#5-1-2-Pillow-扩展库主要功能" class="headerlink" title="5.1.2 Pillow 扩展库主要功能"></a>5.1.2 Pillow 扩展库主要功能</h5><ol><li><p>图像归档<br>对图像进行批处理、生成图像预览、图像格式转换等。</p></li><li><p>图像处理<br>图像基本处理、像素处理、颜色处理。</p></li></ol><h5 id="5-1-3-Pillow-扩展库主要操作"><a href="#5-1-3-Pillow-扩展库主要操作" class="headerlink" title="5.1.3 Pillow 扩展库主要操作"></a>5.1.3 Pillow 扩展库主要操作</h5><ol><li><p>在 PIL 中，任何一个图像文件都可以用 Image 对象表示。</p></li><li><p><strong>Image 类的图像读取和创建方法</strong></p></li></ol><table><thead><tr><th align="left">方法</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">Image.open(filename)</td><td align="left">根据参数加载图像文件</td></tr><tr><td align="left">Image.new(mode, size, color)</td><td align="left">根据给定参数创建一个新的图像</td></tr><tr><td align="left">Image.open(StringIO.StringIO(buffer)</td><td align="left">从字符串中获取图像</td></tr><tr><td align="left">Image.frombytes(mode, size, data)</td><td align="left">根据像素点data创建图像</td></tr><tr><td align="left">Image.verify()</td><td align="left">对图像文件完整性进行检查，返回异常</td></tr></tbody></table><ol start="3"><li><strong>Image 类有 4 个处理图片的常用属性</strong></li></ol><table><thead><tr><th align="left">属性</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">Image.format</td><td align="left">标识图像格式或来源，如果图像不是从文件读取，值是None</td></tr><tr><td align="left">Image.mode</td><td align="left">图像的色彩模式，“L”灰度图像、“RGB”真彩色图像、“CMYK”出版图像</td></tr><tr><td align="left">Image.size</td><td align="left">图像宽度和高度，单位是像素(px)，返回值是二元元组(tuple)</td></tr><tr><td align="left">Image.palette</td><td align="left">调色板属性，返回一个ImagePalette类型</td></tr></tbody></table><ol start="4"><li><strong>Image 类的图像转换和保存方法</strong></li></ol><table><thead><tr><th align="left">方法</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">Image.save(filename,format)</td><td align="left">将图像保存为filename文件名，format是图片格式</td></tr><tr><td align="left">Image.convert(mode)</td><td align="left">使用不同的参数，转换图像为新的模式</td></tr><tr><td align="left">Image.thumbnail(size)</td><td align="left">创建图像的缩略图，size是缩略图尺寸的二元元组</td></tr></tbody></table><p><strong>Pillow 扩展库缩略图示例</strong><br>生成图像的缩略图，其中（128，128）是缩略图的尺寸</p><pre><code>from PIL import Imageim = Image.open(&quot;\img0.jpg&quot;) im.thumbnail((128, 128)) im.save(&quot;img0TN&quot;, &quot;JPEG&quot;)</code></pre><ol start="5"><li>Image 类能够对每个像素点或者一幅 RGB 图像的每个通道单独进行操作，split() 方法能够将 RGB 图像各颜色通道提取出来，merge() 方法能够将各独立通道再合成一幅新的图像。</li></ol><table><thead><tr><th align="left">方法</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">Image.point(func)</td><td align="left">根据函数 func 功能对每个元素进行运算，返回图像副本</td></tr><tr><td align="left">Image.split()</td><td align="left">提取 RGB 图像的每个颜色通道，返回图像副本</td></tr><tr><td align="left">Image.merge(mode,bands)</td><td align="left">合并通道，采用 mode 色彩，bands 是新色的色彩通道</td></tr><tr><td align="left">Image.blend(img1, img2, alpha)</td><td align="left">将两幅图片 img1 和 img2 按照如下公式插值后生成新的图像： img1 * (1.0 - alpha) + img2* alpha</td></tr></tbody></table><ol start="6"><li><strong>颜色变换</strong></li></ol><pre><code>from PIL import Imageim = Image.open(&#39;img1.jpg&#39;)r,g,b = im.split()om = Image.merge(&quot;RGB&quot;, (b, g, r)) om.save(&#39;img1BGR.jpg&#39;)</code></pre><ol start="7"><li><strong>通道颜色变换</strong></li></ol><pre><code>from PIL import Imageim = Image.open(&#39;img1.jpg&#39;)r,g,b = im.split() # 获得RGB通道数据newr = g.point(lambda i: i*0.9)newg = g.point(lambda i: i&lt;200) # 选择g通道值低于200的像素点newb = b.point(lambda i: i)om = Image.merge(im.mode, (newr, newg, b)) # 将3个通道合成新图像om.save(&#39;img1Merge.jpg&#39;)</code></pre><ol start="8"><li><strong>PIL 库的 ImageFilter 提供的过滤图像方法</strong></li></ol><table><thead><tr><th align="left">方法</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">ImageFilter.BLUR</td><td align="left">图像的模糊效果</td></tr><tr><td align="left">ImageFilter.CONTOUR</td><td align="left">图像的轮廓效果</td></tr><tr><td align="left">ImageFilter.DETAIL</td><td align="left">图像的细节效果</td></tr><tr><td align="left">ImageFilter.EDGE_ENHANCE</td><td align="left">图像的边界加强效果</td></tr><tr><td align="left">ImageFilter.EDGE_ENHANCE_MORE</td><td align="left">图像的阈值边界加强效果</td></tr><tr><td align="left">ImageFilter.EMBOSS</td><td align="left">图像的浮雕效果</td></tr><tr><td align="left">ImageFilter.FIND_EDGES</td><td align="left">图像的边界效果</td></tr><tr><td align="left">ImageFilter.SMOOTH</td><td align="left">图像的平滑效果</td></tr><tr><td align="left">ImageFilter.SMOOTH_MORE</td><td align="left">图像的阈值平滑效果</td></tr><tr><td align="left">ImageFilter.SHARPEN</td><td align="left">图像的锐化效果</td></tr></tbody></table><ol start="9"><li><strong>Pillow 扩展库生成模糊图片</strong><br>生成图像模糊效果：套用滤镜</li></ol><pre><code>from PIL import Imagefrom PIL import ImageFilterim = Image.open(&quot;img1.jpg&quot;)om = im.filter(ImageFilter.BLUR) # 为图片使用模糊滤镜om.save(&#39;img1_blur.jpg&#39;)</code></pre><ol start="10"><li>PIL 库的 ImageEnhance 类提供了更高级的图像增强需求，提供调整色彩度、亮度、对比度、锐化等功能。</li></ol><table><thead><tr><th align="left">方法</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">ImageEnhance.enhance(factor)</td><td align="left">对选择属性的数值增强factor倍</td></tr><tr><td align="left">ImageEnhance.Color(im)</td><td align="left">调整图像的颜色平衡</td></tr><tr><td align="left">ImageEnhance.Contrast(im)</td><td align="left">调整图像的对比度</td></tr><tr><td align="left">ImageEnhance.Brightness(im)</td><td align="left">调整图像的亮度</td></tr><tr><td align="left">ImageEnhance.Sharpness(im)</td><td align="left">调整图像的锐度</td></tr></tbody></table><ol start="11"><li><strong>高级图像增强示例</strong></li></ol><pre><code>from PIL import Imagefrom PIL import ImageEnhanceim = Image.open(&quot;img1.jpg&quot;)# 调整图像对比度om = ImageEnhance.Contrast(im) # 图像对比度增强3倍om.enhance(3).save(&#39;img1_EnContras t.jpg’)</code></pre><h4 id="5-2-验证码生成实战"><a href="#5-2-验证码生成实战" class="headerlink" title="5.2 验证码生成实战"></a>5.2 验证码生成实战</h4><h5 id="5-2-1-题目要求"><a href="#5-2-1-题目要求" class="headerlink" title="5.2.1 题目要求"></a>5.2.1 题目要求</h5><p>利用 PIL 的 ImageDraw 提供的绘图方法生成如下图的验证码，要求字母随机、填充颜色随机。</p><h5 id="5-2-2-思路"><a href="#5-2-2-思路" class="headerlink" title="5.2.2 思路"></a>5.2.2 思路</h5><ol><li>使用库 PIL、random</li><li>画布：随机填充色</li><li>字母：指定字体（注意路径）、模糊滤镜 BLUR</li></ol><h5 id="5-2-3-验证码生成实战代码"><a href="#5-2-3-验证码生成实战代码" class="headerlink" title="5.2.3 验证码生成实战代码"></a>5.2.3 验证码生成实战代码</h5><pre><code>from PIL import Image, ImageDraw, ImageFont, ImageFilterimport random# 随机字母（65-90表示26个大写英文字母）def rndChar():    return chr(random.randint(65, 90))# 随机颜色1（验证码背景颜色）def rndColor():    return (random.randint(64, 255), random.randint(64, 255), random.randint(64, 255))# 随机颜色2（文字颜色）def rndColor2():    return (random.randint(32, 127), random.randint(32, 127), random.randint(32, 127))# 240*60width = 60 * 4height = 60image = Image.new(&#39;RGB&#39;, (width, height), (255, 255, 255))# 创建 Font 对象font = ImageFont.truetype(&#39;ARIALN.TTF&#39;, 36)# 创建 Draw 对象draw = ImageDraw.Draw(image)# 填充每个像素for x in range(width):    for y in range(height):        draw.point((x, y), fill=rndColor())# 输出文字for t in range(4):    draw.text((60 * t + 10, 10), rndChar(), font=font, fill=rndColor2())# 模糊image = image.filter(ImageFilter.BLUR)image.save(&#39;code.jpg&#39;, &#39;jpeg&#39;)</code></pre><p><strong>运行结果</strong><br><img src="/medias/1591338787052.png" alt="验证码生成"></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何阅读一本书</title>
      <link href="/2020/05/12/ru-he-yue-du-yi-ben-shu-du-shu/"/>
      <url>/2020/05/12/ru-he-yue-du-yi-ben-shu-du-shu/</url>
      
        <content type="html"><![CDATA[<h2 id="第一篇-阅读的层次"><a href="#第一篇-阅读的层次" class="headerlink" title="第一篇 阅读的层次"></a>第一篇 阅读的层次</h2><h3 id="第一章-阅读的活力与艺术"><a href="#第一章-阅读的活力与艺术" class="headerlink" title="第一章  阅读的活力与艺术"></a>第一章  阅读的活力与艺术</h3><h4 id="1-1-主动的阅读"><a href="#1-1-主动的阅读" class="headerlink" title="1.1 主动的阅读"></a>1.1 主动的阅读</h4><p>阅读可以是一件多少主动的事。</p><p>阅读越主动，效果越好。</p><h4 id="1-2-阅读的目标：为获得资讯而读，以及为求得理解而读"><a href="#1-2-阅读的目标：为获得资讯而读，以及为求得理解而读" class="headerlink" title="1.2  阅读的目标：为获得资讯而读，以及为求得理解而读"></a>1.2  阅读的目标：为获得资讯而读，以及为求得理解而读</h4><h4 id="1-3-阅读就是学习：指导型的学习，以及自我发现型的学习之间的差异"><a href="#1-3-阅读就是学习：指导型的学习，以及自我发现型的学习之间的差异" class="headerlink" title="1.3 阅读就是学习：指导型的学习，以及自我发现型的学习之间的差异"></a>1.3 阅读就是学习：指导型的学习，以及自我发现型的学习之间的差异</h4><p>只有<strong>真正学习到的人才是主动的学习者</strong>。 </p><p><strong>在阅读与倾听时我们必须要思考，就像我们在研究时一定要思考。</strong></p><p><strong>思考只是主动阅读的一部分。</strong>一个人还必须运用他的感觉与想像力。一个人必须观察，记忆，在看不到的地方运用想像力。这就是在非辅助型的学习中经常想要强调的任务，而在被教导型的阅读，或倾听学习中被遗忘或忽略的过程。</p><p>阅读的艺术包括了所有<strong>非辅助型自我发现学习的技巧</strong>：<strong>敏锐的观察、灵敏可靠的记忆、想像的空间，</strong>再者当然就是<strong>训练有素的分析、省思能力</strong>。这么说的理由在于：<strong>阅读也就是一种发现虽然那是经过帮助，而不是未经帮助的一个过程</strong>。</p><h4 id="1-4-老师的出席与缺席"><a href="#1-4-老师的出席与缺席" class="headerlink" title="1.4 老师的出席与缺席"></a>1.4 老师的出席与缺席</h4><p>如果你问一本书一个问题，你就必须自己回答这个问题。在这样的情况下，这本书就跟自然或世界一样。当你提出问题时，只有等你自己作了思考与分析之后，才会在书本上找到答案。</p><p>对我们这些已经不在学校的人来说，当我们试着要读一本既非主修也非选修的书籍时，也就是我们的成人教育要完全依赖书籍本身的时候，我们就不能再有老师的帮助了。因此，如果我们打算继续学习与发现，我们就要懂得如何让书本来教导我们。</p><h3 id="第二章-阅读的层次"><a href="#第二章-阅读的层次" class="headerlink" title="第二章  阅读的层次"></a>第二章  阅读的层次</h3><p><strong>四种层次的阅读</strong></p><h4 id="2-1-第一种层次的阅读—–基础阅读-elementary-reading"><a href="#2-1-第一种层次的阅读—–基础阅读-elementary-reading" class="headerlink" title="2.1 第一种层次的阅读—–基础阅读 (elementary reading)"></a>2.1 第一种层次的阅读—–基础阅读 (elementary reading)</h4><p>这个阅读层次的学习通常是在小学时完成的。<strong>在这个层次的阅读中，要问读者的问题是：”这个句子在说什么？”</strong></p><h4 id="2-2-第二种层次的阅读—–检视阅读-inspectional-reading"><a href="#2-2-第二种层次的阅读—–检视阅读-inspectional-reading" class="headerlink" title="2.2 第二种层次的阅读—–检视阅读 (inspectional reading)"></a>2.2 第二种层次的阅读—–检视阅读 (inspectional reading)</h4><p>检视阅读是系统化略读 (skimming systematically)的一门艺术。在这个层次的阅读上，你的目标是从表面去观察这本书，学习到光是书的表象所教给你的一切。在<strong>这个层次要问的典型问题就是：”这本书在谈什么？”</strong>这是个表象的问题。还有些类似的问题是：<strong>“这本书的架构如何？”</strong>或是：<strong>“这本书包含哪些部分？”</strong></p><p>用检视阅读读完一本书之后，无论你用了多短的时间，你都该回答得出这样的问题：＂这是哪一类的书——小说、历史，还是科学论文？”</p><h4 id="2-3-第三种层次的阅读—–分析阅读-analytical-reading"><a href="#2-3-第三种层次的阅读—–分析阅读-analytical-reading" class="headerlink" title="2.3 第三种层次的阅读—–分析阅读(analytical reading)"></a>2.3 第三种层次的阅读—–分析阅读(analytical reading)</h4><p><strong>分析阅读</strong>就是<strong>全盘的阅读、完整的阅读，或是说优质的阅读</strong>——你能做到的最好的阅读方式。我们要在这里强调的是，<strong>分析阅读永远是一种专注的活动</strong>。<strong>在这个层次的阅读中，读者会紧抓住一本书——这个比喻蛮恰当的——一直要读到这本书成为他自己为止</strong>。<strong>分析阅读就是要咀嚼与消化一本书</strong>。</p><p>如果你的目标只是获得资讯或消遣，就完全没有必要用到分析阅读。分析阅读就是特别在追寻理解的。</p><h4 id="2-4-第四种层次的阅读—–主题阅读-syntopical-reading"><a href="#2-4-第四种层次的阅读—–主题阅读-syntopical-reading" class="headerlink" title="2.4 第四种层次的阅读—–主题阅读(syntopical reading)"></a>2.4 第四种层次的阅读—–主题阅读(syntopical reading)</h4><p>主题阅读是所有阅读中最复杂也最系统化的阅读。<strong>主题阅读是最主动、也最花力气的一种阅读</strong>。主题阅读不是个轻松的阅读艺术，规则也并不广为人知。虽然如此，<strong>主题阅读却可能是所有阅读活动中最有收获的</strong>。就是因为<strong>你会获益良多</strong>，所以<strong>绝对值得你努力学习如何做到这样的阅读</strong>。</p><h3 id="第三章-阅读的第一个层次：基础阅读"><a href="#第三章-阅读的第一个层次：基础阅读" class="headerlink" title="第三章  阅读的第一个层次：基础阅读"></a>第三章  阅读的第一个层次：基础阅读</h3><h4 id="3-1-学习阅读的阶段"><a href="#3-1-学习阅读的阶段" class="headerlink" title="3.1 学习阅读的阶段"></a>3.1 学习阅读的阶段</h4><p>1）<strong>第一个阶段</strong>，称为“<strong>阅读准备阶段</strong>” (reading readiness)。</p><p>2）<strong>第二个阶段</strong>，<strong>孩子会学习读一些简单的读物</strong>。这个阶段要结束时，小学生应该就能自己阅读简单的书，而且很喜欢阅读了。</p><p>3）<strong>第三个阶段</strong>，<strong>特征是快速建立字汇的能力</strong>，所用的方法是从上下文所提供的线索，＂揭发”不熟悉的字眼。</p><p>4）<strong>第四个阶段</strong>，<strong>特征是精练与增进前面所学的技巧</strong>。最重要的是，学生开始能消化他的阅读经验——从一本书所提出来的一个观点转化到另一个观点，在同一个主题上，对不同的作者所提出来的观点作比较。这是阅读的成熟阶段，应该是一个青少年就该达到的境界，也是终其一生都该持续下去的。</p><h4 id="3-2-阅读的阶段与层次"><a href="#3-2-阅读的阶段与层次" class="headerlink" title="3.2 阅读的阶段与层次"></a>3.2 阅读的阶段与层次</h4><ol><li><p>基础阅读的<strong>第一个阶段，阅读准备阶段</strong>——相当于学前教育或幼稚园的学习经验。 </p></li><li><p>基础阅读的<strong>第二阶段，认字</strong>——相当于一年级学生典型的学习经验（尽管相当多正常的孩子在某方面来说并非都很“经典”。</p></li><li><p>基础阅读的<strong>第三个阶段，字汇的增长及对课文的运用</strong>——通常是（但非全面性，就算正常孩子也一样）在四年级结束时就学会的方法。 </p></li><li><p>基础阅读的<strong>第四个阶段</strong>，也就是最后一个阶段，到这个时期，学生要从小学或初中毕业了。</p></li></ol><p>只有当一个孩子精通了基础阅读的四个阶段，才是他准备好往更高层次的阅读迈进的时候。只有当他能自己阅读时，才能够自已开始学习。也只有这样，他才能变成一个真正优秀的阅读者。</p><h4 id="3-3-更高层次的阅读与高等教育"><a href="#3-3-更高层次的阅读与高等教育" class="headerlink" title="3.3 更高层次的阅读与高等教育"></a>3.3 更高层次的阅读与高等教育</h4><p>一个优秀的大学，就算什么也没贡献，也该培育出能进行主题阅读的读者。大学的文凭应该代表着一般大学毕业生的阅读水平．不但能够阅读任何一种普通的资料，还能针对任何一种主题做个人的研究（这就是在所有阅读中，主题阅读能让你做到的事）。然而，通常大学生要在毕业以后，再读三四年的时间才能达到这样的程度，并且还不见得一定达到。</p><h4 id="3-4-阅读与民主教育的理念"><a href="#3-4-阅读与民主教育的理念" class="headerlink" title="3.4 阅读与民主教育的理念"></a>3.4 阅读与民主教育的理念</h4><p>无限制的受教育机会是一个社会能提供给人民最有价值的服务——或说得正确一点，只有当一个人的自我期许，能力与需要受限制时，教育机会才会受到限制。</p><p>我们一定要比一个人人识字的国家更进一步。我们的国人应该变成一个个真正”有能力”的阅读者，能够真正认知”有能力”这个字眼中的涵义。达不到这样的境界，我们就无法应付未来世界的需求。</p><h3 id="第四章-阅读的第二个层次：检视阅读"><a href="#第四章-阅读的第二个层次：检视阅读" class="headerlink" title="第四章  阅读的第二个层次：检视阅读"></a>第四章  阅读的第二个层次：检视阅读</h3><p><strong>检视阅读，才算是真正进入阅读的层次。</strong></p><h4 id="4-1-检视阅读一：有系统的略读或粗读"><a href="#4-1-检视阅读一：有系统的略读或粗读" class="headerlink" title="4.1 检视阅读一：有系统的略读或粗读"></a>4.1 检视阅读一：有系统的略读或粗读</h4><p> <strong>略读或粗读是检视阅读的第一个子层次</strong>。你脑中的<strong>目标是要发现这本书值不值得多花时间仔细阅读</strong>。其次，就算你决定了不再多花时间仔细阅读这本书，略读也能告诉你许多跟这本书有关的事。</p><p><strong>略读如何去做的一些建议</strong></p><ol><li><p><strong>先看书名页，然后如果有序就先看序</strong><br>要很快地看过去。 特别注意副标题，或其他的相关说明或宗旨，或是作者写作本书的特殊角度。在完成这个步骤之前，你对这本书的主题已经有概念了。 如果你愿意，你会暂停一下，在你脑海中将这本书归类为某个特定的类型。而在那个类型中，已经包含了哪些书。</p></li><li><p><strong>研究目录页，对这本书的基本架构做概括性的理解</strong><br>这就像是在出发旅行之前，要先看一下地图一样。很惊讶的是，除非是真的要用到那本书了，许多人连目录页是看都不看一眼的。<br>通常，一本书，特别是一些论说性的书都会有目录，但是有时小说或诗集也会写上一整页的纲要目录，分卷分章之后再加许多小节的副标，以说明题旨。目录纲要还是很有价值的，在你开始阅读整本书之前，你应该先仔细阅读目录才对。</p></li><li><p><strong>如果书中附有索引，也要检阅一下</strong><br>索引快速评估一下这本书涵盖了哪些议题的范围，以及所提到的书籍种类与作者等等。如果你发现列举出来的哪一条词汇很重要， 至少要看一下引用到这个词目的某几页内文。你所阅读的段落很可能就是个要点——这本书的关键点——或是关系到作者意图与态度的新方法。</p></li><li><p><strong>如果那是本包着书衣的新书，不妨读一下出版者的介绍</strong><br>作者尽力将书中的主旨正确地摘要出来，已经不是稀奇的事了。</p></li><li><p><strong>从一本书的目录开始挑几个看来跟主题息息相关的篇章来看</strong><br>如果这些篇章在开头或结尾有摘要说明（很多会有），就要仔细地阅读这些说明。</p></li><li><p><strong>把书打开来，东翻翻西翻翻，念个一两段，有时候连续读几页，但不要太多</strong><br>就用这样的方法<strong>把全书翻过一遍，随时寻找主要论点的讯号</strong>，<strong>留意主题的基本脉动</strong>。最重要的是，<strong>不要忽略最后的两三页</strong>。就算最后有后记，一本书最后结尾的两三页也还是不可忽视的。很少有作者能拒绝这样的诱惑，而不是结尾几页将自己认为既新又重要的观点重新整理一遍的。虽然有时候作者自己的看法不一定正确，但你不应该错过这个部分。</p></li></ol><p>附带一提的是，这是一种非常主动的阅读。<strong>一个人如果不够灵活，不能够集中精神来阅读，就没法进行检视阅读。</strong></p><p><strong>问题：</strong><br>有多少次你在看一本好书的时候，翻了好几页，脑海却陷人了白日梦的状态中，等清醒过来，竟完全不明白自己刚看的那几页在说些什么？</p><p><strong>解决方法：</strong><br>你可以<strong>把自己想成是一个侦探，在找寻一本书的主题或思想的线索</strong>。<strong>随时保持敏感，就很容易让一切状况清楚</strong>。留意我们所提出的建议，会帮助你保持这样的态度。你会很惊讶地发现自己节省了更多时间，高兴自己掌握了更多重点，然后轻松地发现原来阅读是比想像中还更要简单的一件事。</p><h4 id="4-2-检视阅读二：粗浅的阅读"><a href="#4-2-检视阅读二：粗浅的阅读" class="headerlink" title="4.2 检视阅读二：粗浅的阅读"></a>4.2 检视阅读二：粗浅的阅读</h4><p><strong>什么叫对的方向？</strong><br>答案是一个很重要又有帮助的阅读规则，但却经常被忽略。</p><p>这个规则很简单：<strong>头一次面对一本难读的书的时候，从头到尾先读完一遍，碰到不懂的地方不要停下来查询或思索。</strong></p><p><strong>只注意你能理解的部分，不要为一些没法立即了解的东西而停顿</strong>。<strong>继续读下去，略过那些不懂的部分，很快你会读到你看得懂的地方</strong>。<strong>集中精神在这个部分。继续这样读下去。将全书读完，不要被一个看不懂的章节、注解、评论或参考资料阻挠或泄气</strong>。如果你让自已被困住了，如果你容许自已被某个顽固的段落绑住了，你就是被打败了。在大多数情况里，你一旦和它纠缠，就很难脱困而出。在读第二遍的时候，你对那个地方的了解可能会多一些，但是在那之前，你必须至少将这本书先从头到尾读一遍才行。</p><p>你从头到尾读了一遍之后的了解——就算只有50%或更少——能帮助你在后来重读第一次略过的部分时，增进理解。能帮助你在后来重读第一次略过的部分时，增进理解。就算你不重读，对一本难度很高的书了解了一半，也比什么都不了解来得要好些——如果你让自己在一碰上困难的地方就停住，最后就可能对这本书真的一无所知了。</p><p>这个规则也适用于论说性的作品。事实上，第一次看这样一本书的时候要粗浅阅读的这个规则，在你违反的时候正可以不证自明。</p><p>如果你坚持要了解每一页的意义，才肯再往下读，那你一定读不了多少。</p><h4 id="4-3-阅读的速度"><a href="#4-3-阅读的速度" class="headerlink" title="4.3 阅读的速度"></a>4.3 阅读的速度</h4><p>检视阅读是一种在有限的时间当中，充分了解一本书的艺术。检视阅读的两个方式都需要快速地阅读。一个熟练的检视阅读者想要读一本书时，不论碰到多难读或多长的书，都能够很快地运用这两种方式读完。</p><p>一个很好的速读课程应该要教你不同的阅读速度，而<strong>不是一味求快</strong>，而<strong>忽略了你目前能掌握的程度</strong>。应该是<strong>依照读物的性质与复杂程度，而让你用不同的速度来阅读。</strong></p><p>我们的重点真的很简单。许多书其实是连略读都不值得的，另外一些书只需要快速读过就行了。有少数的书需要用某种速度，通常是相当慢的速度，才能完个理解。<strong>一个只需要快速阅读的书却用很慢的速度来读</strong>，就是在浪费时间，这时<strong>速读的技巧就能帮你解决问题</strong>。但这只是阅读问题中的一种而已。要了解一本难读的书，其间的障碍，非一般所谓生理或心理障碍所能比拟甚或涵盖。</p><p>所谓阅读速度，理想上来说，<strong>不只是要能读得快，还要能用不同的速度来阅读</strong>——<strong>要知道什么时候用什么样的速度是恰当的</strong>。<strong>检视阅读是一种训练有素的快速阅读</strong>，但这不只是因为你读的速度快——虽然你真的读得很快而是因为在检视阅读时，<strong>你只读书中的一小部分，而且是用不同的方式来读，不一样的目标来读</strong>。</p><p><strong>分析阅读通常比检视阅读来得慢一些</strong>，但就算你拿到一本书要做分析阅读，也不该用同样的速度读完全书。每一本书，不论是多么难读的书，在无关紧要的间隙部分就可以读快一点。而一本好书，总会包含一些比较困难，应该慢慢阅读的内容。</p><h4 id="4-4-逗留与倒退"><a href="#4-4-逗留与倒退" class="headerlink" title="4.4 逗留与倒退"></a>4.4 逗留与倒退</h4><p>许多人会从最初学会阅读之后，多年一直使用“半出声”(sub-vocalize)的方式来阅读。此外，拍摄下来的眼睛在活动时的影片，显示年轻或未受过训练的阅读者，在阅读一行字的时候会在五六个地方发生“逗留”(fixate)现象。（眼睛在移动时看不见，只有停下来时才能看见。）因此，他们在读这一行字的时候，只能间隔着看到一个个单字或最多两三个字的组合。更糟的是，这些不熟练的阅读者在每看过两三行后，眼睛就自然地“倒退”(regress)到原点——也就是说，他们又会倒退到先前读过的句子与那一行去了。</p><p>许多人在阅读时会“逗留”会“倒退”，因而使他们的速度慢下来的习惯。</p><p>要矫正眼睛逗留于一点的工具有很多种，有些很复杂又很昂贵。无论如何，任何复杂的工具其实都比不上你的一双手来得有用，<strong>你可以利用双手训练自己的眼睛，跟着章节段落移动得越来越快</strong>。你可以自己做这样的训练：<strong>将大拇指与食指、中指合并在一起，用这个“指针”顺着一行一行的字移动下去，速度要比你眼睛感觉的还要快一点</strong>。<strong>强迫自己的眼睛跟着手部的动作移动</strong>。一旦你的眼睛能跟着手移动时，你就能读到那些字句了。继续练习下去，继续增快手的动作，等到你发觉以前，你的速度已经可以比以前快两三倍了。</p><h4 id="4-5-理解的问题"><a href="#4-5-理解的问题" class="headerlink" title="4.5 理解的问题"></a>4.5 理解的问题</h4><p><strong>一个优秀的阅读者就是读得很主动，很专心。</strong></p><p>专心并不一定等于理解力——如果大家对“理解力”并没有误解的话。理解力，是比回答书本内容一些简单问题还要多一点的东西。那种有限的理解力，不过是小学生回答”这是在说什么？”之类问题的程度而已。一个读者要能够正确地回答许多更进一步的问题，才表示有更高一层的理解力，而这是速读课程所不要求的东西，也几乎没有人指导要如何回答这类的问题。</p><p>速读的问题就出在理解力上。事实上，这里所谓的理解力是超越基础阅读层次以上的理解力，也是造成问题的根源。大多数的速读课程都没有包括这方面的指导。因此，有一点值得在这里强调的是，本书之所以想要改进的．正是这一种阅读的理解力。没有经过分析阅读，你就没法理解一本书。正如我们前面所言，分析阅读，是想要理解（或了解）一本书的基本要件。</p><h4 id="4-6-检视阅读的摘要"><a href="#4-6-检视阅读的摘要" class="headerlink" title="4.6 检视阅读的摘要"></a>4.6 检视阅读的摘要</h4><p><strong>阅读的速度并非只有单一的一种，重点在如何读出不同的速度感，知道在阅读某种读物时该用什么样的速度</strong>。超快的速读法是引人怀疑的一种成就，那只是表现你在阅读一种根本不值得读的读物。更好的秘方是：<strong>在阅读一本书的时候，慢不该慢到不值得，快不该快到有损于满足与理解。</strong>不论怎么说，阅读的速度，不论是快还是慢，只不过是阅读问题一个微小的部分而已。</p><p>当你并<strong>不清楚手边的一本书是否值得细心阅读时</strong>（经常发生这种情况），<strong>必须先略读一下</strong>。一般来说，<strong>就算你想要仔细阅读的书也要先略读一下，从基本架构上先找到一些想法。</strong></p><p>最后，<strong>在第一次阅读一本难读的书时，不要企图了解每一个字句</strong>。这是最最重要的一个规则。<strong>这也是检视阅读的基本概念。</strong>不要害怕，或是担忧自己似乎读得很肤浅。就算是<strong>最难读的书也快快地读一遍</strong>。当你再读第二次时，你就已经准备好要读这本书了。</p><h3 id="第五章-如何做一个自我要求的读者"><a href="#第五章-如何做一个自我要求的读者" class="headerlink" title="第五章 如何做一个自我要求的读者"></a>第五章 如何做一个自我要求的读者</h3><p><strong>在阅读的时候想要保持清醒，或昏昏入睡，主要看你的阅读目标是什么</strong>。如果你的<strong>阅读目标是获得利益</strong>——<strong>不论是心灵或精神上的成长</strong>——<strong>你就得保持清醒</strong>。这也意味着<strong>在阅读时要尽可能地保持主动</strong>，同时还<strong>要做一番努力</strong>——而这番努力是<strong>会有回馈</strong>的。</p><h4 id="5-1-主动的阅读基础：一个阅读者要提出的四个基本问题"><a href="#5-1-主动的阅读基础：一个阅读者要提出的四个基本问题" class="headerlink" title="5.1 主动的阅读基础：一个阅读者要提出的四个基本问题"></a>5.1 主动的阅读基础：一个阅读者要提出的四个基本问题</h4><p><strong>主动阅读的核心</strong>：你在阅读时要<strong>提出问题</strong>来——<strong>在阅读的过程中，你自己必须尝试去回答的问题。</strong></p><ol><li><p><strong>整体来说，这本书到底在谈些什么？</strong><br>你一定要<strong>想办法找出这本书的主题</strong>，作者<strong>如何依次发展这个主题，如何逐步从核心主题分解出从打属的关键议题来</strong>。</p></li><li><p><strong>作者细部说了什么，怎么说的？</strong><br>你一定要想办法<strong>找出主要的想法、声明与论点</strong>。这些<strong>组合成作者想要传达的特殊讯息</strong>。</p></li><li><p><strong>这本书说得有道理吗？是全部有道理，还是部分有道理？</strong><br>除非你能回答前两个问题，否则你没法回答这个问题。在你<strong>判断这本书是否有道理之前</strong>，你必须先<strong>了解整本书在说些什么才行</strong>。然而，等你了解了一本书，如果你又读得很认真的话，你会觉得有责任为这本书做个自己的判断。光是知道作者的想法是不够的。</p></li><li><p><strong>这本书跟你有什么关系？</strong><br>如果这本书给了你一些资讯，你一定要问问这些资讯有什么意义。为什么这位作者会认为知道这件事很重要？你真的有必要去了解吗？如果这本书不只提供了资讯，还启发了你，就更有必要找出其他相关的、更深的含意或建议，以获得更多的启示。</p></li></ol><p>任何一种超越基础阅读的阅读层次，<strong>核心就在你要努力提出问题</strong>（然后尽你可能地找出答案）。这是绝不可或忘的原则。这也是有自我要求的阅读者，与没有自我要求的阅读者之间，有天壤之别的原因。后者提不出问题——当然也得不到答案。</p><h4 id="5-2-如何让一本书真正属于你自己"><a href="#5-2-如何让一本书真正属于你自己" class="headerlink" title="5.2 如何让一本书真正属于你自己"></a>5.2 如何让一本书真正属于你自己</h4><p>要真正完全拥有一本书，必须把这本书变成你自己的一部分才行，而要让你成为书的一部分最好的方法——书成为你的一部分和你成为书的一部分是同一件事——就是要去写下来。</p><p><strong>为什么对阅读来说，在书上做笔记是不可或缺的事？</strong><br><strong>第一，那会让你保持清醒——不只是不昏睡，还是非常清醒。</strong><br><strong>其次，阅读，如果是主动的，就是一种思考，而思考倾向于用语言表达出来——不管是用讲的还是写的。</strong>一个人如果说他知道他在想些什么，却说不出来，通常是他其实并不知道自己在想些什么。<br><strong>第三，将你的感想写下来，能帮助你记住作者的思想。</strong></p><p>阅读一本书应该像是你与作者之间的对话。有关这个主题，他知道的应该比你还多，否则你根本用不着去跟这本书打交道了。但是了解是一种双向沟通的过程，学生必须向自己提问题，也要向老师提问题。一旦他了解老师的说法后，还要能够跟老师争辩。在书上做笔记，其实就是在表达你跟作者之间相异或相同的观点。这是你对作者所能付出的最高的敬意。</p><h4 id="5-3-做笔记的多种方法"><a href="#5-3-做笔记的多种方法" class="headerlink" title="5.3 做笔记的多种方法"></a>5.3 做笔记的多种方法</h4><ol><li><p><strong>画底线</strong>——在主要的重点，或<strong>重要又有力量的句子下画线</strong></p></li><li><p><strong>在画底线处的栏外再加画一道线</strong>——把你已经<strong>画线的部分再强调一遍</strong>，或是某一段很重要，但要画底线太长了，便在这一整段外加上一个记号</p></li><li><p><strong>在空白处做星号或其他符号</strong>——要<strong>慎用</strong>，<strong>只用来强调书中十来个最重要的声明或段落</strong>即可。你可能想要将<strong>做过这样记号的地方每页折一个角，或是夹一张书签</strong>。这样你随时从书架上拿起这本书，打开你做记号的地方，就能唤醒你的记忆</p></li><li><p><strong>在空白处编号</strong>——作者的某个论点发展出一连串的重要陈述时，可以<strong>做顺序编号</strong></p></li><li><p><strong>在空白处记下其他的页码</strong>——强调作者在书中其他部分也有过同样的论点，或相关的要点，或是与此处观点不同的地方。这样做能让散布全书的想法统一集中起来。许多读者会用 Cf 这样的记号，表示比较或参照的意思</p></li><li><p><strong>将关键字或句子圈出来</strong>——这跟画底线是同样的功能。</p></li><li><p><strong>在书页的空白处做笔记</strong>——在阅读某一章节时，你可能会有些问题（或答案），在空白处记下来，这样可以帮你回想起你的问题或答案。你也可以将复杂的论点简化说明在书页的空白处。或是记下全书所有主要论点的发展顺序。书中最后一页可以用来作为个人的索引页，将作者的主要观点依序记下来。</p></li></ol><p><strong>书前的空白页最好是用来记载你的思想</strong>。 你读完一本书，在最后的空白页写下个人的索引后，再翻回前面的空白页，试着将全书的大纲写出来，用不着一页或一个重点一个重点地写（你已经在书后的空白页做过这件事了），试着将全书的整体架构写出来，列出基本的大纲与前后篇章秩序。</p><h4 id="5-4-三种做笔记的方法"><a href="#5-4-三种做笔记的方法" class="headerlink" title="5.4 三种做笔记的方法"></a>5.4 三种做笔记的方法</h4><h5 id="5-4-1-结构笔记-structural-note-making"><a href="#5-4-1-结构笔记-structural-note-making" class="headerlink" title="5.4.1 结构笔记(structural note-making)"></a>5.4.1 结构笔记(structural note-making)</h5><p>用检视阅读来读一本书时，可能没有太多时间来做笔记。检视阅读，就像我们前面所说过的，所花的时间永远有限。虽然如此，你在这个层次阅读时，还是会提出一些重要的问题，而且最好是在你记忆犹新时，将答案也记下来——只是有时候不见得能做得到。</p><p><strong>在检视阅读中，要回答的问题是：</strong><br><strong>第一，这是什么样的一本书？</strong><br><strong>第二，整本书在谈的是什么？</strong><br><strong>第三，作者是借着怎样的整体架构，来发展他的观点或陈述他对这个主题的理解？</strong></p><p><strong>你应该做下笔记，把这些问题的答案写下来。</strong>尤其如果你知道终有一天，或许是几天或几个月之后，你会重新拿起这本书做分析阅读时，就更该将问题与答案先写下来。要做这些笔记最好的地方是目录页，或是书名页，这些是我们前面所提的笔记方式中没有用到的页数。</p><p>在这里要注意的是，这些笔记主要的重点是全书的架构，而不是内容——至少不是细节。因此我们称这样的笔记为<strong>结构笔记(structural note-making)</strong>。</p><h5 id="5-4-2-概念笔记-conceptual-note-making"><a href="#5-4-2-概念笔记-conceptual-note-making" class="headerlink" title="5.4.2 概念笔记(conceptual note-making)"></a>5.4.2 概念笔记(conceptual note-making)</h5><p>在检视阅读的过程中，特别是又长又难读的书，你有可能掌握作者对这个主题所要表达的一些想法。但是通常你做不到这一点。 而除非你真的再仔细读一遍全书，否则就不该对这本书立论的精确与否、有道理与否遽下结论。 之后，等你做分析阅读时，关于这本书准确性与意义的问题，你就要提出答案了。<strong>在这个层次的阅读里，你做的笔记就不再是跟结构有关，而是跟概念有关了</strong>。这些概念是作者的观点，而<strong>当你读得越深越广时，便也会出现你自己的观点了</strong>。</p><p><strong>结构笔记</strong>与<strong>概念笔记(conceptual note-making)</strong>是截然不同的。而当你同时在读好几本书，在做主题阅读——就同一个主题，阅读许多不同的书时，你要做的又是什么样的笔记呢？同样的，这样的笔记也应该是概念性的。你在书中空自处所记下的页码不只是本书的页码，也会有其他几本书的页码。</p><h5 id="5-4-3-辩证笔记-dialectical-note-making"><a href="#5-4-3-辩证笔记-dialectical-note-making" class="headerlink" title="5.4.3 辩证笔记(dialectical note making)"></a>5.4.3 辩证笔记(dialectical note making)</h5><p>对一个已经熟练同时读好几本相同主题书籍的专业阅读者来说，还有一个更高层次的记笔记的方法。那就是针对一场讨论情境的笔记——这场讨论是由许多作者所共同参与的，而且他们可能根本没有觉察自己的参与。我们喜欢称这样的笔记为<strong>辩证笔记(dialecticalnote making)</strong>。因为<strong>这是从好多本书中摘要出来的</strong>，而不是一本，因而<strong>通常需要用单独的一张纸来记载</strong>。这时，我们会再用上概念的结构——就一个单一主题，把所有相关的陈述和疑问顺序而列。</p><h4 id="5-5-培养阅读的习惯"><a href="#5-5-培养阅读的习惯" class="headerlink" title="5.5 培养阅读的习惯"></a>5.5 培养阅读的习惯</h4><p><strong>要养成习惯，除了不断地运作练习之外，别无他法。</strong></p><p><strong>在你养成习惯的前后，最大的差异就在于阅读能力与速度的不同</strong>。经过练习后，同一件事，你会做得比刚开始时要好很多。这也就是俗话说的<strong>熟能生巧</strong>。<strong>一开始你做不好的事，慢慢就会得心应手</strong>，像是自然天生一样。你好像生来就会做这件事，就跟你走路或吃饭一样自然。这也是为什么说习惯是第二天性的道理。</p><h4 id="5-6-由许多规则中养成一个习惯"><a href="#5-6-由许多规则中养成一个习惯" class="headerlink" title="5.6 由许多规则中养成一个习惯"></a>5.6 由许多规则中养成一个习惯</h4><p><strong>一定要学会忘掉那些分开的步骤，才能表现出整体的动作，而每一个单一的步骤都还要确实表现得很好。</strong> 但是，为了<strong>要忘掉这些单一的动作</strong>，一开始你<strong>必须先分别学会每一个单一的动作</strong>。只有这样，你<strong>才能将所有的动作连结起来。</strong></p><p>阅读就跟滑雪一样，除非你对每一个步骤都很熟练之后，你才能将所有不同的步骤连结起来，变成一个复杂却和谐的动作。你无法压缩其中不同的部分，好让不同的步骤立刻紧密连结起来。你在做这件事时，每一个分开来的步骤都需要你全神贯注地去做。<strong>在你分别练习过这些分开来的步骤后</strong>，你不但<strong>能放下你的注意力</strong>，<strong>很有效地将每个步骤做好</strong>，还<strong>能将所有的动作结合起来，表现出一个整体的顺畅行动</strong>。</p><p><strong>一个人只要学习过一种复杂的技巧，就会知道要学习一项新技巧，一开始的复杂过程是不足为惧的</strong>。也知道他用不着担心这些个别的行动，因为只有<strong>当他精通这些个别的行动时，才能完成一个整体的行动</strong>。</p><p>规则的多样化，意味着要养成一个习惯的复杂度，而非表示要形成许多个不同的习惯。在到达一个程度时，每个分开的动作自然会压缩、连结起来，变成一个完整的动作。当<strong>所有相关动作都能相当自然地做出来时，你就已经养成做这件事的习惯了</strong>。然后你就能想一下如何读一本以前你觉得对自己来说很困难的书。<strong>一开始时，学习者只会注意到自己与那些分开来的动作。等所有分开的动作不再分离，渐渐融为一体时，学习者便能将注意力转移到目标上，而他也具备了要达成目标的能力了</strong>。</p><p>要学习做一个很好的阅读者并不容易。而且不单单只是阅读．还是分析式的阅读。</p><h2 id="第二篇-阅读的第三个层次：分析阅读"><a href="#第二篇-阅读的第三个层次：分析阅读" class="headerlink" title="第二篇 阅读的第三个层次：分析阅读"></a>第二篇 阅读的第三个层次：分析阅读</h2><h3 id="第六章-一本书的分类"><a href="#第六章-一本书的分类" class="headerlink" title="第六章 一本书的分类"></a>第六章 一本书的分类</h3><p>当我们提到读书的时候，所说明的<strong>阅读规则也同样适用于其他比较易于阅读的资料</strong>。虽然这些规则程度不尽相当，应用在后者身上时，有时候作用不尽相同，但是<strong>只要你拥有这些技巧，懂得应用，总可以比较轻松。</strong></p><h4 id="6-1-书籍分类的重要性（规则一：分类）"><a href="#6-1-书籍分类的重要性（规则一：分类）" class="headerlink" title="6.1 书籍分类的重要性（规则一：分类）"></a>6.1 书籍分类的重要性（规则一：分类）</h4><p><strong>分析阅读的第一个规则可以这么说：</strong></p><p><strong>规则一：</strong><br><strong>你一定要知道自己在读的是哪一类书，而且要越早知道越好。最好早在你开始阅读之前就先知道。</strong></p><p>一开始时，你要<strong>先检视这本书用检视阅读先浏览一遍。</strong>你读读书名、副标题、目录，然后最少要看看作者的序言、摘要介绍及索引。如果这本书有书衣，要看看出版者的宣传文案。这些都是作者在向你传递讯号，让你知道风朝哪个方向吹。如果你不肯停、看、听，那也不是他的错。</p><h4 id="6-2-从一本书的书名中你能学到什么"><a href="#6-2-从一本书的书名中你能学到什么" class="headerlink" title="6.2 从一本书的书名中你能学到什么"></a>6.2 从一本书的书名中你能学到什么</h4><p>如果读者忽略了这一切，却答不出“这是－本什么样的书”的问题，那他只该责怪自己了。事实上，他只会变得越来越困惑。如果他不能回答这个问题，如果他从没问过自己这个问题，他根本就不可能回答随之而来的，关于这本书的其他问题。</p><p>阅读书名很重要，但还不够。除非你能在心中有一个分类的标准，否则世上再清楚的书名，再详尽的目录、前言，对你也没什么帮助。</p><p><strong>只有当你自己心中有一个分类的标准，你才能做明智的判断。</strong>换句话说，如果<strong>你想简单明白地运用这个规则，那就必须先使这个规则更简单明白一些</strong>。只有当你在不同的书籍之间能找出区别，并且定出一些合理又经得起时间考验的分类时，这个规则才会更简单明白一些。</p><p>我们要确定的是一个基本的分类原则，这个原则适用于所有的论说性作品。这也就是用来区分理论性与实用性作品的原则。</p><h4 id="6-3-实用性vs-理论性作品"><a href="#6-3-实用性vs-理论性作品" class="headerlink" title="6.3 实用性vs.理论性作品"></a>6.3 实用性vs.理论性作品</h4><p><strong>实用是与某种有效的做法有关，不管是立即或长程的功效。而理论所关注的却是去明白或了解某件事。</strong></p><p>知识可以用在许多方面，不只是控制自然，发明有用的机器或工具，还可以指导人类的行为，在多种技术领域中校正人类的运作技巧。这里我们要举的例子是纯科学与应用科学的区别，或是像通常非常粗糙的那种说法，也就是科学与科技之间的区别。</p><p><strong>要让知识变成实用，就要有操作的规则。</strong>我们一定要超越** ”知道这是怎么回事”，进而明白“如果我们想做些什么，应该怎么利用它”**。概括来说，这也就是知与行的区别。理论性的作品是在教你这是什么，实用性的作品在教你如何去做你想要做的事，或你认为应该做的事。</p><p><strong>任何一本指南类的书都是实用的。任何一本书告诉你要该做什么，或如何去做，都是实用的书。</strong></p><p>所有<strong>说明某种艺术的学习技巧</strong>，<strong>任何一个领域的实用手册</strong>，像是工程、医药或烹任，或<strong>所有便于分类为“教导性”</strong>(moral)<strong>的深奥论述</strong>，如经济、伦理或政治问题的书，<strong>都是实用的书。</strong> </p><p>严格来说，<strong>任何一本教我们如何生活，该做什么，不该做什么，同时说明做了会有什么奖赏，不做会有什么惩罚的伦理的书</strong>，不论我们是否同意他的结论，<strong>都得认定这是一本实用的书。</strong></p><p>实用书所用到的典型陈述，是某件事应该做完（或做到）；这样做（或制造）某个东西是对的；这样做会比那样做的结果好；这样选择要比那样好，等等。相反的，理论型的作品却常常说”是“，没有“应该”或“应当”之类的字眼。那是在表示某件事是真实的，这些就是事实，不会说怎样换一个样子更好，或者按照这个方法会让事情变得更好等等。</p><p>照传统的分法，理论性的作品会被分类为<strong>历史、科学和哲学</strong>等等。</p><p><strong>历史</strong>就是<strong>纪事</strong>(Chronotopic)。</p><p><strong>科学</strong>则不会太在意过去的事，它所面对的是可能发生在任何时间、地点的事。科学家寻求的是<strong>定律或通则</strong>。 </p><p>如果一本理论的书所强调的内容，超乎你日常、例行、正常生活的经验，那就是<strong>科学</strong>的书。否则就是一本<strong>哲学</strong>的书。</p><h3 id="第七章-透视一本书"><a href="#第七章-透视一本书" class="headerlink" title="第七章 透视一本书"></a>第七章 透视一本书</h3><h4 id="7-0-规则二：简单描述整本书的内容、规则三：列举重要篇章，说明怎么组成一个整体的架构"><a href="#7-0-规则二：简单描述整本书的内容、规则三：列举重要篇章，说明怎么组成一个整体的架构" class="headerlink" title="7.0 规则二：简单描述整本书的内容、规则三：列举重要篇章，说明怎么组成一个整体的架构"></a>7.0 规则二：简单描述整本书的内容、规则三：列举重要篇章，说明怎么组成一个整体的架构</h4><p>每一本书的封面之下都有一套自己的骨架。作为一个分析阅读的读者，你的责任就是要找出这个骨架。</p><p>知道<strong>掌握一本书的架构是绝对需要的</strong>，这能带引你发现阅读任何一本书的第二及第三个规则。 我们说的是“任何一本书”。这些规则适用于诗集，也适用于科学书籍．或任何一种论说性作品。当然，根据书本的不同这些规则在应用时会各不相同。</p><p><strong>分析阅读的第二个规则是：</strong><br><strong>使用一个单一的句子，或最多几句话（一小段文字）来叙述整本书的内容。</strong></p><p>从某一方面来说，每一本书都有一个“干什么”的主题，整本书就是针对这个主题而展开。如果你知道了，就明白了这是什么样的书。我们也可以揣测－个作者想要干什么．想要做什么。<strong>找出一本书在干什么，也就是在发现这本书的主题或重点。</strong></p><p>对于“整体内容”这件事，光是一个模糊的认知是不够的，你必须要确切清楚地了解才行。只有一个方法能知道你是否成功了。你必须能用几句话，告诉你自己，或别人，这整本书在说的是什么。不要满足于“感觉上的整体”，自己却说不出口。</p><p><strong>第三个规则可以说成是：</strong><br><strong>将书中重要篇章列举出来，说明它们如何按照顺序组成一个整体的架构。</strong></p><p>一本好书，就像一栋好房子，每个部分都要很有秩序地排列起来。每个重要部分都要有一定的独立性。就像我们看到的，每个单一部分有自己的室内架构，装潢的方式也可能跟其他部分不同。但是却一定要跟其他部分连接起来这是与功能相关——否则这个部分便无法对整体的智能架构作出任何贡献了。</p><h4 id="7-1-结构与规划：叙述整本书的大意"><a href="#7-1-结构与规划：叙述整本书的大意" class="headerlink" title="7.1 结构与规划：叙述整本书的大意"></a>7.1 结构与规划：叙述整本书的大意</h4><p>首先，一位作者，特别是好的作者，会经常想要帮助你整理出他书中的重点。尽管如此，当你要求读者择要说出一本书的重点时，大多数人都会一脸茫然。一个原因是今天的人们普遍不会用简明的语言表达自己，另一个原因，则是他们忽视了阅读的这一条规则。当然，这也说明太多读者根本就不注意作者的前言，也不注意书名，才会有这样的结果。</p><p>其次，是要小心，不要把我们提供给你的那些书的重点摘要，当作是它们绝对又惟一的说明。一本书的整体精神可以有各种不同的诠释，没有哪一种一定对。当然，某些诠释因为够精简、准确、容易理解，就是比另一些诠释好。不过，也有些南辕北辙的诠释，不是高明得不相上下，就是烂得不相上下。</p><h4 id="7-2-驾驭复杂的内容：为一本书拟大纲的技巧"><a href="#7-2-驾驭复杂的内容：为一本书拟大纲的技巧" class="headerlink" title="7.2 驾驭复杂的内容：为一本书拟大纲的技巧"></a>7.2 驾驭复杂的内容：为一本书拟大纲的技巧</h4><p>我们可以依照第三个规则，将内容大纲排列如下：<br>(1)  作者将全书分成五个部分，<strong>第一部分谈的是什么，第二部分谈的是什么，第三部分谈的是别的事，第四部分则是另外的观点，第五部分又是另一些事。</strong></p><p>(2)  <strong>第一个主要的部分又分成三个段落，第一段落为X，第二段落为Y，第三段落为Z。</strong></p><p>(3)  在<strong>第一部分的第一阶段，作者有四个重点，第一个重点是A，第二个重点是B，第三个重点是C，第四个重点是D</strong>等等。</p><p>一本书的生命也是有限的，就算不死，也跟所有人造的东西一样是不完美的。 因为没有一本书是完美的，所以也不值得为任何一本书写出一个完美的纲要。你只要尽力而为就行了。 </p><h4 id="7-3-阅读与写作的互惠技巧"><a href="#7-3-阅读与写作的互惠技巧" class="headerlink" title="7.3 阅读与写作的互惠技巧"></a>7.3 阅读与写作的互惠技巧</h4><p>一个作品应该有整体感，清楚明白，前后连贯。如果这本书有整体的精神，那我们就一定要找出来。如果全书是清楚明白又前后一贯的，我们就要找出其间的纲要区隔，与重点的秩序来当作回报。所谓文章的清楚明白，就是跟纲要的区隔是否清楚有关，所谓文章的前后一贯，就是能把不同的重点条理有序地排列出来。</p><p><strong>规则二和规则三这两个规则不但可以用来阅读一整本论说性的书，也可以用来阅读其中某个特别重要的部分</strong>。如果书中某个部分是一个相当独立又复杂的整体，那么就要分辨出这部分的整体性与复杂性，才能读得明白。 传达知识的书，与文学作品、戏剧、小说之间，有很大的差异。 前者的各个部分可以是独立的，后者却不能。如果一个人说他把那本小说已经”读到够多，能掌握主题了”，那他一定根本不知道自己在说些什么。 </p><h4 id="7-4-发现作者的意图（规则四：找出作者要问的问题）"><a href="#7-4-发现作者的意图（规则四：找出作者要问的问题）" class="headerlink" title="7.4 发现作者的意图（规则四：找出作者要问的问题）"></a>7.4 发现作者的意图（规则四：找出作者要问的问题）</h4><p><strong>第四个规则可以说是：</strong><br><strong>找出作者要问的问题。</strong></p><p>如果主要的问题很复杂，又分成很多部分，你还要能说出次要的问题是什 么。你应该不只是有办法完全掌握住所有相关的问题，还要能明智地将这些问题整合出顺序来。哪一个是主要的，哪个是次要的？哪个问题要先回答，哪些是后来才要回答的？</p><p>从某方面来说，你可以看出这个规则是在重复一些事情，这些事情在你掌握一本书的整体精神和重要部分的时候已经做过了。然而，这个规则的确可以帮你做好这些事。换句话说，遵守规则四，能让你和遵守前两条规则产生前后呼应的效果。</p><p><strong>如何找出作者的问题？</strong><br>某件事存在吗？是什么样的事？发生的原因是什么？或是在什么样的情况下存在？或为什么会有这件事的存在？这件事的目的是什么？造成的影响是什么？特性及特征是什么？与其他类似事件，或不相同事件的关联是什么？这件事是如何进行的？以上这些都是<strong>理论性的问题</strong>。</p><p>有哪些结果可以选择？应该采取什么样的手段才能获得某种结果？要达到某个目的，应该采取哪些行动？以什么顺序？在这些条件下，什么事是对的，或怎样才会更好，而不是更糟？在什么样的条件下，这样做会比那样做好一些？以上这些都是<strong>实用的问题</strong>。</p><h4 id="7-5-分析阅读的第一个阶段"><a href="#7-5-分析阅读的第一个阶段" class="headerlink" title="7.5 分析阅读的第一个阶段"></a>7.5 分析阅读的第一个阶段</h4><p>阅读的前四个规则是分析阅读的规则。如果在运用之前能先做好检视阅读，会更能帮助你运用这些规则。这前四个规则是有整体性，有同一个目标的。这四个规则在一起，能提供读者对一本书架构的认识。当你运用这四个规则来阅读一本书，或任何又长又难读的书时，你就完成了分析阅读的第一个阶段。</p><p><strong>真正实际的读者是一次就完成所有的阶段。</strong></p><h4 id="7-6-总结"><a href="#7-6-总结" class="headerlink" title="7.6 总结"></a>7.6 总结</h4><p>分析阅读的第一阶段，或，找出一本书在谈些什么的四个规则：<br>(1)    <strong>依照书本的种类与主题作分类。</strong><br>(2)    <strong>用最简短的句子说出整本书在谈些什么。</strong><br>(3)    <strong>按照顺序与关系，列出全书的重要部分。将全书的纲要拟出来之后，再将各个部分的纲要也一一列出。</strong><br>(4) <strong>找出作者在问的问题，或作者想要解决的问题。</strong></p><h3 id="第八章-与作者找出共通的词义"><a href="#第八章-与作者找出共通的词义" class="headerlink" title="第八章 与作者找出共通的词义"></a>第八章 与作者找出共通的词义</h3><p>如果你运用了前一章结尾时所谈到的前四个规则，你就完成了分析阅读的第一个阶段。</p><p>现在进行第二个阶段。这也包括了四个阅读规则。</p><p><strong>第一个规则——称为“找出共通的词义”</strong></p><p>在任何一个成功的商业谈判中，双方找出共同的词义，也就是达成共识(coming to terms)，通常是最后一个阶段。</p><p>但是在用<strong>分析阅读阅读一本书时，找出共通的词义却是第一个步骤。</strong>除非读者与作者能找出共通的词义，否则想要把知识从一方传递到另一方是不可能的事。因为<strong>词义(term)是可供沟通的知识的基本要素</strong>。</p><h4 id="8-1-单字vs-词义"><a href="#8-1-单字vs-词义" class="headerlink" title="8.1 单字vs.词义"></a>8.1 单字vs.词义</h4><p>词义和单字(word)不同——至少，不是一个没有任何进一步定义的单字。如果词义跟单字完全相同，你只需要找出书中重要的单字，就能跟作者达成共识了。但是一个单字可能有很多的意义，特别是一个重要的单字。如果一个作者用了一个单字是这个意义，而读者却读成其他的意义，那这个单字就在他们之间擦身而过，他们双方没有达成共识。</p><p><strong>沟通是一个人努力想要跟别人</strong>（也可能是动物或机器）<strong>分享他的知识、判断与情绪</strong>。只有当双方对一些事情达成共识，譬如彼此对一些资讯或知识都有分享，沟通才算成功。</p><p>只要模糊地带还存在，就表示作者和读者之间对这些单字的意义还没有共识。为了要达成完全的沟通，最重要的是双方必须要使用意义相同的单字——简单来说，就是，<strong>找出共通的词义达成共识</strong>。双方找出共通的词义时，沟通就完成了，两颗心也奇迹似地拥有了相同的想法。</p><p>词义可以定义为没有模糊地带的字。当一个单字使用得没有模糊意义的时候，就是一个词义了。字典中充满了单字。就这些单字都有许多意义这一点而言，它们几乎都意义模糊。当某个时间，作者与读者同时在使用同一个单字，并采取惟一相同的意义时，在那种毫无模糊地带的状态中，他们就是找出共通的词义了。</p><p>可以将达成共识看作是一种<strong>使用文字的技巧，以达到沟通知识</strong>的目的。</p><p>必须<strong>抓住书中重要的单字．搞清楚作者是如何使用这个单字的</strong>。不过我们可以说得更精确又优雅一些。</p><h4 id="8-2-规则五：找出重要单字，透过它们与作者达成共识"><a href="#8-2-规则五：找出重要单字，透过它们与作者达成共识" class="headerlink" title="8.2 规则五：找出重要单字，透过它们与作者达成共识"></a>8.2 规则五：找出重要单字，透过它们与作者达成共识</h4><p><strong>第一部分：</strong> <strong>找出重要单字</strong>，那些举足轻重的单字。</p><p><strong>第二部分：</strong> <strong>确认这些单字在使用时的最精确的意义</strong>。</p><p>这是分析阅读第二阶段的第一个规则，目标是<strong>诠释内容与讯息</strong>。</p><p><strong>第一个步骤</strong>是<strong>处理语言的问题</strong>。<br><strong>第二个步骤</strong>是<strong>超越语言，处理语言背后的思想涵义</strong>。</p><p>因为语言并不是完美的传递知识的媒介，因而在沟通时也会有形成障碍的作用。追求具备诠释能力的阅读，规则就在克服这些障碍。身为读者，我们应该从我们这一边来努力打通障碍。两个心灵想透过语言来接触，需要作者与读者双方都愿意共同努力才行。就像教学，除非被教的学生产生<strong>呼应的活力</strong>，否则光靠老师是行不通的。</p><p>语言与思想的问题——特别是单字与词义之间的差异——是非常重要的。因此我们宁愿冒着重复的风险，也要确定这个重点被充分了解。这个重点就是，一个单字可能代表许多不同的词义，而一个词义可以用许多不同的单字来解释。</p><h4 id="8-3-找出关键字"><a href="#8-3-找出关键字" class="headerlink" title="8.3 找出关键字"></a>8.3 找出关键字</h4><p>如果你不想办法了解这些关键字所出现的那些段落的意思，你就没法指出哪些字是关键字了。如果你了解那些段落的意思，当然会知道其中哪几个字是非常重要的。如果你并不完全了解那些段落的意思，很可能是因为你并不清楚作者是如何使用一些特定的字眼。如果你把觉得有困扰的字圈出来，很可能就找出了作者有特定用法的那些字了。 </p><p>从一个读者的角度来看，<strong>最重要的字就是那些让你头痛的字</strong>。这些字很可能对作者来说也很重要。不过，有时也并非如此。</p><p>也很可能，对作者来说很重要的字．对你却不是问题——因为你已经了解了这些字。在这种状况下，你与作者就是已经找出共通的词义，达成共识了。只有那些还未达成共识的地方，还需要你的努力。</p><h4 id="8-4-专门用语及特殊字汇"><a href="#8-4-专门用语及特殊字汇" class="headerlink" title="8.4 专门用语及特殊字汇"></a>8.4 专门用语及特殊字汇</h4><p><strong>第一个，也是最明显的信号是，作者开诚布公地强调某些特定的字，而不是其他的字</strong>。他会用很多方法来做这件事。他会用不同的字体来区分，如加括号，斜体字等记号以提醒你。他也会明白地讨论这些字眼不同的意义．并指出他是如何在书中使用这些不同的字义，以引起你对这些字的注意。或是他会借着这个字来命名另外一个东西的定义，来强调这个字。 </p><p>每一个知识领域都有独特的专门用语(technical vocabulary)。</p><p>如果你知道这是什么种类的书，整本书在谈的主题是什么，有哪些重要的部分，将大大帮助你把专门用语从一般用语中区分出来。作者的书名、章节的标题、前言，在这方面也都会有些帮助。</p><p>某些知识领域有一套完整的专门用语，在一本这种主题的书中找出重要的单字，相形之下就很容易了。就积极面来说．只要熟悉一下那个领域，你就能找出这些专门的单字；就消极面来说，你只要看到不是平常惯见的单字，就会知道那些字一定是专门用语。</p><p><strong>另外一个线索是，作者与其他作者争执的某个用语就是重要的字</strong>。当你发现一位作者告诉你某个特定的字曾经被其他人如何使用，而他为什么选择不同的用法时，你就可以知道这个字对他来说意义非凡。</p><p><strong>大多数人都习惯于没有主动的阅读。</strong>没有主动的阅读或是毫无要求的阅读，最大的问题就在读者对字句毫不用心，结果自然无法跟作者达成共识了。</p><h4 id="8-5-找出字义"><a href="#8-5-找出字义" class="headerlink" title="8.5 找出字义"></a>8.5 找出字义</h4><p><strong>你一定要利用上下文自己已经了解的所有字句，来推敲出你所不了解的那个字的意义。</strong></p><p>如果一个定义里的每个字都还需要去定义时，那没有任何一个东西可以被定义了。</p><p>事实上，本书之所以能给你带来新的洞察力或启发，就是因为其中有一些你不能一读即懂的字句。如果你不能自己努力去了解这些字，那就不可能学会我们所谈的这种阅读方法。你也不可能作到自己阅读一本书的时候，从不太了解进展到逐渐了解的境界。</p><p>一个单字是可以代表许多不同词义的。记住这件事的一个方法，是区分作者的用语(vocabulary)与专业术语(terminology) 之间的不同。</p><p>另外还有一些更复杂的情况。首先，<strong>一个可以有许多不同意义的字，在使用的时候可以只用其中一个意义，也可以把多重意义合起来用</strong>。让我们再用“阅读”来当例子。在本书某些地方，我们用来指阅读任何一种书籍。在另一些地方，我们指的是教导性的阅读，而非娱乐性的阅读。还有一些其他地方，我们指的更是启发性的阅读，而非只是获得资讯。</p><p>为了确定你跟我们对于阅读这件事达成了共识，我们用类似“启发性的阅读”的句子来代替“阅读”这两个字。为了更确定清楚，我们又用了类似“如何运用你的心智来阅读一本书，也就是如何让自己从不太理解到逐渐理解的一个过程”的长句子来说明一个词义，这个词义也就是本书最强调的一种阅读。但这个词义却分别用了一个字、一个片语及一个长句子来作说明。</p><p><strong>如果因为练习分析阅读而引发你的兴趣，你可以利用这种阅读多读一点和这些主题相关的书。在阅读这些书时，你会获得更多的好处，因为你是在阅读的经验中，提出了自己的问题而去找这些书的。</strong></p><p>你也可能并不想再研究下去。就算你不想，只<strong>要你肯花一点精神，在读一本书的时候，找出重要的关键字，确认每个字不同意义的转换，并与作者找出共通的词义．你对一本书的理解力就会大大增加了</strong>。很少有一些习惯上的小小改变，会产生如此宏大的效果。</p><h3 id="第九章-判断作者的主旨"><a href="#第九章-判断作者的主旨" class="headerlink" title="第九章 判断作者的主旨"></a>第九章 判断作者的主旨</h3><p>书里的提案，也就是主旨，也是一种声明。 那是作者在表达他对某件事的判断。他断言某件他认为是真的事，或否定某件他判断是假的事。他坚持这个或那个是事实。 这样的提案，是一种知识的声明，而不是意图的声明。作者的意图可能在前言的一开头就告诉我们了。就一部论说性的作品来说．通常他会承诺要指导我们做某件事。为了确定他有没有澄守这些承诺．我们就一定要找出他的<strong>主旨(propositions)</strong>才行。 </p><h4 id="9-0-规则六：与句子及提案有关、规则七：与各种论述-arguments-有关"><a href="#9-0-规则六：与句子及提案有关、规则七：与各种论述-arguments-有关" class="headerlink" title="9.0 规则六：与句子及提案有关、规则七：与各种论述(arguments)有关"></a>9.0 规则六：与句子及提案有关、规则七：与各种论述(arguments)有关</h4><p><strong>分析阅读的第六个规则，是与句子及提案有关的规则。</strong></p><p>第七个规则与第六个规则是息息相关的。一位作者可能借着事件、事实或知识，诚实地表达自己的想法。通常我们也是抱着对作者的信任感来阅读的。但是除非我们对作者的个性极端感兴趣，否则只是知道他的观点并不能满足我们。<strong>作者的主旨如果没有理论的支持，就只是在抒发个人想法罢了</strong>。如果是这本书、这个主题让我们感兴趣，而不是作者本身，那么我们不只想要知道作者的主张是什么，还想知道<strong>为什么他认为我们该被说服，以接受这样的观点</strong>。</p><p><strong>因此，第七个规则与各种论述(arguments)有关。</strong></p><p>一种说法总是受到许多理由、许多方法的支持。有时候我们可以强力主张真实，有时候则顶多谈谈某件事的可能。但不论哪种论点都要包含一些用某种方式表达的陈述。</p><p>我们说明<strong>这些规则的顺序，都是有文法与逻辑的根据的。</strong>我们从共识谈到主旨，再谈到论点，表达的方法是从字（与词）到一个句子，再到一连串的句子（或段落）来作说明。我们从最简单的组合谈到复杂的组合。当然，一本书含有意义的最小单位就是”字”。但是如果说一本书就是一连串字的组合，没有错，却并不恰当。书中也经常把一组组的字，或是一组组的句子来当单位。－个主动的读者，不只会注意到字，也会注意到句子与段落。 除此之外，没有其他方法可以发现一个作者的共识、主旨与论点。</p><p>我们把分析阅读谈到这里时——<strong>目的是在诠释作者的意图</strong>——似乎和第一个阶段的发展方向背道而驰——<strong>第一阶段的目的是掌握结构大纲</strong>。我们原先从将一本书当作是个整体，谈到书中的主要部分，再谈到次要的部分。 </p><p>因此，这两个过程，掌握大纲与诠释意图，在主旨与论述的层次中互相交集了。你<strong>将一本书的各个部分细分出来</strong>，就可以<strong>找出主旨与论述</strong>。然后你再仔细<strong>分析一个论述由哪些主旨，甚至词义而构成</strong>。等这两个步骤你都完成时，就可以说是真的了解一本书的内容了。</p><h4 id="9-1-句子与主旨"><a href="#9-1-句子与主旨" class="headerlink" title="9.1 句子与主旨"></a>9.1 句子与主旨</h4><p>句子与段落是文法的单位、语言的单位。主旨与论述是逻辑的单位，也就是思想与知识的单位。</p><p>让我们说明句子与主旨之间的关系。并不是一本书中的每一句话都在谈论主旨。有时候，一些句子在表达的是疑问。他们提出的是问题，而不是答案。<strong>主旨则是这些问题的答案</strong>。<strong>主旨所声明的是知识或观点。</strong>这也是为什么我们说表达这种声明的句子是叙述句(declarative)，而提出问题的句子是疑问句(interrogative)。其他有些句子则在表达希望或企图。这些句子可能会让我们了解一些作者的意图，却并不传达他想要仔细推敲的知识。</p><p>并不是每一个叙述句都能当作是在表达一个主旨。这么说至少有两个理由。第一个是<strong>事实上，字都有歧义，可以用在许多不同的句子中</strong>。因此，如果字所表达的意思改变了，很可能同样的句子却在阐述不同的主旨。 </p><p><strong>当一个简单的句子使用的字都毫无歧义时，通常在表达的是一个单一的主旨</strong>。但就算用字没有歧义，一个复合句也可能表达一个或两个主旨。一个复合句其实是一些句子的组合，其间用一些字如 “与”、“如果……就”或“不但……而且”来作连接。</p><p>不只是一个单一的句子可以表达出不同的主旨，不管是有歧义的句子或复合句都可以，而且同一个主旨也能用两个或更多不同的句子来说明。如果你能抓住我们在字里行间所用的同义字，你就会知道我们在说：“教与学的功能是互相连贯的”与“传授知识与接受知识是息息相关的过程”这两句话时，所谈的是同一件事。</p><p>你应该注意一个句子中字的排列顺序，与彼此之间的关系。对一个阅读者来说，有一些文法的知识是必要的。除非你能越过语言的表象，看出其中的意义，否则你就无法处理有关词义、主旨与论述思想的要素——的问题。只要文字、句子与段落是不透明的、未解析的，他们就是沟通的障碍，而不是媒介。你阅读了一些字，却没有获得知识。</p><h4 id="9-2-谈规则（规则五、规则六、规则七）"><a href="#9-2-谈规则（规则五、规则六、规则七）" class="headerlink" title="9.2 谈规则（规则五、规则六、规则七）"></a>9.2 谈规则（规则五、规则六、规则七）</h4><p><strong>第五个规则是：</strong> <strong>找出关键字，与作者达成共识</strong>。</p><p><strong>第六个规则是：</strong> <strong>将一本书中最重要的句子圈出来，找出其中的主旨</strong>。</p><p><strong>第七个规则是：</strong> <strong>从相关文句的关联中，设法架构出一本书的基本论述</strong>。</p><h4 id="9-3-找出关键句"><a href="#9-3-找出关键句" class="headerlink" title="9.3 找出关键句"></a>9.3 找出关键句</h4><p>一本书中真正的关键句中只有少数的几句话，并不是说你就可以忽略其他的句子。当然，你应该要了解每一个句子。而大多数的句子，就像大多数的文字一样，对你来说都是亳无困难的。我们在谈速读时提到过，在读这些句子时可以相当快地读过去。</p><p><strong>从一个读者的观点来看，</strong>对你<strong>重要的句子就是一些需要花一点努力来诠释的句子</strong>，因为你第一眼看到这些句子时并不能完全理解。你对这些句子的理解，只及于知道其中还有更多需要理解的事。这些句子你会读得比较慢也更仔细一点。这些句子对作者来说也许并不是最重要的，但也很可能就是，因为<strong>当你碰到作者认为最重要的地方时，应该会特别吃力</strong>。用不着说，<strong>你在读这些部分时应该特别仔细才好。</strong></p><p>最重要的句子就是在整个论述中，阐述作者判断的部分。一本书中通常包含了一个以上或一连串的论述。作者会解释为什么他现在有这样的观点，或为什么他认为这样的情况会导致严重的后果。他也可能会讨论他要使用的一些字眼。他会批评别人的作品。他会尽量加入各种相关与支持的论点。但<strong>他沟通的主要核心是他所下的肯定与否定的判断</strong>，<strong>以及他为什么会这么做的理由</strong>。因此，<strong>要掌握住重点，就要从文章中看出浮现出来的重要句子。</strong></p><p>有些作者会帮助你这么做。他们会在这些字句底下划线。他们不是告诉你说这些是重点，就是用不同的印刷字体将主要的句子凸显出来。当然，如果你阅读时昏昏沉沉的，这些都帮不上忙了。我们碰到过许多读者或学生，根本不注意这些已经弄得非常清楚的记号。他们只是一路读下去，而不肯停下来仔细地观察这些重要的句子。</p><p>阅读的一部分本质就是被困惑，而且知道自己被困惑。怀疑是智慧的开始，从书本上学习跟从大自然学习是一样的。如果你对一篇文章连一个问题也提不出来，那么你就不可能期望一本书能给你一些你原本就没有的视野。</p><p><strong>另一个找出关键句的线索是</strong>——<strong>找出组成关键句的文字来。</strong></p><p><strong>如果你已经将重要的字圈出来了，它一定会引导你看到值得注意的句子</strong>。因此在诠释阅读法中，第一个步骤是为第二个步骤作准备的。反之亦然。 很可能你是因为对某些句子感到困惑，而将一些字作上记号的。事实上，虽然我们在说明这些规则时都固定了前后的顺序，但你却不一定要依照这个顺序来阅读。词义组成了主旨，主旨中又包含了词汇。如果你知道这个字要表达的意思，你就能抓住这句话中的主旨了<strong>。如果你了解了一句话要说明的主旨，你也就是掌握了其中词义的意思。</strong></p><p><strong>接下来的是更进一步找出最主要的主旨的线索。</strong>这些主旨一定在一本书最主要的论述中——不是前提就是结论。因此，如果你能<strong>依照顺序找出这些前后相关的句子——找出有始有终的顺序</strong>，你<strong>可能就已经找到那些重要的关键句子了</strong>。</p><p>我们所说的顺序，要有始有终。任何一种论述的表达，都需要花点时间。你可以一口气说完一句话，但你要表达一段论述的时候却总要有些停顿。你要先说一件事，然后说另一件事．接下来再说另一件事。一个论述是从某处开始，经过某处，再到达某处的。那是思想的演变移转。可能开始时就是结论，然后再慢慢地将理由说出来。也可能是先说出证据与理由，再带引你达到结论。</p><p>许多人认为他们知道如何阅读，因为他们能用不同的速度来阅读。但是他们经常在错误的地方暂停，慢慢阅读。他们会为了一个自己感兴趣的句子而暂停，却不会为了感到困扰的句子而暂停。事实上，在阅读非当代作品时，这是最大的障碍。</p><h4 id="9-4-找出主旨"><a href="#9-4-找出主旨" class="headerlink" title="9.4 找出主旨"></a>9.4 找出主旨</h4><p><strong>当你发现一段话里所使用的文字的意义时，你就和作者找到了共识</strong>。同样的，<strong>诠释过组成句子的每个字，特别是关键字之后，你就会发现主旨。</strong></p><p>在找出文字所表达的意思与句子所阐述的主旨之间，只有两个不同之处。一个是后者所牵涉的内容比较多。就像你要用周边的其他字来解释一个特殊的字一样，你也要借助前后相关的句子来了解那个问题句。在两种情况中，都是从你了解的部分，进展到逐渐了解你原来不懂的部分。</p><p><strong>另一个不同是书复杂的句子通常要说明的不只一个主旨</strong>。除非你能分析出所有不同，或相关的主旨，否则你还是没有办法完全诠释一个重要的句子。要熟练地做到这一点，就需要常常练习。<strong>试着在本书中找出一些复杂的句子，用你自己的话将其中的主旨写出来。</strong></p><p>“用你自己的话来说“，是测验你懂不懂一个句子的主旨的最佳方法。如果要求你针对作者所写的某个句子作解释，而你只会重复他的话或在前后顺序上作一些小小的改变，你最好怀疑自己是否真的了解了这句话。</p><p>如果你想要确定自己是否吸收了主旨，而不只是生吞活剥了字句，最好是用这种翻译来测试一下。就算你的测验失败了，你还是会发现自己的理解不及在哪里。</p><h4 id="9-5-找出论述"><a href="#9-5-找出论述" class="headerlink" title="9.5 找出论述"></a>9.5 找出论述</h4><p>指导我们阅读的第七个规则的逻辑单位，是“论述”——一系列先后有序，其中某些还带有提出例证与理由作用的主旨。如同”意思”之于文字，“主旨”之于句子，“论述“这个逻辑单位也不会只限定于某种写作单位里。一个论述可能用一个复杂的句子就能说明。可能用一个段落中的某一组句子来说明。可能等于一个段落，但又有可能等于好几个段落。</p><p>另外还有一个困难点。在任何一本书中都有许多段落根本没有任何论述——就连一部分也没有。这些段落可能是一些说明证据细节，或者如何收集证据的句子。</p><p><strong>第七个规则可以有另一个公式：</strong></p><p><strong>如果可以，找出书中说明重要论述的段落。但是，如果这个论述井没有这样表达出来，你就要去架构出来。你要从这一段或那一段中挑选句子出来，然后整理出前后顺序的主旨，以及其组成的论述。</strong></p><p>等你找到主要的句子时，架构一些段落就变得很容易了。有很多方法可试。你可以用一张纸，写下构成一个论述的所有主旨。通常更好的方法是，就像我们已经建议过的，在书的空白处作上编号，再加上其他记号，把一些应该排序而读的句子标示出来。</p><p><strong>在阅读的过程中你能让大脑不断地活动，能跟作者达成共识，找到他的主旨，那么你就能看出他的论述是什么了</strong>。而这也就是人类头脑的自然本能。</p><p><strong>进一步应用这个阅读规则：</strong></p><p><strong>首先</strong>，要记住<strong>所有的论述都包含了一些声明</strong>。其中有些是你为什么该接受作者这个论述的理由。如果你先找到结论，就去看看理由是什么。如果你先看到理由，就找找看这些理由带引你到什么样的结论上。</p><p><strong>其次</strong>，要<strong>区别出两种论述的不同之处</strong>。 一种是以一个或多个特殊的事实证明某种共通的概念，另一种是以连串的通则来证明更进一步的共通概念。前者是归纳法，后者是演绎法。但是这些名词并不重要。重点作如何区分二者的能力。</p><p><strong>第三</strong>，找出作者认为哪些事情是<strong>假设</strong>，哪些是能<strong>证实</strong>的或有根据的，以及哪些是不需要证实的<strong>自明之理</strong>。</p><p>换句话说，每个论述都要有开端。基本上。有两种开始的方法或地方：一种是<strong>作者与读者都同意的假设</strong>，－种是<strong>不论作者或读者都无法否认的自明之理</strong>。在第一种状况中，<strong>只要彼此认同，这个假设可以是任何东西</strong>。第二个情况就需要<strong>多一点的说明了</strong>。</p><h4 id="9-6-找出解答（规则八：找出作者的解答）"><a href="#9-6-找出解答（规则八：找出作者的解答）" class="headerlink" title="9.6 找出解答（规则八：找出作者的解答）"></a>9.6 找出解答（规则八：找出作者的解答）</h4><p>在你想发现一本书到底在谈些什么的最后一个步骤是：<strong>找出作者在书中想要解决的主要问题</strong>（如果你回想下，这在第四个规则中已经谈过了）。现在，你已经跟作者有了共识，抓到他的主旨与论述了，你就该检视一下你收集到的是什么资料，并提出一些更进一步的问题来。作者想要解决的问题哪些解决了？为了解决问题，他是否又提出了新问题？无论是新问题或旧问题，哪些是他知道自己还没有解决的？</p><p>诠释作品的阅读技巧的最后一部分就是：<br><strong>规则八，找出作者的解答。</strong></p><p>你在应用这个规则及其他三个规则来诠释作品时，你可以很清楚地感觉到自己已经开始在了解这本书了。如果你开始读一本超越你能力的书——也就是能教导你的书——你就有一段长路要走了。更重要的是，你现在已经能用分析阅读读完一本书了。这第三个，也是最后一个阶段的工作很容易。你的心灵及眼睛都已经打开来了，而你的嘴闭上了。做到这一点时，你已经在伴随作者而行了。从现在开始，你可以有机会与作者辩论，表达你自己的想法。</p><h4 id="9-7-分析阅读的第二个阶段"><a href="#9-7-分析阅读的第二个阶段" class="headerlink" title="9.7 分析阅读的第二个阶段"></a>9.7 分析阅读的第二个阶段</h4><p>分析阅读的第二个阶段，或找出一本书到底在说什么的规则（诠释一本书的内容）：<br>(5)   <strong>诠释作者使用的关键字，与作者达成共识。</strong><br>(6)   <strong>从最重要的句子中抓出作者的重要主旨。</strong><br>(7)   <strong>找出作者的论述，重新架构这些论述的前因后果，以明白作者的主张。</strong><br>(8)   <strong>确定作者已经解决了哪些问题，还有哪些是未解决的。在未解决的问题中，确定哪些是作者认为自已无法解决的问题。</strong></p><h3 id="第十章-公正地评断一本书"><a href="#第十章-公正地评断一本书" class="headerlink" title="第十章 公正地评断一本书"></a>第十章 公正地评断一本书</h3><p>读者需要还作者一个深思熟虑的评断。</p><p>一本好书值得主动地阅读。主动的阅读不会为了已经了解一本书在说些什么而停顿下来，必须能评论，提出批评，才算真正完成了这件事。没有自我期许的读者没法达到这个要求，也不可能作到分析或诠释一本书。</p><h4 id="10-1-受教是一种美德"><a href="#10-1-受教是一种美德" class="headerlink" title="10.1 受教是一种美德"></a>10.1 受教是一种美德</h4><p>受教的美德——这是一种长久以来一直受到误解的美德。受教通常与卑躬屈膝混为一谈。一个人如果被动又顺从，可能就会被误解为他是受教的人。相反的，受教或是能学习是一种极为主动的美德。一个人如果不能自动自发地运用独立的判断力，他根本就不可能学习到任何东西。<strong>或许他可以受训练，却不能受教</strong>。因此，<strong>最能学习的读者，也就是最能批评的读者</strong>。这样的读者在最后终于能对一本书提出回应，对于作者所讨论的问题，会努力整理出自己的想法。</p><p>光是努力，并不足以称得上受教。<strong>读者必须懂得如何评断一本书，就像他必须懂得如何才能了解一本书的内容</strong>。 这第三组的阅读规则，也就是引导读者在最后一个阶段训练自己受教的能力。</p><h4 id="10-2-修辞的作用"><a href="#10-2-修辞的作用" class="headerlink" title="10.2 修辞的作用"></a>10.2 修辞的作用</h4><p>我们经常发现教学与受教之间的关系是互惠的，而一个作者能深思熟虑地写作的技巧，和一个读者能深思熟虑地掌握这本书的技巧之间，也有同样的互惠关系。</p><p>最后阶段的一些规则，则超越理解的范畴，要作出评论。于是，这就涉及修辞。</p><p>如果我们在说话，我们不只希望别人了解我们，也希望别人能同意我们的话。 如果我们沟通的目的是很认真的，我们就希望能说服或劝导对方——更精确地说，说服对方接受我们的理论，劝导对方最终受到我们的行为与感觉的影响。在作这样的沟通时，接受的一方如果也想同样认真，那就不但要有回应，还要做一个负责的倾听者。你对自己所听到的要有回应，还要注意到对方背后的意图。同时，你还要能有自己的主见。当你有自己的主见时，那就是你的主张，不是作者的主张了。如果你不靠自己，只想依赖别人为你作判断，那你就是在做奴隶，不是自由的人了。思想教育之受推崇．正因如此。</p><p>站在叙述者或作者的角度来看，<strong>修辞就是要知道如何去说服对方</strong>。因为这也是最终的目标，所有其他的沟通行为也必须做到这个程度才行。相对的，在读者或听者的立场，修辞的技巧是知道当别人想要说服我们时，我们该如何反应。同样的，文法及逻辑的技巧能让我们了解对方在说什么．并准备作出评论。</p><h4 id="10-3-暂缓评论的重要性（规则九：暂缓评论）"><a href="#10-3-暂缓评论的重要性（规则九：暂缓评论）" class="headerlink" title="10.3 暂缓评论的重要性（规则九：暂缓评论）"></a>10.3 暂缓评论的重要性（规则九：暂缓评论）</h4><p>除非你听清楚了，也确定自己了解了，否则就不要回话。除非你真的很满意自已完成的前两个阅读阶段，否则不会感觉到可以很自由地表达自己的想法。只有<strong>当你做到这些事时，你才有批评的权力，也有责任这么做。</strong></p><p><strong>第九个规则：</strong></p><p>在你说出“我同意”，”我不同意”，或“我暂缓评论”之前，你一定要能肯定地说：“我了解了。”上述三种意见代表了所有的评论立场。我们希望你不要弄错了．以为所谓评论就是要不同意对方的说法。这是非常普遍的误解。同意对方说法，与不同意对方说法都一样要花心力来作判断的。同意或不同意都有可能对，也都有可能不对。毫无理解便同意只是愚蠢，还不清楚便不同意也是无礼。</p><p><strong>暂缓评论也是评论的一种方式。那是一种有些东西还未表达的立场。你在说的是．无论如何．你还没有被说服。</strong></p><p>你可能会怀疑，这些不过是普通常识，为什么要大费周章地说明？<strong>有两个理由。</strong><br><strong>第一点，</strong>前面已经说过，<strong>许多人会将评论与不同意混为一谈</strong>（就算是“建设性”的批评也是不同意）。<br>其<strong>次，</strong>虽然这些规则看起来很有理，<strong>在我们的经验中却发现很少有人能真正运用</strong>。这就是古人说的<strong>光说不练的道理</strong>。</p><p>当然，说出“我不懂”也是个很重要的评断，但这只能在你尽过最大努力之后，因为书而不是你自己的理由才能说这样的话。如果你已经尽力，却仍然无法理解，可能是这本书真的不能理解。对一本书，尤其是一本好书来说，这样的假设是有利的。在阅读一本好书时，无法理解这本书通常是读者的错。 </p><h4 id="10-4-避免争强好辩的重要性（规则十：避免争强好辩）"><a href="#10-4-避免争强好辩的重要性（规则十：避免争强好辩）" class="headerlink" title="10.4 避免争强好辩的重要性（规则十：避免争强好辩）"></a>10.4 避免争强好辩的重要性（规则十：避免争强好辩）</h4><p>评论式阅读的第二个规则的道理，与第一个一样清楚，但需要更详尽的说明与解释。</p><p><strong>规则十：</strong></p><p><strong>当你不同意作者的观点时，要理性地表达自己的意见，不要无理地辩驳或争论。</strong>如果你知道或怀疑自己是错的，就没有必要去赢得那场争辩。事实上，你赢得争辩可能真的会在世上名噪一时，但长程来说，诚实才是更好的策略。</p><p>在与作者——活着或死了的老师——对话中，真正的好处是他能从中学到什么；如果他知道所谓的赢只在于增进知识，而不是将对方打败，他就会明自争强好辩是毫无益处的。我们并不是说读者不可以极端反对或专门挑作者的毛病，我们要说的只是：就像他反对一样，他也要有同意的心理准备。不论要同意还是反对，他该顾虑的都只有一点——事实，关于这件事的真理是什么。</p><p>这里要求的不只是诚实。读者看到什么应该承认是不必说的。当必须同意作者的观点，而不是反对的，也不要有难过的感觉。如果有这样的感觉，他就是个积习己深的好辩者。就这第二个规则而言，这样的读者是情绪化的，而不是理性的。</p><h4 id="10-5-化解争议（规则十一：化解争议）"><a href="#10-5-化解争议（规则十一：化解争议）" class="headerlink" title="10.5 化解争议（规则十一：化解争议）"></a>10.5 化解争议（规则十一：化解争议）</h4><p>大部分争论形式——只要排除误解，增加知识就能解决这些争论。</p><p>一个人在与别人对话时，就算有不同的意见，最后还是有希望达成共识。他应该准备好改变自己的想法，才能改变别人的想法。他永远要先想到自己可能误解了，或是在某一个问题上有盲点。在争论之中，一个人绝不能忘了这是教导别人，也是自己受教的一个机会。</p><p><strong>第三个规则要如何应用在读者与作者的对话中呢？这个规则要怎样转述成阅读的规则呢？</strong><br>当读者发现自己与书中某些观点不合时，就要运用到这个规则了。这个规则要求他先确定这个不同的意见不是出于误解。再假设这个读者非常注意，除非自己真的了解，而且确实毫无疑问，否则不会轻易提出评断的规则，那么，接下来呢？</p><p>接下来，这个规则要求他就真正的知识与个人的意见作出区别。还要相信就知识而言，这个争议的议题是可以解决的。如果他继续进一步追究这个问题，作者的观点就会指引他，改变他的想法。如果这样的状况没有发生，就表示他的论点可能是正确的，至少在象征意义上，他也有能力指导作者。至少他可以希望如果作者还活着，还能出席的话，作者也可能改变想法。</p><p>如果一个作者的主旨没有理论基础，就可以看作是作者个人的意见。一个读者如果不能区别出知识的理论说明与个人观点的阐述，那他就无法从阅读中学到东西。他感兴趣的顶多只是作者个人，把这本书当作是个人传记来读而已。当然，这样的读者无所谓同意或不同意，他不是在评断这本书，而是作者本身。</p><p>如果读者基本的兴趣是书籍本身，而不是作者本身，对于自己有责任评论这件事就要认真地对待。在这一点上，读者要就真正的知识与他个人观点以及作者个人观点之不同之处，作出区分。因此，除了表达赞成或反对的意见之外，读者还要作更多的努力。他必须为自己的观点找出理由来。当然，如果他赞同作者的观点、就是他与作者分享同样的理论。但是如果他不赞同，他一定要有这么做的理论基础。否则他就只是把知识当作个人观点来看待了。</p><p><strong>规则十一：</strong></p><p><strong>尊重知识与个人观点的不同，在作任何评断之前，都要找出理论基础。</strong></p><h4 id="10-6-总结"><a href="#10-6-总结" class="headerlink" title="10.6 总结"></a>10.6 总结</h4><p>这三个规则在一起所说明的是批评式阅读的条件，而在这样的阅读中，读者应该能够与作者“辩论”。</p><p><strong>第一：</strong>要求<strong>读者先完整地了解一本书，不要急着开始批评。</strong><br><strong>第二：</strong>恳请<strong>读者不要争强好辩或盲目反对。</strong><br><strong>第三：</strong> <strong>将知识上的不同意见看作是大体上可以解决的问题。</strong></p><p>这个规则再进一步的话，就是要求读者要为自己不同的意见找到理论基础，这样这个议题才不只是被说出来，而且会解释清楚。只有这祥，才有希望解决这个问题。</p><h3 id="第十一章-赞同或反对作者"><a href="#第十一章-赞同或反对作者" class="headerlink" title="第十一章 赞同或反对作者"></a>第十一章 赞同或反对作者</h3><p>当读者不只是盲目地跟从作者的论点，还能和作者的论点针锋相对时，他最后才能提出同意或反对的有意义的评论。</p><p>同意或反对所代表的意义值得我们进一步讨论。一位读者与作者达成共识后，掌握住他的主旨与论述，便是与作者心意相通了。事实上，<strong>诠释一本书的过程是透过言语的媒介，达到心及上的沟通。读懂一本书可以解释为作者与读者之间的一种认同。</strong> 他们同慈用这样的说法来说明一种想法。因为这样的认同，读者便能透过作者所用的语言，看出他想要表达的想法。</p><p><strong>当你透过对一本书的诠释理解，与作者达成了共识之后，才可以决定同意他的论点，或是不同意他的立场。</strong></p><h4 id="11-1-偏见与公正"><a href="#11-1-偏见与公正" class="headerlink" title="11.1 偏见与公正"></a>11.1 偏见与公正</h4><p><strong>第一点，</strong>因为<strong>人有理性的一面，又有动物的一面，所以在争辩时就要注意你会带进去的情绪，或是在当场引发的脾气。否则你的争论会流于情绪化，而不是在说理了</strong>。当你的情绪很强烈时，你可能会认为自已很有道理。</p><p><strong>第二点，</strong>你<strong>要把自己的前提或假设摊出来。</strong>你要知道你的偏见是什么——这也是<strong>你的预先评断。</strong>否则你就不容易接受对手也有不同假设的权利。<strong>一场好的辩论是不会为假设而争吵的。</strong></p><p><strong>第三点，</strong>也是最后一点，派别之争几乎难以避免地会造成一些盲点，要化解这些盲点，<strong>应尽力尝试不偏不倚。</strong>当然，争论而不想有派别之分是不可能的事。但是<strong>在争论时应该多一点理性的光，少一点激情的热，每个参与辩论的人至少都该从对方的立场来看想</strong>。如果你不能用同理心来阅读一本书，你的反对意见会更像是争吵，而不是文明的意见交流。</p><p>理想上，这三种心态是明智与有益的对话中必要的条件。这三种要件显然也适用在阅读上——那种作者与读者之间的对话上。对一个愿意采取理性争论方式的读者来说，每个建议对他都是金玉良言。</p><p>我们仍然相信，作者与读者的对话及批评式的阅读，是可以相当有纪律的。因此，我们要介绍一套比较容易遵守，可以取代这三种规则的替代方法。这套方法指出四种站在对立角度来评论一本书之道。我们希望即使读者想要提出这四种评论时，也不会陷入情绪化或偏见的状态中。</p><p><strong>以下是这四点的摘要说明</strong><br>(1)  你的<strong>知识不足</strong>(uninformed)。<br>(2)  你的<strong>知识有错误</strong>(misinformed)。<br>(3)  你<strong>不合逻辑</strong>——你的推论无<strong>法令人信服</strong>。<br>(4)  你的<strong>分析不够完</strong>整。</p><p>无论如何，这确实是一位读者在不同意时．基本上可以作出的重点声明。这四个声明多少有点独立性。只用其中一点，不会妨害到其他重点的运用。每个重点或全部的重点都可以用上，因为这些重点是不会互相排斥的。</p><p>读者不能任意使用这些评论，除非他确定能证明这位作者是知识不足、知识有误或不合逻辑。一本书不可能所有的内容都是知识不足或知识有误。一本书也不可能全部都不合逻辑。而要作这样评论的读者，除了要能精确地指认作者的问题之外，还要能进一步证明自己的论点才行。他要为自己所说的话提出理由来。</p><h4 id="11-2-判断作者的论点是否正确"><a href="#11-2-判断作者的论点是否正确" class="headerlink" title="11.2 判断作者的论点是否正确"></a>11.2 判断作者的论点是否正确</h4><p>(1)  <strong>说一位作者知识不足，就是在说他缺少某些与他想要解决的问题相关的知识。</strong>在这里要注意的是，除非这些知识确实相关，否则就没有理由作这样的评论。<strong>要支持你的论点，你就要能阐述出作者所缺乏的知识，并告诉他这些知识如何与这个问题有关，如果他拥有这些知识会如何让他下一个不同的结论。</strong></p><p>(2)  <strong>说一位作者的知识错误，就是说他的理念不正确。</strong>这样的错误可能来自缺乏知识，但也可能远不只于此。不论是哪一种，他的论点就是与事实相反。作者所说的事实或可能的事实，其实都是错的，而且是不可能的。这样的作者是在主张他自己其实并没有拥有的知识，当然，<strong>除非这样的缺点影响到作者的结论，否则并没必要指出来</strong>。<strong>要作这个评论，你必须要能说明事实，或是能采取比作者更有可能性的相反立场来支持你的论点</strong>。</p><p>前两点的批评是互相有关联的。<strong>知识不足，就可能造成我们所说的知识错误。</strong>此外，任何人的某种知识错误，也就是在那方面知识不足。不过，这两种不足在消极与积极面上的影响，还是有差别的。缺乏相关的知识，就不太可能解决某个特定的问题，或支持某一种结论。错误的知识却会引导出错误的结论，与站不住脚的解答。这两个评论合在一起，<strong>指出的是作者的前提有缺陷。</strong>他<strong>需要充实知识。他的证据与论点无论在质与量上都还不够好。</strong></p><p>(3)  <strong>说一位作者是不合逻辑的，就是说他的推论荒谬。</strong>一般来说，荒谬有两种形态。<strong>一种是缺乏连贯，也就是结论冒出来了，却跟前面所说的理论连不起来。另一种是事件变化的前后不一致，也就是作者所说的两件事是前后矛盾的。</strong>要批评这两种问题，读者一定要能例举精确的证据，而那是作者的论点中所欠缺的使人信服的力量。只要当主要的结论受到这些荒谬推论的影响时，这个缺点才要特别地提出来。一本书中比较无关的部分如果缺乏信服力，也还说得过去。</p><p>第三个批评点与前两个是互相关联的。当然，有时候作者可能没法照他自己所提的证据或原则得出结论。这样他的推论就不够完整。但是这里我们主要关心的还是一个作者的理论根据很好，导出来的结论却很差的情况。发现作者的论点没有说服人的力量，是因为前提不正确或证据不足，虽然很有趣，但却一点也不重要。</p><p>如果一个人设定了很完整的前提，结论却问题百出，那从某个角度而言，就是他的知识有错误。不过，到底这些错误的论述来自推论有毛病的问题，还是因为一些其他的缺点，特别像是相关知识不足等等，这两者之间的差异倒是值得我们细细推敲的。</p><h4 id="11-3-判断作者论述的完整性"><a href="#11-3-判断作者论述的完整性" class="headerlink" title="11.3 判断作者论述的完整性"></a>11.3 判断作者论述的完整性</h4><p><strong>前面三个批评点，是与作者的声明与论述有关的。</strong></p><p>第四个批评点。这是在讨论<strong>作者是否实际完成了他的计划——也就是对于他的工作能否交待的满意度</strong>。</p><p>在开始之前，我们必须先澄清一件事。如果你说你读懂了，而你却找不出证据来支持前面任何一个批评点的话，这时你就有义务要同意作者的任何论点。这时你完全没有自主权。你没有什么神圣的权利可以决定同意或不同意。</p><p>如果你不能用相关证据显示作者是知识不足、知识有误，或不合逻辑，你就不能反对他。</p><p><strong>前面三个批评点与作者的共识、主旨与论述有关。</strong>这些是作者开始写作时要用来解决问题的要素。第四点——这本书是否完整了——与整本书的架构有关。</p><p>(4)  说一位作者的分析是不完整的，就是说他并没有解决他一开始提出来的所有问题，或是他并没有尽可能善用他手边的资料，或是他并没有看出其间的含意与纵横交错的关系，或是他没法让自己的想法与众不同。但这还不够去说一本书是不完整的。任何人都可以这样评论一本书。人是有限的，他们所做的任何工作也都是有限的，不完整的。因此，作这样的评论是毫无意义的。<strong>除非读者能精确地指出书中的问题点——不论是来自他自己的努力求知，或是靠其他的书帮忙——才能作这样的批评。</strong></p><p>严格来说，第四点并不能作为不同意一个作者的根据。我们只能就作者的成就是有限的这一点而站在对立面上。然而，当读者找不出任何理由提出其他批评点而同意一本书的部分理论时，或许会因为这第四点，关于一本书是不完整的论点，而暂缓评论整本书。站在读者的立场，暂缓评论一本书就是表示作者并没有完全解决他提出的问题。</p><p>阅读同样领域的书，可以用这四种评论的标准来作比较。如果一本书能比另一本书说出较多的事实，错误也较少，就比较好一点。但如果我们想要借读书来增进知识，显然一本能对主题作最完整叙述的书是最好的。唯有比较每位作者在分析论点时的完整性，才是真正有深度的比较。比较每本书里有效而且突出的论点有多少，就可以当作评断其完整性的参考了。这时你会发现能与作者找出共同的词义是多么有用了。<strong>突出的词义越多，突出的论述也就越多。</strong></p><p>在拟大纲的最后阶段，就是要知道作者想要解决的问题是什么。诠释一本书的最后阶段，就是要知道作者解决了哪些问题，还有哪些问题尚未解决。批评一本书的最后阶段，就是要检视作者论述的完整性。这跟全书大纲，作者是否把问题说明清楚，也跟诠释一本书，衡量他多么完满地解决了问题都有关。</p><h4 id="11-4-分析阅读的三阶段"><a href="#11-4-分析阅读的三阶段" class="headerlink" title="11.4 分析阅读的三阶段"></a>11.4 分析阅读的三阶段</h4><p><strong>所有的规则按适当的次序，用合宜的标题写出来：</strong></p><p>一、<strong>分析阅读的第一阶段：找出一本书在谈些什么的规则</strong><br>(1)    依照书的<strong>种类与主题</strong>来<strong>分类</strong>。<br>(2)    使用<strong>最简短的文字说明整本书在谈些什么</strong>。<br>(3) 将<strong>主要部分按顺序与关联性列举出来</strong>。<strong>将全书的大纲列举出来</strong>，井<strong>将各个部分的大纲也列出来</strong>。<br>(4) 确定作者<strong>想要解决的问题</strong>。</p><p>二、<strong>分析阅读的第二阶段：诠释一本书的内容规则</strong><br>(5)    诠释作者的<strong>关键字</strong>，与他<strong>达成共识</strong>。<br>(6)    由<strong>最重要的句子</strong>中，<strong>抓住作者的</strong>重要主旨。<br>(7)    知道作者的<strong>论述是什么</strong>，从内容中<strong>找出相关的句子</strong>，再<strong>重新架构</strong>出来。<br>(8)    确定作者已经<strong>解决了哪些问题</strong>，<strong>还有哪些是没解决的</strong>。再<strong>判断哪些是作者知道他没解决的问题</strong>。</p><p>三、<strong>分析阅读的第三阶段：像是沟通知识一样地评论一本书的规则</strong><br>A.   <strong>智慧礼节的一般规则</strong><br>(9)    除非你已经完成大纲架构，也能诠释整本书了，否则<strong>不要轻易批评</strong>。（在你说出：“我读懂了！”之前，不要说你同意、不同意或暂缓评论。）<br>(10)    <strong>不要争强好胜</strong>，非辩到底不可。<br>(11)    在<strong>说出评论之前</strong>，你要<strong>能证明自己区别得出真正的知识与个人观点的不同</strong>。</p><p>B.   <strong>批评观点的特别标准</strong><br>(12)    证明作者的<strong>知识不足</strong>。<br>(13)    证明作者的<strong>知识错误</strong>。<br>(14)    证明作者<strong>不合逻辑</strong>。<br>(15)    证明作者的<strong>分析与理由是不完整的</strong>。</p><p><strong>注意：</strong>关于最后这四点，前三点是表示不同意见的准则，如果你无法提出相关的佐证，就必须同意作者的说法，或至少一部分说法。你只能因为最后一点理由，对这本书暂缓评论。</p><p>如果你是为了追求知识而阅读，除非你能判断作者所提出的事实的意义，或者应该具备的意义，否则称不上有头脑的阅读。作者所提出的事实，很少没经过有意无意的诠释。尤其如果你读的是文摘类的作品，那都是根据某种意义，或某种诠释原则而过滤过的事实。如果你阅读的是启发性的作品，这个问题更是没有终了的时刻。在学习的任何一个阶段，你都要回顾一下这个问题：<strong>“这究竟有没有意义？”</strong></p><p>我们已经提过的这四个问题，总结了身为读者应尽的义务。前三个，与人类语言的沟通天性有关。如果沟通并不复杂，就用不着做出大纲来。如果语言是完美的沟通媒介，而不是有点不透明，就用不着诠释彼此的想法了。如果错误与无知不会局限真实或知识，我们也根本用 不着批评了。第四个问题区别了讯息(information)与理解(under­standing)之间的差异。如果你阅读的读物是以传递讯息为主，你就要自己更进一步，找出其中的启发性来。即使你被自己阅读的东西所启发了，你也还要继续往前探索其中的意义。</p><p>分析阅读的规则是一个理想化的阅读。这些规则只是衡量阅读层次的理想标准。你是个好读者，也就能达到你应该达到的阅读层次。</p><h4 id="11-5-小结"><a href="#11-5-小结" class="headerlink" title="11.5 小结"></a>11.5 小结</h4><p>当我们说某人读书“读得很好”(Well-read)时，我们心中应该要有这些标准来作衡量的依据。太多时候，我们是用这样的句子来形容一个人阅读的量，而非阅读的质。<strong>一个读得很广泛，却读不精的人，与其值得赞美，不如值得同情。</strong></p><p>伟大的作者经常也是伟大的读者，但这并不是说他们阅读所有的书。他们精通自己所阅读的书，他们的程度就可以跟作者相匹敌。他们有权被称作权威人士。在这种状况下，很自然地，一个好学生通常会变成老师，而一位好的读者也会变成作者。</p><p>运用本书所提供的规则，<strong>仔细地阅读一本书</strong>，而不是浮面地阅读大量的书，就是一个好读者能达到的理想境界了。当然，许多书都值得精读。但有<strong>更多的书只要浏览一下</strong>就行了。<strong>要成为一个好读者，就要懂得依照一本书的特质，运用不同的阅读技巧来阅读。</strong></p><h3 id="第十二章-辅助阅读"><a href="#第十二章-辅助阅读" class="headerlink" title="第十二章 辅助阅读"></a>第十二章 辅助阅读</h3><p>除了书籍本身之外，任何辅助阅读我们都可以称作是外在的阅读。所谓“内在阅读”(intrinsic reading)，意思是指阅读书籍的本身，与所有其他的书都是不相关的。而“外在阅读”(extrinsic reading)指的是我们借助其他一些书籍来阅读一本书。</p><p>一直将焦点集中在身为读者的基本工作上——<strong>拿起一本书来研究，运用自己的头脑来努力，不用其他的帮助</strong>。但是如果一直这样做下去，可能就错了。外在阅读可以帮上这个忙。有时候还非要借助外在阅读，才能完全理解一本书呢！</p><p>在理解与批评一本书的过程中，内在与外在的阅读通常会混在一起。在诠释、批评与做大纲时，我们都难免受到过去经验的影响。在阅读这本书之前，我们一定也读过其他的书。没有人是从分析阅读开始阅读第一本书的。我们可能不会充分对照其他书籍或自己生活里的经验，但是我们免不了会把某一位作者对某件事的声明与结论，拿来跟我们所知的，许多不同来源的经验作比较。这也就是俗话说的，我们不应该，也不可能完全孤立地阅读一本书。</p><p>整体来说，在你找寻外力帮助之前，最好能自己一个人阅读。如果你经常这么做，最后你会发现越来越不需要外界的助力了。</p><p>外在的辅助来源可以分成四个部分。<br><strong>第一，相关经验。</strong><br><strong>第二，其他的书。</strong><br><strong>第三，导论与摘要。</strong><br><strong>第四，工具书。</strong></p><p>根据一般的阅读常识来说，<strong>你依照内在阅读的规则尽力将一本书读完之后，却还是有一部分不懂或全部都不懂时，就应该要找外在的帮助了。</strong></p><h4 id="12-1-相关经验的角色"><a href="#12-1-相关经验的角色" class="headerlink" title="12.1 相关经验的角色"></a>12.1 相关经验的角色</h4><p><strong>特殊经验</strong>则需要<strong>主动地寻找</strong>，只有当一个人碰到困难时才会用得上。特殊经验的最佳例子就是在实验室中进行的实验，但也不一定需要有实验室。  </p><p><strong>一般经验</strong>并不一定要每个人都有才叫一般。一般(Common)与全体(Universal)是有点差别的。</p><p>这两种经验主要是跟不同的书籍有关。</p><p>一般经验在一方面与阅读小说有关，另一方面与阅读哲学书籍有关。判断一本小说的写实性，完全要依赖一样，我们从自己的生活体验来看这本书是真实或不够真实。</p><p>特殊经验主要是与阅读科学性作品有关。要理解与判断一本科学作品所归纳的论点，你就必须了解科学家所作的实验报告与证明。有时候科学家在形容一个实验时栩栩如生，你读起来一点困难也没有。有时说明图表会帮助你了解这些像是奇迹般的描述。</p><p>阅读历史作品，同时与一般经验及特殊经验都有关。这是因为历史掺杂着虚构与科学的部分。</p><p>要怎样才能知道你是否适当地运用自己的经验，来帮助你读懂一本书呢？问问你自己：在你觉得自己了解了的某一点上，能不能举出一个实例来？<strong>在你不太确定自己有没有掌握一本书时，不妨这样测验一下你自己。</strong> </p><h4 id="12-2-其他的书可以当作阅读时的外在助力"><a href="#12-2-其他的书可以当作阅读时的外在助力" class="headerlink" title="12.2 其他的书可以当作阅读时的外在助力"></a>12.2 其他的书可以当作阅读时的外在助力</h4><p>我们的建议尤其适用于所谓巨著。一般人总是抱着热忱想要阅读巨著，但是当他绝望地感觉到自已无法理解这本书时，热忱很快便消退了。其中一个原因，当然是因为一般人根本不知道要如何好好地阅读一本书。但还不只如此，还有另一个原因：他们认为自己应该能够读懂自己所挑选的第一本书，用不着再读其他相关的著作。</p><p>许多伟大的作品不只是互相有关联，而且在写作时还有特定的先后顺序，这都是不该忽略的事。后人的作品总是受到前人的影响。如果你先读前一位的作品，他可能会帮助你了解后人的作品。 <strong>阅读彼此相关的书籍，依照写作的时间顺序来读，对你了解最后写的作品有很大帮助。这就是外在辅助阅读的基本常识与规则。</strong></p><p><strong>外在辅助阅读的主要功用在于延伸与一本书相关的内容脉络。</strong>我们说过文章的脉络有助于诠释字义与句子，找出共识与主旨。就像一整本书的脉络是由各个部分贯穿起来一样，相关的书籍也能提供一个大型的网路脉络，以帮助你诠释你正在阅读的书。</p><p>我们经常会发现，一本伟大的著作总会有很长的对话部分。伟大的作者也是伟大的读者，想要了解他们，不妨读一读他们在读的书。身为读者，他们也是在与作者对话，就像我们在跟我们所阅读的书进行对话一样。只不过我们可能没写过其他的书。</p><p>想要加入这样的谈话，我们一定要读与巨著相关的著作，而且要依照写作前后的年表来阅读。有关这些书的对话是有时间顺序的。时间顺序是最基本的，千万不要忽略了。阅读的顺序可以是从现代到过去，也可以从过去到现代。虽然从过去读到现代的作品因为顺其自然而有一定的好处，不过年代的角度也可以倒过来观察。</p><p>顺便提醒一下，比起科学与小说类的书，阅读历史与哲学的书时，比较需要阅读相关的书籍。尤其是阅读哲学书时更重要，因为哲学家互相都是彼此了不起的读者。在小说与戏剧中，这就比较不重要了。如果真是好作品，可以单独阅读。当然一些文评家并不想限制自己这么做。</p><h4 id="12-3-如何运用导读与摘要"><a href="#12-3-如何运用导读与摘要" class="headerlink" title="12.3 如何运用导读与摘要"></a>12.3 如何运用导读与摘要</h4><p>第三种外在的<strong>辅助阅读包括导读(commentary)与摘要(ab­stract)</strong>。这里要强调的是，<strong>在运用这些资料时要特别聪明，也就是要尽量少用</strong>。这么说有两个理由。</p><p><strong>第一，</strong> <strong>一本书的导读并不一定都是对的</strong>。当然这些导读的用处很大，但却并不像我们希望的那样经常有用。</p><p><strong>第二，</strong> <strong>就算他们写对了，可能也不完整</strong>。因此，你可能在书中发现一些重点，而那些导读者却没有发现到。阅读这类导读，尤其是自以为是的导读，会限制你对一本书的理解，就算你的理解是对的。</p><p><strong>一些关于如何使用导读的建议</strong></p><p>事实上，这已经很相当于<strong>外在阅读的基本规则</strong>。内在阅读的规则是在阅读一本书之前，你要先看作者的序与前言。相反地，外在的阅读规则是除非你看完了一本书，否则不要看某个人的导读。这个规则尤其适用于一些学者或评论家的导言。要正确地运用这些导读，必须<strong>先尽力读完一本书</strong>，然后还有些问题在干扰着你时，你才<strong>运用这些导读来解答问题</strong>。<strong>如果你先读了这些导读，可能会让你对这本书产生曲解。你会只想看那些学者或批评家提出的重点，而无法看到可能同样重要的其他论点。</strong></p><p>如果是用这样的方法阅读，附带读一些这类的导读书籍是很有趣的事。你已经读过全书，也都了解了。而那位导读者也读过这本书，甚至可能读了好几次，他对这本书有自己的理解。你接近他的作品时，基本上是与他站在同一个水平上的。然而如果你在阅读全书之前，先看了他的导读手册，你就隶属于他了。</p><p><strong>要特别注意的是，</strong>你必须读完全书之后，才能看这类诠释或导读手册，而不是在之前看。如果你已经看过全书，知道这些导读如果有错，是错在哪里，那么这样的导读就不会对你造成伤害。但是如果你完全依赖这样的书，根本没读过原书，你的麻烦就大了。</p><p><strong>还有一个重点，</strong>如果你养成了依赖导读的习惯，当你找不到这类书时，你会完全不知所措。你可能可以借着导读来了解某一本作品，但一般而言，你不会是个好读者。</p><p>这里所说的外在阅读的规则也适用于摘录或情节摘要之类的作品。他们有两种相关的用途，也只有这两种。</p><p><strong>第一，</strong>如果你<strong>已经读过一本书，这些摘要能唤醒你的记忆</strong>。理想上，<strong>在分析阅读时，你就该自己作这样的摘要</strong>。 </p><p><strong>第二，</strong>在主题阅读时，<strong>摘要的用处很大，</strong>你可以因此知道某些特定的议题是与你的主题密切相关的。<strong>摘要绝不能代替真正的阅读，但有时却能告诉你，你想不想或需不需要读这本书。</strong></p><h4 id="12-4-如何运用工具书"><a href="#12-4-如何运用工具书" class="headerlink" title="12.4 如何运用工具书"></a>12.4 如何运用工具书</h4><p>工具书的类型有许多种。下面是我们认为最主要的两种：<strong>字典与百科全书</strong>。 无论如何，对于其他类型的工具书，我们也还是有很多话要说的。</p><p>虽然这是事实，但可能很多人不了解，那就是在你能运用工具书之前，你自己已经具备了很多知识：尤其是你必须有四种基本的知识。因此，工具书对矫正无知的功能是有限的。那并不能帮助文盲，也不能代替你思考。</p><p><strong>要善用工具书，</strong>首先你必须有一些想法，不管是多模糊的想法，那就是你想要知道些什么？你的无知就像是被光圈围绕着的黑暗。你一定要能将光线带进黑暗之中才行。而除非光圈围绕着黑暗，否则你是无法这么做的。换句话说，你一定要能对工具书问一个明智的问题。否则如果你只是彷惶迷失在无知的黑幕中，工具书也帮不上你的忙。</p><p><strong>其次，</strong>你一定要知道在哪里找到你要找的答案。你要知道自己问的是哪一类的问题，而哪一类的工具书是回答这类问题的。没有一本工具书能回答所有的问题，无论过去或现在，所有的工具书都是针对特定问题而来的。尤其是，事实上，<strong>在你能有效运用工具书之前，你必须要对主要类型的工具书有一个全盘的了解。</strong></p><p>在工具书对你发挥功用之前，你还必须有第三种知识。你必须要知道这本书是怎么组织的。在阅读工具书时也一样，要看完编辑说明如何使用这本书之后，才开始阅读内容。</p><p>当然，<strong>工具书并不能回</strong>答所有的问题。</p><p><strong>要明智地运用工具书的第四个条件就是：</strong><br><strong>你必须知道你想要找的是什么，在哪一种工具书中能找到这样的东西。</strong>你也要知道如何在工具书中找到你要的资料，还要能确定该书的编者或作者知道哪个答案。在你使用工具书之前，这些都是你应该清楚知道的事。对一无所知的人来说，工具书可说是毫无用处。工具书并不是茫然无知的指南。</p><h4 id="12-5-如何使用字典"><a href="#12-5-如何使用字典" class="headerlink" title="12.5 如何使用字典"></a>12.5 如何使用字典</h4><p>字典是一种工具书，以上所说的工具书问题在使用时都要考虑进去。</p><p>不论<strong>字典</strong>是如何编辑的，<strong>主要目的还是教育的工具</strong>。</p><p><strong>阅读任何一本书的第一个规则是</strong>：<strong>知道这是一本什么样的书。</strong></p><p><strong>从四个方面来看待文字：</strong></p><p>(1)  <strong>文字是物质的——可以写成字，也可以说出声音</strong>。因此，在拼字与发音上必须统一，虽然这种统一常被特例变化所破坏，但并不像你某些老师所说的那样重要。</p><p>(2)  <strong>文字是语言的一部分</strong>。在一个较复杂的句子或段落的结构中，文字扮演了文法上的角色。同一个字可以有多种不同的用法，随着不同的谈话内容而转变意义，特别是在语音变化不明显的英文中更是如此。</p><p>(3)  <strong>文字是符号——这些符号是有意义的，不只一种意义，而是很多种意义</strong>。这些意义在许多方面是互相关联的。 有时候会细微地变化成另一种意义，有时候一个字会有一两组完全不相干的意义。 因为意义上的相通，不同的字也可能互相连接起来——就像同义字，不同的字却有同样的意义。或是反义字，不同的字之间有相反或对比的意义。</p><p>(4)  <strong>文字是约定俗成的——这是人类创造的符号</strong>。 这也是为什么每个字都有历史．都有历经变化的文化背景。从文字的字根、字首、字尾，到词句的来源，我们可以看出文字的历史。</p><p>字典是一种完美的自修工具书，因为它告诉你要注意什么，如何诠释不同的缩写字，以及上面所说的四种有关文字符号的知识。任何人不善读一本字典开头时所作的解释以及所列的缩写符号，那用不好字典就只能怪他自己了。</p><h4 id="12-6-如何使用百科全书"><a href="#12-6-如何使用百科全书" class="headerlink" title="12.6 如何使用百科全书"></a>12.6 如何使用百科全书</h4><p>百科全书和一般光提供讯息的书不同，它所能提供的理解取决于你对这些相关事实之间的关系的了解。</p><p>使用百科全书，读者必须要依赖编者的帮忙与建议。任何一本好的百科全书都有引言，指导读者如何有效地运用这本书，你一定要照着这些指示阅读。通常，这些引言都会要使用者在翻开字母排列的内容之前，先查证一下索引。</p><p>字典是关于文字的，而百科全书是关于事实的。</p><p>(1)  <strong>事实是一种说法(proposition)——说明一个事实时，会用一组文字来表达</strong>。为了全盘地了解知识，你必须知道事实的意义——这个意义又如何影响到你在找寻的真理。如果你知道的只是事实本身，表示你了解的并不多。</p><p>(2)  <strong>事实是一种“真实”的说法(“True” proposition)——事实不是观点</strong>。当有人说：”事实上……”的时候，表示他在说的是一般人同意的事。由于百科全书必须只报导事实，不掺杂观点（除了上述的方法），因而也限制了记载的范围。它不能处理一些未达成共识的主题——如果真的要处理这些问题，只能列举人们各种不同的说法。</p><p>(3)  <strong>事实是真相的反映</strong>——事实可能是</p><ol><li>一个资讯；</li><li>不受怀疑的推论。 </li></ol><p>不管是哪一种，都代表着事情的真相。因此，事实如果只是对真相提出一点揣测，那就称不上是观念或概念，以及理论。同样地，对真相的解释（或部分解释），除非众所公认是正确的，否则就不能算是事实。</p><p>(4)  <strong>事实是某种程度上的约定俗成——我们说事实会改变</strong>。我们的意思是，某个时代的事实，到了另一个时代却不是事实了。但既然事实代表 ”真实” ，当然是小会变的。因为真实．严格来说是不会变的，事实也不会变。不过所有我们认为是真实的主旨．并不一定都是真实的。我们一定要承认的是，任何我们认为是真实的主旨，都可能被更有包容力或更正确的观察与调查证明是错的。与科学有关的事实更是如此。</p><p>事实——在某种程度上——也受到文化的影响。</p><p>如果你记住前面有关事实的叙述，一本好的百科全书会回答你有关事实的所有问题。将百科全书当作是辅助阅读的艺术，也就是能对事实提出适当问题的艺术。就跟字典一样，我们只是帮你提出问题来，百科全书会提供答案的。</p><p>还要记得一点，百科全书不是追求知识最理想的途径。你可能会从其中条理分明的知识中．获得启发，但就算是在最重要的问题上，百科全书的启发性也是有限的。理解需要很多相关条件，在百科全书中却找不到这样的东西。</p><h2 id="第三篇-阅读不同读物的方法"><a href="#第三篇-阅读不同读物的方法" class="headerlink" title="第三篇 阅读不同读物的方法"></a>第三篇 阅读不同读物的方法</h2><h3 id="第十三章-如何阅读实用型的书"><a href="#第十三章-如何阅读实用型的书" class="headerlink" title="第十三章 如何阅读实用型的书"></a>第十三章 如何阅读实用型的书</h3><p><strong>越通用的规则就越少</strong>，这算是一个好处。而<strong>越通用的规则，也越容易理解——容易学会与使用这些规则</strong>。但是，说实在的，当你置身错复杂的实际情况，想要援用一些规则的时候你也会发现越通用的规则离题越远。</p><p>我们前面谈过分析阅读的规则，一般来说是适用于论说性的作品也就是说任何一种传达知识的书。但是你不能只用一般通用的方法来读任何一本书。你可能读这本书那本书，或是任何一种特殊主题的书,可能是历史、数学、政治论文或科学研究，或是哲学及神学理论，因此，在运用以下这些规则时，你一定要有弹性，并能随时调整。幸运的是，当你开始运用这些规则时你会慢慢感觉到这些规则是如何在不同的读物上发挥作用。</p><p>要特别提醒的是，在第十一章结尾时所说明的十五个阅读规则并不适用于阅读小说或诗集。</p><p>这四个问题与任何本书都有关，不论是虚构或非虚构，不论是诗、历史、科学或哲学。</p><h4 id="13-1-两种实用性的书"><a href="#13-1-两种实用性的书" class="headerlink" title="13.1 两种实用性的书"></a>13.1 两种实用性的书</h4><p>关于实用性的书有一件事要牢记在心：<strong>任何实用性的书都不能解决该书所关心的实际问题</strong>。一本理论性的作品可以解决自己提出的问题。但是<strong>实际的问题却只能靠行动来解决。</strong></p><p>你必须自已进行有活力的阅读过程，不只是读这本书，还要读很多其他的书。这也是为什么老话说：<strong>只有行动能解决问题</strong>。<strong>行动只能在现世发生，而不是在书本中发生。</strong></p><p><strong>读者一定要能加上一点自己的想法，才能运用在实际的状况中</strong>。他要能<strong>更了解实际状况</strong>，<strong>更有判断力</strong>，知道如何将规则应用在这样的状况中。</p><p><strong>任何书里包含了规则——原理、准则或任何一种一般的指导你都要认定是一本实用性的书。</strong>规则底下的原理通常都很科学，换言之，属于理论性的知识。规则与原理的结合，就是事物的理论。因此，我们谈造桥的理论，也谈打桥牌的理论。我们的意思是，<strong>理论性的原则会归纳出出色的行事规则</strong>。</p><p><strong>实用性的书因此可分为两种类型：</strong></p><ol><li><strong>说明规则</strong>的实用性的书</li><li><strong>阐述形成规则</strong>的原理的实用性的书</li></ol><p>不管是在什么领域中，谈规则的书都可以立刻认出来是实用性的。一本谈实用原理的书，乍看之下会以为是理论性的书。</p><p>在阅读一本以规则为主的书时，要找寻的主旨当然是那些规则。阐述这些规则通常是用命令句，而不是叙述句。</p><p>无论是叙述句或命令句，你总是能认出一个规则来，因为它在建议你某件事是值得做的，而且一定会有收获。</p><p><strong>无法让一本实用的书被实用地阅读，就是失败的阅读。</strong>如果在原理中能找到可以理解的规则，那么也就可以在由原理引导出来的规则或建议的行动中，找到实用原理的意义。</p><p>行为规则要谈得上是真理，有两种情况：</p><ol><li><strong>真的有效</strong></li><li><strong>是这样做能带引你到正确的结果，达到你的期望</strong></li></ol><p>如果你不认同仔细、头脑清楚地阅读是件值得做的事情，那么纵使本书的规则真的有效，这本书对你来说还是没什么实用性。</p><p>在评断一本理论性的书时，读者必须观察他自己与作者之间的原理与假设的一致性或差异性。<strong>在评断一本实用性的书时，所有的事都与结果及目标有关。</strong></p><h4 id="13-2-说服的角色"><a href="#13-2-说服的角色" class="headerlink" title="13.2 说服的角色"></a>13.2 说服的角色</h4><p>当你在阅读任何一种实用书时，一定要问你自己两个主要的问题。<br><strong>第一、作者的目的是什么？</strong><br><strong>第二、他建议用什么方法达到这个目的？</strong></p><p>对作品最终的评断是<strong>你是否接受他的结论，与他提议的方法。</strong></p><p>实用书的特性，<strong>一个人必须要被说服，以采取特定的思想与行动</strong>。<strong>实际的思考与行动除了需要理智以外，情感也是重要的因素</strong>。没有人可以没有受到感动，却认真采取实际评论或行动的。</p><p>一个人如果真正读懂了一本实用的书，他知道这本书的基本共识、主旨、论述是什么，就能觉察出作者的雄辩。但是，一个读者如果完全不接受所有内容的诉求，那就不必阅读实用性的书了。</p><h4 id="13-3-赞同实用书之后"><a href="#13-3-赞同实用书之后" class="headerlink" title="13.3 赞同实用书之后"></a>13.3 赞同实用书之后</h4><p>在读一本书时要提出的四个问题，到了读实用性的书时有了一点变化。</p><p><strong>第一个问题：这本书是在谈些什么？</strong><br>读任何书都得想办法找出一个<strong>作者的问题是什么</strong>（规则四涵盖这一点），不过在读实用性的书时，格外是一个决定性的关键。你一定要了解作者的目的是什么。换句话说，一定要知道他想解决的问题是什么。你一定要知道他想要做些什么——因为，在实用性的书中，知道<strong>他要做的是什么</strong>，就等于是知道<strong>他想要你做的是什么</strong>。这当然是非常重要的事了。</p><p><strong>第二个问题：回答关于这本书的意义或内容。</strong><br>你仍然要能够找出<strong>作者的共识、主旨与论述</strong>。规则八要你说出哪些是作者已经解决的问题，哪些是还没有解决的问题。你要发现并了解作者所建议的、达到他目标的方法。换句话说，在阅读实用性书时，如果规则四调整为：<strong>＂找出作者想要你做什么。”</strong>规则八就该调整为：<strong>”了解他要你这么做的目的。”</strong></p><p><strong>第三个问题：内容真实吗？</strong><br>在理论性作品中，当你根据自己的知识来比较作者对事物的描绘与说明时，这个问题的答案便出来了。如果这本书所描述的大致与你个人的体验相似时，你就必须承认那是真实的，或至少部分是真实的。实用性的书，虽然也会与真实作比较，但最主要的却是你能不能接受作者的宗旨——他最终的目标，加上他建议的达成目标的方法——这<strong>要看你认为追求的是什么，以及什么才是最好的追求方法而定。</strong></p><p><strong>第四个问题：这本书与我何干？</strong><br>(1)   <strong>同意分析阅读是值得做的。</strong><br>(2)   <strong>接受这些阅读规则，当作是达到目标的基本要件</strong>，你会像我们现在所说的一样，开始照着阅读起来。<br>如果你没有这么做，可能并不是你偷懒或太累了，而是你并不真的同意(1)或(2)。</p><p>某些作者提出的结论是很通用或一般性的——可供所有的人类使用——另外一些作者的结论却只有少数人能运用。如果结论是通用的——譬如像本书，所谈的是使所有人都能阅读得更好，而不是只有少数人——那么我们所讨论的便适用于每位读者。</p><p>心理问题会影响我们阅读实用性的作品。</p><h3 id="第十四章-如何阅读想像文学"><a href="#第十四章-如何阅读想像文学" class="headerlink" title="第十四章 如何阅读想像文学"></a>第十四章 如何阅读想像文学</h3><p>阅读想像文学的问题比阅读论说性作品的问题更为困难。然而，比起阅读科学、哲学、政治、经济与历史，一般人却似乎更广泛地拥有阅读文学的技巧。</p><p><strong>如何阅读想像文学的建议</strong><br><strong>开始</strong>，<strong>从否定的说法谈起</strong>，而不建立一些规则。<br><strong>其次</strong>，用<strong>类推的方法</strong>，简短地<strong>将阅读非小说的规则转化为阅读小说</strong>的规则。<br><strong>最后</strong>，<strong>阅读特殊形态的想像文学时所发生的问题</strong>，像是小说、戏剧与抒情诗。</p><h4 id="14-1-读想像文学的“不要”"><a href="#14-1-读想像文学的“不要”" class="headerlink" title="14.1 读想像文学的“不要”"></a>14.1 读想像文学的“不要”</h4><p>为了要用否定的形态来作说明，一开始就有必要掌握论说性作品与文学作品的差异。</p><p><strong>论说性作品要传达的是知识</strong>——在<strong>读者经验中曾经有过或没有过的知识</strong>。<br><strong>想像文学是在阐述一个经验本身</strong>——那是<strong>读者只能借着阅读才能拥有或分享的经验</strong>如果成功了，就<strong>带给读者一种享受</strong>。</p><p>我们都是运用判断与推论，也就是理智，才能理解事情。这并不是说我们在思考时用不上想像力，或我们的感官经验完全独立于理性的洞察与反应之外。</p><p>有关想像文学的事实，带引出我们要建议的否定的指令：<strong>不要抗拒想像文学带给你的影响力</strong>。</p><p>阅读一部<strong>伟大的文学作品</strong>的规则应该<strong>以达成某种深沉的经验为目标</strong>。这些规则应该尽可能去除我们体验这种深刻感受的阻碍。</p><p><strong>在想像文学中，不要去找共识、主旨或论述</strong>。</p><p>最后一个否定的指令：<strong>不要用适用于传递知识的，与真理一致的标准来批评小说</strong>。</p><p>这是真的吗？我们也有这样的感觉吗？ 我们是不是总是这样想，却从来没有意识到？以前或许很模糊的事，现在是不是却很明显了？作者的理论或说明虽然可能很复杂，是不是却比我们过去对这个观念的混淆来得清楚，也简单多了？</p><p>如果我们能很肯定地回答上述问题，我们与作者之间的沟通便算是建立起来了。当我们了解，也不反对作者的观点时，我们一定要说：”<strong>这确实是我们共通的观念。我们测验过你的理论，发现是正确的</strong>。”</p><h4 id="14-2-阅读想像文学的一般规则"><a href="#14-2-阅读想像文学的一般规则" class="headerlink" title="14.2 阅读想像文学的一般规则"></a>14.2 阅读想像文学的一般规则</h4><p><strong>阅读论说性作品的三组规则</strong><br><strong>第一组：</strong>找出作品的如整体及部分结构<br><strong>第二组：</strong>定义与诠释书中的共识、主旨与论述<br><strong>第三组：</strong>评论作者的学说，以赞同或反对的意见完成我们对他的作品的理解</p><p>称这三组规则为<strong>架构性、诠释性与评论性</strong>的。</p><p>首先，我们可以将架构性的规则——拟大纲的规则——改变为适合阅读小说的规则：<br>(1)  你必须<strong>将想像文学作品分类</strong>。抒情诗在叙述故事时，基本上是<strong>以表达个人情绪的经验为主</strong>。<br>(2)  你要能<strong>抓住整本书的大意</strong>。你能不能<strong>掌握</strong>这一点，要看你<strong>能不能用一两句话来说明整本书的大意</strong>。<br>(3)  你不仅要能将<strong>整本书简化为大意</strong>，还要能<strong>发现整本书各个部分是如何架构</strong>起来的。</p><p><strong>阅读小说时候的诠释规则是什么？</strong><br>(1)  小说的要素是插曲、事件、角色与他们的思想、言语、感觉及行动。<br>(2)  <strong>共识与主旨有关</strong>。小说的要素与整个表现的场景或背景有关。<br>(3)  如果说论说性作品中有任何活动，那就是<strong>论述的发展</strong>。由证据与理由到结构的一个逻辑性的演变。</p><p>任何经验都有时间顺序，无论多么短暂飘渺的经验都是如此。</p><p><strong>小说的阅读批评规则是什么？</strong><br><strong>在论说性作品中</strong>的规则是：<strong>在你还不了解一本书之前，不要评论一本书——不要说你同意或反对这个论点</strong>。</p><p><strong>小说的阅读批评</strong>的规则是：<strong>在你衷心感激作者试着为你创造的经验之前，不要批评一本想像的作品</strong>。</p><p>你不只要能说明你自已为什么喜欢或不喜欢，还要能<strong>表达出</strong>这本书中<strong>哪些地方是好的</strong>，<strong>哪些是不好的</strong>，并<strong>说明理由</strong>才行。</p><p>你会慢慢<strong>建立起批评的标准</strong>，你也会发现许多跟你有同样品味的人与你一起分享你的论点。你还可能会发现一件我们相信如此的事：<strong>懂得阅读方法的人，文学品味都很高</strong>。</p><h3 id="第十五章-阅读故事、戏剧与诗的一些建议"><a href="#第十五章-阅读故事、戏剧与诗的一些建议" class="headerlink" title="第十五章 阅读故事、戏剧与诗的一些建议"></a>第十五章 阅读故事、戏剧与诗的一些建议</h3><p><strong>三个问题：</strong><br>第一，<strong>这整本书的内容是在谈些什么？</strong><br>第二，<strong>内容的细节是什么？是如何表现出来的？</strong><br>第三，<strong>这本书说的是真实的吗？全部真实或部分真实？</strong></p><p>要回答第一个问题，就是你<strong>能说出关于一个故事、戏剧或诗的情节大意，并要能广泛地包括故事或抒情诗中的动作与变化。</strong><br>要回答第二个问题，你就<strong>要能辨识剧中所有不同的角色，并用你自己的话重新叙述过发生在他们身上的关键事件。</strong><br>要回答第三个问题，就是你<strong>能合理地评断一本书的真实性。</strong>这像一个故事吗？这本书能满足你的心灵与理智吗？你欣赏这本书带来的美吗？不管是哪一种观点，你能说出理由吗？</p><p>第四个问题是：<strong>这本书与我何关？</strong><br>在想像文学作品中，这个问题与阅读诗与故事毫无关系。严格说起来，在你读好了——也就是分析好了小说、戏剧或诗之后，是用不着采取什么行动的。在你采取类似的分析阅读，回答前面三个问题之后，你身为读者的责任就算尽到了。</p><p>不过，阅读故事与小说的主要目的并不是要采取实际的行动。想像文学可以引导出行动，但却并非必要，因为它们属于纯艺术的领域。</p><p>如果你受到一本书的影响，而走出户外进行任何行动时，要问问你自己，那本书是否包含了激励你的宣言，让你产生行动力？这是想像文学本身就拥有的自主权。<strong>要把这些文学作品读通，你惟一要做的事就是去感受与体验</strong>。</p><h4 id="15-1-如何阅读故事书"><a href="#15-1-如何阅读故事书" class="headerlink" title="15.1 如何阅读故事书"></a>15.1 如何阅读故事书</h4><p>第一个建议是：<strong>快读，并且全心全意地读。</strong></p><p>要达到这个理想，最接近的方法就是<strong>将阅读一篇好故事</strong>的<strong>时间压缩到合理的长度</strong>。否则你可能会忘了其间发生的事情，也会漏掉一些完整的情节，最后不知道自己在读的是什么了。</p><p>建议是<strong>要读得很快，而且全神投入</strong>。</p><p>让<strong>角色进人你的心灵</strong>之中，<strong>相信</strong>其中<strong>发生的事件</strong>，就算有疑惑也不要怀疑。在你了解一个角色为什么要做这件事之前，不要心存疑虑。尽量<strong>试着活在他的世界</strong>里，而不是你的世界，这样他所做的事就很容易理解了。除非你真的尽力”活在”这样的虚构世界中，否则不要任意批评这个世界。</p><p>阅读每一本书时要提出的问题——<strong>这整本书在谈些什么？</strong><br>除非你能很快读完，否则你没法看到整个故事的大要。如果你不专心一致地读，你也会漏掉其中的细节。</p><p>故事就像我们的人生一样，在生命中，我们不可能期望了解每一件发生在我们身上的事，或把一生全都看清楚。但是，<strong>当我们回顾过去时，我们便了解为什么了</strong>。所以，读者在阅读小说时，全部看完之后再<strong>回顾一下</strong>，就会了解事件的关联与活动的前后顺序了。</p><p>所有这些都回到同一个重点：<strong>你一定要读完一本小说之后，才能谈你是否把这个故事读通了</strong>。在阅读一本小说时，在第一页之前， 到最后一页之后，你对那些角色会发生些什么事所产生的想像，跟下一个阅读的人没什么两样。</p><p>对人类而言，小说或虚构的故事似乎是不可或缺的。为什么？<br>其中一个理由是：<strong>小说能满足我们潜意识或意识中许多的需要</strong>。如果只是<strong>触及意识的层面</strong>，像<strong>论说性作品</strong>一样，当然是很重要的。但<strong>小说</strong>一样也很重要，因为它<strong>触及潜意识的层面</strong>。</p><h4 id="15-2-关于史诗的重点"><a href="#15-2-关于史诗的重点" class="headerlink" title="15.2 关于史诗的重点"></a>15.2 关于史诗的重点</h4><p>阅读任何一部重要的史诗对读者来说都有额外的要求——<strong>要求你集中注意力，全心参与并运用想像力</strong>。阅读史诗所要求的努力确实是不简单的。</p><p>好的阅读我们该说是<strong>分析阅读能让我们收获良多</strong>，而阅读史诗，至少就像阅读其他小说作品一样，能让我们的<strong>心灵更上层楼</strong>。不幸的是，如果读者不能善用阅读技巧来阅读这些史诗，将会一无所获。</p><h4 id="15-3-如何阅读戏剧"><a href="#15-3-如何阅读戏剧" class="headerlink" title="15.3 如何阅读戏剧"></a>15.3 如何阅读戏剧</h4><p>如果你没有<strong>将剧本搬上心灵的舞台演出过</strong>，或许你还不能算是读过剧本了。就算你读得再好，也只是读了一部分而已。</p><p><strong>把剧本大声地读出来</strong>倒经常是不错的方法。<strong>要慢慢读，就像是听众在听你说话一样，还是带着感情读也就是说要让那些句子对你别有深意。</strong>这个简单的建议会帮助你解决许多问题。只有<strong>当这样做之后还有问题，才要找注解来帮助你阅读</strong>。</p><h4 id="15-4-关于悲剧的重点"><a href="#15-4-关于悲剧的重点" class="headerlink" title="15.4 关于悲剧的重点"></a>15.4 关于悲剧的重点</h4><p>第一，<strong>记住悲剧的精髓在时间，或是说缺乏时间</strong>。<br>对我们来说很容易看出来该做些什么，但我们能在有限的时间中看清楚一切吗？在阅读希腊悲剧时，你要一直把这个问题放在心中。</p><p>第二，我们确实知道在希腊的戏剧中，所有的悲剧演员都穿一种高出地面几英寸的靴子（他们也戴面具）。<br>因此你要记得，在读旁白的部分时，你要想像这些台词是跟你一般身高的人所说出来的话，而在读悲剧人物的台词时，你要想像这是出自一个大人物的口中，他们不只是在形象上，在实际身高上也高出你一截。</p><h4 id="15-5-如何阅读抒情诗-Lyric-Poetry"><a href="#15-5-如何阅读抒情诗-Lyric-Poetry" class="headerlink" title="15.5 如何阅读抒情诗(Lyric Poetry)"></a>15.5 如何阅读抒情诗(Lyric Poetry)</h4><p>最简单的有关诗的定义，就是诗人所写的东西。</p><p>无论我们心中如何激荡着原始的诗情，但是诗仍是由<strong>文字组成的</strong>，而且是以条理分明，精巧熟练的方式所组合出来的。</p><p><strong>两个观念：</strong><br><strong>第一</strong>，抒情诗，任何现代诗，只要你<strong>肯拿起来读</strong>，你会发现并不像你想的要花那么大的功夫。<br><strong>其次</strong>，那绝对是值得你花时间与精力去做的事。</p><p>一首好诗可以<strong>用心研读，一读再读</strong>，并在你一生当中<strong>不断地想起这首诗</strong>。你会在诗中<strong>不断地找到新点子、新的乐趣与启示</strong>，对你自己及这个世界<strong>产生新的想法</strong>。</p><p>阅读抒情诗的<strong>第一个规则</strong>是：<strong>不论你觉得自己懂不懂，都要一口气读完，不要停</strong>。</p><p>任何一首诗都有个<strong>整体大意</strong>。除非我们一次读完，否则无法理解大意是什么，也很难发现诗中<strong>隐藏的基本感觉与经验</strong>是什么。尤其是在一首诗中，中心思想绝不会在第一行或第一段中出现的。那是整首诗的意念，而不是在某一个部分里面。</p><p>阅读抒情诗的<strong>第二个规则</strong>是：<strong>重读一遍——大声读出来</strong>。<br><strong>大声朗诵诗句</strong>，会发现似乎说出来的字句<strong>可以帮助你更了解这首诗</strong>。<strong>如果你朗诵出来，比较不容易略过那些不了解的字句，你的耳朵会抗议你的眼睛所忽略的地方。</strong>诗中的节奏或是有押韵的地方，能帮助你把该强调的地方突显出来，增加你对这首诗的了解。最后，你会对这首诗打开心灵，让它对你的心灵发生作用一如它应有的作用。</p><p>如果一个人觉得自己不能读诗，只要能遵守前面这两个规则来读，就会发现比较容易一些了。一旦你掌握住一首诗的大意时，就算是很模糊的大意，你也可以开始提出问题来。</p><p><strong>只要一个人愿意努力，几乎任何人都能读任何诗</strong>。你发现任何有关作者生活与时代的资讯，只要是确实的都有帮助。要了解一首诗，一定要去读它——<strong>一遍又一遍地读</strong>。</p><h3 id="第十六章-如何阅读历史书"><a href="#第十六章-如何阅读历史书" class="headerlink" title="第十六章 如何阅读历史书"></a>第十六章 如何阅读历史书</h3><h4 id="16-1-难以捉摸的史实"><a href="#16-1-难以捉摸的史实" class="headerlink" title="16.1 难以捉摸的史实"></a>16.1 难以捉摸的史实</h4><p>一件历史的”事实”——虽然我们感觉很相信这两个字代表的意义，但却是世上<strong>最难以捉摸的</strong>。当然，某一种历史事实是可以很确定的。（比如一些重大事件的历史事件的记载，时间是应该不会错的。）</p><h4 id="16-2-历史的理论"><a href="#16-2-历史的理论" class="headerlink" title="16.2 历史的理论"></a>16.2 历史的理论</h4><p>当然，一个好的历史学家是不会编造过去的。他认为自己对某些观念、事实，或精准的陈述责无旁贷。不过，有一点不能忘记的是，历史学家一定要编纂一些事情。他不是在许多事件中找出一个共通的模式，就是要套上一个模式。他也总不免要指出事件发生的原因及行为的动机。</p><p>如果我们真的想要了解一个事件或时期的历史，就很有必要多看一些相关的论著。如果我们所感兴趣的事件对我们又有特殊意义的话，就更值得这么做了。我们认为每一种历史的写作都必定是从某个观点出发的。为了追求真相，我们必须从更多不同的角度来观察才行。</p><h4 id="16-3-历史中的普遍性"><a href="#16-3-历史中的普遍性" class="headerlink" title="16.3 历史中的普遍性"></a>16.3 历史中的普遍性</h4><p><strong>阅读历史的两个要点是：</strong><br><strong>第一</strong>，对你感兴趣的事件或时期，尽可能阅读一种以上的历史书。<br><strong>第二</strong>，阅读历史时，不只要关心在过去某个时间、地点真正发生了什么事，还要读懂在任何时空之中，尤其是现在，人们为什么会有如此这般行动的原因。</p><h4 id="16-4-阅读历史书要提出的问题"><a href="#16-4-阅读历史书要提出的问题" class="headerlink" title="16.4 阅读历史书要提出的问题"></a>16.4 阅读历史书要提出的问题</h4><p>在阅读历史时，我们也要像阅读论说性作品一样，提出基本的问题。<br><strong>第一个问题</strong>：每一本历史书都有一个<strong>特殊而且有限定范围</strong>的<strong>主题</strong>。通常读者很容易就看出这样的主题，不过，不见得会仔细到看出作者为自己所设定的范围。</p><p><strong>第二个问题</strong>：历史书在说一个故事，而这个故事当然是发生在一个特定的时间里。一般的纲要架构因此决定下来了，用不着我们去搜寻。</p><p>历史告诉我们人类过去所做的事，也经常引导我们作改变，尝试表现出更好的自我。历史会建议一些<strong>可行性</strong>，因为那是以前的人已经做过的事。既然是做过的事，就可能再做一次——或是可以<strong>避免再做</strong>。</p><h4 id="16-5-如何阅读传记与自传"><a href="#16-5-如何阅读传记与自传" class="headerlink" title="16.5 如何阅读传记与自传"></a>16.5 如何阅读传记与自传</h4><p>传记是一个<strong>真人的故事</strong>。这种作品一直以来就是有混合的传统，因此也保持着混杂的特性。</p><p>读者也要问同样的问题——<strong>作者的目的是什么</strong>？他所谓<strong>真实包含哪些条件</strong>？——这也是在读任何一本书时都要提出的问题。</p><p>我们要记得，没有任何文字是自己写出来的——我们所阅读到的文字都是由人所组织撰写出来的。</p><p>自传揭露了多少有关作者的秘密，我们都用不着花上一堆时间来研究作者并未言明的秘密。当然，如果你想知道一个人的一生，你就该尽可能去阅读你能找到的资料，包括他对自己一生的描述（如果他写过）。对于任何自传都要有一点<strong>怀疑心</strong>，同时别忘了，在你还不了解一本书之前，<strong>不要妄下论断</strong>。</p><p>传记是有<strong>启发性</strong>的。那是生命的故事，通常是成功者一生的故事——也可以当作我们<strong>生活的指引</strong>。</p><h4 id="16-6-如何阅读关于当前的事件"><a href="#16-6-如何阅读关于当前的事件" class="headerlink" title="16.6 如何阅读关于当前的事件"></a>16.6 如何阅读关于当前的事件</h4><p>在阅读时，<strong>四个基本问题是一定要提出来</strong>的。</p><p>所谓当前发生的事件，也就是跟“新闻”这两个字很类似。</p><p>阅读当代作品时，我们不会有时空的隔阂，因此我们除了要厘清作者心中的过滤器之外，也要弄清楚自己的想法才行。</p><p>读者要<strong>擦亮眼睛</strong>(Caveat lector)!要搞清楚他们的利益考虑，阅读任何东西都要小心翼翼。</p><h4 id="16-7-关于文摘的注意事项"><a href="#16-7-关于文摘的注意事项" class="headerlink" title="16.7 关于文摘的注意事项"></a>16.7 关于文摘的注意事项</h4><p>当我们尽心阅读这些摘要，就像他们在之前的尽心阅读以帮助我们作摘要一样，他们的功能对我们才会真正有帮助。</p><p><strong>越是浓缩过的摘要，筛选得越厉害</strong>。内容被浓缩得越多，我们对浓缩者的特质就更要有所了解。毕竟，在经过专业浓缩过的句子中，读者更要能读出言外之意才行。<strong>阅读文摘</strong>，有时是<strong>最困难又自我要求最多</strong>的一种阅读方式。</p><h3 id="第十七章-如何阅读科学与数学"><a href="#第十七章-如何阅读科学与数学" class="headerlink" title="第十七章 如何阅读科学与数学"></a>第十七章 如何阅读科学与数学</h3><p><strong>限定讨论两种形式的书：</strong><br>一种是在我们传统中，<strong>伟大的科学与数学</strong>的<strong>经典之作</strong>。另一种则是<strong>现代科普著作</strong>。</p><h4 id="17-1-了解科学这一门行业"><a href="#17-1-了解科学这一门行业" class="headerlink" title="17.1 了解科学这一门行业"></a>17.1 了解科学这一门行业</h4><p>我们毫不迟疑地要推荐你<strong>最少要阅读一些伟大的科学经典巨著</strong>。事实上，你真的没有借口不阅读这样的书。其中没有一本真的很难读，就算牛顿的《自然哲学的数学原理》(Mathematical Principles of Natural Philosophy)，<strong>只要你真的肯努力，也是可以读得通的</strong>。</p><p>你要做的就是<strong>运用阅读论说性作品的规则</strong>，而且要很<strong>清楚地知道作者想要解决的问题是什么</strong>。这个分析阅读的规则<strong>适用于任何论说性的作品</strong>，尤其<strong>适用于科学与数学的作品</strong>。</p><p>要<strong>跟上科学发展的脚步</strong>，<strong>找出</strong>事实、假定、原理与证据之间的<strong>相互关联</strong>，就是<strong>参与了人类理性的活动</strong>，而那可能是人类最成功的领域。也许，光这一点就能印证有关科学历史研究的价值了。此外，这样的研究还能在<strong>某种程度</strong>上<strong>消除一些对科学的谬误</strong>。</p><h4 id="17-2-阅读科学经典名著的建议"><a href="#17-2-阅读科学经典名著的建议" class="headerlink" title="17.2 阅读科学经典名著的建议"></a>17.2 阅读科学经典名著的建议</h4><p>所谓科学作品，就是在某个研究领域中，经过实验或自然观察得来的结果，所写成的研究报告或结论。叙述科学的问题总要尽量描述出正确的现象，<strong>找出不同现象之间的互动关系</strong>。</p><p>伟大的科学作品，尽管最初的假设不免个人偏见，但<strong>不会有夸大或宣传</strong>。你要注意作者最初的假设，放在心上，然后把他的假设与经过论证之后的结论作个区别。一个越“客观”的科学作者，越会明白地要求你接受这个、接受那个假设。<strong>科学的客观不在于没有最初的偏见，而在于坦白承认</strong>。</p><p>在科学作品中，<strong>主要的词汇</strong>通常都是<strong>一些不常见的或科技的用语</strong>。这些用语很容易找出来，你也<strong>可以经由这些用语找到主旨</strong>。主旨通常都是很一般性的。</p><p>在阅读科学作品时，似乎有两个主要的难题。<br>一个是有关论述的问题。<strong>科学基本上是归纳法</strong>，基本的论述也就是经由研究查证，建立出来的一个通则——可能是经由实验所创造出来的一个案例，也可能是长期观察所收集到的一连串案例。还有另外一些论述是<strong>运用演绎法来推论的</strong>。这样的论述是<strong>借着其他已经证明过的理论，再推论出来的</strong>。在讲求证据这一点上，科学与哲学其实差异不大。不过<strong>归纳法是科学的特质</strong>。</p><p>出现第一个困难的原因是：为了了解科学中归纳法的论点，你就必须<strong>了解科学家引以为理论基础的证据</strong>。如果这本书不能启发一个人时，读者只有一个解决办法，就是自己<strong>亲身体验以获得必要的特殊经验</strong>。他可能要亲眼看到实验的过程，或是去观察与操作书中所提到的相同的实验仪器。他也可能要去博物馆观察标本与模型。</p><p>任何人想要了解科学的历史<strong>，除了阅读经典作品外，还要能自己做实验</strong>，以熟悉书中所谈到的关系重大的实验。</p><p>这并不是说你一定要依序完成所有的实验才能开始阅读这本书。不过在当时他所提出来的方法仍是革命性的，他所构思的化学元素大体上我们仍然沿用至今。因此阅读这本书的重点是：<strong>你用不着读完所有的细节才能获得启发</strong>。譬如他的前言<strong>便强调了科学方法的重要，便深具启发性</strong>。</p><h4 id="17-3-面对数学的问题"><a href="#17-3-面对数学的问题" class="headerlink" title="17.3 面对数学的问题"></a>17.3 面对数学的问题</h4><p>数学其实是一种语言，我们可以像学习自己的语言一样学习它。在学习自己的语言时，我们要学两次：<strong>第一次是学习如何说话</strong> <strong>，第二次是学习如何阅读</strong>。幸运的是，数学只需要学一次，因为它完全是书写的语言。</p><p><strong>学习新的书写语言，牵涉到基础阅读的问题</strong>。当第一次接受阅读指导时，我们的问题在要学习认出每一页中出现的特定符号，还要记得这些符号之间的关系。如果我们被一个句子的句法搞昏头时，也得从<strong>基础的层次来解决</strong>。只有当我们解决了这些问题时，我们的阅读能力才能更上层楼。</p><p>任何一种语言都是一种沟通的媒介，借着语言人们能彼此了解共同的主题。</p><p><strong>基本几何学的命题有两种：</strong><br>(1)   有关<strong>作图问题</strong>的叙述。<br>(2)   有关<strong>几何图形与各相关部分之间</strong>的关系的定理。作图的问题必须着手去做，定理的问题就得去证明。</p><p>作图很明显地与公设(postulate)相似，两者都声称几何的运作是可以执行出来的。在公设的案例中，这个可能性是<strong>假定</strong>(assumed)出来的。在命题的案例中，那是要证明(proved)出来的。当然，<strong>要这样证明，需要用到公设</strong>。</p><p>命题要确定的不是假设是否为真，也不是结论是否为真——除非假设为真的时候。而除非命题得到证明，否则我们就无法确认假设和结论的关系是否为真。命题所证明的，纯粹是这种关系是否为真。别无其他。</p><p>我们只是针对一个<strong>真正有范围限制的问题，作出真正逻辑的解释</strong>。在解释的清晰与问题范围有限制的特质之中，有一种特别的吸引力。</p><h4 id="17-4-掌握科学作品中的数学问题"><a href="#17-4-掌握科学作品中的数学问题" class="headerlink" title="17.4 掌握科学作品中的数学问题"></a>17.4 掌握科学作品中的数学问题</h4><p>我们所关心的是在科学作品中有<strong>相当多的数学问题</strong>，而这也是一个主要的阅读障碍。关于这一点有几件事要说明如下。</p><p>第一，你<strong>至少可以把一些比你想像的基础程度的数学读得更明白</strong>。<br>我们已经建议你从欧几里得开始，我们确定你只要花几个晚上把《几何原理》读好，就能<strong>克服对数学的恐惧心理</strong>。读完欧几里得之后，你可以进一步，看看其他经典级的希腊数学大师的作品——阿基米德(Archimedes)、阿波罗尼乌斯(Apollonius)、尼科马科斯(Nicomachus)。这些书并不真的很难，而且你可以<strong>跳着略读</strong>。</p><p>第二，如果你阅读数学书的企图是<strong>要了解数学本身</strong>，当然你<strong>要读数学</strong>，<strong>从头读到尾——手上还要拿枝笔</strong>，这会比阅读任何其他的书还需要<strong>在书页空白处写些笔记</strong>。但是你的企图可能并非如此，而是只想<strong>读一本有数学在内的科学书</strong>，这样<strong>跳着略读反而是比较聪明</strong>的。</p><p>以牛顿的《自然哲学的数学原理》为例，<strong>先看定理的说明，再看看结论，掌握一下这是如何证明出来的。</strong> <strong>读读引理</strong> (lemmas)及<strong>系理</strong>(corollaries)的<strong>说明</strong>，再<strong>读</strong>所谓<strong>旁注</strong> (scholiums)（基本上这是讨论命题与整个问题之间的关系）。这么做了之后，你会<strong>看到整本书的全貌</strong>，也会发现牛顿是<strong>如何架构这个系统</strong>的——哪个先哪个后，各个部分又如何密切呼应起来。用这样的方法读这本书，觉得困难就<strong>不要看图表（</strong>许多读者是这么做的），<strong>只挑你感兴趣的内容</strong>来看，但要<strong>确定没错过牛顿所强调的重点</strong>。其中一个重点出现在第三卷的结尾，名称是“<strong>宇宙系统</strong>”，牛顿称之为<strong>一般的旁注</strong>，不但<strong>总结了前人的重点</strong>，也<strong>提出</strong>了一个物理学上几乎所有<strong>后人都会思考的伟大问题</strong>。</p><p>科学作品中经常会包括数学，主要因为我们前面说过数学精确、清晰与范围限定的特质。有时候你能读懂一些东西，却用不着深入数学的领域，像牛顿的书就是个例子。奇怪的是，就算数学对你来说可怕得不得了，但是一点也没有数学有时造成的麻烦还可能更大呢！</p><p>当然，并不是所有的科学经典作品都用上了数学，或是一定要用数学。只要你记住，<strong>你的责任不是成为这个主题的专家，而是要去了解相关的问题，在阅读时就会轻松许多</strong>。</p><h4 id="17-5-关于科普书的重点"><a href="#17-5-关于科普书的重点" class="headerlink" title="17.5 关于科普书的重点"></a>17.5 关于科普书的重点</h4><p>从某一方面而言，关于阅读科普书，我们没有什么更多的话要说了。就定义上来说，这些书——不论是书或文章——都是为广泛的大众而写的，而不只是为专家写的。因此，如果你已经读了一些科学的经典名作，这类流行书对你来说就毫无问题了。<br>这是因为这些书虽然与科学有关，但一般来说，读者都已经<strong>避免了阅读原创性科学巨著的两个难题</strong>。<br>第一，<strong>他们只谈论一点相关的实验内容</strong>（他们只报告出实验的结果）。<br>第二，<strong>内容只包括一点数学</strong>（除非是以数学为主的畅销书）。</p><p><strong>阅读科普书绝对比阅读故事书要困难得多</strong>。就算是一篇三页没有实验报告，没有图表，也没有数学方程式需要读者去计算的有关DNA的文章，<strong>阅读的时候如果你不全神贯注，就是没法理解</strong>。因此，在阅读这种作品时<strong>所需要的主动性比其他的书还要多</strong>。</p><ol><li>要<strong>确认主题</strong>。</li><li>要<strong>发现整体与部分之间的关系</strong>。</li><li>要<strong>与作者达成共识</strong>。</li><li>要<strong>找出主旨与论述</strong>。</li><li><strong>在评估或衡量意义之前</strong>，要能<strong>完全了解这本书</strong>才行。</li></ol><p>现在这些规则对你来说应该都很熟悉了。但是在这里运用起来更有作用。</p><p><strong>短文通常都是在传递资讯</strong>，你阅读的时候用<strong>不着太多主动的思考</strong>。你要做的<strong>只是去了解</strong>，<strong>明白作者所说的话</strong>，除此之外大多数情况就<strong>用不着花太大的力气</strong>了。</p><p>如果我们想要<strong>了解我们存活的这个年代</strong>，我们就该<strong>了解一下数学是什么，数学家是如何运用数学，如何思考的</strong>。</p><h3 id="第十八章-如何阅读哲学书"><a href="#第十八章-如何阅读哲学书" class="headerlink" title="第十八章 如何阅读哲学书"></a>第十八章 如何阅读哲学书</h3><p><strong>为什么孩子天生就有的心态，我们却要努力去发展呢？</strong><br>在我们成长的过程中，不知是什么原因，成人便失去了孩提时代原本就有的好奇心。或许是因为学校教育使头脑僵化了——死背的学习负荷是主因，尽管其中有大部分或许是必要的。另一个更可能的原因是父母的错。就算有答案，我们也常告诉孩子说没有答案，或是要他们不要再问问题了。碰到那些看来回答不了的问题时，我们觉得困窘，便想用这样的方法掩盖我们的不自在。所有这些都在打击一个孩子的好奇心。他可能会以为问问题是很不礼貌的行为。人类的好问从来没有被扼杀过，但却很快地降格为大部分大学牛所提的问题他们就像接下来要变成的成人一样，只会问一些资讯而已。</p><p>对这个问题我们没有解决方案，当然也不会自以为是，认为我们能告诉你如何回答孩子们所提出来的深刻问题。但是我们要提醒你一件很重要的事，<strong>就是最伟大的哲学家所提出来的深刻问题，正是孩子们所提出的问题。能够保留孩子看世界的眼光，又能成熟地了解到保留这些问题的意义，确实是非常稀有的能力——拥有这种能力的人也才可能对我们的思想有重大的贡献</strong>。</p><p>我们并不一定要像孩子般地思考，才能了解存在的问题。孩子们其实并不了解，也没法了解这样的问题——就算真有人能了解的话。但是我们一定要能够<strong>用赤子之心来看世界，怀疑孩子们怀疑的问题，问他们提出的问题</strong>。成人复杂的生活阻碍了寻找真理的途径。<strong>伟大的哲学家</strong>总能<strong>厘清生活中的复杂</strong>，看出简单的差别——只要经由他们说明过，原先困难无比的事就变得很简单了。<strong>如果我们要学习他们，提问题的时候就一定也要有孩子气的单纯——而回答时却成熟而睿智</strong>。</p><h4 id="18-1-哲学家提出的问题"><a href="#18-1-哲学家提出的问题" class="headerlink" title="18.1 哲学家提出的问题"></a>18.1 哲学家提出的问题</h4><p>这些哲学家所提出的“孩子气的单纯”问题，到底是些什么问题？我们写下来的时候，这些问题看起来并不简单，因为要回答起来是很困难的。不过，由于这些问题都很根本也很基础，所以乍听之下很简单。</p><p>一个哲学家想要<strong>探索存在的特质与存在的领域</strong>时，这些就是他们会提出来的典型问题。因为是问题，并不难说明或理解，但要回答，却难上加难——事实上困难到即使是近代的哲学家，也无法作出满意的解答。</p><p>哲学家会提的<strong>另一组问题不是存在，而是跟改变或形成有关</strong>。根据我们的经验，我们会毫不迟疑地指出某些事物是存在的，但是我们也会说所有这些事物都是会改变的。它们存在过，却又消失了。当它们存在时，大多数都会从一个地方移动到另一个地方，其中有许多包括了质与量上的改变：它们会变大或变小，变重或变轻，或是像成熟的苹果与过老的牛排，颜色会有改变。</p><p><strong>哲学并不只限于理论性的问题而已</strong>。以善与恶为例。孩子特别关心好跟坏之间的差别，如果他们弄错了，可能还会挨打。但是直到我们成人之后，对这两者之间的差异也不会停止关心。在善与恶之间，是否有普遍被认可的区别？无论在任何情况中，是否某些事永远是好的，某些事永远是坏的？或是就像哈姆雷特引用蒙田的话：“<strong>没有所谓好跟坏，端看你怎么去想它</strong>。”当然，善与恶跟对与错并不相同。这两组词句所谈的似乎是两种不同的事。尤其是，就算我们会觉得凡是对的事情就是善的，但我们可能不觉得凡是错的事情就一定是恶的。</p><p>我们所讨论的两种问题，<strong>区分出两种主要不同的哲学领域</strong>。<br>第一组，<strong>关于存在与变化的问题，与这个世界上存在与发生的事有关。这类问题在哲学领域中属于理论或思辩型的部分</strong>。<br>第二组，<strong>关于善与恶，好与坏的问题，和我们应该做或探寻的事有关，我们称这是隶属于哲学中实用的部分，更正确来说该是规范 (normative)的哲学</strong>。<br><strong>哲学规范的书基本上关心的是所有人都应该追求的目标</strong>——像过好生活，或组织一个好社会，他们就应该运用什么方法来达成目的的这一点上，却仅仅只会提供一些<strong>最普遍的共识</strong>。</p><p>哲学家提出来的问题，也有助于哲学两大领域中次分类的区分。</p><ol><li>如果<strong>思辩或理论型</strong>的哲学主要在<strong>探讨存在</strong>的问题，那就<strong>属于形上学</strong>。</li><li>如果<strong>问题与变化有关</strong>——关于特质与种类的演变，变化的条件与原因——就是<strong>属于自然哲学</strong>的。</li><li>如果主要<strong>探讨的是知识的问题</strong>——关于我们的认知，人类知识的起因、范围与限制，确定与不确定的问题——那就属于认识论(epistemology)的部分，也称作<strong>知识论</strong>。</li><li>就<strong>理论与规范哲学</strong>的区分而言，如果是<strong>关于如何过好生活，个人行为中善与恶的标准</strong>，这都与伦理学有关，也就是<strong>理论哲学的领域；</strong>如果是<strong>关于良好的社会，个人与群体之间的行为问题</strong>，则是政治学或政治哲学的范畴，也就是<strong>规范哲学的领域</strong>。</li></ol><h4 id="18-2-现代哲学与传承"><a href="#18-2-现代哲学与传承" class="headerlink" title="18.2 现代哲学与传承"></a>18.2 现代哲学与传承</h4><p>为了说明简要，<strong>让我们把世上存在及发生了什么事，或人类该做该追求的问题</strong>当作“<strong>第一顺位问题</strong>”。我们要认知这样的问题。<br>然后是“<strong>第二顺位问题</strong>”：<strong>关于我们在第一顺位问题中的知识，我们在回答第一顺位问题时的思考模式，我们如何用语言将思想表达出来等问题</strong>。</p><p><strong>区别出第一顺位与第二顺位问题</strong>是有帮助的。因为那会<strong>帮助</strong>我们<strong>理解近年来的哲学界发生了什么变化</strong>。当前主要的专业哲学家不再相信第一顺位的问题是哲学家可以解决的问题。目前大多数专业哲学家将心力投注在第二顺位的问题上，<strong>经常提出来的是如何用言语表达思想的问题</strong>。</p><p>问题在于今天大家几乎全然放弃了第一顺位的疑问，也就是对门外汉读者来说最可能感兴趣的那些问题。事实上，今天的哲学，就像当前的科学或数学一样，已经<strong>不再为门外汉写作</strong>了。第二顺位的问题，几乎可以顾名思义，都是些诉求比较窄的问题，而专业的哲学家，就像科学家一样，他们<strong>惟一关心的只有其他专家的意见</strong>。</p><p>这使得现代哲学作品对一个非哲学家来说格外难读——就像科学书对非科学家来说一样的困难。只要是关于第二顺位的哲学作品，我们都无法指导你如何去阅读。不过，还是有一些你可以读的哲学作品， 我们相信也是你该读的书。这些作品提出的问题是我们所说的第一顺位问题。毫无意外的，这些书主要也是为门外汉而写的，而不是专业哲学家写给专业同行看的。</p><p>上溯至1930年或稍晚一点，哲学书是为一般读者而写作的。哲学家希望同行会读他们的书，但也希望一般有知识的读者也能读。因为他们所提的问题，想要回答的问题都是与一般人切身相关的，因此他们认为一般人也该知道他们的思想。</p><h4 id="18-3-哲学的方法"><a href="#18-3-哲学的方法" class="headerlink" title="18.3 哲学的方法"></a>18.3 哲学的方法</h4><p>至少就提出与回答第一顺位问题的哲学而言，了解哲学方法的立足点是很重要的。假设你是一个哲学家，你对我们刚才提的那些孩子气的单纯问题感到很头痛——像任何事物存在的特质，或是改变的特质与成因等问题。那你该怎么做？</p><p>如果你的问题是科学的，你会知道要如何回答。你该进行某种特定的研究，或许是发展一种实验，以检验你的回答，或是广泛地观察各种现象以求证。如果你的问题是关于历史的，你会知道也要做一些研究，当然是不同的研究。但是要找出普遍存在的特质，却没有实验方法可循。而要找出改变是什么，事情为什么会改变，既没有特殊的现象可供你观察，更没有文献记载可以寻找阅读。你惟一能做的是思考问题本身，简单来说，<strong>哲学就是一种思考，别无他物</strong>。</p><p>当然，你并不是在茫然空想。<strong>真正好的哲学并不是“纯“思维——脱离现实经验的思考</strong>。观念是不能任意拼凑的。回答哲学问题，有严格的检验，以确认答案是否合乎逻辑。但这样的检验纯粹是来自一般的经验——你身而为人就有的经验，而不是哲学家才有的经验。你透过人类共同经验而对“改变”这种现象的了解，并不比任何人差——有关你的一切，都是会改变的。只要改变的经验持续下去，你就可以像个伟大的哲学家一样，思考有关改变的特质与起因。而他们之所以与你不同，就在他们的思想极为续密：他们能整理出所有可能问到的最尖锐的问题，然后再仔细清楚地找出答案来。他们用什么方法找出答案来呢？不是观察探索，也不是寻找比一般人更多的经验，而是<strong>比一般人更深刻地思考这个问题</strong>。</p><p>当我们在阅读一本哲学书时，所感兴趣的是哲学的问题，而不是科学或历史的问题。在这里我们要冒着重复的风险再说一次，我们要强调的是，<strong>要回答哲学的问题，除了思考以外，别无他法</strong>。如果我们能建造一架望远镜或显微镜，来检验所谓存在的特质，我们当然该这么做，但是不可能有这种工具的。</p><h4 id="18-4-哲学的风格"><a href="#18-4-哲学的风格" class="headerlink" title="18.4 哲学的风格"></a>18.4 哲学的风格</h4><p>虽然哲学的方法只有一种，但是在西方传统中，伟大的哲学家们至少采用过五种论述的风格。研究或阅读哲学的人应该能区别出其间的不同之处，以及各种风格的优劣。</p><p>(1) 哲学对话：第一种<strong>哲学的论说形式</strong>，虽然并不是很有效，但首次出现在柏拉图的《对话录》(Dialogues)中。<br>这种<strong>风格是对话的，甚至口语的</strong>，一群人跟苏格拉底讨论一些主题（或是后来一些对话讨论中，是和一个名叫“雅典陌生人”[the  Athenian Stranger]的人来进行的）。通常在一阵忙乱的探索讨论之后，苏格拉底会开始<strong>提出一连串的问题，然后针对主题加以说明</strong>。在柏拉图这样的大师手中，这样的风格是启发性的，的确能引领读者自己去发现事情。这样的风格再加上苏格拉底的故事的高度戏剧性 或是说高度的喜剧性——就变得极有力量。<br>柏拉图却一声不响地做到了。怀特海有一次强调，全部西方哲学，不过是“柏拉图的注脚”。后来的希腊人自己也说：“无论我想到什么，都会碰到柏拉图的影子。“无论如何，不要误会了这些说法。柏拉图自已显然并没有哲学系统或教条——<strong>若不是没有教条，我们也没法单纯地保持对话，提出问题。因为柏拉图，以及在他之前的苏格拉底，已经把后来的哲学家认为该讨论的所有重要问题，几乎都整理、提问过了</strong>。</p><p>(2)  <strong>哲学论文或散文</strong>：亚里士多德是柏拉图最好的学生，他在柏拉图门下学习了二十年。据说他也写了对话录，却完全没有遗留下来。所遗留下来的是一些针对不同的主题，异常难懂的散文或论文。</p><p>(3)  <strong>面对异议</strong>：中世纪发展的哲学风格，以圣托马斯·阿奎那的《神学大全》为极致，兼有前述两者的风貌。我们说过，哲学中不断提到的问题大部分是柏拉图提出的；我们应该也谈到，苏格拉底在对话过程中问的是那种小孩子才会问的简单又深刻的问题。而亚里士多德，我们也说过，他会<strong>指出其他哲学家的不同意见，并作出回应</strong>。</p><p>在阿奎那的作品中，最重要的是，他能明确指<strong>陈各种冲突，将不同的观点都说明出来，然后再面对所有不同的意见，提出自己的解决方案</strong>。从对立与冲突中，让真理逐渐浮现，这是中世纪非常盛行的想法。在阿奎那的时代，哲学家接受这样的方式，事实上是因为他们随时要准备当众，或在公开的论争中为自己的观点作辩护——这些场合通常群聚着学生和其他利害相关的人。中世纪的文化多半以口述方式流传，部分原因可能是当时书籍很少，又很难获得。一个主张要被接受，被当作是真理，就要能接受公开讨论的测试。<strong>哲学家不再是孤独的思考者，而是要在智力的市场上，接受对手的挑战</strong>。因此，《神学大全》中便渗透了这种<strong>辩论与讨论的精神</strong>。</p><p>(4)  <strong>哲学系统化</strong>：在17世纪，第四种哲学论说形式又发展出来了。这是两位著名的哲学家，笛卡尔与斯宾诺莎所发展出来的。他们着迷于数学如何组织出一个人对自然的知识，因此他们想<strong>用类似数学组织的方式，将哲学本身整理出来</strong>。</p><p>斯宾诺莎将这样的概念发展到更深的层次。他的《伦理学》(Eth­ics)是用严格的数学方式来表现的，其中有命题、证明、系理、引理、旁注等等。然而，关于形上学或伦理道德的问题，用数学的方法来解析不能让人十分满意，数学的方法还是比较适合几何或其他的数学问题，而不适合用在哲学问题上。当你阅读<strong>斯宾诺莎</strong>的时候，可以像你在阅读牛顿的时候那样<strong>略过很多地方</strong>，在阅读<strong>康德或亚里士多德</strong>时，你什么也不能略过，因为他们的理论是<strong>一直连续下来</strong>的。读<strong>柏拉图时也不能省略</strong>，你漏掉一点就像看一幕戏或读一首诗时，<strong>错过了其中一部分</strong>，这样<strong>整个作品就不完整了</strong>。</p><p>(5)  <strong>格言形式</strong>：还有另一种哲学论说形式值得一提，只不过没有前面四种那么重要。这就是<strong>格言的形式</strong>，是由尼采在他的书《查拉图斯特拉如是说》(Thus Spake Zarathustra)中所采用的，一些现代的法国哲学家也运用这样的方式。</p><p>上个世纪这样的风格之所以受到欢迎，可能是因为西方的读者对东方的哲学作品特别感兴趣，而那些作品就多是用格言的形式写作的。这样的形式可能也来自帕斯卡尔的《沉思录》(Pensees)。当然，帕斯卡尔并不想让自己的作品就以这样简短如谜的句子面世，但是在他想要以文章形式写出来之前，他就已经去世了。</p><p>用<strong>格言的形式来解说哲学</strong>，最大的好处在于<strong>有启发性</strong>。这会给读者一个印象，就像在这些简短的句子中还有言外之意，他必须自己<strong>运用思考来理解</strong>——他<strong>要能够自己找出各种陈述之间的关联，以及不同论辩的立足点</strong>。同样地，这样的形式也有<strong>很大的缺点</strong>，因为<strong>这样的形式完全没法论说</strong>。作者就像个撞了就跑的司机，他碰触到一个主题，谈到有关的真理与洞见，然后就跑到另一个主题上，却并没有为自己所说的话作适当的辩解。因此，格言的形式对喜欢诗词的人来说是很有意思的，但对严肃的哲学家来说却是很头痛的，因为他们希望能跟随着作者的思想，对他作出评论。</p><p>也就是说，所有伟大的哲学作品都不出这五种写作形式，当然，有时哲学家会尝试一种以上的写作方式。不论过去或现在，<strong>哲学论文或散文都可能是最普遍的形式</strong>，从最高超最困难的作品，像康德的书，到最普遍的哲学论文都包括在其中。对话形式是出了名的难写，而几何形式是既难读又难写。格言形式对哲学家来说是绝对不能满意的。而托马斯形式则是现代较少采用的一种方式。或许这也是现代读者不喜欢的一种方式，只是很可惜这样的方式却有很多的好处。</p><h4 id="18-5-阅读哲学的提示"><a href="#18-5-阅读哲学的提示" class="headerlink" title="18.5 阅读哲学的提示"></a>18.5 阅读哲学的提示</h4><p>到目前为止，读者应该很清楚在阅读任何哲学作品时，最重要的就是要<strong>发现问题</strong>，或是<strong>找到书中想要回答的问题</strong>。这些问题可能详细说明出来了，也可能隐藏在其中。不管是哪一种，你都要试着找出来。</p><p>作者会如何回答这些问题，<strong>完全受他的中心思想与原则的控制</strong>。在这一方面作者可能也会说明出来，但不一定每本书都如此。我们前面已经引述过巴兹尔·威利的话，要<strong>找出作者隐藏起来、并未言明的假设</strong>，是多么困难——也多么重要的事情。这适用于每一种作品。运用在哲学书上尤其有力。</p><p>伟大的哲学家在他的作品背后，都有<strong>自己特定的中心思想与原则</strong>。你可以很容易就看出他是否清楚地写在你读的那本书里。但是他也可能不这么做，保留起来在下一本书里再说明白。也可能他永远都不会明讲，但是在每本书里都有点到。</p><p><strong>哲学所询问的不只是现象之间的联系</strong>，更要<strong>追寻潜藏在其中的最终原因与条件</strong>。要回答这些问题，<strong>只有清楚的论述与分析，才能让我们感到满意</strong>。</p><p><strong>读者最要花力气的就是作者的词义与基本主旨。</strong>虽然哲学家跟科学家一样，有一些专门的技术用语，但他们表达思想的词句通常来自日常用语，只是用在很特殊的意义上。读者需要特别注意这一点。如果他不能克服自己，总是想将一个熟悉的字看作一般意义的想法，最后他会让整本书变成胡说八道又毫无意义。</p><p><strong>哲学讨论的基本词义就像科学作品一样，当然是抽象的</strong>。其实，任何具有共通性的知识，除了抽象的词义外，无从表达。</p><p><strong>哲学作品</strong>几乎没有不陈述一些作者认为不证自明的主旨。这种主旨都<strong>直接来自经验</strong>，而不是由其他主旨证明而来。</p><p>因此要了解并测验一位哲学家的主要原则，你用不着借重经由方法调查而获得的特殊经验，这种额外的助力。他诉求的是你<strong>自己的普通常识</strong>，<strong>以及对你自己所生存的这个世界的日常观察</strong>。</p><h4 id="18-6-厘清你的思绪"><a href="#18-6-厘清你的思绪" class="headerlink" title="18.6 厘清你的思绪"></a>18.6 厘清你的思绪</h4><p>一本好的哲学理论的书，就像是好的科学论文，不会有滔滔雄辩或宣传八股的文字。</p><p>哲学家彼此意见往往不合这一点，不应该是你的困扰。这有<strong>两个原因</strong>。<br>第一，<strong>如果这些不同的意见一直存在，可能就指出一个没有解决或不能解决的大问题</strong>。知道真正的奥秘所在是件好事。<br>第二，<strong>哲学家意见合不合其实并不重要，你的责任只是要厘清自己的思路</strong>。就哲学家透过他们的作品而进行的长程对话，你一定要能判断什么成立，什么不成立才行。如果你把一本哲学书读懂了——意思是也读懂了其他讨论相同主题的书——你就可以有评论的立场了。</p><h4 id="18-7-关于神学的重点"><a href="#18-7-关于神学的重点" class="headerlink" title="18.7 关于神学的重点"></a>18.7 关于神学的重点</h4><p>神学有两种类型，<strong>自然神学</strong>(natural theology)与<strong>教义神学</strong>(dogmatic theology)。</p><p><strong>自然神学是哲学的一支，也是形而上学的最后一部分</strong>。譬如你提出一个问题，因果关系是否永无止境？每件事是否都有起因？如果你的答案是肯定的，你可能会陷入一种永无止境的循环当中。因此，你可能要设定某个不因任何事物而发生的原始起因的别称。亚里士多德称这种没有起因的原因是“不动的原动者”(unmoved mover)。你可以另外命名甚至可以说那不过是上帝的别称——但是重点在，你要<strong>透过不需要外力支援的——自然进行的思考，达成这番认知</strong>。</p><p>教义神学与哲学则不同，因为教义神学的首要原则就是某个宗教的教徒所信奉的经文。教义神学永远依赖教义与宣扬教义的宗教权威人士。如果你没有这样的信仰，也不属于某个教派，想要把教义神学的书读好，你就得拿出读数学的精神来读。但是你得永远记住，在有关信仰的文章中，信仰不是一种假设。对有信仰的人来说，那是一种确定的知识，而不是一种实验性的观点。</p><p>一般来说，在面对教义神学的书时，他们会犯一两个错。<br>第一个错是<strong>拒绝接受——即使是暂时的接受——作者首要原则的经文</strong>。<br>第二个错是<strong>认为既然整本书的首要原则是教义的，依据这些教义而来的论述，这些教义所支持的推论，以及所导引出来的结论，都必然也都是属于教义的</strong>。</p><p>一个<strong>没有信仰的读者</strong>在阅读这样的书时，他要做的就是<strong>接受首要原则是成立的，然后用阅读任何一本好的论说性作品都该有的精神来阅读</strong>。至于一个<strong>有信仰的读者</strong>在阅读与自己信仰有关的书籍时，要<strong>面对的则是另一些困难</strong>了。这些问题并不只限于阅读神学才出现。</p><h4 id="18-8-如何阅读“经书”"><a href="#18-8-如何阅读“经书”" class="headerlink" title="18.8 如何阅读“经书”"></a>18.8 如何阅读“经书”</h4><p>有一种很有趣的书，一种阅读方式，是我们还没提到的。我们用“经书”(canonical)来称呼这种书，如果传统一点，我们可能会称作“圣书” (sacred)或“神书”(holy)。但是今天这样的称呼除了在某些这类书上还用得着之外，已经不适用于所有这类书籍了。</p><p>任何一个机构——教会、政党或社会——在其他的功能之外，如果<br>(1) 有<strong>教育的功能</strong><br>(2) 有<strong>一套要教育的课本</strong>(a body of doctrine to teach)<br>(3) 有<strong>一群虔诚又顺服的成员</strong>，那么属于这类组织的成员在阅读的时候都会<strong>必恭必敬</strong>。<br>他们不会——也不能——质疑这些对他们而言就是“经书”的书籍的权威与正确的阅读方法。信仰使得这些信徒根本不会发现“神圣的“经书中的错误，更别提要找出其中道理不通的地方。</p><p><strong>一个忠诚的读者在阅读经书时，有义务要从中找到意义，并能从其他的”事实”中举证其真实性</strong>。如果他自己不能这么做，他就有义务去找能做到的人。这个人可能是牧师或祭司，或是党派中的上级指导者，或是他的教授。在任何状况中，他都必须接受对方提供给他的解决之道。他的阅读基本上是没有自由可言的。相对地，他也会获得阅读其他书所没有的一种满足感当作回报。</p><h3 id="第十九章-如何阅读社会科学"><a href="#第十九章-如何阅读社会科学" class="headerlink" title="第十九章 如何阅读社会科学"></a>第十九章 如何阅读社会科学</h3><p>社会科学的观念与术语几乎渗透了所有我们今天在阅读的作品中。</p><p>譬如像现代的新闻记者，不再限定自己只报导事实。只有在报纸头版出现，简短的“<strong>谁——发生了什么事——为什么发生——何时何地发生</strong>“新闻提要，才是以事实为主。一般来说，记者都会将事实加上诠释评论、分析，再成为新闻报导。这些诠释与评论都是来自社会科学的观念与术语。</p><p>这些观念与术语也影响到当代许多书籍与文章，甚至可以用社会评论来作一个归类。我们也看到许多文学作品是以这类的主题来写作的：种族问题、犯罪、执法、贫穷、教育、福利、战争与和平、好政府与坏政府。这类文学作品便是向社会科学借用了思想意识与语言。</p><p>社会科学作品并不只限定于非小说类。仍然有一大批重要的当代作家所写的是社会科学的小说。</p><p>此外，无论是任何社会、经济或政治的问题，几乎全都有专家在作研究。这些专家不是自己作研究，就是由直接面对这些问题的官方单位邀请来做。在社会科学专家的协助下，这些问题有系统地阐释出来， 并要想办法解决这些问题。</p><p>社会科学的成长与普及，最重要的因素是在高中与大专教育中引进了社会科学。</p><h4 id="19-1-什么是社会科学？"><a href="#19-1-什么是社会科学？" class="headerlink" title="19.1 什么是社会科学？"></a>19.1 什么是社会科学？</h4><p><strong>究竟社会科学是什么呢？</strong><br>有一个方法可以找出答案，就是去看看大学中将哪些学科与训练课程安排在这样的科系之下。社会科学的部门中通常包括了人类学、经济学、政治学与社会学。</p><p><strong>为什么没有包括法律、教育、商业、社会服务与公共行政呢？所有这些学科也都是运用社会科学的概念与方法才发展出来的啊？</strong><br>对于这个问题，最常见的回答是：后面这些学科的目的，在于训练大学校园以外的专业工作者，而前面所提的那些学科却是比较专注于追求人类社会的系统知识，通常是在大学校园中进行的。</p><p><strong>那么心理学呢？</strong><br>一些划分严格的社会科学家会将心理学排除在社会科学之外，因为他们认为心理学所谈的是个人的特质问题，而社会科学关心的却是文化、制度与环境因素。一些区分比较没那么严格的学者，则认为生理心理学应该归类为生物科学，而不论是正常或变态心理学则该隶属于社会科学，因为个<strong>人与社会整体是不可分割</strong>的。</p><p><strong>那么行为科学呢？他们在社会科学中担任什么样的角色？</strong><br>依照原始的用法，行为科学中包括了社会学、人类学、行为生物学、经济学、地理学、法律、心理学、精神病学与政治科学。<strong>行为科学特别强调对可观察可测量的行为作系统化的研究，以获得可被证实的发现</strong>。近年来，行为科学几乎跟社会科学变成同义词了，但许多讲究传统的人反对这样的用法。</p><p>诸如<strong>人类学、经济学、政治学、社会学的学科，都是组成社会科学的核心</strong>，几乎所有的社会科学家都会将这些学科归纳进来。</p><h4 id="19-2-阅读社会科学的容易处"><a href="#19-2-阅读社会科学的容易处" class="headerlink" title="19.2 阅读社会科学的容易处"></a>19.2 阅读社会科学的容易处</h4><p>绝大部分社会科学看起来都像是非常容易阅读的作品。这些作品的内容<strong>通常取材自读者所熟悉的经验——在这方面</strong>，<strong>社会科学就跟诗与哲学一样——论说的方式也经常是叙述式的</strong>，这对读过小说与历史的读者来说都很熟悉。</p><p><strong>我们都已经很熟悉社会科学的术语，而且经常在使用</strong>。诸如文化（比较文化、反文化、次文化）、集团、疏离、地位、输入／输出、下层结构、伦理、行为、共识等很多这样的术语，几乎是现代人交谈与阅读时经常会出现的字眼。</p><p>想想”社会”，这是一个多么变色龙的词，前面不知可以加上多少形容词，但它总是在表达一种人民群居生活，而非离群索居的广阔定义。我们听到过失序的社会、不健全的社会、沉默的社会、贪婪的社会、富裕的社会……，我们可以从英文字典中第一个字母找起，最后找到“发酵的”(zymotic)社会这样的形容词——这是指持续动荡的社会，就跟我们所处的社会一样。</p><p>把“社会“看作是形容词，同样有许多熟悉的意义。像社会力量、社会压力、社会承诺，当然还有无所不在的社会问题。在阅读或写作杜会科学时，最后一种是<strong>特别容易出现的题材</strong>。</p><p>社会学家在写作时所用的术语及隐喻，加上字里行间充满深刻的情感，让我们误以为这是很容易阅读的。书中所引用的资料对读者来说是很熟悉的，的确，那是他们天天读到或听到的字眼。此外，读者的态度与感觉也都跟着这些问题的发展紧密联系在一起。<strong>对于社会科学所讨论的问题，我们都会有很强烈的意见。</strong></p><h4 id="19-3-阅读社会科学的困难处"><a href="#19-3-阅读社会科学的困难处" class="headerlink" title="19.3 阅读社会科学的困难处"></a>19.3 阅读社会科学的困难处</h4><p>说来矛盾，我们前面所说的让社会科学看来很容易阅读的因素，却也是让社会科学不容易阅读的因素。譬如我们前面所提到的最后一个因素你身为一个读者，要对作者的观点投人一些看法。许多读者担心，<strong>如果承认自己与作者意见不合，而且客观地质疑自己阅读的作品，是一种对自己投入不忠的行为</strong>。但是，只要你是<strong>用分析阅读来阅读，这样的态度是必要的</strong>。我们所谈的阅读规则中已经指出了这样的态度，至少在做大纲架构及诠释作品的规则中指出过。如果你要回答阅读任何作品都该提出的头两个问题，你一定要先检查一下你自己的意见是什么。<strong>如果你拒绝倾听一位作者所说的话，你就无法了解这本书了。</strong></p><p><strong>社会科学中熟悉的术语及观点，同时也造成了理解上的障碍。</strong>许多社会科学家自己很清楚这个问题。他们非常反对在一般新闻报导或其他类型的写作中，任意引用杜会科学的术语及观点。如果在你阅读的作品中，作者将一个自己都不太清楚的词句当作是关键字，那你一定也会跟着摸不着头脑的。</p><p>让我们把这个观点再说明清楚一点。我们要先把社会科学与自然科学——物理、化学等——区分出来。我们已经知道，<strong>科学作品（指的是后面那种“科学”)的作者会把假设与证明说得十分清楚，同时也确定读者很容易与他达成共识，并找到书中的主旨。</strong>因为在阅读任何论说性作品时，与作者达成共识并找到主旨是最重要的一部分，科学家的作法等于是帮你做了这部分的工作。不过你还是会发现用数学形式表现的作品很难阅读，如果你没法<strong>牢牢掌握住论述、实验，以及对结论的观察基础，</strong>你会发现很难对这本书下评论——也就是回答”这是真实的吗？”“这本书与我何干？”的问题。然而，有一点很重要的是，<strong>阅读科学作品要比阅读任何其他论说性作品都来得容易</strong>。</p><p>换句话说，<strong>自然科学的作者必须做的是“把他的用语规定出来”</strong>——这也就是说，他告诉你，在他的论述中有哪些基本的词义，而他会如何运用。这样的说明通常会出现在书的一开头，可能是解释、假设、公理等等。<strong>在阅读自然科学的作品时，你也不会与作者争辩他的使用规则。你接受这些规则，开始阅读。</strong></p><p><strong>在自然科学中已经很普遍的用语说明，在社会科学中却仍然不太普遍</strong></p><ol><li>一个理由是——<strong>社会科学并不能数学化</strong></li><li>另一个理由是——<strong>在社会或行为科学中，要说明用语比较困难</strong></li></ol><p>阅读社会科学作品最困难的地方在于：<strong>事实上，在这个领域中的作品是混杂的，而不是纯粹的论说性作品</strong>。太多社会科学的作品混杂了科学、哲学与历史，甚至为了加强效果，通常还会带点虚构的色彩。</p><p>你还记得<strong>分析阅读的第一个步骤是回答这个问题：这是本什么样的书？</strong>如果是小说，这个问题相当容易回答。如果是科学或哲学作品，也不难。就算是形式混杂的历史，一般来说读者也会知道自己在读的是历史。但是组成社会科学的不同要素——有时是这种，有时是那种，有时又是另一种模式使我们在阅读任何有关社会科学的作品时，很难回答这个问题。事实上，这就跟要给社会科学下定义是同样困难的事。</p><p>不过，<strong>分析阅读的读者还是得想办法回答这个问题</strong>。这不只是他要做的第一件工作，也是最重要的工作。如果他能够<strong>说出他所阅读的这本书是由哪些要素组成的，他就能更进一步理解这本书了</strong>。    </p><h4 id="19-4-阅读社会科学作品"><a href="#19-4-阅读社会科学作品" class="headerlink" title="19.4 阅读社会科学作品"></a>19.4 阅读社会科学作品</h4><p><strong>在阅读社会科学时，关于一个主题通常要读好几本书，而不会只读一本书。</strong>这不只是因为社会科学是个新领域，只有少数经典作品，还因为我们<strong>在阅读社会科学时，主要的着眼点在一个特殊的事件或问题上，而非一个特殊的作者或一本书</strong>。<strong>基本上，在这些领域中，并没有什么权威的著作，因此我们必须读很多本相关的书。</strong>而社会科学家本身也有一个现象，就是<strong>为了要能跟得上时代，他们必须不断地推陈出新，重新修订他们的作品，新作品取代旧作品，过时的论述也不断被淘汰了</strong>。</p><p><strong>分析阅读的规则并不适用于就一个主题同时阅读很多本书的情况</strong>。分析阅读适用于阅读个别的书籍。当然吗如果你想要善用这些规则，就要仔细地研究观察。接下来要介绍的新的阅读规则，则需要我们 通过第三层次的阅读（分析阅读），才能进入这第四层次的阅读（主题阅读）。<strong>我们现在就准备要讨论第四层次的阅读。因为社会科学作品有这样的特质，所以必须要用这样的阅读。</strong></p><p><strong>如何阅读实用性作品，这与其他的阅读完全不同，因为读者有特定的义务，也就是如果他同意作者的观点，就要采取行动</strong>。然后我们讨论小说与诗，提出和阅读论说性作品不同的问题。最后，我们讨论的是三种理论性的论说作品科学与数学、哲学、社会科学。<strong>社会科学放在最后，是因为这样的书需要用上主题阅读。</strong></p><h2 id="第四篇-阅读的最终目标"><a href="#第四篇-阅读的最终目标" class="headerlink" title="第四篇 阅读的最终目标"></a>第四篇 阅读的最终目标</h2><h3 id="第二十章-阅读的第四个层次：主题阅读"><a href="#第二十章-阅读的第四个层次：主题阅读" class="headerlink" title="第二十章 阅读的第四个层次：主题阅读"></a>第二十章 阅读的第四个层次：主题阅读</h3><p>我们在前面提到过，<strong>在讨论某个特定的主题时，牵涉到的往往不只是一本书</strong>。我们也一再非正式地提醒过，甚至其他领域中相关的作者与书籍，都与这个特定的主题有关。</p><p><strong>在作主题阅读时：</strong></p><ol><li>第一个要求就是知道：<strong>对一个特定的问题来说，所牵涉的绝对不是一本书而已</strong>。 </li><li>第二个要求则是：<strong>要知道就总的来说，应该读的是哪些书？</strong></li></ol><p>第二个要求比第一个要求还难做到。</p><p>我们在检验这个句子：”与<strong>同一个主题</strong>相关两本以上的书”时，困难就出现了。我们所说的“同一个主题”是什么意思？如果这个主题是单一的历史时期或事件，就很清楚了，但是在其他的领域中，就很难作这样清楚的区分。</p><p>你可能料到小说有这种情况。因为作品的特性，小说沟通问题的方法跟论说性作品不同。但是，论说性作品也有同样的问题。</p><p>譬如说你对“爱”这个概念很感兴趣，想要阅读相关的读物。因为关于爱的作品很广泛，你要整理出一个相关书目来阅读是有点困难的。假设你向专家求教，到一个完备的图书馆中寻找书目，还对照一位优秀学者所写的论文，终于把书目弄出来了。再假设你进一步舍弃诗人和小说家谈的这个主题，只想从论说性的作品中找答案（在后面我们会说明为什么这样的做法是明智的）。现在你开始依照书目来阅读这些书了。你发现什么？</p><p>面对如此庞大的相关资料，我们要<strong>如何决定我们要研究的主题是什么呢</strong>？我们能确定这中间只有一个单一的主题吗？就像这个问题本身的困难，在我们找到答案之前，我们能说我们已经确认了“<strong>同一个主题</strong>”吗？</p><p>你只不过读了一小部分有关爱的论说性作品，这些问题就会浮现在你脑海中，其实还有更多其他的问题会出现。无论如何，我们已经说到重点了。<strong>在做主题阅读时，会出现一种很矛盾的现象。虽然这个层次的阅读被定义为就同一个主题，阅读两种以上的书，意思也是指在阅读开始之前，这个主题就已经被确认了</strong>，但是换个角度来说，<strong>这个主题也是跟着阅读走的，而不是事前就能定出来的</strong>。以爱这个例子来说，在你决定自己要读些什么之前，你可能已经读了好几百本相关的著作了。等你都读完之后，你会发现有一半的书其实跟主题根本无关。</p><h4 id="20-1-在主题阅读中，检视阅读所扮演的角色"><a href="#20-1-在主题阅读中，检视阅读所扮演的角色" class="headerlink" title="20.1 在主题阅读中，检视阅读所扮演的角色"></a>20.1 在主题阅读中，检视阅读所扮演的角色</h4><p>我们已经说过很多次，<strong>阅读的层次是渐进累积的</strong>。<strong>较高层次的阅读中也包括了前面的，或较低层次的阅读</strong>。在主题阅读中，我们就要说明这一点。</p><p>你可能还记得，在解说检视阅读与分析阅读的关系时，我们指出在检视阅读中的两个步骤——<strong>第一个是浏览，第二个是粗浅地阅读</strong>——也就是<strong>分析阅读的前两个步骤</strong>。</p><p><strong>浏览</strong>能帮助你准备做分析阅读的第一个步骤：<strong>你能确定自己在读的是什么主题，能说明这是什么样的书，并拟出大纲架构。</strong><br><strong>粗浅的阅读</strong>对分析阅读的第一步骤也有帮助。基本上这是进入第二步骤的准备动作。在第二个步骤中，你<strong>要能够与作者达成共识，说明他的主旨，跟随他的论述，才能够诠释整本书的内容</strong>。</p><p>同样的，<strong>检视阅读与分析阅读也可以当作是进人主题阅读的前置作业或准备动作</strong>。事实上，在这个阶段，<strong>检视阅读已经是读者在阅读时主要的工具或手段了</strong>。</p><p>举例来说，你有上百本的参考书目，看起来全是与爱有关的主题。如果你全部用<strong>分析阅读来阅读</strong>，你不只会很<strong>清楚</strong>你在<strong>研究的主题是什么——主题阅读中的“同一主题”——你还会知道你所阅读的书中，那些跟主题无关，是你不需要的书</strong>。但是要用分析阅读将一百本书读完，会花上你十年的时间。就算你能全心投注在这个研究上，仍然要花上好几个月的时间。再加上我们前面谈过的主题阅读中会出现的矛盾问题，显然必要有一些捷径。</p><p>这个捷径是要靠你的<strong>检视阅读技巧来建立</strong>的。你收集好书目之后，要做的第一件事是检视书单上所有的书。<strong>在做检视阅读之前，绝不要用分析阅读来阅读</strong>。<strong>检视阅读</strong>不会让你明白有关主题的所有错综复杂的内容，或是作者所有的洞察力，但却<strong>具有两种基本的功能</strong>。<strong>第一，它会让你对自己想要研究的主题有个清晰的概念，这样接下来你针对某几本书做分析阅读时，会大有助益。其次，它会简化你的书目到一个合理的程度。</strong></p><p><strong>对学生，尤其是研究生来说，我们很难想到还有比这更管用的方式。只要他们肯照着做，一定会有帮助。</strong>根据我们的经验，在研究生程度的学生中，确实有些人能做到主动的阅读与分析阅读。这对他们来说还不够，他们或许不是完美的读者，但是至少他们知道要如何掌握一本书的重点，能明确地说出书中的要点，并把这些观点纳入他们研究主题的一部分。但是他们的努力有一大半是浪费掉了，因为他们不知道要<strong>如何才能比别人读得快一点</strong>。他们阅读每一本书或每一篇文章都花上同样的时间与努力，结果他们该花精神好好阅读的书却没有读好，倒把时间花在那些不太值得注意的书上了。</p><p>能够<strong>熟练检视阅读的读者，不但能在心中将书籍分类，而且能对内容有一个粗浅的了解。</strong>他也会<strong>用非常短的时间就发现，这本书谈的内容对他研究的主题到底重不重要</strong>。这时他可能还不清楚哪些资料才是最重要的这可能要等到读下本书的时候才能发现。但是有两件事至少他已经知道其中之一。那就是他不是发现这本书必须回头再读一次，以获得启发，便是知道不论这本书多有趣又多丰富，却毫无启发性，因此不值得重新再读。</p><p>这个忠告通常会被忽略是有原因的。我们说过，<strong>在分析阅读中，技巧熟练的阅读者可以同时用上许多技巧，而初学者却必须把步骤分开来。</strong>同样的，<strong>主题阅读的准备工作——先检视书目上所有的书，在开始做分析阅读之前先检视一遍——可以在做分析阅读时一并进行</strong>。但我们不相信任何读者能做到这一点，就算技巧再熟练也不行。这也是许多年轻研究生所犯的毛病。他们自以为两个步骤可以融合为一个，结果阅读任何书都用同样的速度，对某些特殊的作品来说不是太快就是太慢，但无论如何，对他们阅读的大部分书来说，这样的方法都是不对的。</p><p><strong>一旦你检视过，确定某些书跟你研究的主题相关后，你就可以开始做主题阅读了</strong>。要注意的是，我们并没有像你以为的说：“开始做分析阅读”。当然，你需要研读每一本书，再组合起跟你主题相关的资料，你在做分析阅读时就已经学会了这些技巧。但是绝不要忘了，<strong>分析阅读的技巧只适用于单一的作品，主要的目标是要了解这本书</strong>。而我们会看到，主题阅读的目标却大不相同。</p><h4 id="20-2-主题阅读的五个步骤"><a href="#20-2-主题阅读的五个步骤" class="headerlink" title="20.2 主题阅读的五个步骤"></a>20.2 主题阅读的五个步骤</h4><p>现在我们准备好要说明如何做主题阅读了。我们的假设是：<strong>你已经检视了相当多的书，你至少对其中一些书在谈些什么有点概念了，而且你也有想要研究的主题了。接下来你该怎么办？</strong></p><p>在<strong>主题阅读中一共有五个步骤</strong>。这些步骤我们不该称之为规则——虽然也许我们会因为只要漏掉其中一个步骤，主题阅读就会变得很困难，甚至读不下去了。我们会简略地介绍一下这些步骤的顺序，不过这些步骤彼此之间还是可以互相取代的。</p><p><strong>主题阅读步骤一：找到相关的章节</strong>。<br>当然，我们假设你已经学会分析阅读了，如果你愿意，你能把所有相关的书都看透彻了。但是你可能会把阅读单本的书放在第一顺位，而把自己的主题放在其次。事实上，这个顺序应该颠倒过来，<strong>在主题阅读中，你及你关心的主题才是基本的重点，而不是你阅读的书。</strong></p><p>在你已经<strong>确定哪些书是相关的之后</strong>，<strong>主题阅读的第一个步骤就是把这些书整体检视阅读一遍</strong>。你的<strong>目标是找出书中与你的主题极为相关的章节</strong>。你选择的书不太可能全本都与你的主题或问题相关。就算是如此，也一定是少数，你应该很快地把这本书读完。你不该忘了，你的阅读是别有用心的——也就是说，<strong>你是为了要解决自己的问题才阅读——而不是为了这本书本身的目的而阅读。</strong></p><p>看起来，这个步骤似乎与前面所说的，为了发现这本书是否与你主题相关的检视阅读当同一件事来进行。许多状况的确可以这么做。但是如果你认为永远都可以这么做的话，可能就不太聪明了。记住，<strong>第一步的检视阅读是要集中焦点在你要进一步做主题阅读的主题上</strong>。我们说过，除非你已经检阅过书单上大部分的书，否则你无法完全理解这个问题。因此，<strong>在确认哪些是相关的书籍的同时，还要确认哪些是相关的章节，其实是很危险的做法</strong>。除非你的技巧已经很熟练，而且对你要研究的主题已经很清楚了，否则你<strong>最好是将两部分分开来做</strong>。</p><p><strong>在主题阅读中，能够把你所阅读的第一批书，与你后来针对这个主题阅读的许多本书的差别区分出来，是很重要的事</strong>。对后来的这些书来说，你可能对自己的主题已经有了很清楚的概念，这时就可以把两种检视阅读合并在一起。但是<strong>在一开始时．却要明显地区分出来，否则你在找相关章节时会犯下严重的错误，到后来要更正这些错误时义要花上很多的时间与精力</strong>。</p><p>总之，<strong>要记得你最主要的工作不是理解整本书的内容，而是找出这本书对你的主题有什么帮助，而这可能与作者本身的写作目的相去甚远</strong>。在这个阶段的过程中，这并不重要。作者可能是在无意之间帮你解决了问题。我们已经说过，<strong>在主题阅读中，是书在服务你，而不是你在服务书</strong>。因此，<strong>主题阅读是最主动的一种阅读法</strong>。当然，<strong>分析阅读也需要主动的阅读方式</strong>。但是你<strong>在分析阅读一本书时，你就像是把书当作主人，供他使唤</strong>。而你<strong>在做主题阅读时，却一定要做书的主人</strong>。</p><p><strong>主题阅读步骤二：带引作者与你达成共识。</strong><br><strong>在诠释阅读中</strong>（分析阅读的第二步骤），<strong>第一个规则是要你与作者达成共识，也就是要能找出关键字，发现他是如何使用这些字的。</strong>但是现在你<strong>面对的是许多不同的作者</strong>，他们不可能每个人都使用同样的字眼，或相同的共识。在这时候就是要<strong>由你来建立起共识，带引你的作者们与你达成共识，而不是你跟着他们走</strong>。</p><p><strong>在主题阅读中</strong>，这可能是<strong>最困难的一个步骤</strong>。<strong>真正的困难在于要强迫作者使用你的语言，而不是使用他的语言</strong>。这跟我们一般的阅读习惯都不相同。我们也指出过很多次，我们假设：我们想要用分析阅读来阅读的作者，是比我们优秀的人。尤其如果这是一本伟大的著作时，就更可能如此。无论我们在了解他的过程中花了多少力气，我们都会倾向于接受他的词义与他安排的主题结构。但<strong>在主题阅读中，如果我们接受任何一位作者所提出来的词汇</strong>(terminology)，<strong>我们很快就会迷失</strong>。<strong>我们可能会了解他的书，却无法了解别人的书。我们也很难找到与自己感兴趣的主题的资料</strong>。</p><p>我们不只要<strong>能够坚决拒绝接受任何一位作者的词汇</strong>，还得<strong>愿意面对可能没有任何一位作者的词汇对我们来说是有用的事实</strong>。</p><p><strong>简单来说，主题阅读是一种大量的翻译工作</strong>。我们并不是将一种语言翻成另一种语言，像法语翻成英语，但是我们要将一种共通的词汇加诸在许多作者身上，无论他们所使用的是不是相同的语言，或是不是关心我们想解决的问题，是否创造了理想的词汇供我们使用。</p><p>这就是说，<strong>在进行主题阅读时，我们要建立一组词汇，首先帮助我们了解所有的作者，而不是其中一两个作者；其次帮助我们解决我们的问题</strong>。这一点认识会带我们进入第三个步骤。</p><p><strong>主题阅读步骤三：厘清问题。</strong><br><strong>诠释阅读的第二个规则是要我们找出作者的关键句子。</strong>然后<strong>从中逐步了解作者的主旨</strong>。主旨是由词义组成的，在主题阅读中，当然我们也要做同样的工作。最好的方法是<strong>先列出一些可以把我们的问题说得比较明白的问题，然后让那些作者来回答这些问题</strong>。</p><p>事实上，有时候我们<strong>必须接受作者可能一个问题也回答不了</strong>。在这样的状况中，我们<strong>必须要将他视为是对这个问题保持沉默，或是尚未作出决定</strong>。但是就算他并没有很清楚地讨论这个问题，有时我们也可以在他书中找到间接的回答。我们会得出这么一个结论：<strong>如果他考虑到这个问题的话，那就会如何如何回答这个问题</strong>。在这里<strong>需要一点自我约束</strong>。我们<strong>不能把思想强加在作者脑海中，也不能把话语放进他们的口中</strong>。但是我们也<strong>不能完全依赖他们对这个问题的解说</strong>。如果我们真的能靠其中任何一位作者来解释这个问题，或许我们根本就没有问题要解决。</p><p>我们说过要把问题照秩序排列出来，好帮助我们在研究时使用。当然，这个秩序是跟主题有关的，不过还是有一般的方向可循。第一个问题通常<strong>跟我们在研究的概念或现象的存在或特质有关</strong>。如果一位作者说这种现象的确存在，或这种概念有一种特质，那么对于他的书我们就要提出更进一步的问题了。这个问题可能跟<strong>这个现象是如何被发现，或这个概念是如何表现出来的有关</strong>。最后一部分的问题则是<strong>与回答前面问题所产生的影响有关。</strong></p><p>我们不该期望所有的作者都用同一种方法来回答我们的问题。如果他们这么做了，我们就又没有问题要解决了。那个问题会被一致的意见解决了。正因为每个作者都不相同，因此我们要再面对主题阅读的下一个步骤。</p><p><strong>主题阅读步骤四：界定议题。</strong><br>如果一个问题很清楚，如果我们也确定各个作者会用不同的方式来回答——不论赞成或反对——那么这个议题就被定义出来了。这是介于用这种方法回答问题的作者，和用另外一种（可能是相反的）方法来回答问题的作者之间的议题。</p><p>如果检验过后，所有的作者提供的答案只有正反两面的意见，那么这个问题算是简单的问题。通常，<strong>对一个问题会有超过两种以上的答案</strong>。在<strong>这种情况</strong>下，我们就要<strong>找出不同意见彼此之间的关联，再根据作者的观点来作分类。</strong></p><p><strong>当两个作者对同一个问题有相当的了解，所作的回答却完全相反或矛盾时，这才是一个真正有参与的议题</strong>。但是这样的现象并不像我们希望的那样经常发生。通常，<strong>答案之不同固然来自于各人对这个主题有不同的观点，但也有很多情况是来自于对问题本身的认知不同</strong>。所以<strong>在做主题阅读的读者，要尽可能地确保议题是大家所共同参与的</strong>。有时候这会迫使他在列出问题的时候，小心不采取任何一位作者明自采用的方法。</p><p>我们要处理的问题．<strong>可能会出现很多种不同的议题，不过通常都可以分门别类</strong>。譬如像考虑到某种概念的特质的问题，就会出现一堆相关的议题。<strong>许多议题绕着一组相互关联密切的问题打转</strong>，就会<strong>形成这个主题的争议</strong>。这样的争议可能很复杂，这时主题阅读的读者就要将所有争议的前后关系整理清楚——尽管没有任何作者做这件事。厘清争议，同时将相关议题整理出来之后，我们便要进入主题阅读的最后一个步骤。</p><p><strong>主题阅读步骤五：分析讨论。</strong><br>到目前为止，我们已经检验过作品，找出相关的章节，设定了一个不偏不倚的共识，适用于所有被检视过的作者，再设定出一整套的问题，其中大部分都能在作者的说明中找到答案。然后就不同的答案界定并安排出议题。接下来该怎么做呢?    </p><p>前面四个步骤与分析阅读的前两组规则是互相辉映的。这些规则应用在任何一本书中，都会要我们<strong>回答一个问题：这本书在说些什么？是如何说明的？</strong>在<strong>主题阅读中</strong>，对于与我们的问题相关的讨论，我们<strong>也要回答类似的问题</strong>。在<strong>只阅读一本书的分析阅读</strong>中，剩下<strong>还有两个问题要回答：这是真实的吗？这与我何干？</strong>而在<strong>主题阅读</strong>中，我们对于讨论<strong>也要准备回答同样的问题</strong>。</p><p>因此，就可以发现的真理而言，就我们可以找到的问题答案而言，与其说是立足于任何一组主旨或主张上，不如说是<strong>立足于顺序清楚的讨论的本身</strong>。因此，为了要让我们的头脑接受这样的真相——也让别人接受我们要多做一点工作，不只是问问题与回答问题而已。我们要依照特定的顺序来提问题，也要能够辨认为什么是这个顺序。我们必须说明这些问题的不同答案，并说明原因。我们也<strong>一定要能够从我们检视过的书中找出支持我们把答案如此分类的根据</strong>。只有<strong>当我们做到这一切时</strong>，我们<strong>才能号称针对我们问题的讨论作了分析，也才能号称真正了解了问题</strong>。</p><p>事实上，我们所做的可能超过这些。<strong>对一个问题完整地分析过后</strong>，<strong>将来其他人对同一个问题要作研究时</strong>，我们的<strong>分析讨论就会提供他一个很好的研究基础</strong>。那会<strong>清除一些障碍，理出一条路，让一个原创性的思考者能突破困境</strong>。如果没有这个分析的工作，就没法做到这一点，因为这个问题的各个层面就无法显现出来。</p><h4 id="20-3-客观的必要性"><a href="#20-3-客观的必要性" class="headerlink" title="20.3 客观的必要性"></a>20.3 客观的必要性</h4><p><strong>要完整地分析一个问题或某个主题，得指出这个讨论中的主要议题，或是一些基本的知性反对立场。</strong>这并不是说在所有的讨论中，反对的意见总是占主导的。相反，<strong>同意或反对的意见总是互相并存的</strong>。也就是说，在大多数的议题中，正反两面的意见总是有几个，甚至许多作者在支持。在一个争议性的立场上，我们很少看到一个孤零零的支持者或反对者。</p><p>换句话说，<strong>主题阅读的目的，并不是给阅读过程中发展出来的问题提供最终答案，也不是给这个计划开始时候的问题提供最终解答</strong>。当我们要给这样的主题阅读写一份读者报告的时候，这个道理特别清楚。如果这份报告就任何所界定并分析过的重要议题，想要主张或证明某一种观点的真实或虚假，都会太过教条，失去对话的意义。如果这么做，主题阅读就不再是主题阅读，而只是讨论过程中的另一个声音，失去了疏离与客观性。</p><p>我们要说的是我们在<strong>追求理解的过程中，可以而且应该多贡献一种不同的形式</strong>。而<strong>这样的形式必须是绝对客观又公正的</strong>。<strong>主题阅读所追求的这种特质</strong>，可以用这句话来作总结：＂<strong>辩证的客观。</strong>”</p><p>简单来说，<strong>主题阅读就是要能面面俱到，而自己井不预设立场</strong>。当然，这是个严格的理想，一般人是没法做到的。而<strong>绝对的客观也不是人类所能做到的事</strong>。他可能可以做到不预设立场，毫无偏见地呈现出任何观点，对不同的意见也保持中立。但是<strong>采取中立比面面俱到要容易多了</strong>。在这一方面，主题阅读的读者注定会失败的。一个议题有各种不同的观点，不可能巨细靡遗地全都列出来。虽然如此，<strong>读者还是要努力一试</strong>。</p><p>虽然我们说<strong>保持中立要比面面俱到容易一些，但还是没那么容易</strong>。<strong>主题阅读的读者必须抗拒一些诱惑，厘清自己的思绪。</strong>对于某些冲突性的观点避免作出明白的真伪判断，并不能保证就能做到完全的公正客观。偏见可能会以各种微妙的方式进入你的脑海中——可能是总结论述的方式，可能是因为强调与忽略的比重，可能是某个问题的语气或评论的色彩，甚至可能因为对某些关键问题的不同答案的排列顺序。</p><p>要<strong>避免这样的危险</strong>，谨慎的<strong>主题阅读的读者可以采取一个明显的手段</strong>，<strong>尽量多加利用</strong>。那就是他要<strong>不断回头参阅诸多作者的原文，重新再阅读相关的章节</strong>。并且，<strong>当他要让更多的人能应用他的研究结果时，他必须照原作者的原文来引用他的观点或论述</strong>。虽然看起来有点矛盾，但这并不影响我们前面所说的，在分析问题时必须先建立一套中立的词汇。这样的中立语言还是必要的，而且在总结一个作者的论述时，一定要用这套中立的语言，而不是作者的语言。但是伴随着总结，<strong>一定要有仔细引用的作者原文，以免对文意有所扭曲，这样阅读者才能自己判断你对作者所作的诠释是否正确。</strong></p><p><strong>主题阅读的读者必须能够坚决地避免这个问题，才不会偏离公正客观的立场</strong>。要达到这样的理想，必须要能不偏不倚地<strong>在各种相对立的问题中保持平衡</strong>，<strong>放下一切偏见</strong>，<strong>反省自己是否有过与不及的倾向</strong>。在最后的分析中，一份主题阅读的书面报告是否达到对话形式的客观，虽然也可以由读者来判断，但只有写这份报告的人才真正明白自己是否达到这些要求。</p><h4 id="20-4-主题阅读的练习实例：进步论"><a href="#20-4-主题阅读的练习实例：进步论" class="headerlink" title="20.4 主题阅读的练习实例：进步论"></a>20.4 主题阅读的练习实例：进步论</h4><p>在主题阅读中，要包括小说、戏剧与诗是很困难的，原因有很多个。<br>第一，<strong>故事的精髓在情节，而非对某个议题所秉持的立场</strong>。一般来说，要将小说作者的观点列入议题的某一方时需要作很多很广泛的努力。要花的努力很多，得到的结果却可能是半信半疑的，因此通常最好放弃在这方面的努力。</p><p>可以检验进步这个概念的其他许多作品，一如常见的情况，显得一片混乱。面对这样的问题，我们前面说过，就是要建立起一套中立的语句。这是一个很复杂的工作，下面的例子可以帮助我们说明这是如何进行的。</p><p>所谓“进步”一词，不同的作者有许多不同的用法。这些不同的用法，大部分显示的只是意义的轻重不同，因而可以用分析的方法来处理。但是有些作者也用这个词来指出历史上某种特定的变化，而这种变化不是改善的变化。既然大多数作者都用“进步”来指出历史上某种为了促进人类朝向更美好生活的变化，并且既然往更改善的状态的变化是这个概念的基础，那么同样的字眼就不能适用于两种相反的概念了。因此，本例我们取大多数人的用法，那些主张历史上“非关改善的进展”(non-meliorative advance)的作者，就只好划为少数派了。我们这么说的目的是，在讨论这些少数作者的观点时，<strong>就算他们自己运用了“进步”这样的字眼，我们也不能将他们纳入“进步”的概念中</strong>。</p><p>我们前面说过，<strong>主题阅读的第三步是厘清问题</strong>。在“进步”的例子中，我们对这个问题一开始的直觉，经过检验之后，证明是正确的。第一个要问的问题，也是各个作者被认为提供各种不同答案的问题，是“历史上真的有‘进步’这回事吗？”说历史的演变整体是朝向改善人类的生存条件，的确是事实吗？基本上，对这个问题有三种不同的回答：(1)是(2)否(3)不知道。然而，回答“是“可以用许多不同的方式来表达，回答“否”也有好几种说法，而说“不知道”也至少有三种方式。</p><p><strong>对这个基本问题所产生的各式各样相互牵连的答案，构成我们所谓关于进步的一般性争议</strong>。所谓一般性，是因为我们研究的每个作者，只要对这个主题有话要说，就会在这个主题所界定的各个议题上选边站。但是对于进步还有一种特殊的争论，参与这种议题的，都是一些主张进步论的作者——这些作者主张进步确实发生。身为进步论的作者，他们全都强调进步是一种历史的事实，而所有的议题都应该和进步的本质或特质相关。这里的议题其实只有三种，只是个别讨论起来都很复杂。<br>这三个议题我们可以用问题的形式来说明：<br>(1)  进步是必要的？还是要取决于其他事件?<br>(2)  进步会一直无止境地持续下去？还是会走到终点或高原期而消失?<br>(3)  进步是人类的天性，还是养成的习惯——来自人类动物的本能，或只是外在环境的影响？</p><p>最后，就<strong>进步发生的面向</strong>而言，还有一些次要议题，不过，这些议题仍然只限于在主张进步论的作者之间。有六个面向是某些作者认为会发生，另外有些作者虽然多少会反对其中一两个的发生，但不会全部反对（因为他们在定义上就是肯定进步发生的作者）。这六个面向是：<br>(1)  <strong>知识的进步</strong><br>(2)  <strong>技术的进步</strong><br>(3)  <strong>经济的进步</strong><br>(4)  <strong>政治的进步</strong><br>(5)  <strong>道德的进步</strong><br>(6)  <strong>艺术的进步</strong><br>关于最后一项有些特殊的争议。因为在我们的观点里，没有一位作者坚信在这个面向中真的有进步，甚至有些作者否认这个面向有进步。</p><p>我们列举出“进步”的分析架构，只是要让你明白，在这个主题中包含了多少的议题，与对这些讨论的分析——换句话说，这也是主题阅读的第四及第五个步骤。<strong>主题阅读的读者必须做类似的工作才行</strong>，当然，<strong>他用不着非得就自己的研究写一本厚厚的书不可</strong>。</p><h4 id="20-5-如何应用主题工具书"><a href="#20-5-如何应用主题工具书" class="headerlink" title="20.5 如何应用主题工具书"></a>20.5 如何应用主题工具书</h4><p>如果你仔细阅读过本章，你会注意到，虽然我们花了不少时间谈这件事，但我们并没有解决主题阅读中的矛盾问题。这个<strong>矛盾</strong>可以说明如下：<strong>除非你知道要读些什么书，你没法使用主题阅读。但是除非你能做主题阅读，否则你不知道该读些什么书。</strong>换句话说，这可以算是<strong>主题阅读中的根本问题</strong>。也就是说，<strong>如果你不知道从何开始，你就没法做主题阅读</strong>。就算你对如何开始有粗浅的概念，你花在寻找相关书籍与篇章的时间，远超过其他步骤所需时间的总和。</p><p>当然，至少理论上有一种方法可以解决这个矛盾的问题。理论上来说，你可以对我们传统中的主要经典作品有一番完整的认识，对每本书所讨论的各种观念都有相当的认知。如果你是这样的人，就根本用不着任何人帮忙，我们在主题阅读上也没法再多教给你什么了。</p><p>从另一个角度来看，就算你本身没有这样的知识，你还是<strong>可以找有这种知识的人帮忙</strong>。但<strong>你要认清一点，就算你能找到这样的人，他的建议最后对你来说，在帮助的同时，几乎也都会变成障碍</strong>。如果那个主题<strong>正好是他做过特殊研究</strong>的，对他来说就很难只告诉你哪<strong>些章节是重要相关</strong>的，而不告诉你该如何读这些书——而这一点很可能就造成你的<strong>阻碍</strong>。但是<strong>如果他并没有针对这个主题做过特殊的研究，他知道的也许还没有你多</strong>——尽管你们双方都觉得应该比你多。</p><p>因此，你需要的是一本工具书，能告诉你在广泛的资料当中，到哪里去找与你感兴趣的主题相关的章节，而用不着花时间教你如何读这些章节——也就是对这些章节的意义与影响不抱持偏见。</p><p>当然，<strong>主题工具书</strong>有一个主要的缺点。这仍然是一套书目的索引（尽管是很大的一套），至于这套书没有包含的其他作品里什么地方可以找到你要的东西，则<strong>只有一些粗略的指引</strong>。不过，<strong>不管你要做哪一类主题阅读，这套书至少总能帮助你知道从何处着手</strong>。同时，<strong>在这整套名著中的书，不论是关于哪个主题，也都是你真的想要阅读的书</strong>。因此，<strong>主题工具书</strong>能帮助成熟的学者，或刚开始研究特定问题的初学者<strong>节省许多基本的研究工具</strong>，<strong>能让他很快进人重点，开始做独立的思考</strong>。因为他<strong>已经知道前人的思想是什么了。</strong></p><p>主题工具书对这种研究型的读者很有帮助，而且对初学者更有助益。主题工具书能从三方面帮助刚开始做研究的人：<strong>启动阅读，建议阅读，指导阅读</strong>。</p><p><strong>在启动阅读方面</strong>，主题工具书能帮助我们在面对传统经典作品时，克服最初的困难。这些作品都有点吸引力，我们都很想读这些书，但往往做不到。我们听到很多建议，要我们从不同的角度来阅读这样的书，而且有不同的阅读进度，从简单的作品开始读，再进展到困难的作品。但是所有这类阅读计划都是要读完整本书，或是至少要读完其中的大部分内容。就一般的经验来说，这样的解决方案很少能达到预期的效果。</p><p>对于这类经典巨著，使用主题阅读再加上主题工具书的帮助，就会产生完全不同的解决方案。<strong>主题工具书可以帮读者就他们感兴趣的主题，启动他对一些经典著作的阅读</strong>——在这些主题上，<strong>先阅读来自大量不同作者的一些比较短的章节</strong>。这可以<strong>帮助我们在读完这些经典著作之前，先读进去</strong>。</p><p>使用主题阅读来阅读经典名著，再加上主题工具书的帮助，还能提供我们许多建议。<strong>读者一开始阅读是对某个主题特别感兴趣，但是会逐渐激发出对其他主题的兴趣。而一旦你开始研究某位作者，就很难不去探索他的上下文</strong>。就在你明白过来之前，这本书你已经读了一大半了。</p><p>最后，主题阅读加上主题工具书，还能从<strong>三种不同的方向指导关系</strong>。事实上这是这个层次的阅读最有利的地方。</p><p><strong>第一，读者阅读的章节所涉及的主题，能够给他一个诠释这些章节的方向。</strong>但这并不是告诉他这些章节是什么意思，因为一个章节可能从好几个或许多个方向与主题相关。而读者的责任就是要<strong>找出这个章节与主题真正相关的地方在哪里</strong>。要学习这一点，需要拥有很重要的阅读技巧。</p><p><strong>第二，针对同一个主题，从许多不同的作者与书籍中收集出来的章节，能帮助读者强化对各个章节的诠释能力。</strong>有时候我们从同一本书中依照顺序来阅读的章节，以及<strong>挑出来比对阅读的章节，相互对照之下可以让我们更了解其中的含意</strong>。有时候从不同书中摘出来的章节是互相冲突的，但是当你读到彼此冲突的论点时，就会更明白其中的意义了。有时候从一个作者的书中摘出来的章节，由另一个作者的书的某个章节作补充或评论，实际上可以帮助读者对第二位作者有更多的了解。 </p><p><strong>第三，如果主题阅读运用在许多不同的主题上，当你发现同一个章节被主题工具书引述在许多不同主题之下的时候，这件事情本身就很有指导阅读的效果。</strong>随着读者针对不同的主题要对这些章节进行多少不同的诠释，他会发现这些章节含有丰富的意义。<strong>这种多重诠释的技巧，不只是阅读技巧中的基本练习，同时也会训练我们的头脑面对任何含义丰富的章节时，能习惯性地作出适当的调整。</strong></p><p>因为我们相信，<strong>对想做这个层次的阅读的读者来说，无论他是资深的学者或初学者，主题工具书都很有帮助，因此我们称这一阅读层次为主题阅读</strong>。我们希望读者能原谅我们一点点的自我耽溺。为了回报您的宽容，我们要指出很重要的一点。</p><p><strong>主题阅读</strong>可以说有<strong>两种</strong>：</p><ol><li>一种是<strong>单独使用的主题阅读</strong></li><li>一种是<strong>与主题工具一起并用</strong></li></ol><p><strong>后一种可以当作是构成前一种阅读计划的一部分</strong>，一开始<strong>由这里着手</strong>，是<strong>最聪明的做法</strong>。 而前一种主题阅读所应用的范围要比后一种广义许多。</p><h4 id="20-6-构成主题阅读的原则"><a href="#20-6-构成主题阅读的原则" class="headerlink" title="20.6 构成主题阅读的原则"></a>20.6 构成主题阅读的原则</h4><p>当然，我们对所有这些指控都不同意，我们要依序回答这些指控。让我们一次谈一个。<br><strong>第一，是关于词汇的问题。</strong>否认一个概念可以用不同的词汇来说明，就像否认一种语言可以翻译成另一种语言。当然，这样的否认是刻意制造出来的。譬如最近我们阅读（《古兰经》的一个新译本，前言一开始便说要翻译（《古兰经》是不可能的事。但是因为译者接着又解释他是如何完成的，所以我们只能假设他的意思是：要翻译这样一本被众人视为神圣的典籍，是一件极为困难的事。我们也同意。<strong>不过困难并不代表做不到。</strong></p><p>事实上，所谓作者本身的词汇是神圣不可侵犯的说法，其实只是在说要将一种说法翻译成另一种说法是非常困难的。这一点我们也同意。但是，<strong>同样的，困难并非不可能做到。</strong></p><p><strong>其次．谈到作者各自区隔与独立的特性。</strong>这就像说有一天亚里士多德走进我们办公室（当然穿着长袍），身边跟着一位又懂现代英语又懂古希腊语的翻译，而我们却无法听懂他讲什么，他也无法听懂我们讲什么一样。我们不相信有这回事。毫无疑问，亚里士多德对他看到的许多事一定觉得很讶异，但我们确信在十分钟之内，只要我们想，我们就能跟他一起讨论某个我们共同关心的问题。<strong>对于一些特定的概念一定会发生困难，但是只要我们能够发现，就能解决。</strong></p><p>如果这是可行的（我们不认为任何人会否认），那么让一本书经由翻译——也就是主题阅读的读者——与另一本书的作者”谈话“，并不是不可能的事。当然，这需要很谨慎，而且你要把双方的语言——也就是两本书的内容——了解得越透彻越好。<strong>这些问题并非不能克服，如果你觉得无法克服只是在自欺欺人。</strong></p><p><strong>最后，谈到风格的问题。</strong>我们认为，这就像是说人与人之间无法作理性的沟通，而只能作情绪上的沟通——就像你跟宠物沟通的层次。</p><p>如果你用很愤怒的腔调对你的狗说：“我爱你！＂它会吓得缩成团，并不知道你在说什么。有谁能说：人与人之间的语言沟通，除了语气与姿势外就没有其他的东西？说话的语气是很重要的——尤其当沟通的主要内容是情绪关系的时候；而当我们只能听（或者看？）的时候，肢体语言中可能就有些要告诉我们的事情。但是人类的沟通，不只这此东西。如果你问一个人出口在哪里？他告诉你沿着B走廊就会看到。这时他用的是什么语气并不重要。他可能对也可能错，可能说实话也可能撒谎，但是重点在你沿着B走廊走，很快就能找到出口了。你知道他说的是什么，也照着做了，这跟他如何说这句话一点关系也没有。</p><p>只要<strong>相信翻译是可行的</strong>（因为人类一直在做这件事），<strong>书与书之间就能彼此对谈</strong>（因为人类也一直在这么做）。<strong>只要愿意这么做，人与人之间也有理性客观的沟通能力</strong>（因为我们能彼此互相学习），所以我们相信主题阅读是可行的。</p><h4 id="20-7-主题阅读精华摘要"><a href="#20-7-主题阅读精华摘要" class="headerlink" title="20.7 主题阅读精华摘要"></a>20.7 主题阅读精华摘要</h4><p>我们已经谈完主题阅读了。让我们将这个层次的阅读的每个步骤列举出来。<br>我们说过，在主题阅读中有两个阶段。<br><strong>一个是准备阶段，另一个是主题阅读本身</strong>。让我们复习一下这些不同的步骤： </p><p><strong>一、观察研究范围：主题阅读的准备阶段</strong><br>(1)  <strong>针对你要研究的主题，设计一份试验性的书目。</strong>你可以参考图书馆目录、专家的建议与书中的书目索引。<br>(2)  <strong>浏览这份书目上所有的书</strong>，<strong>确定哪些与你的主题相关</strong>，并<strong>就你的主题建立起清楚的概念</strong>。</p><p><strong>二、主题阅读：阅读所有第一阶段收集到的书籍</strong><br>(1)  <strong>浏览所有在第一阶段被认定与你主题相关的书，找出最相关的章节。</strong><br>(2)  <strong>根据主题创造出一套中立的词汇，带引作者与你达成共识</strong>——无论作者是否实际用到这些词汇，所有的作者，或至少绝大部分的作者都可以用这套词汇来诠释。<br>(3)  <strong>建立一个中立的主旨，列出一连串的问题无论作者是否明白谈过这些问题</strong>，所有的作者，或者<strong>至少大多数的作者都要能解读为针对这些问题提供了他们的回答。</strong><br>(4)  <strong>界定主要及次要的议题。</strong>然后<strong>将作者针对各个问题的不同意见整理陈列在各个议题之旁。</strong>你要记住，各个作者之间或之中，不见得一定存在着某个议题。有时候，你需要针对一些不是作者主要关心范围的事情，把他的观点解读，才能建构出这种议题。<br>(5)  <strong>分析这些讨论。</strong>这得<strong>把问题和议题按顺序排列，以求突显主题</strong>。<strong>比较有共通性的议题，要放在比较没有共通性的议题之前</strong>。<strong>各个议题之间的关系也要清楚地界定出来。</strong></p><p><strong>注意：</strong>理想上，<strong>要一直保持对话式的疏离与客观</strong>。要做到这一点，每<strong>当你要解读某个作家对一个议题的观点时，必须从他自己的文章中引一段话来并列。</strong></p><h3 id="第二十一章-阅读与心智的成长"><a href="#第二十一章-阅读与心智的成长" class="headerlink" title="第二十一章 阅读与心智的成长"></a>第二十一章 阅读与心智的成长</h3><p>我们已经完成了在本书一开始时就提出的内容大要。我们已经说明清楚，<strong>良好的阅读基础在于主动的阅读。阅读时越主动，就读得越好。</strong></p><p><strong>所谓主动的阅读，也就是能提出问题来</strong>。我们也指出在阅读任何一本书时该<strong>提出什么样的问题</strong>，以及<strong>不同种类的</strong>书必须<strong>怎样以不同的方式回答这些问题。</strong></p><p>我们也区分并讨论了阅读的四种层次，并说明这<strong>四个层次是累积渐进的，前面或较低层次的内容包含在后面较高层次的阅读里</strong>。接着，我们刻意强调后面较高层次的阅读，而比较不强调前面较低层次的阅读。因此，我们特别<strong>强调分析阅读与主题阅读</strong>。因为对大多数的读者来说，分析阅读可能是最不熟悉的一种阅读方式，我们特别花了很长的篇幅来讨论，定出规则，并说明应用的方法。不过分析阅读中的所有规则，只要照最后一章所说的略加调整，就同样适用于接下来的主题阅读。</p><p>我们完成我们的工作了，但是你可能还没有完成你的工作。我们用不着再提醒你，这是一本实用性的书，或是阅读这种书的读者有什么特殊的义务。我们认为，<strong>如果读者阅读了一本实用的书，并接受作者的观点，认同他的建议是适当又有效的，那么读者一定要照着这样的建议行事。</strong>你可能不接受我们所支持的<strong>主要目标</strong>——也就是你<strong>应该有能力读得更透彻</strong>——也不同意我们建议达到目标的方法——也就是<strong>检视阅读、分析阅读与主题阅读的规则</strong>。（但如果是这样，你可能也读不到这一页了。）不过如果你<strong>接受这个目标</strong>，也<strong>同意这些方法是适当的</strong>，那你就<strong>一定要以自己以前可能从没有经历过的方式来努力阅读了。</strong></p><h4 id="21-1-好书能给我们什么帮助"><a href="#21-1-好书能给我们什么帮助" class="headerlink" title="21.1 好书能给我们什么帮助"></a>21.1 好书能给我们什么帮助</h4><p>“手段”(means)这两个字可以解释成两种意义。在前面的章节中，我们将<strong>手段当作是阅读的规则</strong>，也就是<strong>使你变成一个更好的阅读者的方法</strong>。但是<strong>手段也可以解释为你所阅读的东西</strong>。空有方法却没有可以运用的材料，就和空有材料却没有可以运用的方法一样是毫无用处的。</p><p>以“手段”的后一种意思来说，<strong>未来提升你阅读能力的手段其实是你将阅读的那些书</strong>。我们说过<strong>，这套阅读方法适用于任何一本</strong>，以及任何一种你所阅读的书——无论是小说还是非小说，想像文学还是论说性作品，实用性还是理论性。但是事实上，起码就我们在探讨分析阅读与主题阅读过程中所显示的这套方法<strong>井不适用于所有的书</strong>。原因是<strong>有些书根本用不上这样的阅读</strong>。</p><p>我们在前面已经提过这一点了，但我们想要再提一遍，因为这与你马上要做的工作有关。<strong>如果你的阅读目的是想变成一个更好的阅读者，你就不能摸到任何书或文章都读。</strong>如果你所读的书都在你的能力范围之内，你就没法提升自己的阅读能力。<strong>你必须能操纵超越你能力的书</strong>，或像我们所说的，<strong>阅读超越你头脑的书</strong>。<strong>只有那样的书能帮助你的思想增长。除非你能增长心智，否则你学不到东西。</strong></p><p>因此，对你来说最重要的是，你<strong>不只要能读得好，还要有能力分辨出哪些书能帮助你增进阅读能力</strong>。</p><p>我们说过很多次，<strong>一个好的读者也是自我要求很高的读者</strong>。他<strong>在阅读时很主动，努力不懈</strong>。现在我们要谈的是另外一些观念。你想要用来练习阅读技巧，尤其是分析阅读技巧的书，一定要对你也有所要求<strong>。这些书</strong>一定要<strong>看起来是超越你的能力才行</strong>。你大可不必担心真的如此，<strong>只要你能运用我们所说的阅读技巧，没有一本书能逃开你的掌握。</strong>当然，这并不是说所有的技巧可以一下子像变魔术一样让你达到目标。<strong>无论你多么努力，总会有些书是跑在你前面的。事实上，这些书就是你要找的书，因为它们能让你变成一个更有技巧的读者。</strong></p><p>有些读者会有错误的观念，以为那些书——对读者的阅读技巧不断提出挑战的书籍——都是自己不熟悉的领域中的书。结果一般人都相信，对大多数读者来说，只有科学作品，或是哲学作品才是这种书。但是事实并非如此。我们已经说过，<strong>伟大的科学作品比一些非科学的书籍还要容易阅读，因为这些科学作者很仔细地想要跟你达成共识，帮你找出关键主旨，同时还把论述说明清楚</strong>。在文学作品中，找不到这样的帮助，所以长期来说，那些书才是要求最多，最难读的书。譬如从许多方面来说，荷马的书就比牛顿的书难读——尽管你在第一次读的时候，可能对荷马的体会较多。荷马之所以难读，是因为他所处理的主题是很难写好的东西。</p><p>我们在这里所谈的困难，跟阅读一本烂书所谈的困难是不同的。<strong>阅读一本烂书</strong>也是很困难的事，因为那样的书<strong>会抵消你为分析阅读所作的努力</strong>，每当你认为能掌握到什么的时候又会溜走。事实上，<strong>一本烂书根本不值得你花时间去努力，甚至根本不值得作这样的尝试。你努力半天还是一无所获</strong>。</p><p>读一本好书，却会计你的努力有所回报。最好的书对你的回馈也最多。当然，<strong>这样的回馈分成两种：</strong><br><strong>第一</strong>，当<strong>你成功地阅读了一本难读的好书</strong>之后，你的<strong>阅读技巧必然增进了。</strong><br><strong>第二</strong>，长期来说这一点更重要——<strong>一本好书能教你了解这个世界以及你自己。</strong>你<strong>不只更懂得如何读得更好，还更懂得生命</strong>。你<strong>变得更有智慧</strong>，而不只是更有知识——像只提供讯息的书所形成的那样。<strong>你会成为一位智者，对人类生命中永恒的真理有更深刻的体认。</strong></p><p><strong>伟大的经典就是在帮助你把这些问题像的更清楚一点，因为这些书的作者都是比一般人思想更深刻的人。</strong></p><h4 id="21-2-书的金字塔"><a href="#21-2-书的金字塔" class="headerlink" title="21.2 书的金字塔"></a>21.2 书的金字塔</h4><p><strong>西方传统所写出的几百万册的书籍中，百分之九十九都对你的阅读技巧毫无帮助。</strong>这似乎是个令人困恼的事实，不过连这个百分比也似乎高估了。但是，想想有这么多数量的书籍，这样的估算还是没错。有许多书只能当作娱乐消遣或接收资讯用。娱乐的方式有很多种，有趣的资讯也不胜枚举，但是你别想从中学习到任何重要的东西。<strong>事实上，你根本用不着对这些书做分析阅读。扫描一下便够了。</strong></p><p><strong>第二种类型的书籍是可以让你学习的书——学习如何阅读，如何生活。</strong>只有百分之一，千分之一，甚或万分之一的书籍合乎这样的标准。<strong>这些书是作者的精心杰作，所谈论的也是人类永远感兴趣，又有特殊洞察力的主题。</strong>这些书可能不会超过几千本，对读者的要求却很严苛，<strong>值得做一次分析阅读</strong>——一次。如果你的技巧很熟练了，<strong>好好地阅读过一次</strong>，你就能<strong>获得所有要获得的主要概念</strong>了。你把这本书读过一遍，便可以放回架上。你知道你用不着再读一遍，但你可能要<strong>常常翻阅，找出一些特定的重点，或是重新复习一下一些想法或片段</strong>。（你<strong>在这类书中的空白处所做的一些笔记</strong>，对你会特别有帮助。）</p><p><strong>你怎么知道不用再读那本书了呢？</strong><br>因为你<strong>在阅读时，你的心智反应已经与书中的经验合而为一</strong>了。这样的书会<strong>增长你的心智，增进你的理解力</strong>。就在<strong>你的心智成长，理解力增加之后</strong>，你<strong>了解到</strong>——这是多少有点<strong>神秘的经验</strong>——这本书对你以后的心智成长不会再有帮助了。你知道你已经<strong>掌握这本书的精髓</strong>了。你将<strong>精华完全吸</strong>收了。你很感激这本书对你的贡献，但你知道它能付出的仅止于此了。</p><p>在几千本这样的书里，还有更少的一些书——很可能不到一百种——却是你读得再通，也不可能尽其究竟。你要如何分辨哪些书是属于这一类的呢？这又是有点神秘的事了，不过当你<strong>尽最大的努力用分析阅读读完</strong>一本书，把书放回架上的时候，你心中会有点疑惑，好像还也有什么你没弄清楚的事。我们说“疑惑＂，是因为在这个阶段可能仅只是这种状态。<strong>如果你确知你错过了什么，身为分析阅读者，就有义务立刻重新打开书来，厘清自己的问题是什么</strong>。事实上，你没法一下子指出问题在哪里，但你知道在哪里。你会发现自己忘不了这本书，一直想着这本书的内容，以及自己的反应。最后，你又<strong>重看一次</strong>。然后<strong>非常特殊的事就发生</strong>了。</p><p>如果这本书是属于前面我们所说第<strong>二种类型的书</strong>，重读的时候，你会发现<strong>书中的内容好像比你记忆中的少了许多</strong>。当然，原因是<strong>在这个阶段中你的心智成长了许多</strong>。你的头脑充实了，理解力也增进了。书籍本身并没有改变，改变的是你自己。这样的重读，无疑是计人失望的。</p><p>但是如果这本书是属于更高层次的书——只占浩瀚书海一小部分的书——你在重读时会发现<strong>这本书好像与你一起成长了</strong>。你<strong>会在其中看到新的事物</strong>——<strong>一套套全新的事物</strong>——那是你<strong>以前没看到的东西</strong>。你<strong>以前对这本书的理解并不是没有价值</strong>（假设你第一次就读得很仔细了），<strong>真理还是真理</strong>．<strong>只是过去是某一种面貌，现在却呈现出不同的面貌</strong>。</p><p>一本书怎么会跟你一起成长呢？当然这是不可能的。<strong>一本书只要写完出版了，就不会改变了。</strong>只是你到这时才会开始明白，你最初阅读这本书的时候，这本书的层次就远超过你，现在你重读时仍然超过你，未来很可能也一直超过你。因为这是<strong>一本真正的好书</strong>——我们可说是<strong>伟大的书——所以可以适应不同层次的需要</strong>。你<strong>先前读过的时候感到心智上的成长，并不是虚假的</strong>。那本书的确<strong>提升了你</strong>。但是现在，<strong>就算你已经变得更有智慧也更有知识，这样的书还是能提升你，而且直到你生命的尽头</strong>。</p><p>显然并没有很多书能为我们做到这一点。我们评估这样的书应该少于一百本。但<strong>对任何一个特定的读者来说，数目还会更少</strong>。<strong>人类除了心智力最的不同之外，还有许多其他的不同</strong>。<strong>他们的品味不同，同一件事对这个人的意义就大过对另一个人。</strong>你对牛顿可能就从没有对莎士比亚的那种感觉，这或许是因为你能把牛顿的书读得很好，所以用不着再读一遍，或许是因为数学系统的世界从来就不是你能亲近的领域。如果你喜欢数学——像达尔文就是个例子——牛顿跟其他少数的几本书对你来说就是伟大的作品，而不是莎士比亚。</p><p>我们并不希望很权威地告诉你，哪些书对你来说是伟大的作品。不过在我们的第一个附录中，我们还是列了一些清单，因为根据我们的经验，这些书对许多读者来说都是很有价值的书。我们的重点是，<strong>你该自己去找出对你有特殊价值的书来</strong>。这样的书能<strong>教你很多关于阅读与生命的事情</strong>。这样的书你会想一读再读。这也是会帮助你不断成长的书。</p><h4 id="21-3-生命与心智的成长"><a href="#21-3-生命与心智的成长" class="headerlink" title="21.3 生命与心智的成长"></a>21.3 生命与心智的成长</h4><p>有一种很古老的测验——上一个世纪很流行的测验——目的在于帮你找出对你最有意义的书目。测验是这样进行的：如果你被警告将在一个无人荒岛度过余生，或至少很长的一段时间，而假设你有时间作一些准备，可以带一些实际有用的物品到岛上，还能带十本书去，你会选哪十本？</p><p>试着列这样一份书单是很有指导性的，这倒不只是因为可以帮助你发现自己最想一读再读的书是哪些。事实上，和另外一件事比起来，这一点很可能是微不足道的。那件事就是：当你想像自已被隔绝在一个没有娱乐、没有资讯、没有可以理解的一般事物的世界时，比较起来你是否会对自己了解得更多一点？记住，岛上没有电视也没有收音机，更没有图书馆，只有你跟十本书。</p><p>你开始想的时候，会觉得这样想像的情况有点稀奇古怪，不太真实。当真如此吗？我们不这么认为。<strong>在某种程度上，我们都跟被放逐到荒岛上的人没什么两样。</strong>我们面对的都是<strong>同样的挑战——如何找出内在的资源，过更美好的人类生活的挑战。</strong></p><p><strong>人类的心智有很奇怪的一点，主要是这一点划分了我们心智与身体的截然不同。</strong>我们的<strong>身体是有限制的，心智却没有限制</strong>。其中一个迹象是，<strong>在力量与技巧上，身体不能无限制地成长</strong>。人们到了30岁左在身体状况就达到了巅峰，<strong>随着时间的变化，身体的状况只有越来越恶化，而我们的头脑却能无限地成长与发展下去</strong>。我们的心智不会因为到了某个年纪死就停止成长，只有当大脑失去活力，僵化了，才会失去了增加技巧与理解力的力量。</p><p>这是人类最明显的特质，也是万物之灵与其他动物最主要不同之处。其他的动物似乎发展到某个层次之后，便不再有心智上的发展。但是人<strong>类独有的特质，却也潜藏着巨大的危险</strong>。<strong>心智就跟肌肉一样，如果不常运用就会萎缩。</strong> <strong>心智的萎缩就是在惩罚我们不经常动脑</strong>。这是个可怕的惩罚，因为证据显示，<strong>心智萎缩也可能要人的命</strong>。除此之外，似乎也没法说明为什么<strong>许多工作忙碌的人一旦退休之后就会立刻死亡</strong>。他们<strong>活着是因为工作对他们的心智上有所要求，那是一种人为的支撑力量，也就是外界的力量</strong>。<strong>一旦外界要求的力量消失之后，他们又没有内在的心智活动，他们便停止了思考，死亡也跟着来了</strong>。</p><p>电视、收音机及其他天天围绕在我们身边的娱乐或资讯，也都是些人为的支撑物。它们会让我们觉得自己在动脑，因为我们要<strong>对外界的刺激作出反应</strong>。但是<strong>这些外界刺激我们的力量毕竟是有限的</strong>。像药品一样，一旦习惯了之后，需要的量就会越来越大。到最后，这些力量就只剩下一点点，甚或毫无作用了。这时，<strong>如果我们没有内在的生命力量，我们的智力、品德与心灵就会停止成长。当我们停止成长时，也就迈向了死亡。</strong></p><p><strong>好的阅读，也就是主动的阅读，不只是对阅读本身有用，也不只对我们的工作或事业有帮助，更能帮助我们的心智保持活力与成长。</strong></p>]]></content>
      
      
      <categories>
          
          <category> 读书 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第二章 Python基础语法</title>
      <link href="/2020/05/02/di-er-zhang-python-ji-chu-yu-fa/"/>
      <url>/2020/05/02/di-er-zhang-python-ji-chu-yu-fa/</url>
      
        <content type="html"><![CDATA[<h2 id="第二章-Python基础语法"><a href="#第二章-Python基础语法" class="headerlink" title="第二章 Python基础语法"></a>第二章 Python基础语法</h2><h3 id="1、Python常用数据类型"><a href="#1、Python常用数据类型" class="headerlink" title="1、Python常用数据类型"></a>1、Python常用数据类型</h3><ol><li>在Python中变量不直接存储值，而是存储值的内存地址或者引用。</li><li>在Python中，不需要事先声明变量名及其类型，使用赋值语句可以直接创建任意类型的变量，变量的类型取决于等号右侧表达式的类型。</li><li>赋值（比如a=’ABC’）时，Python解释器干了两件事：<br>1）在内存中创建一个’ABC’的字符串<br>2）在内存中创建一个名为a的变量，并把它指向’ABC’</li></ol><h3 id="2、Python的核心数据类型"><a href="#2、Python的核心数据类型" class="headerlink" title="2、Python的核心数据类型"></a>2、Python的核心数据类型</h3><h4 id="2-1-Number（数字）"><a href="#2-1-Number（数字）" class="headerlink" title="2.1 Number（数字）"></a>2.1 Number（数字）</h4><p>Python支持int,float,complex三种不同的数字类型</p><pre><code>a = 3b = 3.14c = 3+4jprint(type(a),type(b),type(c))isinstance(a, int)</code></pre><p><strong>结果：</strong><br>&lt;class ‘int’&gt; &lt;class ‘float’&gt; &lt;class ‘complex’&gt;<br>True</p><p><strong>示例代码</strong></p><pre><code>import math print(math.factorial(32))            # 计算32的阶乘print(0.4-0.3 == 0.1)                # 实数之间尽量避免直接比较大小print(math.isclose(0.4-0.3, 0.1))    # 测试两个实数是否足够接近c = 3+4j                            # Python内置支持复数及其运算print(c+c)                            # 复数相加print(c.real)                        # 查看复数的实部print(c.imag)                        # 查看复数的虚部print(3+4j.imag)                    # 相当于3+(4j).imag</code></pre><p><strong>结果：</strong><br>263130836933693530167218012160000000<br>False<br>True<br>(6+8j)<br>3.0<br>4.0<br>7.0</p><h4 id="2-2-String（字符串）"><a href="#2-2-String（字符串）" class="headerlink" title="2.2 String（字符串）"></a>2.2 String（字符串）</h4><ol><li>Python中的字符串可以使用单引号、双引号和三引号（三个单引号或三个双引号）括起来，使用反斜杠<code>\</code>转义特殊字符。</li><li>Python3源码文件默认以UTF-8编码，所有字符串都是unicode字符串</li><li>支持字符串拼接、截取等多种运算</li></ol><pre><code>a = &quot;Hello&quot;b = &quot;Python&quot;print(&quot;a+b 输出结果：&quot;, a+b)print(&quot;a[1:4] 输出结果：&quot;, a[1:4])</code></pre><p><strong>结果：</strong><br>a+b 输出结果： HelloPython<br>a[1:4] 输出结果： ell</p><p><strong>示例代码</strong></p><pre><code>text = &#39;&#39;&#39;Beautiful is better than ugly.Explicit is better than implicit.Simple is better than complex.Complex is better than complicated.Flat is better than nested.Sparse is better than dense.Readability counts.&#39;&#39;&#39;print(len(text))                # 字符串长度，即所有字符的数量print(text.count(&#39;is&#39;))            # 字符串中单词is出现的次数print(&#39;beautiful&#39; in text)        # 测试字符串中是否包含单词beautifulprint(&#39;=&#39;*20)                    # 字符串重复print(&#39;Good&#39;+&#39;Morning&#39;)            # 字符串连接</code></pre><p><strong>结果：</strong><br>208<br>6<br>False<br><code>====================</code><br>GoodMorning</p><h4 id="2-3-List（列表）"><a href="#2-3-List（列表）" class="headerlink" title="2.3 List（列表）"></a>2.3 List（列表）</h4><ol><li>列表可以完成大多数集合类的数据结构实现，列表中元素的类型可以不相同，它支持数字，字符串甚至可以包含列表（所谓嵌套）。</li><li>列表是写在方括号<code>[]</code>之间，用逗号分隔开的元素列表。</li><li>列表索引值以<code>0</code>位开始值，<code>-1</code>为从末尾的开始位置。</li><li>列表可以使用<code>+</code>操作符进行拼接，使用<code>*</code> 表示重复。</li></ol><pre><code>list = [&#39;abcd&#39;, 786, 2.23, &#39;runoob&#39;, 70.2]print(list[1:3])tinylist = [123, &#39;runoob&#39;]print(list + tinylist)</code></pre><p><strong>结果：</strong><br>[786, 2.23]<br>[‘abcd’, 786, 2.23, ‘runoob’, 70.2, 123, ‘runoob’]</p><h4 id="2-4-Tuple（元组）"><a href="#2-4-Tuple（元组）" class="headerlink" title="2.4 Tuple（元组）"></a>2.4 Tuple（元组）</h4><ol><li>tuple与list类似，不同之处在于tuple的元素不能修改。tuple写在小括号里，元素之间用逗号隔开。</li><li>元组的元素不可变，但可以包含可变对象，如list。<br>“注意: “ 定义一个只有一个元素的tuple，<code>必须加逗号</code>。<pre><code>t = (&#39;abcd&#39;, 786, 2.23, &#39;runoob&#39;, 70.2)t1 = (1,)t2 = (&#39;a&#39;,&#39;b&#39;,[&#39;A&#39;,&#39;B&#39;])t[2][0] = &#39;X&#39;t</code></pre></li></ol><p><strong>结果：</strong><br>(‘a’,’b’,[‘’,’B’])</p><h4 id="2-5-dict（字典）"><a href="#2-5-dict（字典）" class="headerlink" title="2.5 dict（字典）"></a>2.5 dict（字典）</h4><ol><li>字典是无序的对象集合，使用键-值（key-value）存储，具有极快的查找速度。</li><li>键（key）必须使用不可变类型。</li><li>同一字典中，键（key）必须是唯一的。</li></ol><pre><code>d = {&#39;Michael&#39;:95, &#39;Bob&#39;:75, &#39;Tracy&#39;:85}d[&#39;Michael&#39;]</code></pre><p><strong>结果：</strong><br>95</p><h4 id="2-6-set（集合）"><a href="#2-6-set（集合）" class="headerlink" title="2.6 set（集合）"></a>2.6 set（集合）</h4><ol><li>set和dict类似，也是一组key 的集合，但不存储value，由于key不能重复，所以，在set中，没有重复的key.</li><li>set是无序的，重复元素在set中自动被过滤。</li></ol><pre><code>s = set([1,2,3])ss = set([1,1,2,2,3,3])s</code></pre><p><strong>结果：</strong><br>{1, 2, 3}<br>{1, 2, 3}</p><p><code>注意：</code>set可以看成数学意义上的无序和无重复元素的集合，因此，两个set可以做数学意义上的交集（&amp;）、并集（|）、差集（-）等操作。</p><h4 id="2-7-列表、元组、字典与集合综合示例代码"><a href="#2-7-列表、元组、字典与集合综合示例代码" class="headerlink" title="2.7 列表、元组、字典与集合综合示例代码"></a>2.7 列表、元组、字典与集合综合示例代码</h4><pre><code># 创建列表对象x_list = [1,2,3]# 创建元组对象x_tuple = (1,2,3)# 创建字典对象，元素形式为“键:值”x_dict = {&#39;a&#39;:97, &#39;b&#39;:98, &#39;c&#39;:99}# 创建集合对象x_set = {1,2,3}# 使用下标访问列表中指定位置的元素，元素下标从0开始print(x_list[1])# 元组也支持使用序号作为下标，1表示第二个元素的下标print(x_tuple[1])# 访问字典中特定“键”对应的“值”，字典对象的下标是“键”print(x_dict[&#39;a&#39;])# 查看列表长度，也就是其中元素的个数print(len(x_list))# 查看元素2在元组中首次出现的位置print(x_tuple.index(2))# 查看字典中哪些“键”对应的“值”为98for key,value in x_dict.items():    if value == 98:        print(key)# 查看集合中的元素的最大值print(max(x_set))</code></pre><p><strong>结果：</strong><br>2<br>2<br>97<br>3<br>1<br>b<br>3</p><h3 id="3、运算符"><a href="#3、运算符" class="headerlink" title="3、运算符"></a>3、运算符</h3><h4 id="3-1-算术运算符"><a href="#3-1-算术运算符" class="headerlink" title="3.1 算术运算符"></a>3.1 算术运算符</h4><p>以下假设变量a为10，变量b为21：</p><table><thead><tr><th align="center">运算符</th><th align="left">描述</th><th align="left">实例</th></tr></thead><tbody><tr><td align="center">+</td><td align="left">加 - 两个对象相加</td><td align="left">a + b 输出结果31</td></tr><tr><td align="center">-</td><td align="left">减 - 得到复数或是一个数减去另一个数</td><td align="left">a - b 输出结果-11</td></tr><tr><td align="center">*</td><td align="left">乘 - 两个数相乘或是返回一个被重复若干次的字符串</td><td align="left">a * b 输出结果210</td></tr><tr><td align="center">/</td><td align="left">除 - x 除以 y</td><td align="left">b / a 输出结果2.1</td></tr><tr><td align="center">%</td><td align="left">取模 - 返回除法的余数</td><td align="left">b % a 输出结果1</td></tr><tr><td align="center">**</td><td align="left">幂 - 返回x的y次幂</td><td align="left">a <code>**</code> b 为10的21次方</td></tr><tr><td align="center">//</td><td align="left">取整除 - 向下取接近除数的整数</td><td align="left">9 // 2 结果是4 <br> -9 // 2的结果是-5</td></tr></tbody></table><p><strong>要点：</strong></p><ol><li><code>+</code> 运算符除了用于算术加法外，还可以用户列表、元组、字符串的连接。</li><li><code>-</code>运算符除了用于整数、实数、复数之间的算术减法和相反数之外，还可以计算集合的差集。需要注意的是，在进行实数之间的运算时，有可能会出现误差。</li><li><code>*</code> 运算符除了表示整数、实数、复数之间的算术乘法，还可以用于列表、元组、字符串这几个类型的对象与整数的乘法，表示序列元素的重复，生成新的列表、元组或字符串。</li><li><code>%</code> 运算符可以用于求余运算，还可以用于字符串格式化。</li></ol><p><strong>算术运算符示例</strong></p><pre><code># + 运算符除了用于算术加法外，还可以用于列表、元组、字符串的连接print(3 + 5)print(3.4 + 4.5)print((3+4j) + (5+6j))print(&#39;abc&#39; + &#39;def&#39;)print([1,2] + [3,4])print((1,2) + (3,))print(&quot;\n&quot;)# - 运算符除了用于整数、实数、复数之间的算术减法和相反数之外，还可以计算集合的差集。print(7.9 - 4.5)                # 注意，结果有误差print(5 -3)num = 3print(-num)print(--num)                    # 注意，这里的--是两个负号，负负得正            print(-(-num))                    # 与上一行代码含义相同print({1,2,3} - {3,4,5})        # 计算差集print({3,4,5} - {1,2,3})print(&quot;\n&quot;)</code></pre><p><strong>结果：</strong><br>8<br>7.9<br>(8+10j)<br>abcdef<br>[1, 2, 3, 4]<br>(1, 2, 3)</p><p>3.4000000000000004<br>2<br>-3<br>3<br>3<br>{1, 2}<br>{4, 5}</p><h4 id="3-2-比较运算符"><a href="#3-2-比较运算符" class="headerlink" title="3.2 比较运算符"></a>3.2 比较运算符</h4><p>以下假设变量a为10，变量b为20：</p><table><thead><tr><th align="center">运算符</th><th align="left">描述</th><th align="left">实例</th></tr></thead><tbody><tr><td align="center">==</td><td align="left">等于 - 比较对象是否相等</td><td align="left">(a == b)  返回Fasle</td></tr><tr><td align="center">!=</td><td align="left">不等于 - 比较两个对象是否不相等</td><td align="left">(a != b)  返回True</td></tr><tr><td align="center">&gt;</td><td align="left">大于 - 返回x是否大于y</td><td align="left">(a &gt; b) 返回Fasle</td></tr><tr><td align="center">&lt;</td><td align="left">小于 - 返回x是否小于y。所有比较运算符返回1表示真，返回0表示假。这分别与特殊的变量True和False等价</td><td align="left">(a &lt; b) 返回True</td></tr><tr><td align="center">&gt;=</td><td align="left">大于等于 - 返回x是否大于等于y</td><td align="left">(a &gt;= b) 返回Fasle</td></tr><tr><td align="center">&lt;=</td><td align="left">小于等于 - 返回x是否小于等于y</td><td align="left">(a &lt;= b) 返回True</td></tr></tbody></table><p><strong>比较运算符示例</strong></p><pre><code>print(3+2 &lt; 7+8)                        # 关系运算符优先级低于算术运算符print(3 &lt; 5 &gt;2)                            # 等价于3&lt;5 and 5&gt;2print(3 == 3 &lt; 5)                        # 等价于3==3 and 3&lt;5print(&#39;12345&#39; &gt; &#39;23456&#39;)                # 第一个字符&#39;1&#39;&lt;&#39;2&#39;，直接得出结论print(&#39;abcd&#39; &gt; &#39;Abcd&#39;)                    # 第一个字符&#39;a&#39;&gt;&#39;A&#39;，直接得出结论print([85,92,73,84] &lt; [91,82,73])        # 第一个数字85&lt;91，直接得出结论print([180,90,101] &gt; [180, 90, 99])        # 前两个数字相等，第三个数字101&gt;99print({1,2,3,4} &gt; {3,4,5})                # 第一个集合不是第二个集合的超集print({1,2,3,4} &lt;= {3,4,5})                # 第一个集合不是第二个集合的子集print([1,2,3,4] &gt; [1,2,3])                # 前三个元素相等，并且，第一个列表有多余的元素</code></pre><p><strong>结果：</strong><br>True<br>True<br>True<br>False<br>True<br>True<br>True<br>False<br>False<br>True</p><h4 id="3-3-赋值运算符"><a href="#3-3-赋值运算符" class="headerlink" title="3.3 赋值运算符"></a>3.3 赋值运算符</h4><p>以下假设变量a为10，变量b为20：</p><table><thead><tr><th align="center">运算符</th><th align="left">描述</th><th align="left">实例</th></tr></thead><tbody><tr><td align="center">=</td><td align="left">简单的赋值运算符</td><td align="left">c = a + b 将 a + b 的运算结果赋值为c</td></tr><tr><td align="center">+=</td><td align="left">加法赋值运算符</td><td align="left">c += a 等效于 c = c + a</td></tr><tr><td align="center">-=</td><td align="left">减法赋值运算符</td><td align="left">c -= a 等效于 c = c - a</td></tr><tr><td align="center">*=</td><td align="left">乘法赋值运算符</td><td align="left">c *= a 等效于 c = c * a</td></tr><tr><td align="center">/=</td><td align="left">除法赋值运算符</td><td align="left">c /= a 等效于 c = c / a</td></tr><tr><td align="center">%=</td><td align="left">取模赋值运算符</td><td align="left">c %= a 等效于 c = c % a</td></tr><tr><td align="center"><code>**=</code></td><td align="left">幂赋值运算符</td><td align="left">c <code>**=</code> a 等效于 c = c ** a</td></tr><tr><td align="center">//=</td><td align="left">取整除赋值运算符</td><td align="left">c //= a 等效于 c = c // a</td></tr></tbody></table><h4 id="3-4-逻辑运算符"><a href="#3-4-逻辑运算符" class="headerlink" title="3.4 逻辑运算符"></a>3.4 逻辑运算符</h4><p>以下假设变量a为10，变量b为20：</p><table><thead><tr><th align="center">运算符</th><th align="left">描述</th><th align="left">实例</th></tr></thead><tbody><tr><td align="center">and</td><td align="left">布尔“与” - 如果x为False，x and y 返回Fasle，否则它返回y的计算值</td><td align="left">(a and b) 返回20</td></tr><tr><td align="center">or</td><td align="left">布尔“或” - 如果x为True，它返回x的值，否则它返回y的计算值</td><td align="left">(a or b) 返回20</td></tr><tr><td align="center">not</td><td align="left">布尔“非” - 如果x为True，返回Fasle，如果x为False，它返回True</td><td align="left">not(a and b) 返回False</td></tr></tbody></table><p> <strong>逻辑运算符示例</strong></p><pre><code>print(3 in range(5) and &#39;abc&#39; in &#39;abcdefg&#39;)print(3-3 or 5-2)print(not 5)print(not [])</code></pre><p><strong>结果：</strong><br>True<br>3<br>False<br>True</p><h4 id="3-5-成员运算符"><a href="#3-5-成员运算符" class="headerlink" title="3.5 成员运算符"></a>3.5 成员运算符</h4><table><thead><tr><th align="center">运算符</th><th align="left">描述</th><th align="left">实例</th></tr></thead><tbody><tr><td align="center">in</td><td align="left">如果在指定的序列中找到值返回True，否则返回Fasle</td><td align="left">x在y序列中，如果x在y序列中返回True</td></tr><tr><td align="center">not in</td><td align="left">如果在指定的序列中没有找到值返回True，否则返回False</td><td align="left">x不在y序列中，如果x不在y序列中返回True</td></tr></tbody></table><p> <strong>成员运算符示例</strong></p><pre><code>print(60 in [70, 60, 50, 80] )print(&#39;abc&#39; in &#39;a1b2c3dfg&#39;)print([3] in [[3], [4], [5]])print(&#39;3&#39; in map(str, range(5)))print(5 in range(5))</code></pre><p><strong>结果：</strong><br>True<br>False<br>True<br>True<br>False</p><h4 id="3-6-身份运算符"><a href="#3-6-身份运算符" class="headerlink" title="3.6 身份运算符"></a>3.6 身份运算符</h4><table><thead><tr><th align="center">运算符</th><th align="left">描述</th><th align="left">实例</th></tr></thead><tbody><tr><td align="center">is</td><td align="left">is 是判断两个标识符是不是引用一个对象</td><td align="left">x is y， 类似id(x) == id(y)，如果引用的是同一个对象则返回True，否则返回False</td></tr><tr><td align="center">is not</td><td align="left">is not 是判断两个标识符是不是引用自不同对象</td><td align="left">x is not y，类似id(a)!=id(b)。如果引用的不是同一个对象则，返回结果True，否则返回False</td></tr></tbody></table><p><code>注意</code>：<code>id(x)</code>函数用于获取对象内存地址</p><p><code>特别的</code>：<code>is</code> 与 <code>==</code> 区别：<code>is</code>用于判断两个变量引用对象是否为同一个，<code>==</code>用于判断引用变量的值是否相等。</p><p><strong>身份运算符示例</strong></p><pre><code>a = 20b = 20 if (a is b):    print(&quot;1 : a 和 b 有相同的标识&quot;)else:    print(&quot;1 : a 和 b 没有相同的标识&quot;)print(id(a))print(id(b))# 修改变量b的值b = 30if(a is b):    print(&quot;3 : a 和 b 有相同的标识&quot;)else:    print(&quot;3 : a 和 b 没有相同的标识&quot;)print(id(a))print(id(b))# is 与 == 的区别a = [1, 2, 3]b = [1, 2, 3]print(b == a)print(b is a)print(id(a))print(id(b))</code></pre><p><strong>结果：</strong><br>1 : a 和 b 有相同的标识<br>140715597430912<br>140715597430912<br>3 : a 和 b 没有相同的标识<br>140715597430912<br>140715597431232<br>True<br>False<br>1500602520840<br>1500602814024</p><p><strong>运算符优先级示例</strong></p><pre><code>a = 20 b = 10c = 15d = 5e = 0 e = (a + b) * c / d         # (30 * 15) / 5print(&quot;(a + b) * c / d 运算结果为：&quot;, e)e = ((a + b) * c) / d         # (30 * 15) / 5print(&quot;((a + b) * c) / d 运算结果为：&quot;, e)e = (a + b) * (c / d)         # (30) * (15/5)print(&quot;(a + b) * (c / d) 运算结果为：&quot;, e)e = a + (b * c) / d         # 20 + (150/5)print(&quot;a + (b * c) / d  运算结果为：&quot;, e)</code></pre><p><strong>结果：</strong><br>(a + b) * c / d 运算结果为： 90.0<br>((a + b) * c) / d 运算结果为： 90.0<br>(a + b) * (c / d) 运算结果为： 90.0<br>a + (b * c) / d  运算结果为： 50.0</p><h3 id="4、列表"><a href="#4、列表" class="headerlink" title="4、列表"></a>4、列表</h3><h4 id="4-1-列表的基本概念"><a href="#4-1-列表的基本概念" class="headerlink" title="4.1  列表的基本概念"></a>4.1  列表的基本概念</h4><ol><li>列表是有序的元素集合，所有元素放在一对<code>中括号</code>中，用<code>逗号</code>隔开，没有长度限制。</li><li>列表索引值以<code>0</code>为开始值， <code>-1</code>为从末尾的开始位置。</li><li>列表可以使用<code>+</code>操作符进行拼接，使用<code>*</code>表示重复。</li><li>当列表元素增加或删除时，列表对象自动进行扩展或收缩內存，保证元素之间没有缝隙。</li></ol><p><strong>列表元素可以通过索引访问单个元素</strong><br>a=[4, 2, 3, 6, 6, 9, 5, 8, 1]<br>a[3]=6<br>a[0]=4<br>a9]=异常</p><p><strong>列表</strong></p><ol><li>列表可以完成大多数集合类的数据结构实现。</li><li>列表中元素的类型可以不相同，它支持数字，字符串甚至可以包含列表（所谓嵌套）。</li></ol><p><strong>列表元素修改</strong></p><ol><li>列表大小没有限制，可以随时修改。</li><li>列表元素变可随时修改。</li></ol><pre><code>a=[4, 2, 3, 6, 6, 9, 5, 8, 1]a.insert(0, 1)print(a)a[0]=10print(a)</code></pre><p><strong>结果：</strong><br>[1, 4, 2, 3, 6, 6, 9, 5, 8, 1]<br>[10, 4, 2, 3, 6, 6, 9, 5, 8, 1]</p><h4 id="4-2-列表的基本操作"><a href="#4-2-列表的基本操作" class="headerlink" title="4.2 列表的基本操作"></a>4.2 列表的基本操作</h4><table><thead><tr><th align="center">列表操作符</th><th align="center">操作符含义</th></tr></thead><tbody><tr><td align="center"><code>&lt;list1&gt;</code> + <code>&lt;list2&gt;</code></td><td align="center">连接两个列表</td></tr><tr><td align="center"><code>&lt;list&gt;</code> * <code>&lt;整数类型&gt;</code></td><td align="center">对列表进行整数次重复</td></tr><tr><td align="center"><code>&lt;list&gt;</code>[<code>&lt;整数类型&gt;</code>]</td><td align="center">索引列表中的元素</td></tr><tr><td align="center">len(<code>&lt;seq&gt;</code>)</td><td align="center">列表中元素个数</td></tr><tr><td align="center"><code>&lt;list&gt;</code>[<code>&lt;整数类型&gt;</code>:<code>&lt;整数类型&gt;</code>]</td><td align="center">取列表的一个子序列</td></tr><tr><td align="center">for <code>&lt;var&gt;</code> in <code>&lt;list&gt;</code></td><td align="center">对列表进行循环列举</td></tr><tr><td align="center"><code>&lt;expr&gt;</code> in <code>&lt;list&gt;</code></td><td align="center">成员检查，判断<code>&lt;expr&gt;</code>是否在列表中</td></tr></tbody></table><h4 id="4-3-列表相关方法"><a href="#4-3-列表相关方法" class="headerlink" title="4.3 列表相关方法"></a>4.3 列表相关方法</h4><table><thead><tr><th align="left">方法</th><th align="left">方法含义</th></tr></thead><tbody><tr><td align="left"><code>&lt;list&gt;</code>.append(x)</td><td align="left">将元素<code>x</code>增加到列表的最后</td></tr><tr><td align="left"><code>&lt;list&gt;</code>.sort()</td><td align="left">将列表元素排序</td></tr><tr><td align="left"><code>&lt;list&gt;</code>.reverse()</td><td align="left">将序列元素反转</td></tr><tr><td align="left"><code>&lt;list&gt;</code>.index(x)</td><td align="left">返回第一次出现元素<code>x</code>的索引值</td></tr><tr><td align="left"><code>&lt;list&gt;</code>.insert(i,x)</td><td align="left">在<code>i</code>位置处插入新元素<code>x</code></td></tr><tr><td align="left"><code>&lt;list&gt;</code>.count(x)</td><td align="left">返回元素<code>x</code>在列表中的数量</td></tr><tr><td align="left"><code>&lt;list&gt;</code>.remove(x)</td><td align="left">删除列表中第一次出现的元素<code>x</code></td></tr><tr><td align="left"><code>&lt;list&gt;</code>.pop(i)</td><td align="left">取出列表中位置<code>i</code>的元素，并删除它</td></tr></tbody></table><h4 id="4-4-创建列表"><a href="#4-4-创建列表" class="headerlink" title="4.4 创建列表"></a>4.4 创建列表</h4><pre><code>&gt;&gt;&gt; list((3,5,7,9,11))    # 将元组转换为列表[3,5,7,9,11]&gt;&gt;&gt; list(range(1, 10, 2)  # 将range对象转换为列表[1, 3, 5, 7, 9]&gt;&gt;&gt; list(&#39;hello world&#39;)   # 将字符串转换为列表，每个字符转换为列中的一个元素[&#39;h&#39;,&#39;e&#39;,&#39;l&#39;,&#39;l&#39;,&#39;o&#39;,&#39;w&#39;,&#39;o&#39;,&#39;r&#39;,&#39;l&#39;,&#39;d&#39;]&gt;&gt;&gt; list({3,7,5})  # 将集合转换为列表，集合中的元素是无序的[3, 5, 7]&gt;&gt;&gt; x = list()     # 创建空列表&gt;&gt;&gt; x = [1, 2, 3]&gt;&gt;&gt; del x          # 删除列表对象&gt;&gt;&gt; x              # 对象删除后无法再访问，抛出异常NameError: name &#39;x&#39; is not defined</code></pre><h4 id="4-5-利用索引访问列表"><a href="#4-5-利用索引访问列表" class="headerlink" title="4.5  利用索引访问列表"></a>4.5  利用索引访问列表</h4><pre><code>data = list(range(10))print(data)print(data[0])      # 第一个元素的下标为0print(data[1])      # 第二个元素的下标为1print(data[-1])     # -1表示最后一个元素的下标print(data[15])     # 15不是有效下标，代码抛出异常</code></pre><p><strong>结果：</strong></p><pre><code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]019Traceback (most recent call last):  File &quot;C:\Users\xz\.spyder-py3\temp.py&quot;, line 12, in &lt;module&gt;    print(data[15])     # 15不是有效下标，代码抛出异常IndexError: list index out of range</code></pre><h4 id="4-6-列表常用方法"><a href="#4-6-列表常用方法" class="headerlink" title="4.6  列表常用方法"></a>4.6  列表常用方法</h4><p><strong>append()、insert()、 extend()</strong></p><pre><code> list = [1,2,3,4] print(list) list. append(5) print(list) list.insert(0, 0) print(list) list.insert(2, 1.5) print(list) list.extend([6, 7]) print(list)</code></pre><p><strong>结果：</strong><br>[1, 2, 3, 4]<br>[1, 2, 3, 4, 5]<br>[0, 1, 2, 3, 4, 5]<br>[0, 1, 1.5, 2, 3, 4, 5]<br>[0, 1, 1.5, 2, 3, 4, 5, 6, 7]</p><p><strong>pop()、remove()</strong></p><pre><code>list = [1, 2, 3, 4, 5, 6]print(list.pop())                # 删除并返回最后一个元素print(list)                            print(list.pop(0))                # 删除并返回下标为0的元素，后面的元素向前移动print(list)print(list.pop(2))                # 删除并返回下标为2的元素，后面的元素向前移动print(list)list = [1, 2, 3,  2, 4, 2]        list.remove(2)                    # 删除第一个2， 该方法没有返回值print(list)</code></pre><p><strong>结果：</strong><br>6<br>[1, 2, 3, 4, 5]<br>1<br>[2, 3, 4, 5]<br>4<br>[2, 3, 5]<br>[1, 3, 2, 4, 2]</p><p><strong>sort()、reverse()</strong></p><pre><code>from random import sample# 在range(10000) 中任选10个不重复的随机数data = sample(range(10000), 10)print(data)data.reverse()            # 翻转，首尾交换，该方法没有返回值print(data)data.sort()                # 按元素大小进行排序，该方法没有返回值print(data)data.sort(key=str)        # 按所有元素转换为字符串后的大小进行排序print(data)</code></pre><p><strong>结果：</strong><br>[1680, 5901, 5733, 45, 8628, 5475, 3446, 4598, 555, 9934]<br>[9934, 555, 4598, 3446, 5475, 8628, 45, 5733, 5901, 1680]<br>[45, 555, 1680, 3446, 4598, 5475, 5733, 5901, 8628, 9934]<br>[1680, 3446, 45, 4598, 5475, 555, 5733, 5901, 8628, 9934]</p><p><strong>count()、index()</strong></p><pre><code>list = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]print(list.count(2))    # 输出2    print(list)print(list.index(4))    # 输出6print(list)print(list.index(5))    # 代码抛出异常，提示5 is not in list</code></pre><p><strong>结果：</strong></p><pre><code>2[1, 2, 2, 3, 3, 3, 4, 4, 4, 4]6[1, 2, 2, 3, 3, 3, 4, 4, 4, 4]---------------------------------------------------------------------ValueError                        Traceback (most recent call last)&lt;ipython-input-28-36d2725a5d78&gt; in &lt;module&gt;      4 print(list.index(4))    # 输出6      5 print(list)----&gt; 6 print(list.index(5))    # 代码抛出异常，提示5 is not in listValueError: 5 is not in list</code></pre><h4 id="4-7-列表推导式"><a href="#4-7-列表推导式" class="headerlink" title="4.7  列表推导式"></a>4.7  列表推导式</h4><pre><code>data = [2**i for i in range(64)]等价于data = []for i in range(64):    data.append(2**i)</code></pre><pre><code>data = [num for num in range(20) if num%2==1]等价于data = []for num in range(20):    if num%2 == 1:        data.append(num)</code></pre><h3 id="5、元组"><a href="#5、元组" class="headerlink" title="5、元组"></a>5、元组</h3><h4 id="5-1-基本概念"><a href="#5-1-基本概念" class="headerlink" title="5.1 基本概念"></a>5.1 基本概念</h4><ol><li><p>元组(tuple)是包含多个元素的类型，元素之间用逗号分割。<br>如：t1 = (123, 456, “hello”)</p></li><li><p>可以通过把若干元素放在一对圆括号中创建元组，如果只有一个元素的话则需要多加一个逗号，例如(3,)</p></li><li><p>也可以使用tuple()函数把列表、字典、集合、字符串以及range对象、map对象、zip对象或其他类似对象转换为元组。</p></li><li><p>元组可以是空的，t2=()</p></li><li><p>一个元组也可以作为另一个元组的元素，此时，作为元素的元组需要增加括号，从而避免歧义，如：t3 = (123, 456, (“hello”, “world”))  </p></li><li><p>元组中各元素存在先后关系，可以通过索引访问元组中的元素。<br>例如：t3[0]</p></li><li><p>元组定义后不能更改，也不能删除。<br>t3[0]  = 789</p></li><li><p>与字符串类型类似，可以通过索引区来访问元组中的部分元素。<br>例如：t3[1:]</p></li><li><p>与字符串一样，元组之间可以用<code>+</code>号和<code>*</code>号进行运算</p></li></ol><h4 id="5-2-元组与列表的区别"><a href="#5-2-元组与列表的区别" class="headerlink" title="5.2 元组与列表的区别"></a>5.2 元组与列表的区别</h4><ol><li><p>元组是<code>不可变</code>的，不能直接修改元组中元素的值，也不能为元组增加或删除元素。因此，元组没有提供 append()、 extend()和insert()等方法，也没有 remove()和pop()方法。</p></li><li><p>元组的访问速度比列表<code>更快，开销更小</code>。如果定义了一系列常量值，主要用途只是对它们进行遍历或其他类似操作，那么一般建议使用元组而不用列表。</p></li><li><p>元组可以使得<code>代码更加安全</code>。例如，调用函数时使用元组传递参数可以防止在函数中修改元组，而使用列表则无法保证这一点。</p></li><li><p>元组可用作字典的键，也可以作为集合的元素，但列表不可以，包含列表的元组也不可以。</p></li></ol><h4 id="5-3-生成器表达式"><a href="#5-3-生成器表达式" class="headerlink" title="5.3 生成器表达式"></a>5.3 生成器表达式</h4><pre><code>gen = (2**i for i in range(8))        # 创建生成器对象print(gen)                                print(list(gen))                    # 转换为列表，用完了生成器对象中的所有元素print(tuple(gen))                    # 转换为元组，得到空元组gen = (2**i for i in range(8))        # 重新创建生成器对象print(next(gen))                    # 使用next()函数访问下一个元素print(next(gen))for item in gen:                    # 使用for循环访问剩余的所有元素    print(item, end=&#39;  &#39;)</code></pre><pre><code>&lt;generator object &lt;genexpr&gt; at 0x0000015D62F32138&gt;(1, 2, 4, 8, 16, 32, 64, 128)()124  8  16  32  64  128  </code></pre><h3 id="6、切片操作"><a href="#6、切片操作" class="headerlink" title="6、切片操作"></a>6、切片操作</h3><p><strong>适用于列表和元组</strong></p><ol><li><p>切片是用来获取列表、元组、字符串等有序序列中部分元素的一种语法。在形式上，切片使用2个冒号分割的3个数字来完成。</p></li><li><p>[start:end:step]</p></li><li><p>其中第一个数字<code>start</code>表示切片开始位置，默认为0；第二个数字<code>end</code>表示切片截止（但不包含）位置（默认为列表长度）；第三个数字<code>step</code>表示切片的步长（默认为1），省略步长时还可以同时省略最后一个冒号。  </p></li><li><p>当step为<code>负整数</code>时，表示反向切片，这时start应该在end的右侧。</p></li></ol><pre><code>data = list(range(20))print(data[:])            # 获取所有元素的副本print(data[:3])            # 前三个元素print(data[3:])            # 下标3之后的所有元素print(data[::3])        # 每3个元素选取1个print(data[-3:])        # 最后3个元素print(data[:-5])        # 除最后5个元素之外的所有元素</code></pre><p><strong>结果：</strong><br>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]<br>[0, 1, 2]<br>[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]<br>[0, 3, 6, 9, 12, 15, 18]<br>[17, 18, 19]<br>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</p><h3 id="7、字典"><a href="#7、字典" class="headerlink" title="7、字典"></a>7、字典</h3><h4 id="7-1-基本概念"><a href="#7-1-基本概念" class="headerlink" title="7.1 基本概念"></a>7.1 基本概念</h4><ol><li>字典是<code>无序</code>的对象集合，使用键-值(key- value)存储，具有极快的查找速度。</li><li>键(key)必须使用不可变类型</li><li>同一个字典中，键(key)必须是唯一的</li><li>字典的每个键值key=&gt;value对用冒号:分割，每个键值对之间用逗号，分割，整个字典包括在花括号{}中，格式如下所示<br>dic = {key1: value1, key2: value2)</li></ol><pre><code>&gt;&gt;&gt; d={&#39;Michael&#39;:95, &#39;Bob&#39;:75, &#39;Tracy&#39;:85}&gt;&gt;&gt; d[&#39;Michael&#39;]95</code></pre><h4 id="7-2-常用方法"><a href="#7-2-常用方法" class="headerlink" title="7.2 常用方法"></a>7.2 常用方法</h4><table><thead><tr><th align="left">方法</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">keys()</td><td align="left">返回字典中键的列表</td></tr><tr><td align="left">values()</td><td align="left">返回字典中值的列表</td></tr><tr><td align="left">items()</td><td align="left">返回tuples的列表。每个tuple由字典的键和相应值组成</td></tr><tr><td align="left">clear()</td><td align="left">删除字典的所有条目</td></tr><tr><td align="left">copy()</td><td align="left">返回字典高层结构的一个拷贝，但不复制嵌入结构，而只复制对那些结构的引用</td></tr><tr><td align="left">update(x)</td><td align="left">用字典x中的键值对更新字典内容</td></tr><tr><td align="left">get(x[,y])</td><td align="left">返回键x，若未找到该键返回none，若提供y，则未找到x时返回y</td></tr></tbody></table><p>字典键<code>一般是唯一</code>的，如果<code>重复最后的一个键值</code>对会<code>替换前面的</code>，<code>值不需要唯一</code>。</p><pre><code>dict = {&#39;a&#39;:1, &#39;b&#39;:2, &#39;b&#39;:&#39;3&#39;}dict[&#39;b&#39;]dict</code></pre><p><strong>jupyter一行一行运行结果：</strong><br>‘3’<br>{‘a’: 1, ‘b’: ‘3’}</p><h4 id="7-3-创建字典：基础语法方式"><a href="#7-3-创建字典：基础语法方式" class="headerlink" title="7.3 创建字典：基础语法方式"></a>7.3 创建字典：基础语法方式</h4><ol><li>字典中值<code>可以取任何数据类型</code>，但<code>键必须是不可变</code>的，如<code>字符串</code>，<code>数字</code>或<code>元组</code>。如<br>dict={‘Alice’: ‘2341’, ‘Beth’: ‘9102’, ‘Cecil’: ‘3258’}<br></li><li>也可如此创建字典，如<br>dict1={‘abc’: 456}<br>dict2={‘abc’: 123, 98.6: 37}</li></ol><h4 id="7-4-创建字典：dict函数"><a href="#7-4-创建字典：dict函数" class="headerlink" title="7.4 创建字典：dict函数"></a>7.4 创建字典：dict函数</h4><ol><li>使用dict函数，通过其他映射(比如字典)或者(键,值)序列对创建字典</li></ol><pre><code>items = [(&#39;name&#39;, &#39;Gumby&#39;),(&#39;age&#39;, 42)]d= dict(items)print(d)</code></pre><p><strong>结果：</strong><br>{‘name’: ‘Gumby’, ‘age’: 42}</p><ol start="2"><li>dict函数也可以通过关键字参数来创建字典</li></ol><pre><code>d = dict(name=&#39;Gumby&#39;, age=42)print(d)</code></pre><p><strong>结果：</strong><br>{‘name’: ‘Gumby’, ‘age’: 42}</p><h4 id="7-5-字典元素访问"><a href="#7-5-字典元素访问" class="headerlink" title="7.5 字典元素访问"></a>7.5 字典元素访问</h4><pre><code>data = dict(name=&#39;张三&#39;, age=18, sex=&#39;M&#39;)    print(data[&#39;name&#39;])                            # 使用“键”作为下标，访问“值”print(data.get(&#39;age&#39;))                        print(data.get(&#39;address&#39;, &#39;不存在这个键&#39;))    # “键”不存在，返回默认值print(list(data))                            # 把所有的“键”转换为列表print(list(data.values()))                    # 把所有的“值”转换为列表print(list(data.items()))                    # 把所有的元素转换为列表for key,value in data.items():                # 遍历字典的“键:值”元素    print(key, value, sep=&#39;\t&#39;)</code></pre><p><strong>结果：</strong><br>张三<br>18<br>不存在这个键<br>[‘name’, ‘age’, ‘sex’]<br>[‘张三’, 18, ‘M’]<br>[(‘name’, ‘张三’), (‘age’, 18), (‘sex’, ‘M’)]<br>name    张三<br>age     18<br>sex     M</p><h4 id="7-6-字典元素修改、添加与删除"><a href="#7-6-字典元素修改、添加与删除" class="headerlink" title="7.6 字典元素修改、添加与删除"></a>7.6 字典元素修改、添加与删除</h4><ol><li>当以指定“键”为下标为字典元素赋值时，有两种含义：<br>1）若该“键”存在，表示修改该“键”对应的值。<br>2）若不存在，表示添加一个新元素。<pre><code>sock = {&#39;IP&#39;: &#39;127.0.0.1&#39;, &#39;port&#39;: 80}sock[&#39;port&#39;] = 8080                        # 修改已有元素的“值”sock[&#39;protocol&#39;] = &#39;TCP&#39;                # 增加新元素print(sock)</code></pre></li></ol><p><strong>结果：</strong><br>{‘IP’: ‘127.0.0.1’, ‘port’: 8080, ‘protocol’: ‘TCP’}</p><ol start="2"><li>使用字典对象的update()方法可以将另一个字典的元素一次性全部添加到当前字典对象，如果两个字典中存在相同的“键”，则以另一个字典中的“值”为准对当前字典进行更新。</li></ol><pre><code>sock = {&#39;IP&#39;: &#39;127.0.0.1&#39;, &#39;port&#39;: 80}# 更新了一个元素的“值”，增加了一个新元素sock.update({&#39;IP&#39;: &#39;192.168.9.62&#39;, &#39;protocol&#39;: &#39;TCP&#39;})print(sock)</code></pre><p><strong>结果：</strong><br>{‘IP’: ‘192.168.9.62’, ‘port’: 80, ‘protocol’: ‘TCP’}</p><ol start="3"><li>可以使用字典对象的pop()删除指定“键”对应的元素，同时返回对应的“值”。<br></li><li>popitem()方法用于删除字典的一个键对，并返回一个包含两个元素的元组，其中的两个元素分别是字典元素的“键”和“值”。<br></li><li>也可以使用del删除指定的“键”对应的元素。</li></ol><pre><code>sock = {&#39;IP&#39;:&#39;192.168.9.62&#39;,&#39;port&#39;:80,&#39;protocol&#39;:&#39;TCP&#39;}print(sock.pop(&#39;IP&#39;))                # 朋除并返回指定键的元素print(sock.popitem())                # 删除并返回一个元素del sock[&#39;port&#39;]                    # 朋除指定键的元素print(sock)</code></pre><p><strong>结果：</strong><br>192.168.9.62<br>(‘protocol’, ‘TCP’)<br>{}</p><h3 id="8、集合"><a href="#8、集合" class="headerlink" title="8、集合"></a>8、集合</h3><h4 id="8-1-概述"><a href="#8-1-概述" class="headerlink" title="8.1 概述"></a>8.1 概述</h4><ol><li>Python集合是<code>无序</code>、<code>可变</code>的容器对象，所有元素放在一对<code>大括号</code>中，元素之间使用逗号分隔，同一个集合内的每个<code>元素都是唯一</code>的，<code>不允许重复</code>。</li><li>集合中只能包含<code>数字、字符串、元组</code>等不可变类型的数据，而<code>不能包含列表、字典、集合</code>等可变类型的数据，包含列表等可变类型数据的元组也不能作为集合的元素。</li><li>集合中的元素是<code>无序</code>的，元素存储顺序和添加顺序并不一致。</li><li>集合不支持使用下标直接访问特定位置上的元素，也不支持使用 random中的 choice()函数从集合中随机选取元素，但支持使用 random模块中的 sample函数随机选取部分元素。</li></ol><h4 id="8-2-set（集合）"><a href="#8-2-set（集合）" class="headerlink" title="8.2 set（集合）"></a>8.2 set（集合）</h4><ol><li>set和dict类似，也是一组key的集合，但不存储value，由于key不能重复，所以，在set中，没有重复的key。<br></li><li>set是无序的，重复元素在set中自动被过滤。</li></ol><pre><code>s = set([1,2,3])ss = set([1,1,2,2,3,3])s</code></pre><p><strong>jupyter一行一行运行结果：</strong><br>{1, 2, 3}<br>{1, 2, 3}</p><p><code>注意；</code>set可以看成数学意义上的无序和无重复元素的集合，因此，两个set可以做数学意义上的交集(&amp;)、并集(|)、差集(-)等操作。</p><h4 id="8-3-集合常用方法"><a href="#8-3-集合常用方法" class="headerlink" title="8.3 集合常用方法"></a>8.3 集合常用方法</h4><ol><li>s = add(x)：将元素x添加到集合s中，如果元素已存在，则不进行任何操作。</li><li>s = update(x)：将x的元素添加到集合s中，x可以是列表、元组、字典等。</li></ol><pre><code>data = {30, 40, 50}data.add(20)            # 增加新元素20    print(data)data.add(50)            # 集合中已包含50，忽略本次操作print(data)data.update({40, 60})    # 忽略40，增加新元素60print(data)</code></pre><p><strong>结果：</strong><br>{40, 50, 20, 30}<br>{40, 50, 20, 30}<br>{50, 20, 40, 60, 30}</p><ol start="3"><li>s.pop(x)：随机删除集合中的一个元素。</li><li>s.remove(x)：将元素x从集合s中移除，如果元素不存在，则会发生错误。</li><li>s.discard(x)：将元素x从集合s中移除，如果元素不存在，<code>不会发生错误</code>。</li></ol><pre><code>data = {30, 40, 50}data.remove(30)            # 删除元素30print(data)data.discard(30)        # 集合中没有30，忽略本次操作print(data.pop())        # 删除并返回集合中的一个元素print(data)</code></pre><p><strong>结果：</strong><br>{40, 50}<br>40<br>{50}</p><h3 id="9、字符串"><a href="#9、字符串" class="headerlink" title="9、字符串"></a>9、字符串</h3><h4 id="9-1-概述"><a href="#9-1-概述" class="headerlink" title="9.1 概述"></a>9.1 概述</h4><ol><li>字符串(str)是用双引号””或者单引号’’括起来的一个或多个字符。</li><li>字符串可以保存在变量中，也可以单独存在。</li><li>字符串属于不可变对象，所有方法都是返回处理后的字符串或字节串，不对原字符串进行任何修改。</li><li>可以用type()函数测试一个字符串的类型。</li></ol><pre><code>In[1]:  type(&quot;1&quot;)Out[1]: strIn[2]:  type(1)Out[2]: int</code></pre><ol start="5"><li><p>字符串是一个字符序列：字符串最左端位置标记为0，依次增加。字符串中的编号叫做”索引”。</p><table><thead><tr><th align="center">H</th><th align="center">e</th><th align="center">l</th><th align="center">l</th><th align="center">o</th><th align="center"></th><th align="center">J</th><th align="center">a</th><th align="center">c</th><th align="center">k</th></tr></thead><tbody><tr><td align="center">0</td><td align="center">1</td><td align="center">2</td><td align="center">3</td><td align="center">4</td><td align="center">5</td><td align="center">6</td><td align="center">7</td><td align="center">8</td><td align="center">9</td></tr></tbody></table></li></ol><h4 id="9-2-转义"><a href="#9-2-转义" class="headerlink" title="9.2 转义"></a>9.2 转义</h4><ol><li>Python语言转义符：<code>\</code>，在字符串中表示转义，即该字符与后面相邻的一个字符共同组，成了新的含义。</li><li>输出带有引号的字符串，可以使用转义符。</li><li>使用<code>\\</code>输出带有转义符的字符串。<br>print(“&quot;你好&quot;“)<br>print(“<code>\\</code>“)</li><li>用转义符可以在字符串中表达一些不可直接打印的信息，例如：用<code>\n</code>表示换行;<code>\t</code>表示制表符。<br>print(“Hello\nWorld\n\nGoodbye\t32”)</li></ol><h4 id="9-3-字符串操作"><a href="#9-3-字符串操作" class="headerlink" title="9.3 字符串操作"></a>9.3 字符串操作</h4><ol><li>单个索引辅助访问字符串中的特定位置，格式为<code>&lt;string&gt;[&lt;索引&gt;]</code>。<br>str1 = “hello, world”<br>print(str1[1])</li><li>Python中字符串索引<code>从0开始</code>，一个长度为L的字符串最后一个字符的<code>L-1</code>。</li><li>Python同时允许使用负数从字符串右边末尾位置是向左边进行反向索引，<code>最右侧索引值是-1</code>。</li><li>可以通过两个索引值确定一个位置范围，返回这个范围的子串，字符串的切片。<br>格式：<code>&lt; string&gt;[&lt; start&gt;:&lt;end&gt;]</code></li><li>start和end都是整数型数值，这个子序列从索引 start开始直到索引end结束，但不包括end位置。</li><li>字符串之间可以通过<code>+或*</code>进行连接。<br>加法操作(+)将两个字符串<code>连接</code>成为一个新的字符串。<br>乘法操作()生成一个由其本身字符串<code>重复连接</code>而成的字符串。<br>x in s：如果x是s的子串，返回True，否则返回 False。<br>str[N:M]：切片，返回子串。</li><li>len()函数能返回一个字符串的长度。<br>str1 = “hello, world”<br>len(str1)</li><li>大多数数据类型都可以通过<code>str()</code>函数转换为字符串：如str(123)。</li><li>type函数测试一个字符串的类型。</li></ol><h4 id="9-4-字符串遍历操作"><a href="#9-4-字符串遍历操作" class="headerlink" title="9.4 字符串遍历操作"></a>9.4 字符串遍历操作</h4><p>可以通过for和in组成的循环来遍历字符串中每个字符</p><pre><code>for &lt;var&gt; in &lt;string&gt;:    操作</code></pre><pre><code>str1 = &quot;Hello, world&quot;for p in str1:    print(p)</code></pre><p><strong>结果：</strong><br>H<br>e<br>l<br>l<br>o<br>,</p><p>w<br>o<br>r<br>l<br>d</p><h4 id="9-5-字符串处理方法"><a href="#9-5-字符串处理方法" class="headerlink" title="9.5 字符串处理方法"></a>9.5 字符串处理方法</h4><table><thead><tr><th align="left">操作</th><th align="left">含义</th></tr></thead><tbody><tr><td align="left">+</td><td align="left">连接</td></tr><tr><td align="left">*</td><td align="left">重复</td></tr><tr><td align="left"><code>&lt;string&gt;</code>[]</td><td align="left">索引</td></tr><tr><td align="left"><code>&lt;string&gt;</code>[:]</td><td align="left">剪切</td></tr><tr><td align="left">len(<code>&lt;sting&gt;</code>)</td><td align="left">长度</td></tr><tr><td align="left"><code>&lt;sting&gt;</code>.upper()</td><td align="left">字符串中字母大写</td></tr><tr><td align="left"><code>&lt;sting&gt;</code>.lower()</td><td align="left">字符串中字母小写</td></tr><tr><td align="left"><code>&lt;sting&gt;</code>。strip()</td><td align="left">去两边空格及去指定字符串</td></tr><tr><td align="left"><code>&lt;sting&gt;</code>.split()</td><td align="left">按指定字符分割字符串为数组</td></tr><tr><td align="left"><code>&lt;sting&gt;</code>.join()</td><td align="left">连接两个字符串序列</td></tr><tr><td align="left"><code>&lt;sting&gt;</code>.find()</td><td align="left">搜索指定字符串</td></tr><tr><td align="left"><code>&lt;sting&gt;</code>.replace()</td><td align="left">字符串替换</td></tr><tr><td align="left">for <code>&lt;var&gt;</code> in <code>&lt;string&gt;</code></td><td align="left">字符串迭代</td></tr></tbody></table><h4 id="9-6-常用方法"><a href="#9-6-常用方法" class="headerlink" title="9.6 常用方法"></a>9.6 常用方法</h4><ol><li>Index(x)、rindex(0)：检测是否包含在字符串中，返回相应的索引值，如果不存在，返回异常。</li><li>count(x)：返回str在string里面出现的次数。</li></ol><pre><code>text = &#39;处处飞花飞处处，声声笑语笑声声。&#39;print(text.rindex(&#39;处&#39;))print(text.index(&#39;声&#39;))print(text.count(&#39;处&#39;))</code></pre><p><strong>结果：</strong><br>6<br>8<br>4</p><ol start="3"><li>replace(str1, str2, [,max])：把将字符串中的str1替换成str2，如果max指定，则替换不超过max次。</li></ol><pre><code>text = &quot;Python是一门非常棒的编程语言。&quot;# replace()方法返回替换后的新字符申，可以直接再次调用replace()方法print(text.replace(&#39;棒&#39;, &#39;优雅&#39;).replace(&#39;编程&#39;, &#39;程序设计&#39;))print(text)</code></pre><p><strong>结果：</strong><br>Python是一门非常优雅的程序设计语言。<br>Python是一门非常棒的编程语言。</p><ol start="4"><li>maketrans()：创建字符映射的转换表。</li><li>translate(str)：根据str给出的映射转换表转换string字符。</li></ol><pre><code>table = &#39;&#39;.maketrans(&#39;0123456789&#39;, &#39;零一二三四伍陆柒捌玖&#39;)print(&#39;Tel:62819743&#39;.translate(table))</code></pre><p><strong>结果：</strong><br>Tel:陆二捌一玖柒四三</p><ol start="6"><li>ljust(width[,fillchar])：返回一个原字符串左对齐，并使用fillchar填充至长度width的新字符串，fillchar默认为空格。rjust()、center()类似。</li></ol><pre><code>print(&#39;居左&#39;.ljust(20)+&#39;结束&#39;)print(&#39;居右&#39;.rjust(20, &#39;#&#39;))            # 左侧使用并号填充print(&#39;居中&#39;.center(20, &#39;=&#39;))        # 两侧使用等号填充</code></pre><p><strong>结果：</strong></p><pre><code>居左                  结束##################居右=========居中=========</code></pre><ol start="7"><li>split(str= “”, num=string.count(str))，其中num=string.count(str)以str为分隔符截取字符串，如果num有指定值，则仅截取num+1个子字符串。rsplit()类似，从右侧开始截取。</li><li>join(seq)：以指定字符串作为分隔符，将seq中所有的元素（的字符串表示）合并为一个新的字符串。</li></ol><pre><code>text = &#39;Beautiful is better than ugly.&#39;print(text.split())                        # 使用空白字符进行分隔print(text.split(maxsplit=1))            # 最多分隔一次print(text.rsplit(maxsplit=2))            # 最多分隔两侧print(&#39;1,2,3,4&#39;.split(&#39;,&#39;))                # 使用逗号作为分隔符print(&#39;,&#39;.join([&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;]))    # 使用逗号作为连接符print(&#39;:&#39;.join(map(str, range(1, 5))))    # 使用冒号作为连接符print(&#39;&#39;.join(map(str, range(1, 5))))    # 直接连接，不插入任何连接符</code></pre><p><strong>结果：</strong><br>[‘Beautiful’, ‘is’, ‘better’, ‘than’, ‘ugly.’]<br>[‘Beautiful’, ‘is better than ugly.’]<br>[‘Beautiful is better’, ‘than’, ‘ugly.’]<br>[‘1’, ‘2’, ‘3’, ‘4’]<br>1,2,3,4<br>1:2:3:4<br>1234</p><ol start="9"><li>lower()、upper()、capitalize()（句子首个单词大写）、title()（每个单词大写）、swapcase()（小写和大写相互转换）。</li></ol><pre><code>text = &#39;Simple is better than complex.&#39;print(text.lower())print(text.upper())print(text.capitalize())print(text.title())print(text.swapcase())</code></pre><p><strong>结果：</strong><br>simple is better than complex.<br>SIMPLE IS BETTER THAN COMPLEX.<br>Simple is better than complex.<br>Simple Is Better Than Complex.<br>sIMPLE IS BETTER THAN COMPLEX.</p><ol start="10"><li>startswith()、endswith()：检查字符串是否是以指定子字符串substr开头或结束，是则返回True。</li></ol><pre><code>text = &#39;Simple is better than complex.&#39;print(text.startswith(&#39;simple&#39;))print(text.startswith(&#39;Simple&#39;))print(text.endswith((&#39;.&#39;, &#39;!&#39;, &#39;?&#39;)))</code></pre><p><strong>结果：</strong><br>False<br>True<br>True</p><ol start="11"><li>strip()、rstrip()、lstrip()：截取字符串的指定字符。</li></ol><pre><code>text = &#39;   ======test===#####    &#39;print(text.strip())                # 删除两侧的空白字符print(text.strip(&#39;=# &#39;))        # 删除两侧的=、#和空格</code></pre><p><strong>结果：</strong></p><pre><code>======test===#####test</code></pre><h3 id="10、程序控制流"><a href="#10、程序控制流" class="headerlink" title="10、程序控制流"></a>10、程序控制流</h3><h4 id="10-1-程序基本结构"><a href="#10-1-程序基本结构" class="headerlink" title="10.1 程序基本结构"></a>10.1 程序基本结构</h4><ol><li>顺序结构是程序的基础，但单一的顺序结构不可能解决所有问题。</li><li>程序由三种基本结构组成：<br>顺序结构<br>分支结构<br>循环结构</li><li>任何算法(程序)都可以由这三种基本结构组合来实现。</li></ol><h5 id="10-1-1-顺序结构"><a href="#10-1-1-顺序结构" class="headerlink" title="10.1.1 顺序结构"></a>10.1.1 顺序结构</h5><p>程序按线性顺序依次执行</p><pre><code>   ↓&lt;语句块1&gt;   ↓&lt;语句块2&gt;</code></pre><h5 id="10-1-2-分支结构"><a href="#10-1-2-分支结构" class="headerlink" title="10.1.2 分支结构"></a>10.1.2 分支结构</h5><p>程序根据条件判断结果而选择不同执行路径<br>有单分支和二分支结构，二分支结构可组合成多分支结构</p><p><img src="/medias/%E5%88%86%E6%94%AF%E7%BB%93%E6%9E%84%E6%B5%81%E7%A8%8B%E5%9B%BE.PNG" alt="分支结构流程图"></p><h5 id="10-1-3-循环结构"><a href="#10-1-3-循环结构" class="headerlink" title="10.1.3 循环结构"></a>10.1.3 循环结构</h5><p>程序根据条件判断结果向后反复执行<br>根据循环体触发条件不同，包括条件循环和遍历循环结构</p><p><img src="/medias/%E5%BE%AA%E7%8E%AF%E7%BB%93%E6%9E%84%E6%B5%81%E7%A8%8B%E5%9B%BE.PNG" alt="循环结构流程图"></p><h4 id="10-2-分支结构（单分支、二分支、多分支）"><a href="#10-2-分支结构（单分支、二分支、多分支）" class="headerlink" title="10.2 分支结构（单分支、二分支、多分支）"></a>10.2 分支结构（单分支、二分支、多分支）</h4><h5 id="10-2-1-单分支结构：if语句"><a href="#10-2-1-单分支结构：if语句" class="headerlink" title="10.2.1 单分支结构：if语句"></a>10.2.1 单分支结构：if语句</h5><p>if 语句语法格式</p><pre><code>if &lt;条件&gt;:    语句块</code></pre><ol><li>语句块是 <code>if条件满足</code>后执行的一个或多个语句序列，语句块中语句通过与 if 所在行形成<code>缩进</code>表达包含关系。</li><li>if 语句首先评估&lt;条件&gt;的结果值，如果结果为True，则执行语句块里的语句序列，然后控制转向程序的下一条语句。</li><li>如果结果为 False，语句块里的语句会被跳过。</li></ol><p><strong>单分支结构示例</strong></p><p><strong>题目：</strong>生成包含两个或三个汉字的人名。</p><pre><code>from random import choice, randomname = choice(&#39;董孙李周赵钱王&#39;)condition = random()if condition&gt;0.5:            # random()函数返回[0,1)区间上的随机数    name += choice(&#39;付玉延邵子凯&#39;)name += choice(&#39;国楠栋涵雪玲瑞&#39;)print(condition)print(name)</code></pre><p><strong>结果：</strong><br>0.05645875733667094<br>钱国</p><h5 id="10-2-2-二分支结构：if-else语句"><a href="#10-2-2-二分支结构：if-else语句" class="headerlink" title="10.2.2 二分支结构：if-else语句"></a>10.2.2 二分支结构：if-else语句</h5><ol><li>if-else 语句语法格式</li></ol><pre><code>if &lt;条件&gt;:    &lt;语句块1&gt;else:    &lt;语句块2&gt;</code></pre><ol start="2"><li>&lt;语句块1&gt;是在 if 条件满足后执行的一个或多个语句序列</li><li>&lt;语句块2&gt;是if条件不满足后执行的语句序列</li></ol><p><strong>二分支结构示例</strong></p><p><strong>题目：</strong>PM2.5空气质量提醒。</p><pre><code>PM = eval(input(&quot;请输入PM2.5数值：&quot;))if PM &gt;= 75:    print(&quot;空气存在污染，请小心！&quot;)else:    print(&quot;空气没有污染，可以展开户外活动！&quot;)</code></pre><p><strong>结果：</strong><br>请输入PM2.5数值：98<br>空气存在污染，请小心！</p><h5 id="10-2-3-二分支结构简洁表达方式"><a href="#10-2-3-二分支结构简洁表达方式" class="headerlink" title="10.2.3 二分支结构简洁表达方式"></a>10.2.3 二分支结构简洁表达方式</h5><ol><li>适合通过判断返回特定值，语法格式：</li></ol><pre><code>&lt;表达式1&gt; if &lt;条件&gt; else &lt;表达式2&gt;</code></pre><pre><code>PM = eval(input(&quot;请输入PM2.5数值：&quot;))print(&quot;空气存在污染，请小心！&quot;) if PM &gt;= 75 else print(&quot;空气没有污染，可以展开户外活动！&quot;)</code></pre><p><strong>结果：</strong><br>请输入PM2.5数值：98<br>空气存在污染，请小心！</p><p>请输入PM2.5数值：50<br>空气没有污染，可以展开户外活动！</p><ol start="2"><li>这种紧凑结构适合对特殊值进行处理的情况</li></ol><h5 id="10-2-4-多分支结：if-elif-else语句"><a href="#10-2-4-多分支结：if-elif-else语句" class="headerlink" title="10.2.4 多分支结：if-elif-else语句"></a>10.2.4 多分支结：if-elif-else语句</h5><p><strong>if-elif-else语句语法格式</strong></p><pre><code>if &lt;条件1&gt;:    &lt;语句块1&gt;elif &lt;条件2&gt;:    &lt;语句块2&gt;...else:    &lt;语句块N&gt;</code></pre><ol><li>多分支结构是二分支结构的扩展。</li><li>通常用于设置同一判断条件的多条执行路径。</li><li>运行时依次评估寻找第一个结果为 True 的条件，执行该条件下的语句块，结束后跳过整个多分支结构，执行后面的语句。</li><li>如果没有任何条件成立，则执行 else 下的语句块。</li><li>else 语句是可选的，不一定要有。</li></ol><p><strong>多分支结构示例</strong><br><strong>题目：</strong>PM2.5空气质量（分级）提醒。</p><pre><code>PM = eval(input(&quot;请输入PM2.5数值：&quot;))if 0 &lt;= PM &lt; 35:    print(&quot;空气优质，快去户外运动！&quot;)elif 35 &lt;= PM &lt; 75:    print(&quot;空气良好，适度户外运动！&quot;)else:    print(&quot;空气污染，请小心！&quot;)</code></pre><hr><p>请输入PM2.5数值：24<br>空气优质，快去户外运动！</p><h4 id="10-3-循环结构（for循环、while循环）"><a href="#10-3-循环结构（for循环、while循环）" class="headerlink" title="10.3 循环结构（for循环、while循环）"></a>10.3 循环结构（for循环、while循环）</h4><h5 id="10-3-1-for-循环"><a href="#10-3-1-for-循环" class="headerlink" title="10.3.1 for 循环"></a>10.3.1 for 循环</h5><ol><li>Python 可以使用 for 语句循环遍历整个序列的值。</li><li>for 循环中，循环变量 var 遍历队列中每一个值，循环的语句体为每个 var 值执行一次。</li></ol><pre><code>for &lt;var&gt; in &lt;sequence&gt;:    &lt;body&gt;</code></pre><ol start="3"><li>for 循环在执行过程中，直接在序列上进行遍历，而非在内存中生成一个新的序列拷贝进行遍历。</li></ol><p><strong>for 循环结构示例</strong><br><strong>题目：</strong>输入N个数，求平均数。</p><pre><code>#average n = eval(input(&quot;How many numbers? &quot;))sum = 0.0for i in range(n):    x = eval(input(&quot;Enter a number&gt;&gt;&quot;))    sum += xprint(&quot;\nThe average is: &quot;, sum / n)</code></pre><p><strong>结果：</strong><br>How many numbers? 4<br>Enter a number&gt;&gt;12<br>Enter a number&gt;&gt;3<br>Enter a number&gt;&gt;4<br>Enter a number&gt;&gt;24<br>The average is:  10.75</p><ol start="4"><li>for 循环非常适合用来遍历容器类对象(列表、元组、字典、集合、字符串以及map、zip等类似对象)中的元素，语法形式为：</li></ol><pre><code>for 循环遍历 in 容器类对象:    循环体else:    [else语句代码块]</code></pre><ol start="5"><li>range() 函数可创建一个整数列表，用 for 语句进行循环</li></ol><pre><code>for i in range (10)</code></pre><ol start="6"><li><strong>for 循环缺点</strong><br>1）程序开始时必须提供输入数字总数。<br>2）大规模数字求平均值需要用户先数清楚个数。<br>3）for 循环是需要提供固定循环次数的循环方式。</li></ol><h5 id="10-3-2-while-循环语法"><a href="#10-3-2-while-循环语法" class="headerlink" title="10.3.2  while  循环语法"></a>10.3.2  while  循环语法</h5><pre><code>while 条件表达式:    循环体else:    [else 语句代码块]</code></pre><p><img src="/medias/while%E5%BE%AA%E7%8E%AF%E7%BB%93%E6%9E%84%E6%B5%81%E7%A8%8B%E5%9B%BE.PNG" alt="while循环结构流程图"></p><p><code>while 循环</code>，只要条件满足，就不断循环，条件不满足时退出循环。<br>在while … else 在条件语句为 false 时执行 else 的语句块</p><pre><code>sum = 0n =99while n &gt; 0:    sum = sum +n    n = n-2print(sum)</code></pre><p><strong>结果：</strong><br>2500</p><pre><code>count = 0while count &lt; 3:    print(count, &quot;小于 3&quot;)    count = count + 1else:    print(count, &quot;大于或等于 3&quot;)</code></pre><p><strong>结果：</strong><br>0 小于 3<br>1 小于 3<br>2 小于 3<br>3 大于或等于 3</p><p><strong>while循环结构示例</strong></p><pre><code># 比较复杂一点的有嵌套的while语句a = 1while a &lt; 8:    if a &lt;= 4:        print(a)    else:        print(&quot;hello&quot;)    a = a + 1else:    print(&quot;test&quot;)</code></pre><p><strong>结果：</strong><br>1<br>2<br>3<br>4<br>hello<br>hello<br>hello<br>test</p><h5 id="10-3-3-循环保留字（break、continue）"><a href="#10-3-3-循环保留字（break、continue）" class="headerlink" title="10.3.3 循环保留字（break、continue）"></a>10.3.3 循环保留字（break、continue）</h5><ol><li><strong>循环保留字：break</strong><br>1）break用来跳出最内层 for 或 while 循环，脱离该循环后程序从循环后代码继续执行。<br>2）break语句跳出了最内层 while 循环，但仍然继续执行外层循环。每个 break语句只有能<br>力跳出当前层次循环。<br></li><li><strong>循环保留字：continue</strong><br>1）continue 结束当前当次循环，即跳出循环体中下面尚未执行的语句，但不跳出当前循环。<br>2）对于 while 循环，继续求解循环条件。而对于 for 循环，程序流程接着遍历循环列表。<br>3）区别：<code>continue 语句只结束本次循环，而不终止整个循环的执行。而 break 语句则是结束整个循环过程，不再判断执行循环的条件是否成立</code>。</li></ol><p><strong>for循环结构break保留字示例</strong></p><pre><code>for letter in &#39;Runoob&#39;:  # 第一个实例    if letter == &#39;o&#39;:        break    print(&quot;当前字母为：&quot;,  letter)var = 10                 # 第二个实例while var &gt; 0:    print(&quot;当前变量值为：&quot;, var)    var = var - 1    if var == 7:        breakprint(&quot;Good bye!&quot;)  </code></pre><p><strong>结果：</strong><br>当前字母为： R<br>当前字母为： u<br>当前字母为： n<br>当前变量值为： 10<br>当前变量值为： 9<br>当前变量值为： 8<br>Good bye!</p><p><strong>for循环结构continue保留字示例</strong><br><strong>题目：</strong>编写程序，输出各位数字都不相同的所有三位数。</p><pre><code>digits = range(10)for i in digits:                # 选择第一个数字    if i == 0:        continue                # 第一位数不能是0    for j in digits:        if j == i:            continue            # 第二位与第一位相同，则忽略后面的操作        for k in digits:            if k in (i,j):                continue        # 三位数字必须互不相同            print(int(str(i)+str(j)+str(k)), end=&#39; &#39;)</code></pre><p><strong>结果：</strong><br>102 103 104 105 106 107 108 109 120 123 124 125 126 127 128 129 130 132 134 135 136 137 138 139 140 142 143 145 146 147 148 149 150 152 153 154 156 157 158 159 160 162 163 164 165 167 168 169 170 172 173 174 175 176 178 179 180 182 183 184 185 186 187 189 190 192 193 194 195 196 197 198 201 203 204 205 206 207 208 209 210 213 214 215 216 217 218 219 230 231 234 235 236 237 238 239 240 241 243 245 246 247 248 249 250 251 253 254 256 257 258 259 260 261 263 264 265 267 268 269 270 271 273 274 275 276 278 279 280 281 283 284 285 286 287 289 290 291 293 294 295 296 297 298 301 302 304 305 306 307 308 309 310 312 314 315 316 317 318 319 320 321 324 325 326 327 328 329 340 341 342 345 346 347 348 349 350 351 352 354 356 357 358 359 360 361 362 364 365 367 368 369 370 371 372 374 375 376 378 379 380 381 382 384 385 386 387 389 390 391 392 394 395 396 397 398 401 402 403 405 406 407 408 409 410 412 413 415 416 417 418 419 420 421 423 425 426 427 428 429 430 431 432 435 436 437 438 439 450 451 452 453 456 457 458 459 460 461 462 463 465 467 468 469 470 471 472 473 475 476 478 479 480 481 482 483 485 486 487 489 490 491 492 493 495 496 497 498 501 502 503 504 506 507 508 509 510 512 513 514 516 517 518 519 520 521 523 524 526 527 528 529 530 531 532 534 536 537 538 539 540 541 542 543 546 547 548 549 560 561 562 563 564 567 568 569 570 571 572 573 574 576 578 579 580 581 582 583 584 586 587 589 590 591 592 593 594 596 597 598 601 602 603 604 605 607 608 609 610 612 613 614 615 617 618 619 620 621 623 624 625 627 628 629 630 631 632 634 635 637 638 639 640 641 642 643 645 647 648 649 650 651 652 653 654 657 658 659 670 671 672 673 674 675 678 679 680 681 682 683 684 685 687 689 690 691 692 693 694 695 697 698 701 702 703 704 705 706 708 709 710 712 713 714 715 716 718 719 720 721 723 724 725 726 728 729 730 731 732 734 735 736 738 739 740 741 742 743 745 746 748 749 750 751 752 753 754 756 758 759 760 761 762 763 764 765 768 769 780 781 782 783 784 785 786 789 790 791 792 793 794 795 796 798 801 802 803 804 805 806 807 809 810 812 813 814 815 816 817 819 820 821 823 824 825 826 827 829 830 831 832 834 835 836 837 839 840 841 842 843 845 846 847 849 850 851 852 853 854 856 857 859 860 861 862 863 864 865 867 869 870 871 872 873 874 875 876 879 890 891 892 893 894 895 896 897 901 902 903 904 905 906 907 908 910 912 913 914 915 916 917 918 920 921 923 924 925 926 927 928 930 931 932 934 935 936 937 938 940 941 942 943 945 946 947 948 950 951 952 953 954 956 957 958 960 961 962 963 964 965 967 968 970 971 972 973 974 975 976 978 980 981 982 983 984 985 986 987 </p><h3 id="11、函数"><a href="#11、函数" class="headerlink" title="11、函数"></a>11、函数</h3><h4 id="11-1-基本概念"><a href="#11-1-基本概念" class="headerlink" title="11.1 基本概念"></a>11.1 基本概念</h4><ol><li>函数是一段具有特定功能的、<code>可重用</code>的语句组，通过函数名进行功能调用。</li><li>函数也可以看作是一段具有名字的<code>子程序</code>，可以在需要的地方调用执行，不需要在每个执行地方重复编写这些语句。</li><li>每次使用函数可以提供不同的参数作为输入，以实现对不同数据的处理；函数执行后，还可以反馈相应的处理结果。</li><li>函数是一个功能抽象：完成特定功能，与黑盒类似，对函数的使用不需要了解函数内部实现原理，只要了解函数的输入输出方式。</li><li>分类：<br>1）用户定义函数：用户自己编写的子程序<br>2）系统自带函数及第三方函数：Python内嵌的函数（如abs()、map()）、Python标准库中的函数（如 math 库中的 sqrt() ）等</li><li>使用函数的目的<br>1）降低编程难度<br>2）代码复用</li></ol><h4 id="11-2-函数定义"><a href="#11-2-函数定义" class="headerlink" title="11.2 函数定义"></a>11.2 函数定义</h4><ol><li>python 定义一个函数使用 def 保留字，语法形式如下：</li></ol><pre><code>def &lt;函数名&gt;(&lt;参数列表&gt;):    &lt;函数体&gt;    return &lt;返回值列表&gt;</code></pre><ol start="2"><li><p>函数名<code>&lt;name&gt;</code>：可以是任何有效的Python标识符。</p></li><li><p>参数列表<code>&lt;parameters&gt;</code>：是调用函数时传递给它的值（可以由零个，一个或者多个参数组成），当有多个参数时，各个参数用逗号分隔。</p></li><li><p>函数体<code>&lt;body&gt;</code>：函数被调用时执行的代码，由一个或多个语句组成</p></li><li><p>函数调用的一般形式：</p><pre><code>&lt;name&gt;(parameters) </code></pre></li></ol><h4 id="11-3-函数定义Tips"><a href="#11-3-函数定义Tips" class="headerlink" title="11.3 函数定义Tips"></a>11.3 函数定义Tips</h4><ol><li>不需要说明形参类型，Python 解释器会根据实参的值<code>自动推断</code>形参类型。</li><li>不需要指定函数返回值类型，这由函数中 return 语句返回的值的类型来确定。如果函数没有明确的返回值，Python 认为返回空值 None。</li><li>即使该函数不需要接受任何参数，也必须保留一对空的英文半角圆括号。</li><li>函数头部括号后面的<code>冒号</code>必不可少。    </li><li>函数体相对于 def 关键字必须保持一定的<code>空格缩进</code>。</li></ol><p><strong>函数定义示例1</strong></p><ol><li>定义函数</li></ol><pre><code>def add1(x):    x = x + 1    return 1</code></pre><ol start="2"><li>函数功能：将传给它的数值增1，返回增加后的值 return 语句：结束函数调用，并将结果返回给调用者。</li><li>return 语句是可选的，可出现在函数体任意位置没有 return 语句，函数在函数体结束位置将控制权返回给调用方。</li></ol><p><strong>函数定义示例2</strong></p><ol><li>编写一个程序打印“Happy Birthday”的歌词。<br>标准的歌词：<pre><code>Happy Birthday to you!Happy Birthday to you!Happy Birthday to you, dear &lt;insert-name&gt;Happy Birthday to you!</code></pre></li></ol><p><strong>方法一：使用4个 print 语句</strong></p><pre><code>print(&quot;Happy Birthday to you!&quot;)print(&quot;Happy Birthday to you!&quot;)print(&quot;Happy Birthday to you, dear Jack!&quot;)print(&quot;Happy Birthday to you!&quot;)</code></pre><p><strong>方法二：使用函数来打印歌词的第一、二、四行</strong></p><pre><code>def happy():    print(&quot;Happy Birthday to you!&quot;)happy()happy()print(&quot;Happy Birthday to you, dear Jack!&quot;)happy()</code></pre><p><strong>写出给 Jack 和 Tom 唱生日歌的程序</strong></p><p><strong>方法三：函数嵌套调用、分别定义singJack()和singTom()函数</strong></p><pre><code>def happy():    print(&quot;Happy Birthday to you!&quot;)def singJack():    happy()    happy()    print(&quot;Happy Birthday to you, dear Jack!&quot;)    happy()def singTom():    happy()    happy()    print(&quot;Happy Birthday to you, dear Tom!&quot;)    happy()singJack()print()singTom()</code></pre><p><strong>方法四：简化程序，编写通用函数sing()函数唱生日歌</strong></p><pre><code>def happy():    print(&quot;Happy Birthday to you!&quot;)def sing(person):    happy()    happy()    print(&quot;Happy Birthday to you,&quot;, person + &quot;!&quot;)    happy()sing(&quot;Jack&quot;)print()sing(&quot;Tom&quot;)</code></pre><p><strong>结果：</strong><br>Happy Birthday to you!<br>Happy Birthday to you!<br>Happy Birthday to you, Jack!<br>Happy Birthday to you!</p><p>Happy Birthday to you!<br>Happy Birthday to you!<br>Happy Birthday to you, Tom!<br>Happy Birthday to you!</p><h4 id="11-4-函数调用"><a href="#11-4-函数调用" class="headerlink" title="11.4 函数调用"></a>11.4 函数调用</h4><ol><li>函数调用执行的四个步骤：<br>1）调用程序在调用处暂停执行。<br>2）函数的形参在调用时被赋值为实参。<br>3）执行函数体。<br>4）函数被调用结束，给出返回值。</li></ol><pre><code>def happy():    print(&quot;Happy Birthday to you!&quot;)def sing(person):    happy()    happy()    print(&quot;Happy Birthday to you,&quot;, person + &quot;!&quot;)    happy()sing(&quot;Jack&quot;)print()sing(&quot;Tom&quot;)</code></pre><h4 id="11-5-函数参数传递"><a href="#11-5-函数参数传递" class="headerlink" title="11.5 函数参数传递"></a>11.5 函数参数传递</h4><ol><li>可选参数和可变数量参数。</li><li>在定义函数时，有些参数可以存在默认值。</li><li>在函数定义时，可以设计可变数量参数，通过参数前增加星号（*）实现。</li></ol><pre><code>def dup(str, times = 2):    print(str * times)dup(&quot;knock~&quot;)dup(&quot;knock~&quot;, 4)</code></pre><p><strong>结果：</strong><br>knock<del>knock</del><br>knock<del>knock</del>knock<del>knock</del></p><h4 id="11-6-函数返回值"><a href="#11-6-函数返回值" class="headerlink" title="11.6 函数返回值"></a>11.6 函数返回值</h4><ol><li>return 语句：程序退出该函数，并返回到函数被调用的地方。</li><li>return 语句返回的值传递给调用程序。</li><li>Python 函数的返回值有两种形式：<br>1）没有返回值：无返回值的 return 语句等价于 return None<br>2）返回一个或多个值</li><li>返回值可以是一个变量，也可以是一个表达式</li></ol><pre><code>def square(x):    y = x*x    return ydef square(x):    return x*x</code></pre><h4 id="11-7-lambda函数"><a href="#11-7-lambda函数" class="headerlink" title="11.7 lambda函数"></a>11.7 lambda函数</h4><ol><li>lambda 保留字用于定义一种特殊的函数——匿名函数，又称 lambda函数</li><li>匿名函数并非没有名字，而是将函数名作为函数结果返回，如下：<br>&lt;函数名&gt; = lambda &lt;参数列表&gt; : &lt;表达式&gt;</li><li>lambda 函数与正常函数一样，等价于下面形式：</li></ol><pre><code>def &lt;函数名&gt;(&lt;参数列表&gt;):    return &lt;表达式&gt;</code></pre><ol start="4"><li>简单说，lambda 函数用于定义简单的、能够在一行内表示的函数，返回一个函数类型。</li></ol><p>f  = lambda x,y : x+y</p><p><strong>lambda函数示例</strong></p><ol><li><p>filter 操作 python 语法实现<br>print [x for x in foo if x % 3 == 0]</p></li><li><p>map 操作 python 语法实现<br>print [x * 2 + 10 for x in foo]</p></li></ol><pre><code>foo = [2, 18, 9, 22, 17, 24, 8, 12, 27]print (list(filter(lambda x: x % 3 == 0, foo)))print (list(map(lambda x: x * 2 + 10, foo)))</code></pre><p><strong>结果：</strong><br>[18, 9, 24, 12, 27]<br>[14, 46, 28, 54, 44, 58, 26, 34, 64]</p><h4 id="11-8-lambda函数特点"><a href="#11-8-lambda函数特点" class="headerlink" title="11.8 lambda函数特点"></a>11.8 lambda函数特点</h4><ol><li>lambda 的使用大量简化了代码，使代码简练清晰。但是值得注意的是，这会在一定程度上降低代码的可读性。</li><li>lambda 定义了一个匿名函数。</li><li>lambda 并不会带来程序运行效率的提高，只会使代码更简洁。</li><li>如果可以使用 for .. in … if 来完成的，坚决不用 lambda。</li><li>如果使用 lambda，lambda 内不要包含循环，如果有，建议定义函数来完成，使代码获得可重用性和更好的可读性。</li><li>总结：lambda 是为了减少单行函数的定义而存在的。</li></ol><h4 id="11-9-变量作用域"><a href="#11-9-变量作用域" class="headerlink" title="11.9 变量作用域"></a>11.9 变量作用域</h4><ol><li>一个程序中的变量包括两类：<code>全局变量</code>和<code>局部变量</code>。</li><li>全局变量指在<code>函数之外</code>定义的变量，一般没有缩进，在程序执行<code>全过程</code>有效。</li><li>局部变量指在<code>函数内部使用</code>的变量，仅在函数内部有效，当函数退出时变量将不存在。</li></ol><pre><code>n = 1def func(a, b):    global n    n = b    c = a*b    return cs = func(&quot;knock~&quot;, 4)print(s, n)</code></pre><p><strong>结果：</strong><br>knock<del>knock</del>knock<del>knock</del> 4</p><h4 id="11-10-递归"><a href="#11-10-递归" class="headerlink" title="11.10 递归"></a>11.10 递归</h4><ol><li>递归定义：函数定义中使用函数自身的方法。</li><li>递归在数学和计算机应用中非常强大，能够非常简洁的解决很多问题。</li><li>经典递归例子：阶乘。</li></ol><p>n! = n(n-1)(n-2)(n-3)…(1)<br>5! = 5(4)(3)(2)(1)<br>n! = n(n-1)!</p><ol start="4"><li>阶乘的递归定义函数：</li></ol><pre><code>def fact(n):    if n == 0:        return 1    else:        return n* fact(n-1)print(fact(5))</code></pre><p><strong>结果：</strong><br>120</p><h4 id="11-11-递归示例：字符串反转"><a href="#11-11-递归示例：字符串反转" class="headerlink" title="11.11 递归示例：字符串反转"></a>11.11 递归示例：字符串反转</h4><p><strong>方法一：字符串转换为字符列表，反转列表，列表转换回字符串。</strong></p><pre><code>def reverseStr(s):    ls = list(s)    ls.reverse()    s = &quot;&quot;.join(ls)    print(s)str = &quot;sdfadfw8r873r323230&quot;reverseStr(str)</code></pre><p><strong>结果：</strong><br>032323r378r8wfdafds</p><p><strong>方法二：递归</strong></p><ol><li>将字符串分割成首字符和剩余子字符串</li><li>反转了剩余部分后把首字符放到末尾，整个字符串反转就完成了</li></ol><pre><code>def reverseRecu(s):    if len(s) &lt;= 0:        return &quot;&quot;    else:        return reverseRecu(s[1:]) + s[0]str = &quot;sdfadfw8r873r323230&quot;print(reverseRecu(str))</code></pre><p><strong>结果：</strong><br>032323r378r8wfdafds</p><h3 id="12、类与对象"><a href="#12、类与对象" class="headerlink" title="12、类与对象"></a>12、类与对象</h3><h4 id="12-1-面向过程与面向对象"><a href="#12-1-面向过程与面向对象" class="headerlink" title="12.1 面向过程与面向对象"></a>12.1 面向过程与面向对象</h4><ol><li><p>程序包括<br>数据：数据类型、数据结构<br>处理过程：算法</p></li><li><p>两种程序设计思想<br>1）面向过程：以操作为中心、面向过程把函数继续切分为子函数，即把大块函数通过切割成小块函数来降低系统的复杂度（C语言）</p></li></ol><p>2）面向对象：以数据为中心、把计算机程序视为一组对象的集合，而每个对象都可以接收其他对象发过来的消息，并处理这些消息，计算机程序的执行就是一系列消息在各个对象之间传递(Python)</p><h4 id="12-2-面向过程与面向对象"><a href="#12-2-面向过程与面向对象" class="headerlink" title="12.2 面向过程与面向对象"></a>12.2 面向过程与面向对象</h4><ol><li><p>Python 虽然是解释型语言，但从设计之初就已经是一门面向对象的语言，对于 Python 来说<code>一切皆为对象</code>。</p></li><li><p>Python 中对象的概念很广泛，Python 中的一切内容都可以称为对象，而不一定必须是某个类的实例。例如，字符串、列表、字典、元组等内置数据类型都具有和类完全相似的语法和用法。</p></li></ol><h4 id="12-3-对象例子"><a href="#12-3-对象例子" class="headerlink" title="12.3 对象例子"></a>12.3 对象例子</h4><ol><li><p>人<br>数据：姓名，出生日期，身高，体重，…<br>操作：计算年龄，判断体重是否标准，…</p></li><li><p>电视机<br>数据：型号，厂商，尺寸，频道数，…<br>操作：开机，关机，调频道，调音量，…</p></li><li><p>室内环境<br>数据：温度，湿度，容积，…<br>操作：调节温度，调节湿度，换算容积单位</p></li></ol><h4 id="12-4-类概述"><a href="#12-4-类概述" class="headerlink" title="12.4 类概述"></a>12.4 类概述</h4><ol><li>类是类型概念的发展<br>1）对象是广义的“数据值”。<br>2）对象所属的数据类型就是“类”。<br>3）用于描述复杂数据的静态和动态行为。</li><li>类（class）：描述相似对象的共性。包括<br>1）数据<br>2）操作：方法（method）</li><li>对象是类的实例（instance）</li></ol><h4 id="12-5-类与对象"><a href="#12-5-类与对象" class="headerlink" title="12.5 类与对象"></a>12.5 类与对象</h4><ol><li>类是对现实事物的抽象<br>1）数据抽象，例如：从具体学生抽象出姓名，年龄，地址等数据。<br>2）行为抽象，例如：从学生日常行为抽象出选课，加入社团等操作。<br>3）于是产生了类 Student 的定义。</li><li>抽象可以在多个层次上进行<br>例如：学生-人-动物-生物</li></ol><h4 id="12-6-类封装的好处"><a href="#12-6-类封装的好处" class="headerlink" title="12.6 类封装的好处"></a>12.6 类封装的好处</h4><ol><li>安全：对象自己的方法处理自己的数据。</li><li>易用：使用者无需了解内部实现细节。</li><li>易维护：实现者修改内部实现不会影响使用者。</li><li>标准化：同类甚至不同类的对象对使用者都呈现同样的操作界面。</li></ol><h4 id="12-7-类定义"><a href="#12-7-类定义" class="headerlink" title="12.7 类定义"></a>12.7 类定义</h4><p>实例化后，可以使用其属性。</p><pre><code>class &lt;类名&gt;:    &lt;方法定义&gt;</code></pre><h4 id="12-8-类对象"><a href="#12-8-类对象" class="headerlink" title="12.8 类对象"></a>12.8 类对象</h4><ol><li>类对象支持两种操作：属性引用和实例化。</li><li>属性引用使用和 Python 中所有的属性引用一样的标准语法：obj.name。</li><li>类对象创建后，类命名空间中所有的命名都是有效属性名。</li></ol><h4 id="12-9-Person类"><a href="#12-9-Person类" class="headerlink" title="12.9 Person类"></a>12.9 Person类</h4><p>Person类定义</p><p><code>这是一个学校 Python 定义的一个 Person类</code></p><pre><code>class Person:    # 下面定义了一个类变量    hair = &#39;black&#39;    def __init__(self, name=&#39;Charlie&#39;, age=8):        # 下面为 Person 对象增加2个实例变量        self.name = name        self.age = age    # 下面定义了一个say方法    def say(self, content):        print(content)</code></pre><p><strong>注释：</strong></p><ol><li>类有一个名为<code>__init__()</code>的特殊方法（构造方法），该方法在类实例化时会自动调用</li><li>类的方法与普通的函数只有一个特别 的区别—-他们必须有一个额外的第一个参数名称，按照惯例它的名称是self</li></ol><h4 id="12-10-Person类实例化与使用"><a href="#12-10-Person类实例化与使用" class="headerlink" title="12.10 Person类实例化与使用"></a>12.10 Person类实例化与使用</h4><pre><code># 调用Person类的构造方法，返回一个Person对象# 将该Person对象赋给p对象p = Person()# 输出p的name、age实例变量print(p.name, p.age)  # Charlie8# 访问p的name实例变量，直接为该实例变量赋值p.name = &#39;李刚&#39;# 调用p的say()方法，声明say()方法时定义了2个形参# 但第一个形参（self）是自动绑定的，因此调用该方法只需为第二个形参指定一个值p.say(&#39;Python语言很简单，学习很容易！&#39;)# 再次输出p的name、age实例变量print(p.name, p.age) # 李刚 8</code></pre><p><strong>结果：</strong><br>Charlie 8<br>Python语言很简单，学习很容易！<br>李刚 8</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一章 Python概述与开发环境安装</title>
      <link href="/2020/04/30/di-yi-zhang-python-gai-shu-yu-kai-fa-huan-jing-an-zhuang/"/>
      <url>/2020/04/30/di-yi-zhang-python-gai-shu-yu-kai-fa-huan-jing-an-zhuang/</url>
      
        <content type="html"><![CDATA[<h2 id="第一章-Python概述与开发环境安装"><a href="#第一章-Python概述与开发环境安装" class="headerlink" title="第一章 Python概述与开发环境安装"></a>第一章 Python概述与开发环境安装</h2><h3 id="1、Python开发环境安装"><a href="#1、Python开发环境安装" class="headerlink" title="1、Python开发环境安装"></a>1、Python开发环境安装</h3><h3 id="2、Anaconda安装"><a href="#2、Anaconda安装" class="headerlink" title="2、Anaconda安装"></a>2、Anaconda安装</h3><p>选择 just me<br>不用勾选添加本地环境变量</p><ol><li>查看Anaconda环境是否安装成功（查看Anaconda版本号）：conda –version</li><li>查看目前安装了哪些环境变量：conda info –envs</li><li>查看 Anaconda 当前版本以及安装了哪些包：conda list</li></ol><h3 id="3、Spyder"><a href="#3、Spyder" class="headerlink" title="3、Spyder"></a>3、Spyder</h3><h4 id="3-1-读取文件里面的行数"><a href="#3-1-读取文件里面的行数" class="headerlink" title="3.1 读取文件里面的行数"></a>3.1 读取文件里面的行数</h4><pre><code>import sys import os.path# 文件目录dir = os.path.dirname(sys.executable)# 打开文件进行操作with open(dir+&#39;\\num.txt&#39;, encoding = &#39;utf-8&#39;) as fp:    content = fp.readlines()# 打印文件内容的类型print(type(content))# 打印文件内容print(content)# 打印文件所在的目录print(dir)# 打印文件里面内容的行数print(len(content))</code></pre><p><strong>结果：</strong><br>&lt;class ‘list’&gt;<br>[‘12\n’, ‘6\n’, ‘2\n’, ‘35\n’, ‘11\n’, ‘22\n’, ‘23\n’, ‘11\n’, ‘254\n’, ‘12’]<br>F:\Anaconda<br>10</p><h3 id="4、Jupyter-Notebook"><a href="#4、Jupyter-Notebook" class="headerlink" title="4、Jupyter Notebook"></a>4、Jupyter Notebook</h3><p>默认地址：<a href="http://localhost:8888" target="_blank" rel="noopener">http://localhost:8888</a></p><pre><code># 使用递归def fib(n):    if n==1 or n==2:        return 1    elif n==0:        return 0    return fib(n-1)+fib(n-2)# 输出第10个斐波那契数列print(fib(10))print(fib(0))</code></pre><p><strong>结果：</strong><br>55<br>0</p><h3 id="5、Python环境管理"><a href="#5、Python环境管理" class="headerlink" title="5、Python环境管理"></a>5、Python环境管理</h3><h4 id="5-1-打开管理终端"><a href="#5-1-打开管理终端" class="headerlink" title="5.1 打开管理终端"></a>5.1 打开管理终端</h4><p>Windows用户打开“Anaconda Prompt”</p><p>macOS和Linux用户打开”Terminal”（终端）</p><h4 id="5-2-创建新环境"><a href="#5-2-创建新环境" class="headerlink" title="5.2 创建新环境"></a>5.2 创建新环境</h4><pre><code>conda create --name &lt;env_name&gt; &lt;package_name&gt;</code></pre><p><strong>注：</strong></p><ol><li><code>env_name</code>–创建的环境名，建议英文命名，且不加空格，名称两边不加尖括号”&lt;&gt;”</li><li><code>package_name</code>–安装环境中的包名，名称两边不加尖括号”&lt;&gt;”</li><li>如果要安装指定的版本号， 则只需要在包名后面以=和版本号的形式执行。如：<code>conda create name python2  python=2.7</code>，即创建一个名为“python2”的环境，环境中安装版本为2.7的python。</li><li>如果要在新创建的环境中创建多个包，则直接在<code>&lt;package_names&gt;</code>后以空格隔开，添加多个即可。如：<code>conda create -n python3 python=3.7 numpy pandas</code>，即创建一个名为“python”的环境，环境中安装版本为3.7的python，同时也安装了<code>numpy</code>和<code>pandas</code>。</li><li>默认情况下，新创建的环境会被保存在<code>/User/&lt;username&gt;/anaconda3/env</code>目录下，其中<code>&lt;user_name&gt;</code>为系统当前用户的用户名。</li></ol><h4 id="5-3-激活-退出环境"><a href="#5-3-激活-退出环境" class="headerlink" title="5.3 激活/退出环境"></a>5.3 激活/退出环境</h4><p>激活：<code>conda activate python3</code></p><p>退出：<code>conda deactivate</code></p><h4 id="5-4-删除环境"><a href="#5-4-删除环境" class="headerlink" title="5.4 删除环境"></a>5.4 删除环境</h4><p> <code>conda  remove --name python3 --all</code></p><h3 id="6、Python扩展库安装"><a href="#6、Python扩展库安装" class="headerlink" title="6、Python扩展库安装"></a>6、Python扩展库安装</h3><h4 id="6-1-添加清华大学的Anaconda镜像"><a href="#6-1-添加清华大学的Anaconda镜像" class="headerlink" title="6.1 添加清华大学的Anaconda镜像"></a>6.1 添加清华大学的Anaconda镜像</h4><pre><code>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/# 设置搜索时显示通道地址conda config --set show_channel_urls yes</code></pre><p><code>conda install numpy</code> 测试</p><p><strong>查询可供安装的扩展库版本</strong><br><code>conda search --full-name pandas</code></p><p><strong>获取当前环境中已安装的扩展库信息</strong><br><code>conda list</code></p><h4 id="6-2-在指定环境中安装包"><a href="#6-2-在指定环境中安装包" class="headerlink" title="6.2 在指定环境中安装包"></a>6.2 在指定环境中安装包</h4><pre><code>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yesconda activate python3conda install numpy</code></pre><h4 id="6-3-在当前环境中卸载包"><a href="#6-3-在当前环境中卸载包" class="headerlink" title="6.3 在当前环境中卸载包"></a>6.3 在当前环境中卸载包</h4><p><code>conda remove &lt;package_name&gt;</code></p><h4 id="6-4-在指定环境中卸载包"><a href="#6-4-在指定环境中卸载包" class="headerlink" title="6.4 在指定环境中卸载包"></a>6.4 在指定环境中卸载包</h4><p><code>conda remove --name &lt;env_name&gt; &lt;package_name&gt;</code></p><h3 id="7、-Python扩展库导入"><a href="#7、-Python扩展库导入" class="headerlink" title="7、 Python扩展库导入"></a>7、 Python扩展库导入</h3><p><strong>建议先导入标准库再导入扩展库对象，只导入确实需要使用的标准库和扩展库对象，提高加载速度，减少打包体积</strong></p><h4 id="7-1-import-模块名-as-别名"><a href="#7-1-import-模块名-as-别名" class="headerlink" title="7.1 import 模块名[as 别名]"></a>7.1 import 模块名[as 别名]</h4><p><strong>使用时需要在对象之前加上模块名作为前缀，即“模块名.对象名”</strong></p><pre><code>import math import randomimport posixpath as pathprint(math.sqrt(16))                # 计算并输出16的平方根print(math.cos(math.pi/4))            # 计算余弦值print(random.choices(&#39;abcd&#39;, k=8))    # 从字符串&#39;abcd&#39;随机选择8个字符                                    # 允许重复print(path.isfile(r&#39;C:Windows\notepad.exe&#39;))    #测试指定路径是否为文件</code></pre><p><strong>结果：</strong><br>4.0<br>0.7071067811865476<br>[‘b’, ‘b’, ‘d’, ‘b’, ‘a’, ‘d’, ‘a’, ‘c’]<br>False</p><h4 id="7-2-from-模块名-import-对象名-as-别名"><a href="#7-2-from-模块名-import-对象名-as-别名" class="headerlink" title="7.2 from  模块名  import  对象名 [as 别名]"></a>7.2 from  模块名  import  对象名 [as 别名]</h4><p>不需要模块名作为前缀，导入方式可以减少查询次数，提高访问速度</p><pre><code>from math import pi as PIfrom os.path import getsizefrom random import choicer = 3print(round(PI*r*r, 2))                     # 计算半径为3的圆面积print(getsize(r&#39;C:Windows\notepad.exe&#39;))    # 计算文件大小，单位为字节print(choice(&#39;Python&#39;))                        # 从字符串中随机选择一个字符</code></pre><p><strong>结果：</strong><br>28.27<br>254464<br>o</p><h4 id="7-3-from-模块名-import"><a href="#7-3-from-模块名-import" class="headerlink" title="7.3 from  模块名  import  *"></a>7.3 from  模块名  import  *</h4><p><strong>不推荐使用</strong></p><pre><code>from itertools import *characters = &#39;1234&#39;for item in combinations(characters, 3):    # 从4个字符中任选3个组合    print(item, end=&#39; &#39;)                    # end=&#39; &#39; 表示输出后不换行print(&#39;\n&#39;+&#39;=&#39;*20)                          # 行号后输出20个等于号for item in permutations(characters, 3):    # 从4个字符中任选3个的排列    print(item, end=&#39; &#39;)                    </code></pre><h3 id="8、Python常用标准库"><a href="#8、Python常用标准库" class="headerlink" title="8、Python常用标准库"></a>8、Python常用标准库</h3><h4 id="8-1-字符串"><a href="#8-1-字符串" class="headerlink" title="8.1 字符串"></a>8.1 字符串</h4><p><code>re</code>：正则表达式。用来判断是否是你指定的特定字符串。<br><code>StringIO</code>：提供以文件形式来读写字符串。<br><code>struct</code>：以二进制字节序列来解释字符串。可以通过格式化参数，指定类型、长度、字节序（大小端）、内存对齐等。</p><pre><code>import re print(re.findall(r&#39;f[a-z]*&#39;,  &#39;which foot or hand fell fastest&#39;))</code></pre><p><strong>结果：</strong><br>[‘foot’, ‘fell’, ‘fastest’]</p><p>如果只需要简单的功能，应该首先考虑字符串，因为简单，易于阅读和调试，如：</p><pre><code>print(&#39;tea for too&#39;.replace(&#39;&#39;too,&#39;&#39;two&#39;))</code></pre><p>结果：<br>‘tea for two’</p><h4 id="8-2-数据类型"><a href="#8-2-数据类型" class="headerlink" title="8.2  数据类型"></a>8.2  数据类型</h4><p><code>datetime</code>：提供操作日期和时间的类。<br><code>collections</code>：高性能容器数据类型。实现了Python的通用内置容器、字典、列表、集合，和元组专门的数据类型。<br><code>pprint</code>：提供“整洁打印”功能，具有打印任意Python数据结构的能力。</p><h4 id="8-3-数学运算"><a href="#8-3-数学运算" class="headerlink" title="8.3 数学运算"></a>8.3 数学运算</h4><p><code>random</code>：各种分布的伪随机数的生成器。<br><code>math</code>：数学函数。提供了由C标准的数学函数访问。该库函数不适用于复数。<br><code>cmath</code>：为复数提供的数学函数。<br><code>operator</code>： 重载运算符。</p><p><strong>math 模块为浮点运算提供了对底层C函数库的访问</strong></p><pre><code>import math print(math.cos(math.pi/4))print(math.log(1024, 2))</code></pre><p><strong>结果：</strong><br>0.7071067811865476<br>10.0</p><p><strong>random 提供了生成随机数的工具</strong></p><pre><code>import randomfruits = random.choice([&#39;apple&#39;, &#39;pear&#39;, &#39;banana&#39;])x = random.sample(range(100), 10)     # 0-100选择不能重复的10个数y = random.random()                    # 随机浮点数z = random.randrange(6)                # 从范围0-6中选择随机整数print(fruits)print(x)print(y)print(z)</code></pre><p><strong>结果：</strong><br>apple<br>[64, 97, 91, 21, 40, 55, 63, 79, 77, 1]<br>0.8885638928051524<br>0</p><h4 id="8-4-文件和目录"><a href="#8-4-文件和目录" class="headerlink" title="8.4 文件和目录"></a>8.4 文件和目录</h4><p><code>os.path</code>：常用路径名操作。<br><code>filecmp</code>：文件和目录的比较。<br><code>shutil</code>：高级的文件操作：支持文件复制和删除。</p><h4 id="8-5-操作系统"><a href="#8-5-操作系统" class="headerlink" title="8.5 操作系统"></a>8.5 操作系统</h4><p><code>time</code>：时间获取和转换，各种与时间相关的函数。<br><code>argparse</code>：命令行选项、参数和子命令的解析器。<br><code>io</code>：提供接口处理的IO流。<br><code>logging</code>： Python的日志工具，提供日志记录的API。<br><code>logging.config</code>：Python日志配置，用于配置日志模块的API。<br><code>os</code>：提供丰富的与MAC，NT，Posix等操作系统进行交互的能力。<br><code>sys</code>：提供访问和维护python解释器的能力。这包括了提示信息，版本，整数的最大值，可用模块，路径钩子，标准错误，标准输入输出的定位和解释器调用的命令参数。</p><p><strong>os模块提供了不少与操作系统相关联的函数</strong></p><pre><code>import os print(os.getcwd())            # 返回当前的工作目录os.chdir(r&#39;C:Users\winner\Python3Learn\Lesson1Code&#39;) # 修改当前的工作目录os.system(&#39;mkdir today&#39;)     # 执行系统命令 mkdirprint(os.getcwd())            # 返回当前的工作目录</code></pre><p>建议使用<code>import os</code> 风格而非<code>from os import *</code>，这样可以保证随操作系统不同而有所变化的os.open()不会覆盖内置函数open()。</p><p>在使用os这样的大型模块时，内置的dir()和help()函数非常有用。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对抗击新冠肺炎疫情斗争牺牲烈士和逝世同胞的深切哀悼</title>
      <link href="/2020/04/04/dui-kang-ji-xin-guan-fei-yan-yi-qing-dou-zheng-xi-sheng-lie-shi-he-shi-shi-tong-bao-de-shen-qie-ai-dao/"/>
      <url>/2020/04/04/dui-kang-ji-xin-guan-fei-yan-yi-qing-dou-zheng-xi-sheng-lie-shi-he-shi-shi-tong-bao-de-shen-qie-ai-dao/</url>
      
        <content type="html"><![CDATA[<h3 id="对抗击新冠肺炎疫情斗争牺牲烈士和逝世同胞的深切哀悼"><a href="#对抗击新冠肺炎疫情斗争牺牲烈士和逝世同胞的深切哀悼" class="headerlink" title="对抗击新冠肺炎疫情斗争牺牲烈士和逝世同胞的深切哀悼"></a>对抗击新冠肺炎疫情斗争牺牲烈士和逝世同胞的深切哀悼</h3><p>牺牲的烈士们，逝世同胞们，一路走好，愿你们天堂安息！！！<br>致敬英雄，缅怀逝者！！！</p>]]></content>
      
      
      <categories>
          
          <category> 全国性哀悼活动 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 哀悼 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL将Windows和Linux时区同步的时区表</title>
      <link href="/2020/04/03/wang-zhan-zai-ai-dao-jie-ri-shi-shi-zheng-ge-ye-mian-hui-se-de-dai-ma/"/>
      <url>/2020/04/03/wang-zhan-zai-ai-dao-jie-ri-shi-shi-zheng-ge-ye-mian-hui-se-de-dai-ma/</url>
      
        <content type="html"><![CDATA[<h3 id="网站在哀悼节日时使整个页面灰色的代码"><a href="#网站在哀悼节日时使整个页面灰色的代码" class="headerlink" title="网站在哀悼节日时使整个页面灰色的代码"></a>网站在哀悼节日时使整个页面灰色的代码</h3><p>html {<br>    -webkit-filter: grayscale(100%);<br>    -moz-filter: grayscale(100%);<br>    -ms-filter: grayscale(100%);<br>    -o-filter: grayscale(100%);<br>    filter: grayscale(100%);<br>    filter: progid:DXImageTransform.Microsoft.BasicImage(grayscale=1);<br>}</p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 哀悼 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>npm换成淘宝的源</title>
      <link href="/2020/02/26/npm-huan-cheng-tao-bao-de-yuan/"/>
      <url>/2020/02/26/npm-huan-cheng-tao-bao-de-yuan/</url>
      
        <content type="html"><![CDATA[<p><strong>安装npm install时，长时间停留在fetchMetadata: sill fetchPackageMetaData error for …</strong></p><p><strong>方法如下</strong></p><h3 id="更换成淘宝的源"><a href="#更换成淘宝的源" class="headerlink" title="更换成淘宝的源"></a>更换成淘宝的源</h3><p>npm config set registry <a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a> </p><h3 id="验证是否成功"><a href="#验证是否成功" class="headerlink" title="验证是否成功"></a>验证是否成功</h3><p>npm config get registry </p>]]></content>
      
      
      <categories>
          
          <category> npm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> npm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL将Windows和Linux时区同步的时区表</title>
      <link href="/2020/02/26/mysql-jiang-windows-he-linux-shi-qu-tong-bu-de-shi-qu-biao/"/>
      <url>/2020/02/26/mysql-jiang-windows-he-linux-shi-qu-tong-bu-de-shi-qu-biao/</url>
      
        <content type="html"><![CDATA[<p><strong>时区表</strong> mysql_tzinfo_to_sql /usr/share/zoneinfo | mysql -u root mysql -p</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Github Hexo 建站</title>
      <link href="/2020/02/24/github-hexo-jian-zhan/"/>
      <url>/2020/02/24/github-hexo-jian-zhan/</url>
      
        <content type="html"><![CDATA[<h2 id="Github-Hexo-建站"><a href="#Github-Hexo-建站" class="headerlink" title="Github Hexo 建站"></a>Github Hexo 建站</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><pre><code> hexo init git init npm install --save hexo-deployer-git</code></pre><h3 id="代码高亮"><a href="#代码高亮" class="headerlink" title="代码高亮"></a>代码高亮</h3><pre><code> npm i -S hexo-prism-plugin</code></pre><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><pre><code>npm install hexo-generator-search --save</code></pre><h3 id="中文链接转拼音"><a href="#中文链接转拼音" class="headerlink" title="中文链接转拼音"></a>中文链接转拼音</h3><pre><code>npm i hexo-permalink-pinyin --save</code></pre><h3 id="文章字数统计插件"><a href="#文章字数统计插件" class="headerlink" title="文章字数统计插件"></a>文章字数统计插件</h3><pre><code>npm i --save hexo-wordcount</code></pre><h3 id="添加-RSS-订阅支持"><a href="#添加-RSS-订阅支持" class="headerlink" title="添加 RSS 订阅支持"></a>添加 RSS 订阅支持</h3><pre><code>npm install hexo-generator-feed --save</code></pre><h3 id="添加百度sitemap-xml"><a href="#添加百度sitemap-xml" class="headerlink" title="添加百度sitemap.xml"></a>添加百度sitemap.xml</h3><pre><code> npm install hexo-generator-sitemap --save-dev npm install hexo-generator-baidu-sitemap --save-dev</code></pre><h3 id="生成静态文件和部署到github"><a href="#生成静态文件和部署到github" class="headerlink" title="生成静态文件和部署到github"></a>生成静态文件和部署到github</h3><pre><code>hexo ghexo d</code></pre>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Django阿里云部署详解之服务器安装安装Nginx</title>
      <link href="/2020/02/22/django-a-li-yun-bu-shu-xiang-jie-zhi-fu-wu-qi-an-zhuang-an-zhuang-nginx/"/>
      <url>/2020/02/22/django-a-li-yun-bu-shu-xiang-jie-zhi-fu-wu-qi-an-zhuang-an-zhuang-nginx/</url>
      
        <content type="html"><![CDATA[<h3 id="Django阿里云部署详解之服务器安装安装Nginx"><a href="#Django阿里云部署详解之服务器安装安装Nginx" class="headerlink" title="Django阿里云部署详解之服务器安装安装Nginx"></a>Django阿里云部署详解之服务器安装安装Nginx</h3><h4 id="（本系统就是阿里云部署的）"><a href="#（本系统就是阿里云部署的）" class="headerlink" title="（本系统就是阿里云部署的）"></a>（本系统就是阿里云部署的）</h4><h4 id="大家照做就行，这我试了很多次才总结出来的"><a href="#大家照做就行，这我试了很多次才总结出来的" class="headerlink" title="大家照做就行，这我试了很多次才总结出来的"></a>大家照做就行，这我试了很多次才总结出来的</h4><p>这个在开始准备环境时已经安装了</p><pre><code>yum install epel-releaseyum install -y nginx</code></pre><p>在/etc/nginx/nginx.conf里面修改sever{}</p><pre><code>    listen 443 ssl;    server_name 你的网站名字;    charset utf-8;    ssl_certificate cert/?.pem;    ssl_certificate_key cert/?.key;    ssl_session_timeout 5m;    ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;    ssl_prefer_server_ciphers on;    client_max_body_size 75M;    location /static {        alias /home/tests/static_collected;    }    location /media {        alias /home/tests/media;    }    location / {        uwsgi_pass 127.0.0.1:8001;        include /etc/nginx/uwsgi_params;    }</code></pre><p><strong>查看运行nginx</strong></p><pre><code>nginx -tservice nginx restartservice nginx reloadservice nginx startservice nginx statusservice nginx stop</code></pre><p><strong>找到运行的端口号</strong></p><pre><code>netstat -antp</code></pre><p><strong>杀死进程</strong></p><pre><code>kill -9  编号</code></pre>]]></content>
      
      
      <categories>
          
          <category> Django </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>github ssh 连接失败解决方法</title>
      <link href="/2020/02/21/github-ssh-lian-jie-shi-bai-jie-jue-fang-fa/"/>
      <url>/2020/02/21/github-ssh-lian-jie-shi-bai-jie-jue-fang-fa/</url>
      
        <content type="html"><![CDATA[<h3 id="github-ssh-连接失败"><a href="#github-ssh-连接失败" class="headerlink" title="github ssh 连接失败"></a>github ssh 连接失败</h3><p>$ ssh -T <a href="mailto:git@github.com">git@github.com</a><br>ssh: connect to host github.com port 22: Connection timed out</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>在.ssh下面新加config文件，不要后缀，内容如下：</p><pre><code>Host github.comUser 417952939@qq.comHostname ssh.github.comPreferredAuthentications publickeyIdentityFile ~/.ssh/id_rsaPort 22</code></pre>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Django阿里云部署详解之服务器安装安装uwsgi</title>
      <link href="/2020/02/16/django-a-li-yun-bu-shu-xiang-jie-zhi-fu-wu-qi-an-zhuang-an-zhuang-uwsgi/"/>
      <url>/2020/02/16/django-a-li-yun-bu-shu-xiang-jie-zhi-fu-wu-qi-an-zhuang-an-zhuang-uwsgi/</url>
      
        <content type="html"><![CDATA[<h3 id="Django阿里云部署详解之服务器安装安装uwsgi"><a href="#Django阿里云部署详解之服务器安装安装uwsgi" class="headerlink" title="Django阿里云部署详解之服务器安装安装uwsgi"></a>Django阿里云部署详解之服务器安装安装uwsgi</h3><h4 id="（本系统就是阿里云部署的）"><a href="#（本系统就是阿里云部署的）" class="headerlink" title="（本系统就是阿里云部署的）"></a>（本系统就是阿里云部署的）</h4><h4 id="大家照做就行，这我试了很多次才总结出来的"><a href="#大家照做就行，这我试了很多次才总结出来的" class="headerlink" title="大家照做就行，这我试了很多次才总结出来的"></a>大家照做就行，这我试了很多次才总结出来的</h4><h4 id="1、安装uwsgi"><a href="#1、安装uwsgi" class="headerlink" title="1、安装uwsgi"></a>1、安装uwsgi</h4><p>注意：<br>    1）在系统环境安装，非虚拟环境<br>    2）使用对应python版本安装<br>    3）要先安装python开发包<br><strong>pip install uwsgi</strong></p><h4 id="2、测试-uwsgi-是否正常"><a href="#2、测试-uwsgi-是否正常" class="headerlink" title="2、测试 uwsgi 是否正常"></a>2、测试 uwsgi 是否正常</h4><p>vim test.py<br>新建 test.py 文件，内容如下：</p><pre><code>def application(env, start_response):    start_response(&#39;200 OK&#39;, [(&#39;Content-Type&#39;,&#39;text/html&#39;)])    return &quot;Hello World&quot;</code></pre><p>然后在终端运行：</p><pre><code>uwsgi --wsgi-file test.py  --http :8001</code></pre><p>注意：需要开启端口才可以正常访问<br><strong>杀死uwsgi</strong></p><pre><code>ps -aux | grep uwsgi + awk &#39;{print $2}&#39; | xargs kill -9</code></pre><h4 id="3、可以用uwsgi的http协议访问django写的网站"><a href="#3、可以用uwsgi的http协议访问django写的网站" class="headerlink" title="3、可以用uwsgi的http协议访问django写的网站"></a>3、可以用uwsgi的http协议访问django写的网站</h4><p>执行如下命令可以测试自己的项目</p><pre><code>uwsgi --http :8001 --chdir /home/tests --home /home/test_env --module tests.wsgi:application</code></pre><p><strong>mkdir tests_uwsgi</strong><br><strong>tests.ini</strong></p><pre><code>[uwsgi]chdir=/home/tests    #项目地质home=/home/test_env  #环境地质module=tests.wsgi:applicationmaster=Trueprocesses=4          #工作进程数harakiri=60          #60秒重启max-requests=5000      #服务5000个请求后重新启动进程socket=127.0.0.1:8001uid=nginxgid=nginxpidfile=/home/tests_uwsgi/master.piddaemonize=/home/tests_uwsgi/tests.logvacuum=True   #清理</code></pre><p><strong>初始化ini</strong><br>uwsgi –ini /home/tests_uwsgi/tests.ini</p><p><strong>重新运行uwsgi</strong><br>uwsgi –reload /home/tests_uwsgi/master.pid</p><p><strong>查看uwsgi是否运行</strong><br>ps aux | grep uwsgi</p>]]></content>
      
      
      <categories>
          
          <category> Django </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Django阿里云部署详解之服务器安装MySQL8.0系列的版本（服务器是Linux的系统）</title>
      <link href="/2020/02/10/django-a-li-yun-bu-shu-xiang-jie-zhi-fu-wu-qi-an-zhuang-mysql8.0-xi-lie-de-ban-ben-fu-wu-qi-shi-linux-de-xi-tong/"/>
      <url>/2020/02/10/django-a-li-yun-bu-shu-xiang-jie-zhi-fu-wu-qi-an-zhuang-mysql8.0-xi-lie-de-ban-ben-fu-wu-qi-shi-linux-de-xi-tong/</url>
      
        <content type="html"><![CDATA[<h3 id="Django阿里云部署详解之服务器安装MySQL8-0系列的版本（服务器是Linux的系统）"><a href="#Django阿里云部署详解之服务器安装MySQL8-0系列的版本（服务器是Linux的系统）" class="headerlink" title="Django阿里云部署详解之服务器安装MySQL8.0系列的版本（服务器是Linux的系统）"></a>Django阿里云部署详解之服务器安装MySQL8.0系列的版本（服务器是Linux的系统）</h3><h4 id="（本系统就是阿里云部署的）"><a href="#（本系统就是阿里云部署的）" class="headerlink" title="（本系统就是阿里云部署的）"></a>（本系统就是阿里云部署的）</h4><h4 id="大家照做就行，这我试了很多次才总结出来的"><a href="#大家照做就行，这我试了很多次才总结出来的" class="headerlink" title="大家照做就行，这我试了很多次才总结出来的"></a>大家照做就行，这我试了很多次才总结出来的</h4><pre><code>wget https://dev.mysql.com/get/mysql80-community-release-el7-3.noarch.rpmrpm -ivh mysql80-community-release-el7-3.noarch.rpmyum -y install mysql-community-server</code></pre><h4 id="修改-etc-my-cnf"><a href="#修改-etc-my-cnf" class="headerlink" title="修改/etc/my.cnf"></a>修改/etc/my.cnf</h4><pre><code>[mysqld]# 设置mysql的安装目录basedir=/usr/local/mysql# 设置mysql数据库的数据的存放目录datadir=/usr/local/mysql/data# 设置默认使用的端口port=3306# 允许最大连接数max_connections=200# 允许连接失败的次数。这是为了防止有人试图攻击数据库max_connect_errors=10# 服务端使用的字符集character-set-server=utf8mb4# 数据库字符集对应一些排序等规则使用的字符集collation-server=utf8mb4_general_ci# 创建新表时将使用的默认存储引擎default-storage-engine=INNODB# 默认使用“mysql_native_password”插件作为认证加密方式# MySQL8.0默认认证加密方式为caching_sha2_passworddefault_authentication_plugin=mysql_native_password#server_id=socket=/var/lib/mysql/mysql.sock#这里可以加也可以不加，如果有lc_messages_dir警告就加上，，lc_messages_dir=/usr/local/mysql/share lc_messages=en_USlog-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pid</code></pre><h4 id="创建目录（一定要，不然初始化不成功，因为是你自己设定好的文件夹）"><a href="#创建目录（一定要，不然初始化不成功，因为是你自己设定好的文件夹）" class="headerlink" title="创建目录（一定要，不然初始化不成功，因为是你自己设定好的文件夹）"></a>创建目录（一定要，不然初始化不成功，因为是你自己设定好的文件夹）</h4><p>这里要自己创建/usr/local/mysql和/usr/local/mysql/share<br>报错的时候这里加（看自己报什么错，可选）</p><pre><code>copy /usr/share/mysql-8.0/bulgarian/errmsg.sys /usr/local/mysql/share </code></pre><pre><code>systemctl start mysqldsystemctl status mysqldsystemctl stop mysqld</code></pre><p>初始化MySQL(有时候不一定要，在/var/log/mysqld.log里面可能有，基本上systemctl start mysqld是可以找到临时密码的)</p><pre><code>mysqld --initialize --user=mysql</code></pre><h4 id="重置root密码"><a href="#重置root密码" class="headerlink" title="重置root密码"></a>重置root密码</h4><pre><code>alter user  &#39;root&#39;@&#39;localhost&#39; identified by &#39;your passwoed&#39;;</code></pre><h4 id="新建新账户针对一个数据库（为了安全起见的）"><a href="#新建新账户针对一个数据库（为了安全起见的）" class="headerlink" title="新建新账户针对一个数据库（为了安全起见的）"></a>新建新账户针对一个数据库（为了安全起见的）</h4><pre><code>show databases;create database ?_db default charset=utf8 default collate utf8_unicode_ci;create user &#39;&#39;@&#39;localhost&#39; identified by &#39;&#39;;grant all privileges on ?_db.* to &#39;xiezhouHCH&#39;@&#39;localhost&#39;;flush privileges;</code></pre><p>新账户只能访问指定的数据库</p>]]></content>
      
      
      <categories>
          
          <category> Django </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Django阿里云部署详解之服务器安装虚拟环境、Python</title>
      <link href="/2020/01/31/django-a-li-yun-bu-shu-xiang-jie-zhi-fu-wu-qi-an-zhuang-xu-ni-huan-jing-python/"/>
      <url>/2020/01/31/django-a-li-yun-bu-shu-xiang-jie-zhi-fu-wu-qi-an-zhuang-xu-ni-huan-jing-python/</url>
      
        <content type="html"><![CDATA[<h3 id="Django阿里云部署详解之服务器安装虚拟环境、Python"><a href="#Django阿里云部署详解之服务器安装虚拟环境、Python" class="headerlink" title="Django阿里云部署详解之服务器安装虚拟环境、Python"></a>Django阿里云部署详解之服务器安装虚拟环境、Python</h3><h4 id="（本系统就是阿里云部署的）"><a href="#（本系统就是阿里云部署的）" class="headerlink" title="（本系统就是阿里云部署的）"></a>（本系统就是阿里云部署的）</h4><h4 id="大家照做就行，这我试了很多次才总结出来的"><a href="#大家照做就行，这我试了很多次才总结出来的" class="headerlink" title="大家照做就行，这我试了很多次才总结出来的"></a>大家照做就行，这我试了很多次才总结出来的</h4><pre><code>yum update -yyum -y install gcc gcc-c++yum -y groupinstall &quot;Development tools&quot;yum -y install zlib zlib-devel openssl openssl-devel ncurses-devel sqlite sqlite-devel bzip2-deve readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-develyum install epel-releaseyum -y install nginxyum install libffi-devel -y</code></pre><pre><code>wget https://www.python.org/ftp/python/3.8.1/Python-3.8.1.tgz</code></pre><pre><code>mkdir -p /usr/local/python3tar -zxvf Python-3.8.1.tgz</code></pre><pre><code>cd Python-3.8.1./configure --prefix=/usr/local/python3make &amp;&amp; make install</code></pre><pre><code>ln -s /usr/local/python3/bin/python3 /usr/bin/python3</code></pre><p>原来的pip自己备份下：<br>mv /usr/bin/pip /usr/bin/pip.bak<br>这里使用新的pip</p><pre><code>ln -s /usr/local/python3/bin/pip3 /usr/bin/pippip install --upgrade pip</code></pre><h4 id="修改系统默认的python为自己装的版本"><a href="#修改系统默认的python为自己装的版本" class="headerlink" title="修改系统默认的python为自己装的版本"></a>修改系统默认的python为自己装的版本</h4><p>先找到新版本python安装位置，<br>然后</p><pre><code>vi /etc/profile.d/python.sh</code></pre><p>创建新文件，然后输入</p><pre><code>alias python=&#39;/usr/bin/python3&#39;  # 此处的路径为新版本python的路径，通过我上一篇文章来</code></pre><p>查找此路径<br>重启会话使配置生效</p><pre><code>source /etc/profile.d/python.sh</code></pre><pre><code>pip install --upgrade pippip install --upgrade setuptoolspip install virtualenv</code></pre><pre><code>ln -s /usr/local/python3/bin/virtualenv /usr/bin/virtualenvcd /home/virtualenv --python=/usr/bin/python test_envvirtualenv test_env(也一样是python3，之前已经修改了默认Python版本)source test_env/bin/activate</code></pre><h4 id="进入虚拟环境安装各个软件"><a href="#进入虚拟环境安装各个软件" class="headerlink" title="进入虚拟环境安装各个软件"></a>进入虚拟环境安装各个软件</h4><pre><code>pip install Django==3.0.2 pip install uwsgiln -s /usr/local/python3/bin/uwsgi /usr/bin/uwsgipip install pillowpip install django-mdeditorpip install Markdown</code></pre><h4 id="特别的，安装mysqlclient需要系统安装过mysql-devel，不然报错"><a href="#特别的，安装mysqlclient需要系统安装过mysql-devel，不然报错" class="headerlink" title="特别的，安装mysqlclient需要系统安装过mysql-devel，不然报错"></a>特别的，安装mysqlclient需要系统安装过mysql-devel，不然报错</h4><pre><code>yum -y install mysql-devel</code></pre><pre><code>pip install mysqlclient</code></pre>]]></content>
      
      
      <categories>
          
          <category> Django </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Django开启虚拟环境（Windows中）</title>
      <link href="/2020/01/10/django-kai-qi-xu-ni-huan-jing-windows-zhong/"/>
      <url>/2020/01/10/django-kai-qi-xu-ni-huan-jing-windows-zhong/</url>
      
        <content type="html"><![CDATA[<h3 id="Django开启虚拟环境（Windows中）"><a href="#Django开启虚拟环境（Windows中）" class="headerlink" title="Django开启虚拟环境（Windows中）"></a>Django开启虚拟环境（Windows中）</h3><p>开启本地虚拟环境<br>1)避免多个项目之间python库的冲突<br>2)完整便捷导出python库的列表</p><pre><code>pip install virtualenv</code></pre><p>创建：virtualenv &lt;虚拟环境名称&gt;</p><pre><code>virtualenv blog_env</code></pre><p>启动：Scripts\activate</p><pre><code>pip install Django</code></pre><p>退出：deactivate</p><p>建立目录</p><pre><code>django-admin startproject django_introduction</code></pre><p>运行</p><pre><code>python manage.py runserver 80</code></pre><p>创建应用</p><pre><code>python manage.py startapp blog</code></pre><p>制作数据迁移</p><pre><code>python manage.py makemigrations (app)</code></pre><p>迁移动作</p><pre><code>python manage.py migrate</code></pre><pre><code>python manage.py createsuperuser</code></pre>]]></content>
      
      
      <categories>
          
          <category> Django </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pip 一键导出和安装</title>
      <link href="/2020/01/10/pip-yi-jian-dao-chu-he-an-zhuang/"/>
      <url>/2020/01/10/pip-yi-jian-dao-chu-he-an-zhuang/</url>
      
        <content type="html"><![CDATA[<h3 id="pip-一键导出和安装"><a href="#pip-一键导出和安装" class="headerlink" title="pip 一键导出和安装"></a>pip 一键导出和安装</h3><p>将本地使用的虚拟环境pip install 的所有软件导出<br>pip freeze &gt; requirements.txt</p><p>另一个新的环境安装<br>pip install -r requirements.txt</p>]]></content>
      
      
      <categories>
          
          <category> pip </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pip </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>中华人民共和国70周年生日快乐，我爱你中国！！！！！！！</title>
      <link href="/2019/10/01/zhong-hua-ren-min-gong-he-guo-70-zhou-nian-qing/"/>
      <url>/2019/10/01/zhong-hua-ren-min-gong-he-guo-70-zhou-nian-qing/</url>
      
        <content type="html"><![CDATA[<h2 id="热烈祝贺中华人民共和国70周年生日快乐，我爱你中国！！！！！！！"><a href="#热烈祝贺中华人民共和国70周年生日快乐，我爱你中国！！！！！！！" class="headerlink" title="热烈祝贺中华人民共和国70周年生日快乐，我爱你中国！！！！！！！"></a>热烈祝贺中华人民共和国70周年生日快乐，我爱你中国！！！！！！！</h2><h3 id="我爱你祖国！！！！！！！"><a href="#我爱你祖国！！！！！！！" class="headerlink" title="我爱你祖国！！！！！！！"></a>我爱你祖国！！！！！！！</h3><h3 id="让我们一起祝贺祖国母亲生日快乐！！！！！！！"><a href="#让我们一起祝贺祖国母亲生日快乐！！！！！！！" class="headerlink" title="让我们一起祝贺祖国母亲生日快乐！！！！！！！"></a>让我们一起祝贺祖国母亲生日快乐！！！！！！！</h3><h3 id="祝祖国母亲繁荣昌盛！！！！！！！"><a href="#祝祖国母亲繁荣昌盛！！！！！！！" class="headerlink" title="祝祖国母亲繁荣昌盛！！！！！！！"></a>祝祖国母亲繁荣昌盛！！！！！！！</h3><p><img src="/medias/%E5%BA%86%E7%A5%9D%E4%B8%AD%E5%8D%8E%E4%BA%BA%E6%B0%91%E5%85%B1%E5%92%8C%E5%9B%BD%E6%88%90%E7%AB%8B70%E5%91%A8%E5%B9%B4%E6%B4%BB%E5%8A%A8%E6%A0%87%E8%AF%86.PNG" alt="庆祝中华人民共和国成立70周年活动标识"></p><p><img src="/medias/70%E5%91%A8%E5%B9%B4%E5%8D%8E%E8%AF%9E.jpg" alt="70周年华诞"></p><h3 id="今天是你的生日"><a href="#今天是你的生日" class="headerlink" title="今天是你的生日"></a>今天是你的生日</h3><p>今天是你的生日我的中国<br>清晨我放飞一群白鸽<br>为你衔来一枚橄榄叶<br>鸽子在崇山峻岭飞过<br>我们祝福你的生日我的中国<br>愿你永远没有忧患永远宁静<br>我们祝福你的生日我的中国<br>这是儿女们心中期望的歌<br>今天是你的生日我的中国<br>清晨我放飞一群白鸽<br>为你带回远方儿女的思念<br>鸽子在茫茫海天飞过<br>我们祝福你的生日我的中国<br>愿你月儿常圆儿女永远欢乐<br>我们祝福你的生日我的中国<br>这是儿女在远方爱的诉说<br>今天是你的生日我的中国<br>清晨我放飞一群白鸽<br>为你衔来一棵金色麦穗<br>鸽子在风风雨雨中飞过<br>我们祝福你的生日我的中国<br>愿你逆风起飞雨中获得收获<br>我们祝福你的生日我的中国<br>这是儿女们心中期望的歌</p>]]></content>
      
      
      <categories>
          
          <category> 中华人民共和国70周年 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 中华人民共和国70周年 </tag>
            
            <tag> 祖国生日 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>解决IDEA创建Maven项目速度慢问题</title>
      <link href="/2019/08/02/jie-jue-idea-chuang-jian-maven-xiang-mu-su-du-man-wen-ti/"/>
      <url>/2019/08/02/jie-jue-idea-chuang-jian-maven-xiang-mu-su-du-man-wen-ti/</url>
      
        <content type="html"><![CDATA[<p>add Maven Property<br>Name:archetypeCatalog<br>Value:internal</p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IDEA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark-submit提交集群执行</title>
      <link href="/2019/07/29/spark-submit-ti-jiao-ji-qun-zhi-xing/"/>
      <url>/2019/07/29/spark-submit-ti-jiao-ji-qun-zhi-xing/</url>
      
        <content type="html"><![CDATA[<p>spark-submit<br><code>--</code>master yarn-cluster    //集群启动<br><code>--</code>num-executors 1        //分配多少个进程<br><code>--</code>driver-memory 500m  //driver内存<br><code>--</code>executor-memory 1g //进程内存<br><code>--</code>executor-cores 1       //开多少个核，线程<br><code>--</code>jars $(echo /usr/chl/spark8/jars/*.jar | tr ‘ ‘ ‘,’) //加载jar<br><code>--</code>class com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> SparkStreaming </tag>
            
            <tag> spark-sumbit </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IDEA中jar冲突查找快捷键快速定位</title>
      <link href="/2019/07/27/idea-zhong-jar-chong-tu-cha-zhao-kuai-jie-jian-kuai-su-ding-wei/"/>
      <url>/2019/07/27/idea-zhong-jar-chong-tu-cha-zhao-kuai-jie-jian-kuai-su-ding-wei/</url>
      
        <content type="html"><![CDATA[<p><strong>Ctrl+Alt+Shift+N</strong></p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IDEA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>企业网络日志分析</title>
      <link href="/2019/07/27/qi-ye-wang-luo-ri-zhi-fen-xi/"/>
      <url>/2019/07/27/qi-ye-wang-luo-ri-zhi-fen-xi/</url>
      
        <content type="html"><![CDATA[<h3 id="一、背景数据介绍"><a href="#一、背景数据介绍" class="headerlink" title="一、背景数据介绍"></a>一、背景数据介绍</h3><p><strong>1.    WiFi有哪些数据？</strong><br>手机号<br>机构<br>机构<br>机构<br>网页快照<br>论坛帖子<br>微博<br>邮件<br>IM聊天<br>表单数据<br>APP使用</p><p><strong>2.    WiFi价值</strong><br>客户体验：方便客户、基础设施<br>客户数据：精准营销、获取客户上网行为、获取客户信息、客户接触渠道</p><p><strong>3.    WiFi数据获取</strong><br>Wi-Fi 网络可以捕获附近智能手机的 IMSI 号码，无线跟踪并监控用户的根源在于智能手机（包括 Android 和 iOS 设备）连接 Wi-Fi 网络的方式。</p><p>在大多数现代移动操作系统中有两种广泛实现的协议：<br>可扩展认证协议（EAP）<br>认证和密钥协商（AKA）协议</p><p>这些协议允许智能手机通过自身设备的 IMSI 号码切换登录到已知的 Wi-Fi 网络，实现 WiFi 网络自动连接而无需所有者交互。</p><p><strong>4.    wifi数据应用</strong><br><img src="/medias/wifi%E6%95%B0%E6%8D%AE%E5%BA%94%E7%94%A8.PNG" alt="wifi数据应用"></p><p>画像系统<br><img src="/medias/%E7%94%BB%E5%83%8F%E7%B3%BB%E7%BB%9F.PNG" alt="画像系统"></p><p><strong>5.    数据架构</strong><br><img src="/medias/%E7%BD%91%E7%BB%9C%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E6%8D%AE%E6%9E%B6%E6%9E%84%E5%9B%BE.PNG" alt="网络用户行为数据架构图"></p><p><strong>6.    数据结构</strong><br>（1）    <strong>文件命名</strong><br>数据类型_来源_UUID.txt<br>如BASE_SOURCE_UUID.txt</p><p>定一套字段标准 ，类型标准<br>（2）    <strong>字段</strong><br>（3）  <strong>通用字段</strong></p><table><thead><tr><th align="center">参数1</th><th align="center">参数2</th><th align="center">参数3</th><th align="center">参数4</th></tr></thead><tbody><tr><td align="center">imei</td><td align="center">imei号，手机唯一识别码</td><td align="center"></td><td align="center">手机IMEI码由15-17位数字组成</td></tr><tr><td align="center">imsi</td><td align="center">IMSI，SIM卡唯一识别码</td><td align="center">460011418603055</td><td align="center">14-15位数字</td></tr><tr><td align="center">longitude</td><td align="center">经度</td><td align="center"></td><td align="center">精确到小数点6位</td></tr><tr><td align="center">latitude</td><td align="center">纬度</td><td align="center"></td><td align="center">精确到小数点6位</td></tr><tr><td align="center">phone_mac</td><td align="center">手机MAC</td><td align="center"></td><td align="center">格式需要统一（清洗）aa-aa-aa-aa-aa-aa（范围1-9，a-f）</td></tr><tr><td align="center">device_mac</td><td align="center">采集设备MAC</td><td align="center"></td><td align="center">格式需要统一（清洗）aa-aa-aa-aa-aa-aa（范围任意数字加字母）</td></tr><tr><td align="center">device_number</td><td align="center">采集设备号</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">collect_time</td><td align="center">collect_time</td><td align="center"></td><td align="center"></td></tr></tbody></table><p><strong>微信数据(wechat)</strong></p><table><thead><tr><th align="center">参数1</th><th align="center">参数2</th><th align="center">参数3</th><th align="center">参数4</th></tr></thead><tbody><tr><td align="center">username</td><td align="center">微信昵称</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">phone</td><td align="center">手机号</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">object_username</td><td align="center">对方微信号</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">send_message</td><td align="center">发送内容（不能破解）</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">accept_message</td><td align="center">接收内容（不能破解）</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">message_time</td><td align="center">通信时间</td><td align="center"></td><td align="center"></td></tr></tbody></table><p><strong>邮箱数据(Mail)</strong></p><table><thead><tr><th align="center">参数1</th><th align="center">参数2</th><th align="center">参数3</th><th align="center">参数4</th></tr></thead><tbody><tr><td align="center">send_mail</td><td align="center">发送邮箱</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">send_time</td><td align="center">发送时间</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">accept_mail</td><td align="center">接收邮箱</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">accept_time</td><td align="center">接收时间</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">mail_content</td><td align="center">发送内容</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">mail_type</td><td align="center">发送还是接收</td><td align="center"></td><td align="center">send  accept</td></tr></tbody></table><p><strong>搜索数据(Search)</strong></p><table><thead><tr><th align="center">参数1</th><th align="center">参数2</th><th align="center">参数3</th><th align="center">参数4</th></tr></thead><tbody><tr><td align="center">search_content</td><td align="center">搜索内容</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">search_url</td><td align="center">搜索URL</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">search_type</td><td align="center">搜索引擎</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">search_time</td><td align="center">搜索时间</td><td align="center"></td><td align="center"></td></tr></tbody></table><p><strong>基础数据(Base)</strong></p><table><thead><tr><th align="center">参数1</th><th align="center">参数2</th><th align="center">参数3</th><th align="center">参数4</th></tr></thead><tbody><tr><td align="center">name</td><td align="center">姓名</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">is_marry</td><td align="center">是否已婚</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">phone</td><td align="center">手机号</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">address</td><td align="center">户籍所在地</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">address_new</td><td align="center">现在居住地址</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">birthday</td><td align="center">出生日期</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">car_number</td><td align="center">车牌号</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">idcard</td><td align="center">身份证</td><td align="center"></td><td align="center"></td></tr></tbody></table><p>问题：数据结构，数据字段如何确定？<br>根据实际的需求自己确定。</p><h3 id="二．基础架构搭建"><a href="#二．基础架构搭建" class="headerlink" title="二．基础架构搭建"></a>二．基础架构搭建</h3><h4 id="1、创建Maven父项"><a href="#1、创建Maven父项" class="headerlink" title="1、创建Maven父项"></a>1、创建Maven父项</h4><p><strong>总的pom.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;  &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;  &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;  &lt;packaging&gt;pom&lt;/packaging&gt;  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;  &lt;modules&gt;    &lt;module&gt;xz_bigdata_common&lt;/module&gt;    &lt;module&gt;xz_bigdata_es&lt;/module&gt;    &lt;module&gt;xz_bigdata_flume&lt;/module&gt;    &lt;module&gt;xz_bigdata_hbase&lt;/module&gt;    &lt;module&gt;xz_bigdata_kafka&lt;/module&gt;    &lt;module&gt;xz_bigdata_redis&lt;/module&gt;    &lt;module&gt;xz_bigdata_resources&lt;/module&gt;    &lt;module&gt;xz_bigdata_spark&lt;/module&gt;  &lt;/modules&gt;  &lt;name&gt;xz_bigdata2&lt;/name&gt;  &lt;properties&gt;    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;    &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;    &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;    &lt;cdh.version&gt;cdh5.14.0&lt;/cdh.version&gt;    &lt;junit.version&gt;4.12&lt;/junit.version&gt;    &lt;org.slf4j.version&gt;1.7.5&lt;/org.slf4j.version&gt;    &lt;zookeeper.version&gt;3.4.5&lt;/zookeeper.version&gt;    &lt;scala.version&gt;2.10.5&lt;/scala.version&gt;  &lt;/properties&gt;  &lt;repositories&gt;    &lt;repository&gt;      &lt;id&gt;Akka repository&lt;/id&gt;      &lt;url&gt;https://repo.akka.io/releases&lt;/url&gt;    &lt;/repository&gt;    &lt;!--cloudera依赖--&gt;    &lt;repository&gt;      &lt;id&gt;cloudera&lt;/id&gt;      &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;    &lt;/repository&gt;  &lt;/repositories&gt;  &lt;!--日志依赖--&gt;  &lt;dependencies&gt;    &lt;dependency&gt;      &lt;groupId&gt;org.slf4j&lt;/groupId&gt;      &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;      &lt;version&gt;${org.slf4j.version}&lt;/version&gt;    &lt;/dependency&gt;  &lt;/dependencies&gt;  &lt;build&gt;    &lt;plugins&gt;      &lt;plugin&gt;        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;        &lt;version&gt;3.1&lt;/version&gt;        &lt;configuration&gt;          &lt;source&gt;1.8&lt;/source&gt;          &lt;target&gt;1.8&lt;/target&gt;          &lt;encoding&gt;UTF-8&lt;/encoding&gt;        &lt;/configuration&gt;      &lt;/plugin&gt;    &lt;/plugins&gt;  &lt;/build&gt;&lt;/project&gt;</code></pre><h4 id="2、项目整体结构"><a href="#2、项目整体结构" class="headerlink" title="2、项目整体结构"></a>2、项目整体结构</h4><p><img src="/medias/xz_bigdata2%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84.PNG" alt="xz_bigdata2整体结构"></p><h4 id="3、创建子模块"><a href="#3、创建子模块" class="headerlink" title="3、创建子模块"></a>3、创建子模块</h4><p>选中xz_bigdata2，右键选择Module，新建maven子模块，上面图中的那些模块都是这样创建的。<br>注意：开发时使用jdk1.8以上版本，里面使用了jdk1.8特有的内容，低版本开发是报错的，使用jdk1.8方便开发。</p><p>ctrl+shift+alt+s：打开Project Structure里面可以进行操作。</p><p>ctrl+alt+s：打开Settings，可以配置本地Maven（在Build,Execution,Deployment下面的Build Tools下面的Maven配置自己的本地Maven仓库路径）。</p><p>Settings里面还可以看见之前说的Plugins，安装插件，Maven Helper以及后面的Scala插件都可以这里安装。</p><h3 id="三、Common开发"><a href="#三、Common开发" class="headerlink" title="三、Common开发"></a>三、Common开发</h3><p><strong>pom.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_common&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;        &lt;ant.version&gt;1.9.1&lt;/ant.version&gt;        &lt;jaxen.version&gt;1.1.6&lt;/jaxen.version&gt;        &lt;guava.version&gt;12.0.1&lt;/guava.version&gt;        &lt;dom4j.version&gt;1.6.1&lt;/dom4j.version&gt;        &lt;fastjson.version&gt;1.2.5&lt;/fastjson.version&gt;        &lt;disruptor.version&gt;3.3.6&lt;/disruptor.version&gt;        &lt;org.slf4j.version&gt;1.7.5&lt;/org.slf4j.version&gt;        &lt;commons.io.version&gt;2.4&lt;/commons.io.version&gt;        &lt;httpclient.version&gt;4.2.5&lt;/httpclient.version&gt;        &lt;commons.exec.version&gt;1.3&lt;/commons.exec.version&gt;        &lt;commons.lang.version&gt;2.4&lt;/commons.lang.version&gt;        &lt;commons-vfs2.version&gt;2.1&lt;/commons-vfs2.version&gt;        &lt;commons.math3.version&gt;3.4.1&lt;/commons.math3.version&gt;        &lt;commons.logging.version&gt;1.2&lt;/commons.logging.version&gt;        &lt;commons-httpclient.version&gt;3.1&lt;/commons-httpclient.version&gt;        &lt;commons.collections4.version&gt;4.1&lt;/commons.collections4.version&gt;        &lt;commons.configuration.version&gt;1.6&lt;/commons.configuration.version&gt;        &lt;mysql.connector.version&gt;5.1.46&lt;/mysql.connector.version&gt;        &lt;commons-dbutils.version&gt;1.6&lt;/commons-dbutils.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;commons-dbutils&lt;/groupId&gt;            &lt;artifactId&gt;commons-dbutils&lt;/artifactId&gt;            &lt;version&gt;${commons-dbutils.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;mysql&lt;/groupId&gt;            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;            &lt;version&gt;5.1.46&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;            &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;            &lt;version&gt;${org.slf4j.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;            &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;            &lt;version&gt;${org.slf4j.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;commons-io&lt;/groupId&gt;            &lt;artifactId&gt;commons-io&lt;/artifactId&gt;            &lt;version&gt;${commons.io.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;commons-lang&lt;/groupId&gt;            &lt;artifactId&gt;commons-lang&lt;/artifactId&gt;            &lt;version&gt;${commons.lang.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;commons-configuration&lt;/groupId&gt;            &lt;artifactId&gt;commons-configuration&lt;/artifactId&gt;            &lt;version&gt;${commons.configuration.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;dom4j&lt;/groupId&gt;            &lt;artifactId&gt;dom4j&lt;/artifactId&gt;            &lt;version&gt;${dom4j.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;            &lt;version&gt;${fastjson.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- &lt;dependency&gt;             &lt;groupId&gt;log4j&lt;/groupId&gt;             &lt;artifactId&gt;log4j&lt;/artifactId&gt;             &lt;version&gt;1.2.17&lt;/version&gt;         &lt;/dependency&gt;--&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><h4 id="1、config-ConfigUtil-java—配置文件读取"><a href="#1、config-ConfigUtil-java—配置文件读取" class="headerlink" title="1、config/ConfigUtil.java—配置文件读取"></a>1、config/ConfigUtil.java—配置文件读取</h4><pre><code>package com.hsiehchou.common.config;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.io.InputStream;import java.util.Properties;public class ConfigUtil {    private static Logger LOG = LoggerFactory.getLogger(ConfigUtil.class);    private static ConfigUtil configUtil;    public static ConfigUtil getInstance(){        if(configUtil == null){            configUtil = new ConfigUtil();        }        return configUtil;    }    public Properties getProperties(String path){        Properties properties = new Properties();        try {            LOG.info(&quot;开始加载配置文件&quot; + path);            //流式读取配置文件            InputStream insss = this.getClass().getClassLoader().getResourceAsStream(path);            properties = new Properties();            properties.load(insss);        } catch (IOException e) {            LOG.info(&quot;加载配置文件&quot; + path + &quot;失败&quot;);            LOG.error(null,e);        }        LOG.info(&quot;加载配置文件&quot; + path + &quot;成功&quot;);        System.out.println(&quot;文件内容：&quot;+properties);        return properties;    }    public static void main(String[] args) {        ConfigUtil instance = ConfigUtil.getInstance();        Properties properties = instance.getProperties(&quot;common/datatype.properties&quot;);        //Properties properties = instance.getProperties(&quot;spark/relation.properties&quot;);       // properties.get(&quot;relationfield&quot;);        System.out.println(properties);    }}</code></pre><h4 id="2、config-JsonReader-java"><a href="#2、config-JsonReader-java" class="headerlink" title="2、config/JsonReader.java"></a>2、config/JsonReader.java</h4><pre><code>package com.hsiehchou.common.config;import org.apache.commons.io.FileUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.File;public class JsonReader {    private static Logger LOG = LoggerFactory.getLogger(JsonReader.class);    public static String readJson(String json_path){        JsonReader jsonReader = new JsonReader();        return jsonReader.getJson(json_path);    }    private String getJson(String json_path){        String jsonStr = &quot;&quot;;        try {            String path = getClass().getClassLoader().getResource(json_path).toString();            path = path.replace(&quot;\\&quot;, &quot;/&quot;);            if (path.contains(&quot;:&quot;)) {                path = path.replace(&quot;file:/&quot;,&quot;&quot;);            }            jsonStr = FileUtils.readFileToString(new File(path), &quot;UTF-8&quot;);            LOG.error(&quot;读取json文件{}成功&quot;,path);        } catch (Exception e) {            LOG.error(&quot;读取json文件失败&quot;,e);        }        return jsonStr;    }}</code></pre><h4 id="3、adjuster-Adjuster-java—数据调整接口"><a href="#3、adjuster-Adjuster-java—数据调整接口" class="headerlink" title="3、adjuster/Adjuster.java—数据调整接口"></a>3、adjuster/Adjuster.java—数据调整接口</h4><pre><code>package com.hsiehchou.common.adjuster;/** * 数据调整接口 */public interface Adjuster&lt;T, E&gt; {    E doAdjust(T data);}</code></pre><h4 id="4、adjuster-StringAdjuster-java"><a href="#4、adjuster-StringAdjuster-java" class="headerlink" title="4、adjuster/StringAdjuster.java"></a>4、adjuster/StringAdjuster.java</h4><pre><code>package com.hsiehchou.common.adjuster;public abstract class StringAdjuster&lt;E&gt; implements Adjuster&lt;String, E&gt; {}</code></pre><h4 id="5、file-FileCommon-java"><a href="#5、file-FileCommon-java" class="headerlink" title="5、file/FileCommon.java"></a>5、file/FileCommon.java</h4><pre><code>package com.hsiehchou.common.file;import org.apache.commons.io.FileUtils;import org.apache.commons.io.IOUtils;import java.io.File;import java.io.FileInputStream;import java.io.IOException;import java.net.URL;import java.util.List;public class FileCommon {    private FileCommon(){}    /**     * 判断文件是否存在     * @param name     * @return     */    public static boolean exist(String name){        return exist(new File(name));    }    public static boolean exist(File file){        return file.exists();    }    /**     * 创建文件     * @param file     * @return     * @throws IOException     */    public static boolean createFile(String file) throws IOException {        return createFile(new File(file));    }    public static boolean createFile(File file) throws IOException {        if(!file.exists()){            if(file.isDirectory()){                return file.mkdirs();            }else{                File parentDir = file.getParentFile();                if(!parentDir.exists()) {                    if (parentDir.mkdirs()) {                        return file.createNewFile();                    }                }else{                    return file.createNewFile();                }            }        }        return true;    }    /**     * 读取文件内容 按行     * @param file     * @return     * @throws IOException     */    public static List&lt;String&gt; readLines(String file) throws IOException{        return readLines(new File(file), &quot;UTF-8&quot;);    }    public static List&lt;String&gt; readLines(String file, String encording) throws IOException{        return readLines(new File(file), encording);    }    public static List&lt;String&gt; readLines(File file, String encording) throws IOException {        List&lt;String&gt; lines = null;        if(FileCommon.exist(file)) {            FileInputStream fileInputStream = new FileInputStream(file);            lines = IOUtils.readLines(fileInputStream, encording);            fileInputStream.close();        }        return lines;    }    /**     * 获取文件前缀     * @param fileName     * @return     */    public static String getPrefix(String fileName){        String prefix = fileName;        int pos = fileName.lastIndexOf(&quot;.&quot;);        if (pos != -1){            prefix = fileName.substring(0,pos);        }        return prefix;    }    /**     * 获取文件名后缀     * @param fileName     * @return     */    public static String getFilePostfix(String fileName){        String filePostfix = fileName.substring(fileName.lastIndexOf(&quot;.&quot;) + 1);        return filePostfix.toLowerCase();    }    /**     * 删除文件     * @param filePath     * @return     */    public static boolean delFile(String filePath) {        boolean flag = false;        File file = new File(filePath);        if (file.isFile() &amp;&amp; file.exists()) {            flag = file.delete();        }        return flag;    }    /**     * 移动文件     * @param oldPath     * @param newPath     * @return     */    public static boolean mvFile(String oldPath,String newPath){        boolean flag = false;        File oldfile = new File(oldPath);        File newfile = new File(newPath);        if(oldfile.isFile() &amp;&amp; oldfile.exists()){            if(newfile.exists()){                delFile(newfile.getAbsolutePath());            }            flag = oldfile.renameTo(newfile);        }        return flag;    }    /**     * 删除目录     * @param dir     * @return     */    public static boolean deleteDir(File dir){        if (dir.isDirectory()) {            String[] children = dir.list();            //递归删除目录中的子目录下            if(children!=null){                for (int i=0; i&lt;children.length; i++) {                    boolean success = deleteDir(new File(dir, children[i]));                    if (!success) {                        return false;                    }                }            }        }        // 目录此时为空，可以删除        return dir.delete();    }    //递归建立目录，解压缩相关类中使用    public static void mkdirs(File file) {        File parent = file.getParentFile();        if (parent != null &amp;&amp; (!parent.exists())) {            parent.mkdirs();        }    }    public static String getJarFilePathByClass(String clazz) throws ClassNotFoundException {        return getJarFilePathByClass(Class.forName(clazz));    }    public static String getJarFileDirByClass(String clazz) throws ClassNotFoundException {        return getJarFileDirByClass(Class.forName(clazz));    }    public static String getJarFilePathByClass(Class&lt;?&gt; clazz){        return new File(clazz.getProtectionDomain().getCodeSource().getLocation().getFile()).getAbsolutePath();    }    public static String getJarFileDirByClass(Class&lt;?&gt; clazz){        return new File(getJarFilePathByClass(clazz)).getParent();    }    public static String getAbstractPath(String abstractPath) throws Exception{        URL url = FileCommon.class.getClassLoader().getResource(abstractPath);        System.out.println(&quot;配置文件路径为&quot; + url);        File file = new File(url.getFile());        String content= FileUtils.readFileToString(file,&quot;UTF-8&quot;);        return content;    }    public static String getAbstractPath111(String abstractPath) throws Exception{        File file = new File(abstractPath);        String content= FileUtils.readFileToString(file,&quot;UTF-8&quot;);        return content;    }}</code></pre><h4 id="6、filter—数据过滤顶层接口"><a href="#6、filter—数据过滤顶层接口" class="headerlink" title="6、filter—数据过滤顶层接口"></a>6、filter—数据过滤顶层接口</h4><pre><code>package com.hsiehchou.common.filter;/** * 数据过滤顶层接口 */public interface Filter&lt;T&gt; {    boolean filter(T obj);}</code></pre><h4 id="7、net-HttpRequest-java"><a href="#7、net-HttpRequest-java" class="headerlink" title="7、net/HttpRequest.java"></a>7、net/HttpRequest.java</h4><pre><code>package com.hsiehchou.common.net;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.*;import java.net.HttpURLConnection;import java.net.URL;import java.net.URLConnection;import java.net.URLEncoder;import java.util.Map;public class HttpRequest {    private static final Logger LOG = LoggerFactory.getLogger(HttpRequest.class);    /**     * 向指定URL发送GET方法的请求     * @param url  发送请求的URL     * @param param  请求参数，请求参数应该是 name1=value1&amp;name2=value2 的形式。     * @return URL  所代表远程资源的响应结果     */    public static String sendGet(String url, String param) {        String result = &quot;&quot;;        BufferedReader in = null;        try {            String urlNameString = url + &quot;?&quot; + param;            URL realUrl = new URL(urlNameString);            // 打开和URL之间的连接            URLConnection connection = realUrl.openConnection();            // 设置通用的请求属性            connection.setRequestProperty(&quot;accept&quot;, &quot;*/*&quot;);            connection.setRequestProperty(&quot;connection&quot;, &quot;Keep-Alive&quot;);            connection.setRequestProperty(&quot;user-agent&quot;,                    &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)&quot;);            // 建立实际的连接            connection.connect();            // 获取所有响应头字段            //Map&lt;String, List&lt;String&gt;&gt; map = connection.getHeaderFields();            // 遍历所有的响应头字段            // 定义 BufferedReader输入流来读取URL的响应            in = new BufferedReader(new InputStreamReader(connection.getInputStream(),&quot;UTF-8&quot;));            String line;            while ((line = in.readLine()) != null) {                result += line;            }        } catch (Exception e) {            LOG.info(&quot;发送GET请求出现异常！&quot; + (url+param));            System.out.println(&quot;发送GET请求出现异常！&quot; + e);            e.printStackTrace();        }        // 使用finally块来关闭输入流        finally {            try {                if (in != null) {                    in.close();                }            } catch (Exception e2) {                e2.printStackTrace();            }        }        return result;    }    /**     * 向指定URL发送GET方法的请求     * @param url  发送请求的URL     * @param param  请求参数，请求参数应该是 name1=value1&amp;name2=value2 的形式。     * @return URL 所代表远程资源的响应结果     */    public static String sendGet(String url, String param,String authorization) {        String result = &quot;&quot;;        BufferedReader in = null;        try {            String urlNameString = url + &quot;?&quot; + param;            URL realUrl = new URL(urlNameString);            // 打开和URL之间的连接            URLConnection connection = realUrl.openConnection();            // 设置通用的请求属性            connection.setRequestProperty(&quot;accept&quot;, &quot;*/*&quot;);            connection.setRequestProperty(&quot;connection&quot;, &quot;Keep-Alive&quot;);            connection.setRequestProperty(&quot;Authorization&quot;, authorization);            connection.setRequestProperty(&quot;user-agent&quot;,                    &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)&quot;);            // 建立实际的连接            connection.connect();            // 获取所有响应头字段            connection.getHeaderFields();            // 遍历所有的响应头字段/*            for (String key : map.keySet()) {                System.out.println(key + &quot;---&gt;&quot; + map.get(key));            }*/            // 定义 BufferedReader输入流来读取URL的响应            in = new BufferedReader(new InputStreamReader(                    connection.getInputStream(),&quot;UTF-8&quot;));            String line;            while ((line = in.readLine()) != null) {                result += line;            }        } catch (Exception e) {            LOG.info(&quot;发送POST请求出现异常！&quot; + (url+param));            System.out.println(&quot;发送POST请求出现异常！&quot; + e);            e.printStackTrace();        }        // 使用finally块来关闭输入流        finally {            try {                if (in != null) {                    in.close();                }            } catch (Exception e2) {                e2.printStackTrace();            }        }        return result;    }    public static void main(String[] args) throws Exception{    }    /**     * 向指定 URL 发送POST方法的请求     * @param url  发送请求的 URL     * @param param  请求参数，请求参数应该是 name1=value1&amp;name2=value2 的形式。     * @return  所代表远程资源的响应结果     */    public static String sendPost(String url, String param) {        PrintWriter out = null;        BufferedReader in = null;        String result = &quot;&quot;;        try {            URL realUrl = new URL(url);            // 打开和URL之间的连接            URLConnection conn = realUrl.openConnection();            // 设置通用的请求属性            conn.setRequestProperty(&quot;Content-Type&quot;,&quot;application/json&quot;);            //conn.setInstanceFollowRedirects(false);            // conn.setRequestProperty(&quot;Content-Type&quot;,&quot;application/x-www-form-urlencoded&quot;);            conn.setRequestProperty(&quot;accept&quot;, &quot;*/*&quot;);            conn.setRequestProperty(&quot;connection&quot;, &quot;Keep-Alive&quot;);            conn.setRequestProperty(&quot;user-agent&quot;,                    &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)&quot;);            // 发送POST请求必须设置如下两行            conn.setReadTimeout(30000);            conn.setDoOutput(true);            conn.setDoInput(true);            // 获取URLConnection对象对应的输出流            out = new PrintWriter(conn.getOutputStream());            // 发送请求参数            out.print(param);            // flush输出流的缓冲            out.flush();            // 定义BufferedReader输入流来读取URL的响应            InputStream inputStream = conn.getInputStream();            in = new BufferedReader(new InputStreamReader(inputStream,&quot;UTF-8&quot;));            String line;            while ((line = in.readLine()) != null) {                result += line;            }        }        catch (IOException e) {            LOG.info(&quot;发送POST请求出现异常！&quot; + (url+param),e);        }        //使用finally块来关闭输出流、输入流        finally{            try{                if(out!=null){                    out.close();                }                if(in!=null){                    in.close();                }            }            catch(IOException ex){                ex.printStackTrace();            }        }        return result;    }    /*     * params 填写的URL的参数 encode 字节编码     */    public static String sendPostMessage(String url1,Map&lt;String,Object&gt; params){        String response = null;        Reader in = null;        try {            //访问准备            URL url = new URL(url1);            //开始访问            StringBuilder postData = new StringBuilder();            for (Map.Entry&lt;String,Object&gt; param : params.entrySet()) {                if (postData.length() != 0) postData.append(&#39;&amp;&#39;);                postData.append(URLEncoder.encode(param.getKey(), &quot;UTF-8&quot;));                postData.append(&#39;=&#39;);                postData.append(URLEncoder.encode(String.valueOf(param.getValue()), &quot;UTF-8&quot;));            }            byte[] postDataBytes = postData.toString().getBytes(&quot;UTF-8&quot;);            URLConnection conn = url.openConnection();            //URLConnection conn = url.openConnection();            //conn.setRequestMethod(&quot;POST&quot;);            //conn.setInstanceFollowRedirects(false);            //conn.setRequestProperty(&quot;Content-Type&quot;, &quot;application/x-www-form-urlencoded&quot;);            conn.setRequestProperty(&quot;Content-Type&quot;, &quot;application/json&quot;);            conn.setRequestProperty(&quot;Content-Length&quot;, String.valueOf(postDataBytes.length));            conn.setDoOutput(true);            conn.getOutputStream().write(postDataBytes);            in = new BufferedReader(new InputStreamReader(conn.getInputStream(), &quot;UTF-8&quot;));            StringBuilder sb = new StringBuilder();            for (int c; (c = in.read()) &gt;= 0;)                sb.append((char)c);            response = sb.toString();           //System.out.println(response);        } catch (IOException e) {            LOG.error(null,e);        }finally {            if(in != null){                try {                    in.close();                } catch (IOException e) {                    e.printStackTrace();                }            }        }        return response;    }    /**     * 向指定 URL 发送POST方法的请求     * @param url  发送请求的 URL     * @param param  请求参数，请求参数应该是 name1=value1&amp;name2=value2 的形式。     * @return  所代表远程资源的响应结果     */    public static void sendPostWithoutReturn(String url, String param) {        PrintWriter out = null;        BufferedReader in = null;        String result = &quot;&quot;;        try {            URL realUrl = new URL(url);            // 打开和URL之间的连接            HttpURLConnection conn = (HttpURLConnection )realUrl.openConnection();            // 设置通用的请求属性            conn.setRequestProperty(&quot;Content-Type&quot;,&quot;application/json&quot;);            conn.setRequestProperty(&quot;accept&quot;, &quot;*/*&quot;);            conn.setRequestProperty(&quot;connection&quot;, &quot;Keep-Alive&quot;);            conn.setRequestProperty(&quot;user-agent&quot;,                    &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)&quot;);            //根据需求设置读超时的时间            conn.setReadTimeout(1000);            // 发送POST请求必须设置如下两行            conn.setDoOutput(true);            conn.setDoInput(true);            // 获取URLConnection对象对应的输出流            out = new PrintWriter(conn.getOutputStream());            // 发送请求参数            out.print(param);            // flush输出流的缓冲            out.flush();            // 定义BufferedReader输入流来读取URL的响应            if (conn.getResponseCode() == 200) {                System.out.println(&quot;连接成功,传送数据...&quot;);            } else {                System.out.println(&quot;连接失败,错误代码:&quot;+conn.getResponseCode());            }        }        catch (IOException e) {            LOG.info(&quot;发送POST请求出现异常！&quot; + (url+param),e);        }        //使用finally块来关闭输出流、输入流        finally{            try{                if(out!=null){                    out.close();                }                in.close();            }            catch(Exception ex){                ex.printStackTrace();            }        }    }}</code></pre><h4 id="8、netb-db-DBCommon—mysql的连接、关闭基础类"><a href="#8、netb-db-DBCommon—mysql的连接、关闭基础类" class="headerlink" title="8、netb/db/DBCommon—mysql的连接、关闭基础类"></a>8、netb/db/DBCommon—mysql的连接、关闭基础类</h4><pre><code>package com.hsiehchou.common.netb.db;import com.hsiehchou.common.config.ConfigUtil;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.sql.*;import java.util.Properties;public class DBCommon {    private static Logger LOG = LoggerFactory.getLogger(DBCommon.class);    private static String MYSQL_PATH = &quot;common/mysql.properties&quot;;    private static Properties properties = ConfigUtil.getInstance().getProperties(MYSQL_PATH);    private static Connection conn ;    private DBCommon(){}    public static void main(String[] args) {        System.out.println(properties);        Connection xz_bigdata = DBCommon.getConn(&quot;test&quot;);        System.out.println(xz_bigdata);    }    //TODO  配置文件    private static final String JDBC_DRIVER = &quot;com.mysql.jdbc.Driver&quot;;    private static final String USER_NAME = properties.getProperty(&quot;user&quot;);    private static final String PASSWORD = properties.getProperty(&quot;password&quot;);    private static final String IP = properties.getProperty(&quot;db_ip&quot;);    private static final String PORT = properties.getProperty(&quot;db_port&quot;);    private static final String DB_CONFIG = &quot;?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull&amp;autoReconnect=true&amp;failOverReadOnly=false&quot;;    static {        try {            Class.forName(JDBC_DRIVER);        } catch (ClassNotFoundException e) {            LOG.error(null, e);        }    }    /**     * 获取数据库连接     * @param dbName     * @return     */    public static Connection getConn(String dbName) {        Connection conn = null;        String  connstring = &quot;jdbc:mysql://&quot;+IP+&quot;:&quot;+PORT+&quot;/&quot;+dbName+DB_CONFIG;        try {            conn = DriverManager.getConnection(connstring, USER_NAME, PASSWORD);        } catch (SQLException e) {            e.printStackTrace();            LOG.error(null, e);        }        return conn;    }    /**     * @param url eg:&quot;jdbc:oracle:thin:@172.16.1.111:1521:d406&quot;     * @param driver eg:&quot;oracle.jdbc.driver.OracleDriver&quot;     * @param user eg:&quot;ucase&quot;     * @param password eg:&quot;ucase123&quot;     * @return     * @throws ClassNotFoundException     * @throws SQLException     */    public static Connection getConn(String url, String driver, String user,                                     String password) throws ClassNotFoundException, SQLException{        Class.forName(driver);        conn = DriverManager.getConnection(url, user, password);        return  conn;    }    public static void close(Connection conn){        try {            if( conn != null ){                conn.close();            }        } catch (SQLException e) {            LOG.error(null,e);        }    }    public static void close(Statement statement){        try {            if( statement != null ){                statement.close();            }        } catch (SQLException e) {            LOG.error(null,e);        }    }    public static void close(Connection conn,PreparedStatement statement){        try {            if( conn != null ){                conn.close();            }            if( statement != null ){                statement.close();            }        } catch (SQLException e) {            LOG.error(null,e);        }    }    public static void close(Connection conn,Statement statement,ResultSet resultSet) throws SQLException{        if( resultSet != null ){            resultSet.close();        }        if( statement != null ){            statement.close();        }        if( conn != null ){            conn.close();        }    }}</code></pre><h4 id="9、project-datatype-DataTypeProperties-java"><a href="#9、project-datatype-DataTypeProperties-java" class="headerlink" title="9、project/datatype/DataTypeProperties.java"></a>9、project/datatype/DataTypeProperties.java</h4><pre><code>package com.hsiehchou.common.project.datatype;import com.hsiehchou.common.config.ConfigUtil;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.*;public class DataTypeProperties {    private static final Logger logger = LoggerFactory.getLogger(DataTypeProperties.class);    private static final String DATA_PATH = &quot;common/datatype.properties&quot;;    public static Map&lt;String,ArrayList&lt;String&gt;&gt; dataTypeMap = null;    static {        Properties properties = ConfigUtil.getInstance().getProperties(DATA_PATH);        dataTypeMap = new HashMap&lt;&gt;();        Set&lt;Object&gt; keys = properties.keySet();        keys.forEach(key-&gt;{            String[] split = properties.getProperty(key.toString()).split(&quot;,&quot;);            dataTypeMap.put(key.toString(),new ArrayList&lt;&gt;(Arrays.asList(split)));        });    }    public static void main(String[] args) {        Map&lt;String, ArrayList&lt;String&gt;&gt; dataTypeMap = DataTypeProperties.dataTypeMap;        System.out.println(dataTypeMap.toString());    }}</code></pre><h4 id="10、regex-Validation-java—验证工具类"><a href="#10、regex-Validation-java—验证工具类" class="headerlink" title="10、regex/Validation.java—验证工具类"></a>10、regex/Validation.java—验证工具类</h4><pre><code>package com.hsiehchou.common.regex;import java.util.regex.Matcher;import java.util.regex.Pattern;/** * 验证工具类 */public class Validation {    // ------------------常量定义    /**     * Email正则表达式=     * &quot;^([a-z0-9A-Z]+[-|\\.]?)+[a-z0-9A-Z]@([a-z0-9A-Z]+(-[a-z0-9A-Z]+)?\\.)+[a-zA-Z]{2,}$&quot;     * ;     */    // public static final String EMAIL =    // &quot;^([a-z0-9A-Z]+[-|\\.]?)+[a-z0-9A-Z]@([a-z0-9A-Z]+(-[a-z0-9A-Z]+)?\\.)+[a-zA-Z]{2,}$&quot;;;    public static final String EMAIL = &quot;\\w+(\\.\\w+)*@\\w+(\\.\\w+)+&quot;;    /**     * 电话号码正则表达式=     * (^(\d{2,4}[-_－—]?)?\d{3,8}([-_－—]?\d{3,8})?([-_－—]?\d{1,7})?$)|     * (^0?1[35]\d{9}$)     */    public static final String PHONE = &quot;(^(\\d{2,4}[-_－—]?)?\\d{3,8}([-_－—]?\\d{3,8})?([-_－—]?\\d{1,7})?$)|(^0?1[35]\\d{9}$)&quot;;    /**     * 手机号码正则表达式=^(13[0-9]|15[0-9]|18[0-9])\d{8}$     */    public static final String MOBILE = &quot;^((13[0-9])|(14[5-7])|(15[^4])|(17[0-8])|(18[0-9]))\\d{8}$&quot;;    /**     * Integer正则表达式 ^-?(([1-9]\d*$)|0)     */    public static final String INTEGER = &quot;^-?(([1-9]\\d*$)|0)&quot;;    /**     * 正整数正则表达式 &gt;=0 ^[1-9]\d*|0$     */    public static final String INTEGER_NEGATIVE = &quot;^[1-9]\\d*|0$&quot;;    /**     * 负整数正则表达式 &lt;=0 ^-[1-9]\d*|0$     */    public static final String INTEGER_POSITIVE = &quot;^-[1-9]\\d*|0$&quot;;    /**     * Double正则表达式 ^-?([1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0)$     */    public static final String DOUBLE = &quot;^-?([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0)$&quot;;    /**     * 正Double正则表达式 &gt;=0 ^[1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0$　     */    public static final String DOUBLE_NEGATIVE = &quot;^[1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0$&quot;;    /**     * 负Double正则表达式 &lt;= 0 ^(-([1-9]\d*\.\d*|0\.\d*[1-9]\d*))|0?\.0+|0$     */    public static final String DOUBLE_POSITIVE = &quot;^(-([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*))|0?\\.0+|0$&quot;;    /**     * 年龄正则表达式 ^(?:[1-9][0-9]?|1[01][0-9]|120)$ 匹配0-120岁     */    public static final String AGE = &quot;^(?:[1-9][0-9]?|1[01][0-9]|120)$&quot;;    /**     * 邮编正则表达式 [0-9]\d{5}(?!\d) 国内6位邮编     */    public static final String CODE = &quot;[0-9]\\d{5}(?!\\d)&quot;;    /**     * 匹配由数字、26个英文字母或者下划线组成的字符串 ^\w+$     */    public static final String STR_ENG_NUM_ = &quot;^\\w+$&quot;;    /**     * 匹配由数字和26个英文字母组成的字符串 ^[A-Za-z0-9]+$     */    public static final String STR_ENG_NUM = &quot;^[A-Za-z0-9]+&quot;;    /**     * 匹配由26个英文字母组成的字符串 ^[A-Za-z]+$     */    public static final String STR_ENG = &quot;^[A-Za-z]+$&quot;;    /**     * 过滤特殊字符串正则 regEx=     * &quot;[`~!@#$%^&amp;*()+=|{}&#39;:;&#39;,\\[\\].&lt;&gt;/?~！@#￥%……&amp;*（）——+|{}【】‘；：”“’。，、？]&quot;;     */    public static final String STR_SPECIAL = &quot;[`~!@#$%^&amp;*()+=|{}&#39;:;&#39;,\\[\\].&lt;&gt;/?~！@#￥%……&amp;*（）——+|{}【】‘；：”“’。，、？]&quot;;    /***     * 日期正则 支持： YYYY-MM-DD YYYY/MM/DD YYYY_MM_DD YYYYMMDD YYYY.MM.DD的形式     */    public static final String DATE_ALL = &quot;((^((1[8-9]\\d{2})|([2-9]\\d{3}))([-\\/\\._]?)(10|12|0?[13578])([-\\/\\._]?)(3[01]|[12][0-9]|0?[1-9])$)&quot;            + &quot;|(^((1[8-9]\\d{2})|([2-9]\\d{3}))([-\\/\\._]?)(11|0?[469])([-\\/\\._]?)(30|[12][0-9]|0?[1-9])$)&quot;            + &quot;|(^((1[8-9]\\d{2})|([2-9]\\d{3}))([-\\/\\._]?)(0?2)([-\\/\\._]?)(2[0-8]|1[0-9]|0?[1-9])$)|(^([2468][048]00)([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|(^([3579][26]00)&quot;            + &quot;([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)&quot;            + &quot;|(^([1][89][0][48])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|(^([2-9][0-9][0][48])([-\\/\\._]?)&quot;            + &quot;(0?2)([-\\/\\._]?)(29)$)&quot;            + &quot;|(^([1][89][2468][048])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|(^([2-9][0-9][2468][048])([-\\/\\._]?)(0?2)&quot;            + &quot;([-\\/\\._]?)(29)$)|(^([1][89][13579][26])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|&quot;            + &quot;(^([2-9][0-9][13579][26])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$))&quot;;    /***     * 日期正则 支持： YYYY-MM-DD     */    public static final String DATE_FORMAT1 = &quot;(([0-9]{3}[1-9]|[0-9]{2}[1-9][0-9]{1}|[0-9]{1}[1-9][0-9]{2}|[1-9][0-9]{3})-(((0[13578]|1[02])-(0[1-9]|[12][0-9]|3[01]))|((0[469]|11)-(0[1-9]|[12][0-9]|30))|(02-(0[1-9]|[1][0-9]|2[0-8]))))|((([0-9]{2})(0[48]|[2468][048]|[13579][26])|((0[48]|[2468][048]|[3579][26])00))-02-29)&quot;;    /**     * URL正则表达式 匹配 http www ftp     */    public static final String URL = &quot;^(http|www|ftp|)?(://)?(\\w+(-\\w+)*)(\\.(\\w+(-\\w+)*))*((:\\d+)?)(/(\\w+(-\\w+)*))*(\\.?(\\w)*)(\\?)?&quot;            + &quot;(((\\w*%)*(\\w*\\?)*(\\w*:)*(\\w*\\+)*(\\w*\\.)*(\\w*&amp;)*(\\w*-)*(\\w*=)*(\\w*%)*(\\w*\\?)*&quot;            + &quot;(\\w*:)*(\\w*\\+)*(\\w*\\.)*&quot;            + &quot;(\\w*&amp;)*(\\w*-)*(\\w*=)*)*(\\w*)*)$&quot;;    /**     * 身份证正则表达式     */    public static final String IDCARD = &quot;((11|12|13|14|15|21|22|23|31|32|33|34|35|36|37|41|42|43|44|45|46|50|51|52|53|54|61|62|63|64|65)[0-9]{4})&quot;            + &quot;(([1|2][0-9]{3}[0|1][0-9][0-3][0-9][0-9]{3}&quot;            + &quot;[Xx0-9])|([0-9]{2}[0|1][0-9][0-3][0-9][0-9]{3}))&quot;;    /**     * 机构代码     */    public static final String JIGOU_CODE = &quot;^[A-Z0-9]{8}-[A-Z0-9]$&quot;;    /**     * 匹配数字组成的字符串 ^[0-9]+$     */    public static final String STR_NUM = &quot;^[0-9]+$&quot;;    // //------------------验证方法    /**     * 判断字段是否为空 符合返回ture     * @param str     * @return boolean     */    public static synchronized boolean StrisNull(String str) {        return null == str || str.trim().length() &lt;= 0 ? true : false;    }    /**     * 判断字段是非空 符合返回ture     * @param str     * @return boolean     */    public static boolean StrNotNull(String str) {        return !StrisNull(str);    }    /**     * 字符串null转空     * @param str     * @return boolean     */    public static String nulltoStr(String str) {        return StrisNull(str) ? &quot;&quot; : str;    }    /**     * 字符串null赋值默认值     * @param str  目标字符串     * @param defaut  默认值     * @return  String     */    public static String nulltoStr(String str, String defaut) {        return StrisNull(str) ? defaut : str;    }    /**     * 判断字段是否为Email 符合返回ture     * @param str     * @return boolean     */    public static boolean isEmail(String str) {        return Regular(str, EMAIL);    }    /**     * 判断是否为电话号码 符合返回ture     * @param str     * @return boolean     */    public static boolean isPhone(String str) {        return Regular(str, PHONE);    }    /**     * 判断是否为手机号码 符合返回ture     * @param str     * @return boolean     */    public static boolean isMobile(String str) {        return RegularSJHM(str, MOBILE);    }    /**     * 判断是否为Url 符合返回ture     * @param str     * @return boolean     */    public static boolean isUrl(String str) {        return Regular(str, URL);    }    /**     * 判断字段是否为数字 正负整数 正负浮点数 符合返回ture     * @param str     * @return boolean     */    public static boolean isNumber(String str) {        return Regular(str, DOUBLE);    }    /**     * 判断字段是否为INTEGER 符合返回ture     * @param str     * @return boolean     */    public static boolean isInteger(String str) {        return Regular(str, INTEGER);    }    /**     * 判断字段是否为正整数正则表达式 &gt;=0 符合返回ture     * @param str     * @return boolean     */    public static boolean isINTEGER_NEGATIVE(String str) {        return Regular(str, INTEGER_NEGATIVE);    }    /**     * 判断字段是否为负整数正则表达式 &lt;=0 符合返回ture     * @param str     * @return boolean     */    public static boolean isINTEGER_POSITIVE(String str) {        return Regular(str, INTEGER_POSITIVE);    }    /**     * 判断字段是否为DOUBLE 符合返回ture     * @param str     * @return boolean     */    public static boolean isDouble(String str) {        return Regular(str, DOUBLE);    }    /**     * 判断字段是否为正浮点数正则表达式 &gt;=0 符合返回ture     * @param str     * @return boolean     */    public static boolean isDOUBLE_NEGATIVE(String str) {        return Regular(str, DOUBLE_NEGATIVE);    }    /**     * 判断字段是否为负浮点数正则表达式 &lt;=0 符合返回ture     * @param str     * @return boolean     */    public static boolean isDOUBLE_POSITIVE(String str) {        return Regular(str, DOUBLE_POSITIVE);    }    /**     * 判断字段是否为日期 符合返回ture     * @param str     * @return boolean     */    public static boolean isDate(String str) {        return Regular(str, DATE_ALL);    }    /**     * 验证2010-12-10     * @param str     * @return     */    public static boolean isDate1(String str) {        return Regular(str, DATE_FORMAT1);    }    /**     * 判断字段是否为年龄 符合返回ture     * @param str     * @return boolean     */    public static boolean isAge(String str) {        return Regular(str, AGE);    }    /**     * 判断字段是否超长 字串为空返回fasle, 超过长度{leng}返回ture 反之返回false     * @param str     * @param leng     * @return boolean     */    public static boolean isLengOut(String str, int leng) {        return StrisNull(str) ? false : str.trim().length() &gt; leng;    }    /**     * 判断字段是否为身份证 符合返回ture     * @param str     * @return boolean     */    public static boolean isIdCard(String str) {        if (StrisNull(str))            return false;        if (str.trim().length() == 15 || str.trim().length() == 18) {            return Regular(str, IDCARD);        } else {            return false;        }    }    /**     * 判断字段是否为邮编 符合返回ture     * @param str     * @return boolean     */    public static boolean isCode(String str) {        return Regular(str, CODE);    }    /**     * 判断字符串是不是全部是英文字母     * @param str     * @return boolean     */    public static boolean isEnglish(String str) {        return Regular(str, STR_ENG);    }    /**     * 判断字符串是不是全部是英文字母+数字     * @param str     * @return boolean     */    public static boolean isENG_NUM(String str) {        return Regular(str, STR_ENG_NUM);    }    /**     * 判断字符串是不是全部是英文字母+数字+下划线     * @param str     * @return boolean     */    public static boolean isENG_NUM_(String str) {        return Regular(str, STR_ENG_NUM_);    }    /**     * 过滤特殊字符串 返回过滤后的字符串     * @param str     * @return boolean     */    public static String filterStr(String str) {        Pattern p = Pattern.compile(STR_SPECIAL);        Matcher m = p.matcher(str);        return m.replaceAll(&quot;&quot;).trim();    }    /**     * 校验机构代码格式     * @return     */    public static boolean isJigouCode(String str) {        return Regular(str, JIGOU_CODE);    }    /**     * 判断字符串是不是数字组成     * @param str     * @return boolean     */    public static boolean isSTR_NUM(String str) {        return Regular(str, STR_NUM);    }    /**     * 匹配是否符合正则表达式pattern 匹配返回true     * @param str 匹配的字符串     * @param pattern 匹配模式     * @return boolean     */    private static boolean Regular(String str, String pattern) {        if (null == str || str.trim().length() &lt;= 0)            return false;        Pattern p = Pattern.compile(pattern);        Matcher m = p.matcher(str);        return m.matches();    }    /**     * 匹配是否符合正则表达式pattern 匹配返回true     * @param str 匹配的字符串     * @param pattern 匹配模式     * @return boolean     */    private static boolean RegularSJHM(String str, String pattern) {        if (null == str || str.trim().length() &lt;= 0){            return false;        }        if(str.contains(&quot;+86&quot;)){            str=str.replace(&quot;+86&quot;,&quot;&quot;);        }        Pattern p = Pattern.compile(pattern);        Matcher m = p.matcher(str);        return m.matches();    }    /**     * description:匹配yyyyMMddHHmmss格式时间     * @param time     * @return boolean 2016-7-19 下午5:13:25 by      */    public static final String yyyyMMddHHmmss = &quot;[0-9]{14}&quot;;    public static boolean isyyyyMMddHHmmss(String time) {        if (time == null) {            return false;        }        boolean bool = time.matches(yyyyMMddHHmmss);        return bool;    }    /**     * description:匹配yyyyMMddHHmmss格式时间     * @param time     * @return boolean 2016-7-19 下午5:13:25 by      */    public static final String isMac = &quot;^[A-F0-9]{2}(-[A-F0-9]{2}){5}$&quot;;    public static boolean isMac(String mac) {        if (mac == null) {            return false;        }        boolean bool = mac.matches(isMac);        return bool;    }    /**     * description:匹配yyyyMMddHHmmss格式时间     * @param time     * @return boolean 2016-7-19 下午5:13:25 by      */    public static final String longtime = &quot;[0-9]{10}&quot;;    public static boolean isTimestamp(String timestamp) {        if (timestamp == null) {            return false;        }        boolean bool = timestamp.matches(longtime);        return bool;    }    /**     * 判断字段是否为datatype 符合返回ture     * @param str     * @return boolean     */    public static final String DATATYPE = &quot;^\\d{7}$&quot;;    public static boolean isDATATYPE(String str) {        return Regular(str, DATATYPE);    }    /**     * 判断字段是否为QQ 符合返回ture     * @param str     * @return boolean     */    public static final String QQ = &quot;^\\d{5,15}$&quot;;    public static boolean isQQ(String str) {        return Regular(str, QQ);    }    /**     * 判断字段是否为IMSI 符合返回ture     * @param str     * @return boolean     */    //public static final String IMSI = &quot;^4600[0,1,2,3,4,5,6,7,9]\\d{10}|(46011|46020)\\d{10}$&quot;;    public static final String IMSI = &quot;^[1-9][0-9][0-9]0[0,1,2,3,4,5,6,7,9]\\d{10}|[1-9][0-9][0-9](11|20)\\d{10}$&quot;;    public static boolean isIMSI(String str) {        return Regular(str, IMSI);    }    /**     * 判断字段是否为IMEI 符合返回ture     * @param str     * @return boolean     */    public static final String IMEI = &quot;^\\d{8}$|^[a-fA-F0-9]{14}$|^\\d{15}$&quot;;    public static boolean isIMEI(String str) {return Regular(str, IMEI);}    /**     * 判断字段是否为CAPTURETIME 符合返回ture     * @param str     * @return boolean     */    public static final String CAPTURETIME = &quot;^\\d{10}|(20[0-9][0-9])\\d{10}$&quot;;    public static boolean isCAPTURETIME(String str) {return Regular(str, CAPTURETIME);}    /**     * description:检测认证类型     * @param auth     * @return boolean     */    public static final String AUTH_TYPE = &quot;^\\d{7}$&quot;;    public static boolean isAUTH_TYPE(String str) {return Regular(str, CAPTURETIME);}    /**     * description:检测FIRM_CODE     * @param auth     * @return boolean     */    public static final String FIRM_CODE = &quot;^\\d{9}$&quot;;    public static boolean isFIRM_CODE(String str) {return Regular(str, FIRM_CODE);}    /**     * description:检测经度     * @param auth     * @return boolean     */    public static final String LONGITUDE = &quot;^-?(([1-9]\\d?)|(1[0-7]\\d)|180)(\\.\\d{1,8})?$&quot;;    //public static final String LONGITUDE =&quot;^([-]?(\\d|([1-9]\\d)|(1[0-7]\\d)|(180))(\\.\\d*)\\,[-]?(\\d|([1-8]\\d)|(90))(\\.\\d*))$&quot;;    public static boolean isLONGITUDE(String str) {return Regular(str, LONGITUDE);}    /**     * description:检测纬度     * @param auth     * @return boolean     */    public static final String LATITUDE = &quot;^-?(([1-8]\\d?)|([1-8]\\d)|90)(\\.\\d{1,8})?$&quot;;    public static boolean isLATITUDE(String str) {return Regular(str, LATITUDE);}    public static void main(String[] args) {        boolean bool = isLATITUDE(&quot;26.0615854&quot;);        System.out.println(bool);    }}</code></pre><h4 id="11、thread-ThreadPoolManager-java—线程池管理器单例"><a href="#11、thread-ThreadPoolManager-java—线程池管理器单例" class="headerlink" title="11、thread/ThreadPoolManager.java—线程池管理器单例"></a>11、thread/ThreadPoolManager.java—线程池管理器单例</h4><pre><code>package com.hsiehchou.common.thread;import java.io.Serializable;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;/** *     线程池管理器单例 *  默认创建   ewCachedThreadPool ：创建一个可缓存的线程池 *  可通过指定线程的数量来创建：newFixedThreadPool  ： 创建固定大小的线程池 */public class ThreadPoolManager implements Serializable {    private static final long serialVersionUID = 1465361469484903956L;    public static final ThreadPoolManager threadPoolManager =  new ThreadPoolManager();    private static ThreadPoolManager tpm;    private transient ExecutorService newCachedThreadPool;    private transient ExecutorService newFixedThreadPool;    private int poolCapacity;    private ThreadPoolManager(){        if( newCachedThreadPool == null )            newCachedThreadPool = Executors.newCachedThreadPool();    }    @Deprecated    public static ThreadPoolManager getInstance(){        if( tpm == null ){            synchronized(ThreadPoolManager.class){            if( tpm == null )                tpm =  new ThreadPoolManager();            }        }        return tpm;    }    /**      * 返回 newCachedThreadPool     */    public ExecutorService getExecutorService(){        if( newCachedThreadPool == null ){            synchronized(ThreadPoolManager.class){                if( newCachedThreadPool == null )                    newCachedThreadPool = Executors.newCachedThreadPool();            }        }        return newCachedThreadPool;    }    /**       * 返回 newFixedThreadPool     */    public ExecutorService getExecutorService(int poolCapacity){        return getExecutorService(poolCapacity, false);    }    /**      * 返回 newFixedThreadPool     */    public synchronized ExecutorService getExecutorService(int poolCapacity, boolean closeOld){        if(newFixedThreadPool == null || (this.poolCapacity != poolCapacity)){            if(newFixedThreadPool != null &amp;&amp; closeOld){                newFixedThreadPool.shutdown();            }            newFixedThreadPool = Executors.newFixedThreadPool(poolCapacity);            this.poolCapacity = poolCapacity;        }        return newFixedThreadPool;    }}</code></pre><h4 id="12、time-TimeTranstationUtils-java—时间转换工具类"><a href="#12、time-TimeTranstationUtils-java—时间转换工具类" class="headerlink" title="12、time/TimeTranstationUtils.java—时间转换工具类"></a>12、time/TimeTranstationUtils.java—时间转换工具类</h4><pre><code>package com.hsiehchou.common.time;import org.apache.commons.lang.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;import java.util.HashMap;import java.util.Map;/** * Description: 时间转换工具类 */public class TimeTranstationUtils {    private static final Logger logger = LoggerFactory.getLogger(TimeTranstationUtils.class);/*    private static SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);    private static SimpleDateFormat sdFormatternew = new SimpleDateFormat(&quot;yyyyMMddHH&quot;);    private static SimpleDateFormat sdFormatter1 = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);    private static SimpleDateFormat sdFormatter2 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);    private static SimpleDateFormat sdFormatter3 = new SimpleDateFormat(&quot;yyyyMMdd&quot;);*/    private static Date nowTime;    public static String Date2yyyyMMddHHmmss() {        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);        nowTime = new Date(System.currentTimeMillis());        String time = sdFormatter.format(nowTime);        return time;    }    public static String Date2yyyyMMddHHmmss(long timestamp) {        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);        nowTime = new Date(timestamp);        String time = sdFormatter.format(nowTime);        return time;    }    public static String Date2yyyyMMdd(long timestamp) {        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMdd&quot;);        nowTime = new Date(timestamp);        String time = sdFormatter.format(nowTime);        return time;    }    public static String Date2yyyyMMddHH(String str) {        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);        SimpleDateFormat sdFormatternew = new SimpleDateFormat(&quot;yyyyMMddHH&quot;);        try {            nowTime = sdFormatter.parse(str);        } catch (ParseException e) {            e.printStackTrace();        }        String time = sdFormatternew.format(nowTime);        return time;    }    public static String Date2yyyy_MM_dd() {        SimpleDateFormat sdFormatter1 = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);        nowTime = new Date(System.currentTimeMillis());        String time = sdFormatter1.format(nowTime);        return time;    }    public static String Date2yyyy_MM_dd_HH_mm_ss() {        SimpleDateFormat sdFormatter2 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);        nowTime = new Date(System.currentTimeMillis());        String time = sdFormatter2.format(nowTime);        return time;    }    public static String Date2yyyyMMdd() {        SimpleDateFormat sdFormatter3 = new SimpleDateFormat(&quot;yyyyMMdd&quot;);        nowTime = new Date(System.currentTimeMillis());        String time = sdFormatter3.format(nowTime);        return time;    }    public static String Date2yyyyMMdd(String str) {        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);        SimpleDateFormat sdFormatter3 = new SimpleDateFormat(&quot;yyyyMMdd&quot;);        try {            nowTime = sdFormatter.parse(str);        } catch (ParseException e) {            e.printStackTrace();        }        String time = sdFormatter3.format(nowTime);        return time;    }    public static Long Date2yyyyMMddHHmmssToLong() {        return System.currentTimeMillis() / 1000;    }    public static String long2date(String capturetime){        SimpleDateFormat sdf= new SimpleDateFormat(&quot;yyyyMMdd&quot;);        //前面的lSysTime是秒数，先乘1000得到毫秒数，再转为java.util.Date类型        Date dt = new Date(Long.valueOf(capturetime) * 1000);        String sDateTime = sdf.format(dt);  //得到精确到秒的表示：08/31/2006 21:08:00        return sDateTime;    }    public static Long yyyyMMddHHmmssToLong(String time) {        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);        if (StringUtils.isBlank(time)) {            return 0L;        } else {            boolean isNum = time.matches(&quot;[0-9]+&quot;);            if (isNum) {                long long1 = 0;                try {                    long1 = sdFormatter.parse(time).getTime();                } catch (ParseException e) {                    logger.error(time + &quot;时间转换为long错误&quot; + isNum);                    return 0L;                }                return long1 / 1000;            }        }        return 0L;    }    public static Date yyyyMMddHHmmssToDate(String time) {        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);        if (StringUtils.isBlank(time)) {            return new Date();        } else {            boolean isNum = time.matches(&quot;[0-9]+&quot;);            if (isNum) {                Date date = null;                try {                    date = sdFormatter.parse(time);                } catch (ParseException e) {                    logger.error(time + &quot;时间转换为date错误&quot; + isNum, e);                    System.out.println(time);                    System.out.println(isNum);                    e.printStackTrace();                }                return date;            }        }        return new Date();    }    public static Date yyyyMMddHHmmssToDate() {        Date date = null;        SimpleDateFormat sdFormatter2 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);        try {            date = sdFormatter2.parse(Date2yyyy_MM_dd_HH_mm_ss());        } catch (ParseException e) {            // TODO Auto-generated catch block            e.printStackTrace();        }        return date;    }    public static java.sql.Date strToDate(String strDate) {        String str = strDate;        SimpleDateFormat format = new SimpleDateFormat(&quot;yyyy-mm-dd&quot;);        Date d = null;        try {            d = format.parse(str);        } catch (Exception e) {            e.printStackTrace();        }        java.sql.Date date = new java.sql.Date(d.getTime());        return date;    }    public static Long str2Long(String str){        if(!StringUtils.isBlank(str)){            return Long.valueOf(str);        }else{            return 0L;        }    }    public static Double str2Double(String str){        if(!StringUtils.isBlank(str)){            return Double.valueOf(str);        }else{            return 0.0;        }    }    public static HashMap&lt;String,Object&gt; mapString2Long(Map&lt;String,String&gt; map, String key, HashMap&lt;String,Object&gt; objectMap) {        String logouttime = map.get(key);        if (!StringUtils.isBlank(logouttime)) {            objectMap.put(key, Long.valueOf(logouttime));        } else {            objectMap.put(key, 0L);        }        return objectMap;    }    public static void main(String[] args) throws InterruptedException {        System.out.println(long2date(&quot;1463487992&quot;));    }}</code></pre><h3 id="四、Resources开发"><a href="#四、Resources开发" class="headerlink" title="四、Resources开发"></a>四、Resources开发</h3><h4 id="xz-bigdata-resources结构"><a href="#xz-bigdata-resources结构" class="headerlink" title="xz_bigdata_resources结构"></a>xz_bigdata_resources结构</h4><p><img src="/medias/xz_bigdata_resources%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84.PNG" alt="xz_bigdata_resources整体结构"></p><p>注意：这里的resources要选中右键，选择Make Directory as，选择下级的Resources Root，变成Resources配置源文件，项目可以任意调用。</p><h4 id="1、resources下面"><a href="#1、resources下面" class="headerlink" title="1、resources下面"></a>1、resources下面</h4><p><strong>log4j2.properties</strong></p><pre><code>log4j.rootLogger = error,stdout,D,Elog4j.appender.stdout = org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.Target = System.outlog4j.appender.stdout.layout = org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern = [%-5p] %d{yyyy-MM-dd HH:mm:ss,SSS} method:%l%n%m%nlog4j.appender.D = org.apache.log4j.DailyRollingFileAppenderlog4j.appender.D.File = F://logs/log.loglog4j.appender.D.Append = truelog4j.appender.D.Threshold = DEBUG log4j.appender.D.layout = org.apache.log4j.PatternLayoutlog4j.appender.D.layout.ConversionPattern = %-d{yyyy-MM-dd HH:mm:ss}  [ %t:%r ] - [ %p ]  %m%nlog4j.appender.E = org.apache.log4j.DailyRollingFileAppenderlog4j.appender.E.File =F://logs/error.log log4j.appender.E.Append = truelog4j.appender.E.Threshold = ERROR log4j.appender.E.layout = org.apache.log4j.PatternLayoutlog4j.appender.E.layout.ConversionPattern = %-d{yyyy-MM-dd HH:mm:ss}  [ %t:%r ] - [ %p ]  %m%n</code></pre><h4 id="2、common"><a href="#2、common" class="headerlink" title="2、common"></a>2、common</h4><p><strong>datatype.properties</strong></p><pre><code># base = datatype,idcard,name,age,collecttime,imei# wechat = datatype,wechat,phone,collecttime,imeiwechat = imei,imsi,longitude,latitude,phone_mac,device_mac,device_number,collect_time,username,phone,object_username,send_message,accept_message,message_timemail = imei,imsi,longitude,latitude,phone_mac,device_mac,device_number,collect_time,send_mail,send_time,accept_mail,accept_time,mail_content,mail_typeqq = imei,imsi,longitude,latitude,phone_mac,device_mac,device_number,collect_time,username,phone,object_username,send_message,accept_message,message_time</code></pre><p><strong>mysql.properties</strong></p><pre><code>db_ip = 192.168.116.201db_port = 3306user = rootpassword = root</code></pre><h4 id="3、es"><a href="#3、es" class="headerlink" title="3、es"></a>3、es</h4><p><strong>es_cluster.properties</strong></p><pre><code>es.cluster.name=xz_eses.cluster.nodes = hadoop1,hadoop2,hadoop3es.cluster.nodes1 = hadoop1es.cluster.nodes2 = hadoop2es.cluster.nodes3 = hadoop3es.cluster.tcp.port = 9300es.cluster.http.port = 9200</code></pre><p><strong>mapping/base.json</strong></p><pre><code>{  &quot;_source&quot;: {    &quot;enabled&quot;: true  },  &quot;properties&quot;: {    &quot;datatype&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;idcard&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;name&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;age&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;collecttime&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;}  }}</code></pre><p><strong>mapping/fieldmapping.properties</strong></p><pre><code>tables = wechat,mail,qqwechat.imei = stringwechat.imsi = stringwechat.longitude = doublewechat.latitude = doublewechat.phone_mac = stringwechat.device_mac = stringwechat.device_number = stringwechat.collect_time = longwechat.username = stringwechat.phone = stringwechat.object_username = stringwechat.send_message = stringwechat.accept_message = stringwechat.message_time = longwechat.id = stringwechat.table = stringwechat.filename = stringwechat.absolute_filename  = stringmail.imei = stringmail.imsi = stringmail.longitude = doublemail.latitude = doublemail.phone_mac = stringmail.device_mac = stringmail.device_number = stringmail.collect_time = longmail.send_mail = stringmail.send_time = longmail.accept_mail = stringmail.accept_time = longmail.mail_content = stringmail.mail_type = stringmail.id = stringmail.table = stringmail.filename = stringmail.absolute_filename  = stringqq.imei = stringqq.imsi = stringqq.longitude = doubleqq.latitude = doubleqq.phone_mac = stringqq.device_mac = stringqq.device_number = stringqq.collect_time = longqq.username = stringqq.phone = stringqq.object_username = stringqq.send_message = stringqq.accept_message = stringqq.message_time = longqq.id = stringqq.table = stringqq.filename = stringqq.absolute_filename  = string</code></pre><p><strong>mapping/mail.json</strong></p><pre><code>{  &quot;_source&quot;: {    &quot;enabled&quot;: true  },  &quot;properties&quot;: {    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;imsi&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;longitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;latitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;phone_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_number&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;collect_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;send_mail&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;send_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;accept_mail&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;accept_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;mail_content&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;mail_type&quot;:{&quot;type&quot;: &quot;keyword&quot;},     &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;table&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;absolute_filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}  }}</code></pre><p><strong>mapping/qq.json</strong></p><pre><code>{  &quot;_source&quot;: {    &quot;enabled&quot;: true  },  &quot;properties&quot;: {    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;imsi&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;longitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;latitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;phone_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_number&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;collect_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;phone&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;object_username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;send_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;accept_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;message_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;table&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;absolute_filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}  }}</code></pre><p><strong>mapping/test.json</strong></p><pre><code>{  &quot;_source&quot;: {    &quot;enabled&quot;: true  },  &quot;properties&quot;: {    &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;source&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;target&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;library_id&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;source_sign&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;target_sign&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;create_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;create_user_id&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;is_audit&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;is_del&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;last_modify_user_id&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;last_modify_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;init_version&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;version&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;score&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;level&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;example&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;conflict&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;srcLangId&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;srcLangCN&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;tarLangId&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;tarLangCN&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;docId&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;source_simhash&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;sentence_id&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;section_id&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;type&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;industry&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;industry_name&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;querycount&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;reviser&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;comment&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}  }}</code></pre><p><strong>mapping/wechat.json</strong></p><pre><code>{  &quot;_source&quot;: {    &quot;enabled&quot;: true  },  &quot;properties&quot;: {    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;imsi&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;longitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;latitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;phone_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_number&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;collect_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;phone&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;object_username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;send_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;accept_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;message_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;table&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;absolute_filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}  }}</code></pre><h4 id="4、flume"><a href="#4、flume" class="headerlink" title="4、flume"></a>4、flume</h4><p><strong>datatype.properties</strong></p><p><strong>flume-config.properties</strong></p><pre><code>#kafka topickafkatopic=test100</code></pre><p><strong>validation.properties</strong></p><pre><code># 文件名验证开关FILENAME_VALIDATION=1# DATATYPE转换开关DATATYPE_TRANSACTION=1# 经纬度验证开关LONGLAIT_VALIDATION=1# 是否入错误数据到ESERROR_ES=1</code></pre><h4 id="5、hadoop"><a href="#5、hadoop" class="headerlink" title="5、hadoop"></a>5、hadoop</h4><p><strong>core-site.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--Autogenerated by Cloudera Manager--&gt;&lt;configuration&gt;  &lt;property&gt;    &lt;name&gt;fs.defaultFS&lt;/name&gt;    &lt;value&gt;hdfs://hadoop1:8020&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;fs.trash.interval&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;io.compression.codecs&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.authentication&lt;/name&gt;    &lt;value&gt;simple&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.authorization&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.rpc.protection&lt;/name&gt;    &lt;value&gt;authentication&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;    &lt;value&gt;DEFAULT&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.oozie.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.oozie.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.mapred.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.mapred.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.flume.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.flume.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.HTTP.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.HTTP.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hive.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hive.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.httpfs.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.httpfs.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hdfs.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hdfs.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.yarn.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.yarn.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.group.mapping&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.security.ShellBasedUnixGroupsMapping&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.instrumentation.requires.admin&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;net.topology.script.file.name&lt;/name&gt;    &lt;value&gt;/etc/hadoop/conf.cloudera.yarn/topology.py&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;io.file.buffer.size&lt;/name&gt;    &lt;value&gt;65536&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.ssl.enabled&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.ssl.require.client.cert&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;    &lt;final&gt;true&lt;/final&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.ssl.keystores.factory.class&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory&lt;/value&gt;    &lt;final&gt;true&lt;/final&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;    &lt;value&gt;ssl-server.xml&lt;/value&gt;    &lt;final&gt;true&lt;/final&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.ssl.client.conf&lt;/name&gt;    &lt;value&gt;ssl-client.xml&lt;/value&gt;    &lt;final&gt;true&lt;/final&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre><p><strong>hdfs-site.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--Autogenerated by Cloudera Manager--&gt;&lt;configuration&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;    &lt;value&gt;file:///dfs/nn&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.servicerpc-address&lt;/name&gt;    &lt;value&gt;hadoop1:8022&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.https.address&lt;/name&gt;    &lt;value&gt;hadoop1:50470&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.https.port&lt;/name&gt;    &lt;value&gt;50470&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;    &lt;value&gt;hadoop1:50070&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.replication&lt;/name&gt;    &lt;value&gt;3&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.blocksize&lt;/name&gt;    &lt;value&gt;134217728&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.use.datanode.hostname&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;fs.permissions.umask-mode&lt;/name&gt;    &lt;value&gt;022&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.acls.enabled&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.use.legacy.blockreader&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;    &lt;value&gt;/var/run/hdfs-sockets/dn&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.read.shortcircuit.skip.checksum&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.domain.socket.data.traffic&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.datanode.hdfs-blocks-metadata.enabled&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="6、hbase"><a href="#6、hbase" class="headerlink" title="6、hbase"></a>6、hbase</h4><p><strong>core-site.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--Autogenerated by Cloudera Manager--&gt;&lt;configuration&gt;  &lt;property&gt;    &lt;name&gt;fs.defaultFS&lt;/name&gt;    &lt;value&gt;hdfs://hadoop1:8020&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;fs.trash.interval&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;io.compression.codecs&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.authentication&lt;/name&gt;    &lt;value&gt;simple&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.authorization&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.rpc.protection&lt;/name&gt;    &lt;value&gt;authentication&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;    &lt;value&gt;DEFAULT&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.oozie.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.oozie.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.mapred.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.mapred.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.flume.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.flume.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.HTTP.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.HTTP.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hive.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hive.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.httpfs.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.httpfs.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hdfs.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hdfs.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.yarn.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.yarn.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.group.mapping&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.security.ShellBasedUnixGroupsMapping&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.instrumentation.requires.admin&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.ssl.require.client.cert&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;    &lt;final&gt;true&lt;/final&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.ssl.keystores.factory.class&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory&lt;/value&gt;    &lt;final&gt;true&lt;/final&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;    &lt;value&gt;ssl-server.xml&lt;/value&gt;    &lt;final&gt;true&lt;/final&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.ssl.client.conf&lt;/name&gt;    &lt;value&gt;ssl-client.xml&lt;/value&gt;    &lt;final&gt;true&lt;/final&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre><p><strong>hbase-server-config.properties</strong></p><pre><code>#hbase  开发环境need.init.hbase=true# hbase.zookeeper.quorum=hadoop1.ultiwill.com,hadoop2.ultiwill.com,hadoop3.ultiwill.comhbase.zookeeper.quorum=hadoop1,hadoop2,hadoop3hbase.zookeeper.property.clientPort=2181hbase.rpc.timeout=120000hbase.client.scanner.timeout.period=120000</code></pre><p><strong>hbase-site.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--Autogenerated by Cloudera Manager--&gt;&lt;configuration&gt;  &lt;property&gt;    &lt;name&gt;hbase.rootdir&lt;/name&gt;    &lt;value&gt;hdfs://hadoop1:8020/hbase&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.replication&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.client.write.buffer&lt;/name&gt;    &lt;value&gt;2097152&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.client.pause&lt;/name&gt;    &lt;value&gt;100&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.client.retries.number&lt;/name&gt;    &lt;value&gt;35&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.client.scanner.caching&lt;/name&gt;    &lt;value&gt;100&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.client.keyvalue.maxsize&lt;/name&gt;    &lt;value&gt;10485760&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.ipc.client.allowsInterrupt&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.client.primaryCallTimeout.get&lt;/name&gt;    &lt;value&gt;10&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.client.primaryCallTimeout.multiget&lt;/name&gt;    &lt;value&gt;10&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.fs.tmp.dir&lt;/name&gt;    &lt;value&gt;/user/${user.name}/hbase-staging&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.client.scanner.timeout.period&lt;/name&gt;    &lt;value&gt;60000&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.regionserver.thrift.http&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.thrift.support.proxyuser&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.rpc.timeout&lt;/name&gt;    &lt;value&gt;60000&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.snapshot.enabled&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.snapshot.master.timeoutMillis&lt;/name&gt;    &lt;value&gt;60000&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.snapshot.region.timeout&lt;/name&gt;    &lt;value&gt;60000&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.snapshot.master.timeout.millis&lt;/name&gt;    &lt;value&gt;60000&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.security.authentication&lt;/name&gt;    &lt;value&gt;simple&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.rpc.protection&lt;/name&gt;    &lt;value&gt;authentication&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;zookeeper.session.timeout&lt;/name&gt;    &lt;value&gt;60000&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;zookeeper.znode.parent&lt;/name&gt;    &lt;value&gt;/hbase&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;zookeeper.znode.rootserver&lt;/name&gt;    &lt;value&gt;root-region-server&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;    &lt;value&gt;hadoop1,hadoop3,hadoop2&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;    &lt;value&gt;2181&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.rest.ssl.enabled&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre><p><strong>hdfs-site.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--Autogenerated by Cloudera Manager--&gt;&lt;configuration&gt;  &lt;property&gt;    &lt;name&gt;dfs.permissions&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;    &lt;value&gt;file:///dfs/nn&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.servicerpc-address&lt;/name&gt;    &lt;value&gt;hadoop1:8022&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.https.address&lt;/name&gt;    &lt;value&gt;hadoop1:50470&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.https.port&lt;/name&gt;    &lt;value&gt;50470&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;    &lt;value&gt;hadoop1:50070&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.replication&lt;/name&gt;    &lt;value&gt;3&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.blocksize&lt;/name&gt;    &lt;value&gt;134217728&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.use.datanode.hostname&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;fs.permissions.umask-mode&lt;/name&gt;    &lt;value&gt;022&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.acls.enabled&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.use.legacy.blockreader&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;    &lt;value&gt;/var/run/hdfs-sockets/dn&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.read.shortcircuit.skip.checksum&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.domain.socket.data.traffic&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.datanode.hdfs-blocks-metadata.enabled&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="7、kafka"><a href="#7、kafka" class="headerlink" title="7、kafka"></a>7、kafka</h4><p><strong>kafka-data-push-info</strong></p><pre><code>--config                            kafka自动推送数据配置目录--timeOut                           推送超时时间    默认 15 min  单位为分钟kafka自动推送数据配置：data.sources                        数据源列表。  （例如：data.sources =bhdb1,dpxx）  {source}.source.type                某个数据源的类型。 （数据源分为数据库和文件两大类， 若为数据库 则使用 数据的名称 例如 oracle,mysql,sqlserver等， 否则使用 file）                                                                                                            例如：bhdb1.source.type=oracle 或者  dpxx.source.type=file数据源为数据库：{source}.db.name                    数据库的名称{source}.db.host                    数据库的ip或者主机名{source}.db.port                    数据库的访问端口， 若不填写则使用该种数据库的默认端口{source}.db.user                    用户名{source}.db.pwd                     密码                                                                 {source}.push.topic                 推送到topic的全局配置，即该数据库下配置的表没有配置topic的时候，其数据会推送到该topic。   {source}.push.tables                需要推送数的表列表 {source}.{table}.push.sql           只推送使用该sql查询到的数据    。       不填则表示推送全部。{source}.{table}.push.adjusterfactory 对推送的数据进行调整  ， 必须为com.bh.d406.bigdata.kafka.producer.DataAdjuster的子类   ，  需要进行调整数据的时候填写{source}.{table}.push.topic         该表的数据推送到topic名称  ， 若不填则使用全局的topic配置数据源为文件：{source}.file.dir                   文件目录    （注意：只支持本地目录 ）    {source}.file.encoding              文件编码      （默认UTF-8）{source}.file.extensions            需要过滤的文件格式列表{source}.file.data.loaderfactory    文件加载器工厂类   {source}.file.data.fields           记录的字段列表      与顺序有关{source}.file.data.spliter          数据的分割符         默认 \t{source}.file.skip.firstline        是否跳过第一行数据                       false  or true{source}.file.data.adjusterfactory  数据矫正工厂类{source}.push.thread.num            读取文件的线程数{source}.push.batch.size            分批推送数据 ， 每批数据大小{source}.push.topic                 数据推送的目标topic名称{source}.store.table                存储的表名</code></pre><p><strong>kafka-server-config.properties</strong></p><pre><code>#################Kafka 全局配置 ######################## 格式为host1:port1,host2:port2，# 这是一个broker列表，用于获得元数据(topics，partitions和replicas)，建立起来的socket连接用于发送实际数据，# 这个列表可以是broker的一个子集，或者一个VIP，指向broker的一个子集# metadata.broker.list=hadoop1:9092,slaver01:9092,slaver02:9092metadata.broker.list=hadoop1:9092# zookeeper列表zk.connect=hadoop1:2181,hadoop2:2181,hadoop3:2181# 字消息的序列化类，默认是的encoder处理一个byte[]，返回一个byte[]# 默认值为 kafka.serializer.DefaultEncoderserializer.class=kafka.serializer.StringEncoder# 用来控制一个produce请求怎样才能算完成，准确的说，是有多少broker必须已经提交数据到log文件，并向leader发送ack，可以设置如下的值：# 0，意味着producer永远不会等待一个来自broker的ack，这就是0.7版本的行为。这个选项提供了最低的延迟，但是持久化的保证是最弱的，当server挂掉的时候会丢失一些数据。# 1，意味着在leader replica已经接收到数据后，producer会得到一个ack。这个选项提供了更好的持久性，因为在server确认请求成功处理后，client才会返回。如果刚写到leader上，还没来得及复制leader就挂了，那么消息才可能会丢失。# -1，意味着在所有的ISR都接收到数据后，producer才得到一个ack。这个选项提供了最好的持久性，只要还有一个replica存活，那么数据就不会丢失。# 默认值  为 0request.required.acks=1# 请求超时时间     默认为 10000request.timeout.ms=60000#决定消息是否应在一个后台线程异步发送。#合法的值为sync，表示异步发送；sync表示同步发送。#设置为async则允许批量发送请求，这回带来更高的吞吐量，但是client的机器挂了的话会丢失还没有发送的数据。#默认值为 syncproducer.type=sync</code></pre><h4 id="8、redis"><a href="#8、redis" class="headerlink" title="8、redis"></a>8、redis</h4><p><strong>redis.properties</strong></p><pre><code>redis.hostname = 192.168.116.202redis.port  = 6379</code></pre><h4 id="9、spark"><a href="#9、spark" class="headerlink" title="9、spark"></a>9、spark</h4><p><strong>hive_fields_mapping.properties</strong></p><pre><code>datatype= base,wechat#base = datatype,idcard,name,age,collecttime,imei#wechat = datatype,wechat,phone,collecttime,imei#============================================================basebase.datatype = stringbase.idcard = stringbase.name = stringbase.age = longbase.collecttime = stringbase.imei = string#============================================================wechatwechat.datatype = stringwechat.wechat = stringwechat.phone = stringwechat.collecttime = stringwechat.imei = string</code></pre><p><strong>relation.properties</strong></p><pre><code>#需要关联的字段relationfield = phone_mac,phone,username,send_mail,imei,imsicomplex_relationfield = card,phone_mac,phone,username,send_mail,imei,imsi</code></pre><p><strong>spark-batch-config.properties</strong></p><pre><code># spark 常规 配置   不包括 流式处理的 配置#################### 全局  ############################## 在用户没有指定时，用于分布式随机操作(groupByKey,reduceByKey等等)的默认的任务数（ shuffle过程中 task的个数 ）# 默认为 8spark.default.parallelism=16# Spark用于缓存的内存大小所占用的Java堆的比率。这个不应该大于JVM中老年代所分配的内存大小# 默认情况下老年代大小是堆大小的2/3，但是你可以通过配置你的老年代的大小，然后再去增加这个比率# 默认为 0.66# spark 1.6 后 过期# spark.storage.memoryFraction=0.66# 在spark1.6.0版本默认大小为： (“Java Heap” – 300MB) * 0.75# 例如：如果堆内存大小有4G，将有2847MB的Spark Memory,Spark Memory=(4*1024MB-300)*0.75=2847MB# 这部分内存会被分成两部分：Storage Memory和Execution Memory# 而且这两部分的边界由spark.memory.storageFraction参数设定，默认是0.5即50%# 新的内存管理模型中的优点是，这个边界不是固定的，在内存压力下这个边界是可以移动的# 如一个区域内存不够用时可以从另一区域借用内存spark.memory.fraction=0.75spark.memory.storageFraction=0.5# 是否要压缩序列化的RDD分区（比如，StorageLevel.MEMORY_ONLY_SER）# 在消耗一点额外的CPU时间的代价下，可以极大的提高减少空间的使用# 默认为 falsespark.rdd.compress=true# The codec used to compress internal data such as RDD partitions,# broadcast variables and shuffle outputs. By default,# Spark provides three codecs: lz4, lzf, and snappy. You can also use fully qualified class names to specify the codec,# e.g.# 1. org.apache.spark.io.LZ4CompressionCodec, # 2. org.apache.spark.io.LZFCompressionCodec, # 3. org.apache.spark.io.SnappyCompressionCodec.   defaultspark.io.compression.codec=org.apache.spark.io.SnappyCompressionCodec# Block size (in bytes) used in Snappy compression,# in the case when Snappy compression codec is used.# Lowering this block size will also lower shuffle memory usage when Snappy is used.# default : 32Kspark.io.compression.snappy.blockSize=32768# 同时获取每一个分解任务的时候，映射输出文件的最大的尺寸（以兆为单位）。# 由于对每个输出都需要我们去创建一个缓冲区去接受它，这个属性值代表了对每个分解任务所使用的内存的一个上限值，# 因此除非你机器内存很大，最好还是配置一下这个值。# 默认48spark.reducer.maxSizeInFlight=48# 这个配置参数仅适用于HashShuffleMananger的实现，同样是为了解决生成过多文件的问题，# 采用的方式是在不同批次运行的Map任务之间重用Shuffle输出文件，也就是说合并的是不同批次的Map任务的输出数据，# 但是每个Map任务所需要的文件还是取决于Reduce分区的数量，因此，它并不减少同时打开的输出文件的数量，# 因此对内存使用量的减少并没有帮助。只是HashShuffleManager里的一个折中的解决方案。# 默认为false#spark.shuffle.consolidateFiles=false#java.io.Externalizable. Java serialization is flexible but often quite slow, and leads to large serialized formats for many classes.#default java.io.Serializable#spark.serializer=org.apache.spark.serializer.KryoSerializer# Speculation是在任务调度的时候，如果没有适合当前本地性要求的任务可供运行，# 将跑得慢的任务在空闲计算资源上再度调度的行为，这些参数调整这些行为的频率和判断指标，默认是不使用Speculation的# 默认为false# 慎用   可能导致数据重复的现象#spark.speculation=true# task失败重试次数# 默认为4spark.task.maxFailures=8# Spark 是有任务的黑名单机制的，但是这个配置在官方文档里面并没有写，可以设置下面的参数，# 比如设置成一分钟之内不要再把任务发到这个 Executor 上了，单位是毫秒。# spark.scheduler.executorTaskBlacklistTime=60000# 超过这个时间，可以执行 NODE_LOCAL 的任务# 默认为 3000spark.locality.wait.process=1# 超过这个时间，可以执行 RACK_LOCAL 的任务# 默认为 3000spark.locality.wait.node=3 # 超过这个时间，可以执行 ANY 的任务# 默认为 3000spark.locality.wait.rack=1000#################### yarn  ############################ 提交的jar文件  的副本数# 默认为 3spark.yarn.submit.file.replication=1# container中的线程数# 默认为 25spark.yarn.containerLauncherMaxThreads=25# 解决yarn-cluster模式下 对处理  permGen space oom异常很有用# spark.yarn.am.extraJavaOptions=# spark.driver.extraJavaOptions=-XX:PermSize=512M -XX:MaxPermSize=1024M# 对象指针压缩 和 gc日志收集打印# spark.executor.extraJavaOptions=-XX:PermSize=512M -XX:MaxPermSize=1024M -XX:MaxDirectMemorySize=1536M -XX:+UseCompressedOops -XX:+PrintGCDetails -XX:+PrintGCTimeStamps# -XX:-UseGCOverheadLimit# GC默认情况下有一个限制，默认是GC时间不能超过2%的CPU时间，但是如果大量对象创建（在Spark里很容易出现，代码模式就是一个RDD转下一个RDD），# 就会导致大量的GC时间，从而出现OutOfMemoryError: GC overhead limit exceeded，可以通过设置-XX:-UseGCOverheadLimit关掉它。# -XX:+UseCompressedOops  可以压缩指针（8字节变成4字节）spark.executor.extraJavaOptions=-XX:PermSize=512M -XX:MaxPermSize=1024m -XX:+CMSClassUnloadingEnabled -Xmn512m -XX:MaxTenuringThreshold=15 -XX:-UseGCOverheadLimit -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=80 -XX:+UseCompressedOops -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:gc.log -XX:+HeapDumpOnOutOfMemoryError# 当shuffle缓存的数据超过此值  强制刷磁盘  单位为 byte# spark.shuffle.spill.initialMemoryThreshold=671088640################### AKKA 相关 ########################### 在控制面板通信（序列化任务和任务结果）的时候消息尺寸的最大值，单位是MB。# 如果你需要给驱动器发回大尺寸的结果（比如使用在一个大的数据集上面使用collect()方法），那么你就该增加这个值了。# 默认为 10spark.akka.frameSize=1024# 用于通信的actor线程数量。如果驱动器有很多CPU核心，那么在大集群上可以增大这个值。# 默认为 4spark.akka.threads=8# Spark节点之间通信的超时时间，以秒为单位# 默认为20sspark.akka.timeout=120# exector的堆外内存（不会占用 分配给executor的jvm内存）# spark.yarn.executor.memoryOverhead=2560</code></pre><p><strong>spark-start-config.properties</strong></p><pre><code># Spark 任务 使用java -cp 方式启动的参数配置#spark.executor.extraLibraryPath=/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.23/lib/hadoop/lib/nativespark.yarn.jar=local:/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.23/lib/spark/lib/spark-assembly.jarspark.authenticate=falsespark.driver.extraLibraryPath=/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.23/lib/hadoop/lib/nativespark.yarn.historyServer.address=http://BH-LAN-Virtual-hadoop-9:18088spark.yarn.am.extraLibraryPath=/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.23/lib/hadoop/lib/nativespark.eventLog.enabled=truespark.dynamicAllocation.schedulerBacklogTimeout=1SPARK_SUBMIT=truespark.yarn.config.gatewayPath=/opt/cloudera/parcelsspark.ui.killEnabled=truespark.serializer=org.apache.spark.serializer.KryoSerializerspark.shuffle.service.enabled=truespark.dynamicAllocation.minExecutors=0spark.dynamicAllocation.executorIdleTimeout=60spark.yarn.config.replacementPath={{HADOOP_COMMON_HOME}}/../../..spark.shuffle.service.port=7337spark.eventLog.dir=hdfs://nameservice1/user/spark/applicationHistoryspark.dynamicAllocation.enabled=true#/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.23/lib/spark/lib/*#/etc/spark/conf.cloudera.spark_on_yarn/#/etc/hadoop/conf.cloudera.yarn/spark.submit.deployMode=clientspark.app.name=defaultspark.master=yarn-clientspark.driver.memory=1gspark.executor.instances=1spark.executor.memory=4gspark.executor.cores=2spark.jars=</code></pre><p><strong>spark-streaming-config.properties</strong></p><pre><code># spark  流式处理的 配置# job的并行度# 默认为 1spark.streaming.concurrentJobs=1# Spark记忆任何元数据(stages生成，任务生成等等)的时间(秒)。周期性清除保证在这个时间之前的元数据会被遗忘。#当长时间几小时，几天的运行Spark的时候设置这个是很有用的。注意：任何内存中的RDD只要过了这个时间就会被清除掉。# 默认 disablespark.cleaner.ttl=3600# 将不再使用的缓存数据清除# 默认为falsespark.streaming.unpersist=true# 从网络中批量接受对象时的持续时间 , 单位  ms。# 默认为200msspark.streaming.blockInterval=200# 控制Receiver速度  单位 s# 因为当streaming程序的数据源的数据量突然变大巨大，可能会导致streaming被撑住导致吞吐不过来，所以可以考虑对于最大吞吐做一下限制。# 默认为 100000spark.streaming.receiver.maxRate=10000# kafka每个分区最大的读取速度   单位 s# 控制kafka读取的量spark.streaming.kafka.maxRatePerPartition=50# 读取kafka的分区最新offset的最大尝试次数# 默认为1spark.streaming.kafka.maxRetries=5# 1、为什么引入Backpressure# 默认情况下，Spark Streaming通过Receiver以生产者生产数据的速率接收数据，计算过程中会出现batch processing time &gt; batch interval的情况，# 其中batch processing time 为实际计算一个批次花费时间， batch interval为Streaming应用设置的批处理间隔。# 这意味着Spark Streaming的数据接收速率高于Spark从队列中移除数据的速率，也就是数据处理能力低，在设置间隔内不能完全处理当前接收速率接收的数据。# 如果这种情况持续过长的时间，会造成数据在内存中堆积，导致Receiver所在Executor内存溢出等问题（如果设置StorageLevel包含disk, 则内存存放不下的数据会溢写至disk, 加大延迟）。# Spark 1.5以前版本，用户如果要限制Receiver的数据接收速率，可以通过设置静态配制参数“spark.streaming.receiver.maxRate”的值来实现，# 此举虽然可以通过限制接收速率，来适配当前的处理能力，防止内存溢出，但也会引入其它问题。比如：producer数据生产高于maxRate，当前集群处理能力也高于maxRate，这就会造成资源利用率下降等问题。# 为了更好的协调数据接收速率与资源处理能力，Spark Streaming 从v1.5开始引入反压机制（back-pressure）,通过动态控制数据接收速率来适配集群数据处理能力。# 2、Backpressure# Spark Streaming Backpressure:  根据JobScheduler反馈作业的执行信息来动态调整Receiver数据接收率。# 通过属性“spark.streaming.backpressure.enabled”来控制是否启用backpressure机制，默认值false，即不启用spark.streaming.backpressure.enabled=truespark.streaming.backpressure.initialRate=200</code></pre><p><strong>datatype/fieldtype.properties</strong></p><p><strong>hive/hive-server-config.properties</strong></p><pre><code># hbase  开发环境</code></pre><p><strong>hive/hive-site.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--Autogenerated by Cloudera Manager--&gt;&lt;configuration&gt;  &lt;property&gt;    &lt;name&gt;hive.metastore.uris&lt;/name&gt;    &lt;value&gt;thrift://hadoop1:9083&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.metastore.client.socket.timeout&lt;/name&gt;    &lt;value&gt;300&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;    &lt;value&gt;/user/hive/warehouse&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.warehouse.subdir.inherit.perms&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.auto.convert.join&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.auto.convert.join.noconditionaltask.size&lt;/name&gt;    &lt;value&gt;20971520&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.optimize.bucketmapjoin.sortedmerge&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.smbjoin.cache.rows&lt;/name&gt;    &lt;value&gt;10000&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.server2.logging.operation.enabled&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;    &lt;value&gt;/hadoop_log/log/hive/operation_logs&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;mapred.reduce.tasks&lt;/name&gt;    &lt;value&gt;-1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.exec.reducers.bytes.per.reducer&lt;/name&gt;    &lt;value&gt;67108864&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.exec.copyfile.maxsize&lt;/name&gt;    &lt;value&gt;33554432&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.exec.reducers.max&lt;/name&gt;    &lt;value&gt;1099&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.vectorized.groupby.checkinterval&lt;/name&gt;    &lt;value&gt;4096&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.vectorized.groupby.flush.percent&lt;/name&gt;    &lt;value&gt;0.1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.compute.query.using.stats&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.vectorized.execution.enabled&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.vectorized.execution.reduce.enabled&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.merge.mapfiles&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.merge.mapredfiles&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.cbo.enable&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt;    &lt;value&gt;minimal&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.fetch.task.conversion.threshold&lt;/name&gt;    &lt;value&gt;268435456&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.limit.pushdown.memory.usage&lt;/name&gt;    &lt;value&gt;0.1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.merge.sparkfiles&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.merge.smallfiles.avgsize&lt;/name&gt;    &lt;value&gt;16777216&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.merge.size.per.task&lt;/name&gt;    &lt;value&gt;268435456&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.optimize.reducededuplication&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.optimize.reducededuplication.min.reducer&lt;/name&gt;    &lt;value&gt;4&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.map.aggr&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.map.aggr.hash.percentmemory&lt;/name&gt;    &lt;value&gt;0.5&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.optimize.sort.dynamic.partition&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.execution.engine&lt;/name&gt;    &lt;value&gt;mr&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.executor.memory&lt;/name&gt;    &lt;value&gt;1369020825&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.driver.memory&lt;/name&gt;    &lt;value&gt;966367641&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.executor.cores&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.yarn.driver.memoryOverhead&lt;/name&gt;    &lt;value&gt;102&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.yarn.executor.memoryOverhead&lt;/name&gt;    &lt;value&gt;230&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.dynamicAllocation.enabled&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.dynamicAllocation.initialExecutors&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.dynamicAllocation.minExecutors&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.dynamicAllocation.maxExecutors&lt;/name&gt;    &lt;value&gt;2147483647&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.metastore.execute.setugi&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.support.concurrency&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt;    &lt;value&gt;hadoop1,hadoop3,hadoop2&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.zookeeper.client.port&lt;/name&gt;    &lt;value&gt;2181&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.zookeeper.namespace&lt;/name&gt;    &lt;value&gt;hive_zookeeper_namespace_hive&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.cluster.delegation.token.store.class&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.hive.thrift.MemoryTokenStore&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.server2.use.SSL&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.shuffle.service.enabled&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre><h3 id="五．Flume开发"><a href="#五．Flume开发" class="headerlink" title="五．Flume开发"></a>五．Flume开发</h3><p><strong>xz_bigdata_flume</strong></p><p><strong>FTP–&gt;FlumeSource–&gt;拦截器–&gt;FlumeChannel–&gt;FlumeSink–&gt;Kafka</strong></p><p><strong>自定义的内容有：FlumeSource、拦截器、FlumeSink</strong></p><h4 id="1、maven冲突解决和pom-xml"><a href="#1、maven冲突解决和pom-xml" class="headerlink" title="1、maven冲突解决和pom.xml"></a>1、maven冲突解决和pom.xml</h4><p>1.1 安装Maven Helper插件，在Settings里面的Plugins里面搜索Maven Helper，点击Install，安装完毕。</p><p>1.2 ETL包括数据的抽取、转换、加载<br>①数据抽取：从源数据源系统抽取目的数据源系统需要的数据：<br>②数据转换：将从源数据源获取的数据按照业务需求，转换成目的数据源要求的形式，并对错误、不一致的数据进行清洗和加工；<br>③数据加载：将转换后的数据装载到目的数据源。</p><p><img src="/medias/Flume%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.PNG" alt="Flume数据处理流程"></p><p>1.3 pom.xml</p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_flume&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_flume&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;        &lt;flume-ng.version&gt;1.6.0&lt;/flume-ng.version&gt;        &lt;hadoop.version&gt;2.6.0&lt;/hadoop.version&gt;        &lt;jdom.version&gt;1.0&lt;/jdom.version&gt;        &lt;c3p0.version&gt;0.9.5&lt;/c3p0.version&gt;        &lt;hadoop.version&gt;2.6.0&lt;/hadoop.version&gt;        &lt;mybatis.version&gt;3.1.1&lt;/mybatis.version&gt;        &lt;zookeeper.version&gt;3.4.6&lt;/zookeeper.version&gt;        &lt;net.sf.json.version&gt;2.2.3&lt;/net.sf.json.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;fastjson&lt;/artifactId&gt;                    &lt;groupId&gt;com.alibaba&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;commons-configuration&lt;/artifactId&gt;                    &lt;groupId&gt;commons-configuration&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;commons-io&lt;/artifactId&gt;                    &lt;groupId&gt;commons-io&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;commons-lang&lt;/artifactId&gt;                    &lt;groupId&gt;commons-lang&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_kafka&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;snappy-java&lt;/artifactId&gt;                    &lt;groupId&gt;org.xerial.snappy&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;scala-library&lt;/artifactId&gt;                    &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;                    &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;                    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;                    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;log4j&lt;/artifactId&gt;                    &lt;groupId&gt;log4j&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;!--flume核心依赖--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flume&lt;/groupId&gt;            &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt;            &lt;version&gt;${flume-ng.version}-${cdh.version}&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;                    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;guava&lt;/artifactId&gt;                    &lt;groupId&gt;com.google.guava&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;commons-codec&lt;/artifactId&gt;                    &lt;groupId&gt;commons-codec&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;commons-logging&lt;/artifactId&gt;                    &lt;groupId&gt;commons-logging&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;jetty&lt;/artifactId&gt;                    &lt;groupId&gt;org.mortbay.jetty&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;jetty-util&lt;/artifactId&gt;                    &lt;groupId&gt;org.mortbay.jetty&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;servlet-api&lt;/artifactId&gt;                    &lt;groupId&gt;org.mortbay.jetty&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;commons-io&lt;/artifactId&gt;                    &lt;groupId&gt;commons-io&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;commons-lang&lt;/artifactId&gt;                    &lt;groupId&gt;commons-lang&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flume&lt;/groupId&gt;            &lt;artifactId&gt;flume-ng-sdk&lt;/artifactId&gt;            &lt;version&gt;${flume-ng.version}-${cdh.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;!--flume配置依赖--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flume&lt;/groupId&gt;            &lt;artifactId&gt;flume-ng-configuration&lt;/artifactId&gt;            &lt;version&gt;${flume-ng.version}-${cdh.version}&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;guava&lt;/artifactId&gt;                    &lt;groupId&gt;com.google.guava&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;jdom&lt;/groupId&gt;            &lt;artifactId&gt;jdom&lt;/artifactId&gt;            &lt;version&gt;${jdom.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;            &lt;artifactId&gt;log4j-api&lt;/artifactId&gt;            &lt;version&gt;RELEASE&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;log4j&lt;/groupId&gt;            &lt;artifactId&gt;log4j&lt;/artifactId&gt;            &lt;version&gt;RELEASE&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;commons-io&lt;/groupId&gt;            &lt;artifactId&gt;commons-io&lt;/artifactId&gt;            &lt;version&gt;RELEASE&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;commons-lang&lt;/groupId&gt;            &lt;artifactId&gt;commons-lang&lt;/artifactId&gt;            &lt;version&gt;RELEASE&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;commons-configuration&lt;/groupId&gt;            &lt;artifactId&gt;commons-configuration&lt;/artifactId&gt;            &lt;version&gt;RELEASE&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;            &lt;version&gt;RELEASE&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.google.guava&lt;/groupId&gt;            &lt;artifactId&gt;guava&lt;/artifactId&gt;            &lt;version&gt;RELEASE&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;defaultGoal&gt;compile&lt;/defaultGoal&gt;        &lt;sourceDirectory&gt;src/main/java/&lt;/sourceDirectory&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;archive&gt;                        &lt;manifest&gt;                            &lt;addClasspath&gt;true&lt;/addClasspath&gt;                            &lt;classpathPrefix&gt;jars/&lt;/classpathPrefix&gt;                            &lt;mainClass&gt;&lt;/mainClass&gt;                        &lt;/manifest&gt;                    &lt;/archive&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;copy&lt;/id&gt;                        &lt;phase&gt;install&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;outputDirectory&gt;                                ${project.build.directory}/jars                            &lt;/outputDirectory&gt;                            &lt;excludeArtifactIds&gt;javaee-api&lt;/excludeArtifactIds&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt;                &lt;version&gt;2.7&lt;/version&gt;                &lt;configuration&gt;                    &lt;encoding&gt;UTF-8&lt;/encoding&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><h4 id="2、自定义source"><a href="#2、自定义source" class="headerlink" title="2、自定义source"></a>2、自定义source</h4><p><strong>2.1 继承AbstractSource 实现 Configurable, PollableSource接口</strong></p><pre><code>package com.hsiehchou.flume.source;import com.hsiehchou.flume.constant.FlumeConfConstant;import com.hsiehchou.flume.fields.MapFields;import com.hsiehchou.flume.utils.FileUtilsStronger;import org.apache.commons.io.FileUtils;import org.apache.flume.Context;import org.apache.flume.Event;import org.apache.flume.PollableSource;import org.apache.flume.channel.ChannelProcessor;import org.apache.flume.conf.Configurable;import org.apache.flume.event.SimpleEvent;import org.apache.flume.source.AbstractSource;import org.apache.log4j.Logger;import java.io.File;import java.util.*;/** * 固定写法，自定义Source 直接继承 AbstractSource 和 实现 Configurable, PollableSource 接口 * 可参照官网 http://flume.apache.org/releases/content/1.9.0/FlumeDeveloperGuide.html#source */public class FolderSource extends AbstractSource implements Configurable, PollableSource {    private final Logger logger = Logger.getLogger(FolderSource.class);    //tier1.sources.source1.sleeptime=5    //tier1.sources.source1.filenum=3000    //tier1.sources.source1.dirs =/usr/chl/data/filedir/    //tier1.sources.source1.successfile=/usr/chl/data/filedir_successful/    //以下为配置在flume.conf文件中    //读取的文件目录    private String dirStr;    //读取的文件目录，如果多个，以&quot;,&quot;分割，在flume.conf里面配置    private String[] dirs;    //处理成功的文件写入的目录    private String successfile;    //睡眠时间    private long sleeptime = 5;    //每批文件数量    private int filenum = 500;    //以下为配置在txtparse.properties文件中    //读取的所有文件集合    private Collection&lt;File&gt; allFiles;    //一批处理的文件大小    private List&lt;File&gt; listFiles;    private ArrayList&lt;Event&gt; eventList = new ArrayList&lt;Event&gt;();    /**     * @param context 拿到flume配置里面的所有参数     */    @Override    public void configure(Context context) {        logger.info(&quot;开始初始化flume参数&quot;);        initFlumeParams(context);        logger.info(&quot;初始化flume参数成功&quot;);    }    @Override    public Status process() {        //定义处理逻辑        try {            Thread.currentThread().sleep(sleeptime * 1000);        } catch (InterruptedException e) {            logger.error(null, e);        }        Status status = null;        try {            // for (String dir : dirs) {            logger.info(&quot;dirStr===========&quot; + dirStr);            //TODO 1.监控目录下面的所有文件            //读取目录下的文件，获取目录下所有以 &quot;txt&quot;, &quot;bcp&quot; 结尾的文件            allFiles = FileUtils.listFiles(new File(dirStr), new String[]{&quot;txt&quot;, &quot;bcp&quot;}, true);            //如果目录下文件总数大于阈值，则只取 filenum 个文件进行处理            if (allFiles.size() &gt;= filenum) {                //文件数量大于3000 只取3000条                listFiles = ((List&lt;File&gt;) allFiles).subList(0, filenum);            } else {                //文件数量小于3000，取所有文件进行处理                listFiles = ((List&lt;File&gt;) allFiles);            }            //TODO 2.遍历所有的文件进行解析            if (listFiles.size() &gt; 0) {                for (File file : listFiles) {                    //文件名是需要传到channel中的                    String fileName = file.getName();                    //解析文件  获取文件名及文件内容 文件绝对路径  文件内容                    Map&lt;String, Object&gt; stringObjectMap = FileUtilsStronger.parseFile(file, successfile);                    //返回的内容2个参数  一个是文件绝对路径  另一个是lines文件的所有内容                    //获取文件绝对路径                    String absoluteFilename = (String) stringObjectMap.get(MapFields.ABSOLUTE_FILENAME);                    //获取文件内容                    List&lt;String&gt; lines = (List&lt;String&gt;) stringObjectMap.get(MapFields.VALUE);                    //TODO 解析出来之后，需要把解析出来的数据封装为Event                    if (lines != null &amp;&amp; lines.size() &gt; 0) {                        //遍历读取的内容                        for (String line : lines) {                            //封装event Header 将文件名及文件绝对路径通过header传送到channel中                            //构建event头                            Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();                            //文件名                            map.put(MapFields.FILENAME, fileName);                            //文件绝对路径                            map.put(MapFields.ABSOLUTE_FILENAME, absoluteFilename);                            //构建event                            SimpleEvent event = new SimpleEvent();                            //把读取的一行数据转成字节                            byte[] bytes = line.getBytes();                            event.setBody(bytes);                            event.setHeaders(map);                            eventList.add(event);                        }                    }                    try {                        if (eventList.size() &gt; 0) {                            //获取channelProcessor                            ChannelProcessor channelProcessor = getChannelProcessor();                            //通过channelProcessor把eventList发送出去，可以通过拦截器进行拦截                            channelProcessor.processEventBatch(eventList);                            logger.info(&quot;批量推送到 拦截器 数据大小为&quot; + eventList.size());                        }                        eventList.clear();                    } catch (Exception e) {                        eventList.clear();                        logger.error(&quot;发送数据到channel失败&quot;, e);                    } finally {                        eventList.clear();                    }                }            }            // 处理成功，返回成功状态            status = Status.READY;            return status;        } catch (Exception e) {            status = Status.BACKOFF;            logger.error(&quot;异常&quot;, e);            return status;        }    }    /**     * 初始化flume參數     * @param context     */    public void initFlumeParams(Context context) {        //读取flume，conf配置文件，初始化参数        try {            //文件处理目录            //监控的文件目录            dirStr = context.getString(FlumeConfConstant.DIRS);            //监控多个目录            dirs = dirStr.split(&quot;,&quot;);            //成功处理的文件存放目录            successfile = context.getString(FlumeConfConstant.SUCCESSFILE);            //每批处理文件个数            filenum = context.getInteger(FlumeConfConstant.FILENUM);            //睡眠时间            sleeptime = context.getLong(FlumeConfConstant.SLEEPTIME);            logger.info(&quot;dirStr============&quot; + dirStr);            logger.info(&quot;dirs==============&quot; + dirs);            logger.info(&quot;successfile=======&quot; + successfile);            logger.info(&quot;filenum===========&quot; + filenum);            logger.info(&quot;sleeptime=========&quot; + sleeptime);        } catch (Exception e) {            logger.error(&quot;初始化flume参数失败&quot;, e);        }    }    @Override    public long getBackOffSleepIncrement() {        return 0;    }    @Override    public long getMaxBackOffSleepInterval() {        return 0;    }}</code></pre><p><strong>2.2 实现process()方法</strong><br>此处代码已经在2.1里面，不用再写了</p><pre><code> public Status process() {        //定义处理逻辑        try {            Thread.currentThread().sleep(sleeptime * 1000);        } catch (InterruptedException e) {            logger.error(null, e);        }        Status status = null;        try {            // for (String dir : dirs) {            logger.info(&quot;dirStr===========&quot; + dirStr);            //TODO 1.监控目录下面的所有文件            //读取目录下的文件，获取目录下所有以 &quot;txt&quot;, &quot;bcp&quot; 结尾的文件            allFiles = FileUtils.listFiles(new File(dirStr), new String[]{&quot;txt&quot;, &quot;bcp&quot;}, true);            //如果目录下文件总数大于阈值，则只取 filenum 个文件进行处理            if (allFiles.size() &gt;= filenum) {                //文件数量大于3000 只取3000条                listFiles = ((List&lt;File&gt;) allFiles).subList(0, filenum);            } else {                //文件数量小于3000，取所有文件进行处理                listFiles = ((List&lt;File&gt;) allFiles);            }            //TODO 2.遍历所有的文件进行解析            if (listFiles.size() &gt; 0) {                for (File file : listFiles) {                    //文件名是需要传到channel中的                    String fileName = file.getName();                    //解析文件  获取文件名及文件内容 文件绝对路径  文件内容                    Map&lt;String, Object&gt; stringObjectMap = FileUtilsStronger.parseFile(file, successfile);                    //返回的内容2个参数  一个是文件绝对路径  另一个是lines文件的所有内容                    //获取文件绝对路径                    String absoluteFilename = (String) stringObjectMap.get(MapFields.ABSOLUTE_FILENAME);                    //获取文件内容                    List&lt;String&gt; lines = (List&lt;String&gt;) stringObjectMap.get(MapFields.VALUE);                    //TODO 解析出来之后，需要把解析出来的数据封装为Event                    if (lines != null &amp;&amp; lines.size() &gt; 0) {                        //遍历读取的内容                        for (String line : lines) {                            //封装event Header 将文件名及文件绝对路径通过header传送到channel中                            //构建event头                            Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();                            //文件名                            map.put(MapFields.FILENAME, fileName);                            //文件绝对路径                            map.put(MapFields.ABSOLUTE_FILENAME, absoluteFilename);                            //构建event                            SimpleEvent event = new SimpleEvent();                            //把读取的一行数据转成字节                            byte[] bytes = line.getBytes();                            event.setBody(bytes);                            event.setHeaders(map);                            eventList.add(event);                        }                    }                    try {                        if (eventList.size() &gt; 0) {                            //获取channelProcessor                            ChannelProcessor channelProcessor = getChannelProcessor();                            //通过channelProcessor把eventList发送出去，可以通过拦截器进行拦截                            channelProcessor.processEventBatch(eventList);                            logger.info(&quot;批量推送到 拦截器 数据大小为&quot; + eventList.size());                        }                        eventList.clear();                    } catch (Exception e) {                        eventList.clear();                        logger.error(&quot;发送数据到channel失败&quot;, e);                    } finally {                        eventList.clear();                    }                }            }            // 处理成功，返回成功状态            status = Status.READY;            return status;        } catch (Exception e) {            status = Status.BACKOFF;            logger.error(&quot;异常&quot;, e);            return status;        }    }</code></pre><p><strong>source/MySource.java—Flume官网上的案例</strong></p><pre><code>package com.hsiehchou.flume.source;import org.apache.flume.Context;import org.apache.flume.Event;import org.apache.flume.EventDeliveryException;import org.apache.flume.PollableSource;import org.apache.flume.conf.Configurable;import org.apache.flume.event.SimpleEvent;import org.apache.flume.source.AbstractSource;public class MySource extends AbstractSource implements Configurable, PollableSource {    private String myProp;    /**     * 配置读取     * @param context     */    @Override    public void configure(Context context) {        String myProp = context.getString(&quot;myProp&quot;, &quot;defaultValue&quot;);        // Process the myProp value (e.g. validation, convert to another type, ...)        // Store myProp for later retrieval by process() method        this.myProp = myProp;    }    /**     * 定义自己的业务逻辑     * @return     * @throws EventDeliveryException     */    @Override    public Status process() throws EventDeliveryException {        Status status = null;        try {            // This try clause includes whatever Channel/Event operations you want to do            // Receive new data            //需要把自己的数据封装为event进行传输            Event e = new SimpleEvent();            // Store the Event into this Source&#39;s associated Channel(s)            getChannelProcessor().processEvent(e);            status = Status.READY;        } catch (Throwable t) {            // Log exception, handle individual exceptions as needed            status = Status.BACKOFF;            // re-throw all Errors            if (t instanceof Error) {                throw (Error)t;            }        } finally {        }        return status;    }    @Override    public long getBackOffSleepIncrement() {        return 0;    }    @Override    public long getMaxBackOffSleepInterval() {        return 0;    }    @Override    public void start() {        // Initialize the connection to the external client    }    @Override    public void stop () {        // Disconnect from external client and do any additional cleanup        // (e.g. releasing resources or nulling-out field values) ..    }}</code></pre><h4 id="3、自定义interceptor—数据清洗过滤器"><a href="#3、自定义interceptor—数据清洗过滤器" class="headerlink" title="3、自定义interceptor—数据清洗过滤器"></a>3、自定义interceptor—数据清洗过滤器</h4><p><strong>3.1实现Interceptor 接口</strong></p><pre><code>package com.hsiehchou.flume.interceptor;import com.alibaba.fastjson.JSON;import com.hsiehchou.flume.fields.MapFields;import com.hsiehchou.flume.service.DataCheck;import org.apache.commons.io.Charsets;import org.apache.flume.Context;import org.apache.flume.Event;import org.apache.flume.event.SimpleEvent;import org.apache.flume.interceptor.Interceptor;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.ArrayList;import java.util.List;import java.util.Map;/** * 数据清洗过滤器 */public class DataCleanInterceptor implements Interceptor {    private static final Logger LOG = LoggerFactory.getLogger(DataCleanInterceptor.class);    //datatpye.properties    //private static Map&lt;String,ArrayList&lt;String&gt;&gt; dataMap = DataTypeProperties.dataTypeMap;    /**     *  初始化     */    @Override    public void initialize() {    }    /**     * 单条处理     * 拦截方法。数据解析，封装，数据清洗     * @param event     * @return     */    @Override    public Event intercept(Event event) {        SimpleEvent eventNew = new SimpleEvent();        try {            LOG.info(&quot;拦截器Event开始执行&quot;);            Map&lt;String, String&gt; map = parseEvent(event);            if(map == null){                return null;            }            String lineJson = JSON.toJSONString(map);            LOG.info(&quot;拦截器推送数据到channel:&quot; +lineJson);            eventNew.setBody(lineJson.getBytes());        } catch (Exception e) {            LOG.error(null,e);        }        return eventNew;    }    /**     * 批处理     * @param events     * @return     */    @Override    public List&lt;Event&gt; intercept(List&lt;Event&gt; events) {        List&lt;Event&gt; list = new ArrayList&lt;Event&gt;();        for (Event event : events) {            Event intercept = intercept(event);            if (intercept != null) {                list.add(intercept);            }        }        return list;    }    @Override    public void close() {    }    /**     * 数据解析     * @param event     * @return     */    public static Map&lt;String,String&gt; parseEvent(Event event){        if (event == null) {            return null;        }        //000000000000000    000000000000000    24.000000    25.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    andiy    18609765432    judy            1789098763        String line = new String(event.getBody(), Charsets.UTF_8);        //文件名 和 文件绝对路径        String filename = event.getHeaders().get(MapFields.FILENAME);        String absoluteFilename = event.getHeaders().get(MapFields.ABSOLUTE_FILENAME);        //String转map，进行数据校验，检验错误入ES错误表        Map&lt;String, String&gt; map = DataCheck.txtParseAndalidation(line,filename,absoluteFilename);        return map;        //wechat_source1_1111115.txt        //String[] fileNames = filename.split(&quot;_&quot;);        // String转map，并进行数据长度校验，校验错误入ES错误表        //Map&lt;String, String&gt; map = JZDataCheck.txtParse(type, line, source, filename,absoluteFilename);        //Map&lt;String,String&gt; map = new HashMap&lt;&gt;();        //000000000000000    000000000000000    24.000000    25.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    andiy    18609765432    judy            1789098763        //String[] split = line.split(&quot;\t&quot;);        //数据类别        //String dataType = fileNames[0];        //imei,imsi,longitude,latitude,phone_mac,device_mac,device_number,collect_time,username,phone,object_username,send_message,accept_message,message_time        //ArrayList&lt;String&gt; fields = dataMap.get(dataType);        //for (int i = 0; i &lt; split.length; i++) {        //    map.put(fields.get(i),split[i]);        //}        //添加ID        //map.put(MapFields.ID, UUID.randomUUID().toString().replace(&quot;-&quot;,&quot;&quot;));        // map.put(MapFields.TABLE, dataType);        // map.put(MapFields.FILENAME, filename);        // map.put(MapFields.ABSOLUTE_FILENAME, absoluteFilename);//        Map&lt;String, String&gt; map = DataCheck.txtParseAndalidation(line,filename,absoluteFilename);//        return map;    }    /**     * 实例化创建     */    public static class Builder implements Interceptor.Builder {        @Override        public void configure(Context context) {        }        @Override        public Interceptor build() {            return new DataCleanInterceptor();        }    }}</code></pre><h4 id="4、utils工具类"><a href="#4、utils工具类" class="headerlink" title="4、utils工具类"></a>4、utils工具类</h4><p><strong>utils/FileUtilsStronger.java</strong></p><pre><code>package com.hsiehchou.flume.utils;import com.hsiehchou.common.time.TimeTranstationUtils;import com.hsiehchou.flume.fields.MapFields;import org.apache.commons.io.FileUtils;import org.apache.log4j.Logger;import java.io.File;import java.util.*;import static java.io.File.separator;public class FileUtilsStronger {    private static final Logger logger = Logger.getLogger(FileUtilsStronger.class);    /**     * @param file     * @param path     */    public static Map&lt;String,Object&gt; parseFile(File file, String path) {        Map&lt;String,Object&gt; map=new HashMap&lt;String,Object&gt;();        List&lt;String&gt; lines;        String fileNew = path+ TimeTranstationUtils.Date2yyyy_MM_dd()+getDir(file);        try {            if((new File(fileNew+file.getName())).exists()){                try{                    logger.info(&quot;文件名已经存在，开始删除同名已经存在文件&quot;+file.getAbsolutePath());                    file.delete();                    logger.info(&quot;删除同名已经存在文件&quot;+file.getAbsolutePath()+&quot;成功&quot;);                }catch (Exception e){                    logger.error(&quot;删除同名已经存在文件&quot;+file.getAbsolutePath()+&quot;失败&quot;,e);                }            }else{                lines = FileUtils.readLines(file);                map.put(MapFields.ABSOLUTE_FILENAME,fileNew+file.getName());                map.put(MapFields.VALUE,lines);                FileUtils.moveToDirectory(file, new File(fileNew), true);                logger.info(&quot;移动文件到&quot;+file.getAbsolutePath()+&quot;到&quot;+fileNew+&quot;成功&quot;);            }        } catch (Exception e) {            logger.error(&quot;移动文件&quot; + file.getAbsolutePath() + &quot;到&quot; + fileNew + &quot;失败&quot;, e);        }        return map;    }    /**     * @param file     * @param path     */    public static List&lt;String&gt; chanmodName(File file, String path) {        List&lt;String&gt; lines=null;        try {            if((new File(path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;/&quot;+file.getName())).exists()){                logger.warn(&quot;文件名已经存在，开始删除同名文件&quot; +path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;/&quot;+file.getName());                try{                    file.delete();                    logger.warn(&quot;删除同名文件&quot;+file.getAbsolutePath()+&quot;成功&quot;);                }catch (Exception e){                    logger.warn(&quot;删除同名文件&quot;+file.getAbsolutePath()+&quot;失败&quot;,e);                }            }else{                lines = FileUtils.readLines(file);                FileUtils.moveToDirectory(file, new File(path+ TimeTranstationUtils.Date2yyyy_MM_dd()), true);                logger.info(&quot;移动文件到&quot;+file.getAbsolutePath()+&quot;到&quot;+path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;成功&quot;);            }        } catch (Exception e) {            logger.error(&quot;移动文件&quot; + file.getName() + &quot;到&quot; + path+ TimeTranstationUtils.Date2yyyy_MM_dd() + &quot;失败&quot;, e);        }        return lines;    }    /**     * @param file     * @param path     */    public static void moveFile2unmanage(File file, String path) {        try {            if((new File(path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;/&quot;+file.getName())).exists()){                logger.warn(&quot;文件名已经存在，开始删除同名文件&quot; +file.getAbsolutePath());                try{                    file.delete();                    logger.warn(&quot;删除同名文件&quot;+file.getAbsolutePath()+&quot;成功&quot;);                }catch (Exception e){                    logger.warn(&quot;删除同名文件&quot;+file.getAbsolutePath()+&quot;失败&quot;,e);                }            }else{                FileUtils.moveToDirectory(file, new File(path+ TimeTranstationUtils.Date2yyyy_MM_dd()), true);                //logger.info(&quot;移动文件到&quot;+file.getAbsolutePath()+&quot;到&quot;+path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;成功&quot;);            }        } catch (Exception e) {            logger.error(&quot;移动错误文件&quot; + file.getName() + &quot;到&quot; + path+ TimeTranstationUtils.Date2yyyy_MM_dd() + &quot;失败&quot;, e);        }    }    /**     * @param file     * @param path     */    public static void shnegtingChanmodName(File file, String path) {        try {            if((new File(path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;/&quot;+file.getName())).exists()){                logger.warn(&quot;文件名已经存在，开始删除同名文件&quot; +path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;/&quot;+file.getName());                try{                    file.delete();                    logger.warn(&quot;删除同名文件&quot;+file.getAbsolutePath()+&quot;成功&quot;);                }catch (Exception e){                    logger.warn(&quot;删除同名文件&quot;+file.getAbsolutePath()+&quot;失败&quot;,e);                }            }else{                FileUtils.moveToDirectory(file, new File(path+ TimeTranstationUtils.Date2yyyy_MM_dd()), true);                logger.info(&quot;移动文件到&quot;+file.getAbsolutePath()+&quot;到&quot;+path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;成功&quot;);            }        } catch (Exception e) {            logger.error(&quot;移动文件&quot; + file.getName() + &quot;到&quot; + path+ TimeTranstationUtils.Date2yyyy_MM_dd() + &quot;失败&quot;, e);        }    }    /**     * 获取文件父目录     * @param file     * @return     */    public static String getDir(File file){        String dir=file.getParent();        StringTokenizer dirs = new StringTokenizer(dir, separator);        List&lt;String&gt; list=new ArrayList&lt;String&gt;();        while(dirs.hasMoreTokens()){            list.add((String)dirs.nextElement());        }        String str=&quot;&quot;;        for(int i=2;i&lt;list.size();i++){            str=str+separator+list.get(i);        }        return str+&quot;/&quot;;    }}</code></pre><p><strong>utils/Validation.java—验证工具类</strong></p><pre><code>package com.hsiehchou.flume.utils;import java.util.regex.Matcher;import java.util.regex.Pattern;/** * 验证工具类 */@Deprecatedpublic class Validation {     // ------------------常量定义    /**     * Email正则表达式=     * &quot;^([a-z0-9A-Z]+[-|\\.]?)+[a-z0-9A-Z]@([a-z0-9A-Z]+(-[a-z0-9A-Z]+)?\\.)+[a-zA-Z]{2,}$&quot;     * ;     */    // public static final String EMAIL =    // &quot;^([a-z0-9A-Z]+[-|\\.]?)+[a-z0-9A-Z]@([a-z0-9A-Z]+(-[a-z0-9A-Z]+)?\\.)+[a-zA-Z]{2,}$&quot;;;    public static final String EMAIL = &quot;\\w+(\\.\\w+)*@\\w+(\\.\\w+)+&quot;;    /**     * 电话号码正则表达式=     * (^(\d{2,4}[-_－—]?)?\d{3,8}([-_－—]?\d{3,8})?([-_－—]?\d{1,7})?$)|     * (^0?1[35]\d{9}$)     */    public static final String PHONE = &quot;(^(\\d{2,4}[-_－—]?)?\\d{3,8}([-_－—]?\\d{3,8})?([-_－—]?\\d{1,7})?$)|(^0?1[35]\\d{9}$)&quot;;    /**     * 手机号码正则表达式=^(13[0-9]|15[0-9]|18[0-9])\d{8}$     */    public static final String MOBILE = &quot;^((13[0-9])|(14[5-7])|(15[^4])|(17[0-8])|(18[0-9]))\\d{8}$&quot;;    /**     * Integer正则表达式 ^-?(([1-9]\d*$)|0)     */    public static final String INTEGER = &quot;^-?(([1-9]\\d*$)|0)&quot;;    /**     * 正整数正则表达式 &gt;=0 ^[1-9]\d*|0$     */    public static final String INTEGER_NEGATIVE = &quot;^[1-9]\\d*|0$&quot;;    /**     * 负整数正则表达式 &lt;=0 ^-[1-9]\d*|0$     */    public static final String INTEGER_POSITIVE = &quot;^-[1-9]\\d*|0$&quot;;    /**     * Double正则表达式 ^-?([1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0)$     */    public static final String DOUBLE = &quot;^-?([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0)$&quot;;    /**     * 正Double正则表达式 &gt;=0 ^[1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0$　     */    public static final String DOUBLE_NEGATIVE = &quot;^[1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0$&quot;;    /**     * 负Double正则表达式 &lt;= 0 ^(-([1-9]\d*\.\d*|0\.\d*[1-9]\d*))|0?\.0+|0$     */    public static final String DOUBLE_POSITIVE = &quot;^(-([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*))|0?\\.0+|0$&quot;;    /**     * 年龄正则表达式 ^(?:[1-9][0-9]?|1[01][0-9]|120)$ 匹配0-120岁     */    public static final String AGE = &quot;^(?:[1-9][0-9]?|1[01][0-9]|120)$&quot;;    /**     * 邮编正则表达式 [0-9]\d{5}(?!\d) 国内6位邮编     */    public static final String CODE = &quot;[0-9]\\d{5}(?!\\d)&quot;;    /**     * 匹配由数字、26个英文字母或者下划线组成的字符串 ^\w+$     */    public static final String STR_ENG_NUM_ = &quot;^\\w+$&quot;;    /**     * 匹配由数字和26个英文字母组成的字符串 ^[A-Za-z0-9]+$     */    public static final String STR_ENG_NUM = &quot;^[A-Za-z0-9]+&quot;;    /**     * 匹配由26个英文字母组成的字符串 ^[A-Za-z]+$     */    public static final String STR_ENG = &quot;^[A-Za-z]+$&quot;;    /**     * 过滤特殊字符串正则 regEx=     * &quot;[`~!@#$%^&amp;*()+=|{}&#39;:;&#39;,\\[\\].&lt;&gt;/?~！@#￥%……&amp;*（）——+|{}【】‘；：”“’。，、？]&quot;;     */    public static final String STR_SPECIAL = &quot;[`~!@#$%^&amp;*()+=|{}&#39;:;&#39;,\\[\\].&lt;&gt;/?~！@#￥%……&amp;*（）——+|{}【】‘；：”“’。，、？]&quot;;    /***     * 日期正则 支持： YYYY-MM-DD YYYY/MM/DD YYYY_MM_DD YYYYMMDD YYYY.MM.DD的形式     */    public static final String DATE_ALL = &quot;((^((1[8-9]\\d{2})|([2-9]\\d{3}))([-\\/\\._]?)(10|12|0?[13578])([-\\/\\._]?)(3[01]|[12][0-9]|0?[1-9])$)&quot;            + &quot;|(^((1[8-9]\\d{2})|([2-9]\\d{3}))([-\\/\\._]?)(11|0?[469])([-\\/\\._]?)(30|[12][0-9]|0?[1-9])$)&quot;            + &quot;|(^((1[8-9]\\d{2})|([2-9]\\d{3}))([-\\/\\._]?)(0?2)([-\\/\\._]?)(2[0-8]|1[0-9]|0?[1-9])$)|(^([2468][048]00)([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|(^([3579][26]00)&quot;            + &quot;([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)&quot;            + &quot;|(^([1][89][0][48])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|(^([2-9][0-9][0][48])([-\\/\\._]?)&quot;            + &quot;(0?2)([-\\/\\._]?)(29)$)&quot;            + &quot;|(^([1][89][2468][048])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|(^([2-9][0-9][2468][048])([-\\/\\._]?)(0?2)&quot;            + &quot;([-\\/\\._]?)(29)$)|(^([1][89][13579][26])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|&quot;            + &quot;(^([2-9][0-9][13579][26])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$))&quot;;    /***     * 日期正则 支持： YYYY-MM-DD     */    public static final String DATE_FORMAT1 = &quot;(([0-9]{3}[1-9]|[0-9]{2}[1-9][0-9]{1}|[0-9]{1}[1-9][0-9]{2}|[1-9][0-9]{3})-(((0[13578]|1[02])-(0[1-9]|[12][0-9]|3[01]))|((0[469]|11)-(0[1-9]|[12][0-9]|30))|(02-(0[1-9]|[1][0-9]|2[0-8]))))|((([0-9]{2})(0[48]|[2468][048]|[13579][26])|((0[48]|[2468][048]|[3579][26])00))-02-29)&quot;;    /**     * URL正则表达式 匹配 http www ftp     */    public static final String URL = &quot;^(http|www|ftp|)?(://)?(\\w+(-\\w+)*)(\\.(\\w+(-\\w+)*))*((:\\d+)?)(/(\\w+(-\\w+)*))*(\\.?(\\w)*)(\\?)?&quot;            + &quot;(((\\w*%)*(\\w*\\?)*(\\w*:)*(\\w*\\+)*(\\w*\\.)*(\\w*&amp;)*(\\w*-)*(\\w*=)*(\\w*%)*(\\w*\\?)*&quot;            + &quot;(\\w*:)*(\\w*\\+)*(\\w*\\.)*&quot;            + &quot;(\\w*&amp;)*(\\w*-)*(\\w*=)*)*(\\w*)*)$&quot;;    /**     * 身份证正则表达式     */    public static final String IDCARD = &quot;((11|12|13|14|15|21|22|23|31|32|33|34|35|36|37|41|42|43|44|45|46|50|51|52|53|54|61|62|63|64|65)[0-9]{4})&quot;            + &quot;(([1|2][0-9]{3}[0|1][0-9][0-3][0-9][0-9]{3}&quot;            + &quot;[Xx0-9])|([0-9]{2}[0|1][0-9][0-3][0-9][0-9]{3}))&quot;;    /**     * 机构代码     */    public static final String JIGOU_CODE = &quot;^[A-Z0-9]{8}-[A-Z0-9]$&quot;;    /**     * 匹配数字组成的字符串 ^[0-9]+$     */    public static final String STR_NUM = &quot;^[0-9]+$&quot;;    // //------------------验证方法    /**     * 判断字段是否为空 符合返回ture     * @param str     * @return boolean     */    public static synchronized boolean StrisNull(String str) {        return null == str || str.trim().length() &lt;= 0 ? true : false;    }    /**     * 判断字段是非空 符合返回ture     * @param str     * @return boolean     */    public static boolean StrNotNull(String str) {        return !StrisNull(str);    }    /**     * 字符串null转空     * @param str     * @return boolean     */    public static String nulltoStr(String str) {        return StrisNull(str) ? &quot;&quot; : str;    }    /**     * 字符串null赋值默认值     * @param str 目标字符串     * @param defaut 默认值     * @return String     */    public static String nulltoStr(String str, String defaut) {        return StrisNull(str) ? defaut : str;    }    /**     * 判断字段是否为Email 符合返回ture     * @param str     * @return boolean     */    public static boolean isEmail(String str) {        return Regular(str, EMAIL);    }    /**     * 判断是否为电话号码 符合返回ture     * @param str     * @return boolean     */    public static boolean isPhone(String str) {        return Regular(str, PHONE);    }    /**     * 判断是否为手机号码 符合返回ture     * @param str     * @return boolean     */    public static boolean isMobile(String str) {        return RegularSJHM(str, MOBILE);    }    /**     * 判断是否为Url 符合返回ture     * @param str     * @return boolean     */    public static boolean isUrl(String str) {        return Regular(str, URL);    }    /**     * 判断字段是否为数字 正负整数 正负浮点数 符合返回ture     * @param str     * @return boolean     */    public static boolean isNumber(String str) {        return Regular(str, DOUBLE);    }    /**     * 判断字段是否为INTEGER 符合返回ture     * @param str     * @return boolean     */    public static boolean isInteger(String str) {        return Regular(str, INTEGER);    }    /**     * 判断字段是否为正整数正则表达式 &gt;=0 符合返回ture     * @param str     * @return boolean     */    public static boolean isINTEGER_NEGATIVE(String str) {        return Regular(str, INTEGER_NEGATIVE);    }    /**     * 判断字段是否为负整数正则表达式 &lt;=0 符合返回ture     * @param str     * @return boolean     */    public static boolean isINTEGER_POSITIVE(String str) {        return Regular(str, INTEGER_POSITIVE);    }    /**     * 判断字段是否为DOUBLE 符合返回ture     * @param str     * @return boolean     */    public static boolean isDouble(String str) {        return Regular(str, DOUBLE);    }    /**     * 判断字段是否为正浮点数正则表达式 &gt;=0 符合返回ture     * @param str     * @return boolean     */    public static boolean isDOUBLE_NEGATIVE(String str) {        return Regular(str, DOUBLE_NEGATIVE);    }    /**     * 判断字段是否为负浮点数正则表达式 &lt;=0 符合返回ture     * @param str     * @return boolean     */    public static boolean isDOUBLE_POSITIVE(String str) {        return Regular(str, DOUBLE_POSITIVE);    }    /**     * 判断字段是否为日期 符合返回ture     * @param str     * @return boolean     */    public static boolean isDate(String str) {        return Regular(str, DATE_ALL);    }    /**     * 验证     * @param str     * @return     */    public static boolean isDate1(String str) {        return Regular(str, DATE_FORMAT1);    }    /**     * 判断字段是否为年龄 符合返回ture     * @param str     * @return boolean     */    public static boolean isAge(String str) {        return Regular(str, AGE);    }    /**     * 判断字段是否超长 字串为空返回fasle, 超过长度{leng}返回ture 反之返回false     * @param str     * @param leng     * @return boolean     */    public static boolean isLengOut(String str, int leng) {        return StrisNull(str) ? false : str.trim().length() &gt; leng;    }    /**     * 判断字段是否为身份证 符合返回ture     * @param str     * @return boolean     */    public static boolean isIdCard(String str) {        if (StrisNull(str))            return false;        if (str.trim().length() == 15 || str.trim().length() == 18) {            return Regular(str, IDCARD);        } else {            return false;        }    }    /**     * 判断字段是否为邮编 符合返回ture     * @param str     * @return boolean     */    public static boolean isCode(String str) {        return Regular(str, CODE);    }    /**     * 判断字符串是不是全部是英文字母     * @param str     * @return boolean     */    public static boolean isEnglish(String str) {        return Regular(str, STR_ENG);    }    /**     * 判断字符串是不是全部是英文字母+数字     * @param str     * @return boolean     */    public static boolean isENG_NUM(String str) {        return Regular(str, STR_ENG_NUM);    }    /**     * 判断字符串是不是全部是英文字母+数字+下划线     * @param str     * @return boolean     */    public static boolean isENG_NUM_(String str) {        return Regular(str, STR_ENG_NUM_);    }    /**     * 过滤特殊字符串 返回过滤后的字符串     * @param str     * @return boolean     */    public static String filterStr(String str) {        Pattern p = Pattern.compile(STR_SPECIAL);        Matcher m = p.matcher(str);        return m.replaceAll(&quot;&quot;).trim();    }    /**     * 校验机构代码格式     * @return     */    public static boolean isJigouCode(String str) {        return Regular(str, JIGOU_CODE);    }    /**     * 判断字符串是不是数字组成     * @param str     * @return boolean     */    public static boolean isSTR_NUM(String str) {        return Regular(str, STR_NUM);    }    /**     * 匹配是否符合正则表达式pattern 匹配返回true     * @param str 匹配的字符串     * @param pattern 匹配模式     * @return boolean     */    private static boolean Regular(String str, String pattern) {        if (null == str || str.trim().length() &lt;= 0)            return false;        Pattern p = Pattern.compile(pattern);        Matcher m = p.matcher(str);        return m.matches();    }    /**     * 匹配是否符合正则表达式pattern 匹配返回true     * @param str 匹配的字符串     * @param pattern 匹配模式     * @return boolean     */    private static boolean RegularSJHM(String str, String pattern) {        if (null == str || str.trim().length() &lt;= 0){            return false;        }        if(str.contains(&quot;+86&quot;)){            str=str.replace(&quot;+86&quot;,&quot;&quot;);        }        Pattern p = Pattern.compile(pattern);        Matcher m = p.matcher(str);        return m.matches();    }    /**     * description:匹配yyyyMMddHHmmss格式时间     * @param time     * @return boolean     */    public static final String yyyyMMddHHmmss = &quot;[0-9]{14}&quot;;    public static boolean isyyyyMMddHHmmss(String time) {        if (time == null) {            return false;        }        boolean bool = time.matches(yyyyMMddHHmmss);        return bool;    }    /**     * description:匹配yyyyMMddHHmmss格式时间     * @param time     * @return boolean     */    public static final String isMac = &quot;^[A-Fa-f0-9]{2}(-[A-Fa-f0-9]{2}){5}$&quot;;    public static boolean isMac(String mac) {        if (mac == null) {            return false;        }        boolean bool = mac.matches(isMac);        return bool;    }    /**     * description:匹配yyyyMMddHHmmss格式时间     * @param time     * @return boolean     */    public static final String longtime = &quot;[0-9]{10}&quot;;    public static boolean isTimestamp(String timestamp) {        if (timestamp == null) {            return false;        }        boolean bool = timestamp.matches(longtime);        return bool;    }    /**     * 判断字段是否为datatype 符合返回ture     * @param str     * @return boolean     */    public static final String DATATYPE = &quot;^\\d{7}$&quot;;    public static boolean isDATATYPE(String str) {        return Regular(str, DATATYPE);    }    /**     * 判断字段是否为QQ 符合返回ture     * @param str     * @return boolean     */    public static final String QQ = &quot;^\\d{5,15}$&quot;;    public static boolean isQQ(String str) {        return Regular(str, QQ);    }    /**     * 判断字段是否为IMSI 符合返回ture     * @param str     * @return boolean     */    public static final String IMSI = &quot;^4600[0,1,2,3,4,5,6,7,9]\\d{10}|(46011|46020)\\d{10}$&quot;;    public static boolean isIMSI(String str) {        return Regular(str, IMSI);    }    /**     * 判断字段是否为IMEI 符合返回ture     * @param str     * @return boolean     */    public static final String IMEI = &quot;^\\d{8}$|^[a-fA-F0-9]{14}$|^\\d{15}$&quot;;    public static boolean isIMEI(String str) {return Regular(str, IMEI);}    /**     * 判断字段是否为CAPTURETIME 符合返回ture     * @param str     * @return boolean     */    public static final String CAPTURETIME = &quot;^\\d{10}|(20[0-9][0-9])\\d{10}$&quot;;    public static boolean isCAPTURETIME(String str) {return Regular(str, CAPTURETIME);}    /**     * description:检测认证类型     * @param auth     * @return boolean     */    public static final String AUTH_TYPE = &quot;^\\d{7}$&quot;;    public static boolean isAUTH_TYPE(String str) {return Regular(str, CAPTURETIME);}    /**     * description:检测FIRM_CODE     * @param auth     * @return boolean     */    public static final String FIRM_CODE = &quot;^\\d{9}$&quot;;    public static boolean isFIRM_CODE(String str) {return Regular(str, FIRM_CODE);}    /**     * description:检测经度     * @param auth     * @return boolean     */    public static final String LONGITUDE = &quot;^-?(([1-9]\\d?)|(1[0-7]\\d)|180)(\\.\\d{1,6})?$&quot;;    //public static final String LONGITUDE =&quot;^([-]?(\\d|([1-9]\\d)|(1[0-7]\\d)|(180))(\\.\\d*)\\,[-]?(\\d|([1-8]\\d)|(90))(\\.\\d*))$&quot;;    public static boolean isLONGITUDE(String str) {return Regular(str, LONGITUDE);}    /**     * description:检测纬度     *     * @param auth     * @return boolean 2016-7-19 下午4:50:06 by      */    public static final String LATITUDE = &quot;^-?(([1-8]\\d?)|([1-8]\\d)|90)(\\.\\d{1,6})?$&quot;;    public static boolean isLATITUDE(String str) {return Regular(str, LATITUDE);}    public static void main(String[] args) {        boolean bool = isLATITUDE(&quot;25.546685&quot;);        System.out.println(bool);    }}</code></pre><h4 id="5、constant常量"><a href="#5、constant常量" class="headerlink" title="5、constant常量"></a>5、constant常量</h4><p><strong>constant/FlumeConfConstant.java</strong></p><pre><code>package com.hsiehchou.flume.constant;public class FlumeConfConstant {    //flumeSource配置    public static final String UNMANAGE=&quot;unmanage&quot;;    public static final String DIRS=&quot;dirs&quot;;    public static final String SUCCESSFILE=&quot;successfile&quot;;    public static final String ALL=&quot;all&quot;;    public static final String SOURCE=&quot;source&quot;;    public static final String FILENUM=&quot;filenum&quot;;    public static final String SLEEPTIME=&quot;sleeptime&quot;;    //ESSINK配置    public static final String TIMECELL=&quot;timecell&quot;;    public static final String MAXNUM=&quot;maxnum&quot;;    public static final String SINK_SOURCE=&quot;source&quot;;    public static final String THREADNUM=&quot;threadnum&quot;;    public static final String REDISHOST=&quot;redishost&quot;;}</code></pre><p><strong>constant/TxtConstant.java</strong></p><pre><code>package com.hsiehchou.flume.constant;public class TxtConstant {    public static final String TYPE_ES=&quot;TYPE_ES&quot;;    public static final String STATIONCENTER=&quot;STATIONCENTER&quot;;    public static final String APCENTER=&quot;APCENTER&quot;;    public static final String IPLOGINLOG=&quot;IPLOGINLOG&quot;;    public static final String IMSIIMEI=&quot;IMSIIMEI&quot;;    public static final String MACHOUR=&quot;MACHOUR&quot;;    public static final String TYPE_SITEMANAGE=&quot;TYPE_SITEMANAGE&quot;;    public static final String JZWA=&quot;JZWA&quot;;    public static final String FIRMCODE=&quot;FIRMCODE&quot;;    public static final String FILENAME_FIELDS1=&quot;FILENAME_FIELDS1&quot;;    public static final String FILENAME_FIELDS2=&quot;FILENAME_FIELDS2&quot;;    public static final String FILENAME_FIELDS3=&quot;FILENAME_FIELDS3&quot;;    public static final String FILENAME_FIELDS4=&quot;FILENAME_FIELDS4&quot;;    public static final String FILENAME_FIELDS5=&quot;FILENAME_FIELDS5&quot;;    public static final String FILENAME_VALIDATION=&quot;FILENAME_VALIDATION&quot;;    public static final String AUTHTYPE_LIST=&quot;AUTHTYPE_LIST&quot;;    public static final String SOURCE_FEIJING=&quot;SOURCE_FEIJING&quot;;    public static final String SOURCE_650=&quot;SOURCE_650&quot;;    public static final String OFFICE_11=&quot;OFFICE_11&quot;;    public static final String OFFICE_12=&quot;OFFICE_12&quot;;    public static final String WLZK=&quot;WLZK&quot;;    public static final String FEIJING=&quot;FEIJING&quot;;    public static final String HLWZC=&quot;HLWZC&quot;;    public static final String WIFIWL=&quot;WIFIWL&quot;;    // 错误索引    public static final String ERROR_INDEX=&quot;es.errorindex&quot;;    public static final String ERROR_TYPE=&quot;es.errortype&quot;;    //WIFI索引    public static final String WIFILOG_INDEX=&quot;es.index.wifilog&quot;;    public static final String IPLOGINLOG_TYPE=&quot;es.type.iploginlog&quot;;    public static final String EMAIL_TYPE=&quot;es.type.email&quot;;    public static final String FTP_TYPE=&quot;es.type.ftp&quot;;    public static final String GAME_TYPE=&quot;es.type.game&quot;;    public static final String HEARTBEAT_TYPE=&quot;es.type.heartbeat&quot;;    public static final String HTTP_TYPE=&quot;es.type.http&quot;;    public static final String IMINFO_TYPE=&quot;es.type.iminfo&quot;;    public static final String ORGANIZATION_TYPE=&quot;es.type.organization&quot;;    public static final String SEARCH_TYPE=&quot;es.type.search&quot;;    public static final String IMSIIMEI_TYPE=&quot;es.type.imsiimei&quot;;}</code></pre><h4 id="6、field字段"><a href="#6、field字段" class="headerlink" title="6、field字段"></a>6、field字段</h4><p><strong>field/ErrorMapFields.java</strong></p><pre><code>package com.hsiehchou.flume.fields;public class ErrorMapFields {    public static final String RKSJ=&quot;RKSJ&quot;;    public static final String RECORD=&quot;RECORD&quot;;    public static final String LENGTH=&quot;LENGTH&quot;;    public static final String LENGTH_ERROR=&quot;LENGTH_ERROR&quot;;    public static final String LENGTH_ERROR_NUM=&quot;10001&quot;;    public static final String FILENAME=&quot;FILENAME&quot;;    public static final String FILENAME_ERROR=&quot;FILENAME_ERROR&quot;;    public static final String FILENAME_ERROR_NUM=&quot;10010&quot;;    public static final String ABSOLUTE_FILENAME=&quot;ABSOLUTE_FILENAME&quot;;    public static final String SJHM=&quot;SJHM&quot;;    public static final String SJHM_ERROR=&quot;SJHM_ERROR&quot;;    public static final String SJHM_ERRORCODE=&quot;10007&quot;;    public static final String DATA_TYPE=&quot;DATA_TYPE&quot;;    public static final String DATA_TYPE_ERROR=&quot;DATA_TYPE_ERROR&quot;;    public static final String DATA_TYPE_ERRORCODE=&quot;10011&quot;;    public static final String QQ=&quot;QQ&quot;;    public static final String QQ_ERROR=&quot;QQ_ERROR&quot;;    public static final String QQ_ERRORCODE=&quot;10002&quot;;    public static final String IMSI=&quot;IMSI&quot;;    public static final String IMSI_ERROR=&quot;IMSI_ERROR&quot;;    public static final String IMSI_ERRORCODE=&quot;10005&quot;;    public static final String IMEI=&quot;IMEI&quot;;    public static final String IMEI_ERROR=&quot;IMEI_ERROR&quot;;    public static final String IMEI_ERRORCODE=&quot;10006&quot;;    public static final String MAC=&quot;MAC&quot;;    public static final String CLIENTMAC=&quot;CLIENTMAC&quot;;    public static final String STATIONMAC=&quot;STATIONMAC&quot;;    public static final String BSSID=&quot;BSSID&quot;;    public static final String MAC_ERROR=&quot;MAC_ERROR&quot;;    public static final String MAC_ERRORCODE=&quot;10003&quot;;    public static final String DEVICENUM=&quot;DEVICENUM&quot;;    public static final String DEVICENUM_ERROR=&quot;DEVICENUM_ERROR&quot;;    public static final String DEVICENUM_ERRORCODE=&quot;10014&quot;;    public static final String CAPTURETIME=&quot;CAPTURETIME&quot;;    public static final String CAPTURETIME_ERROR=&quot;CAPTURETIME_ERROR&quot;;    public static final String CAPTURETIME_ERRORCODE=&quot;10019&quot;;    public static final String EMAIL=&quot;EMAIL&quot;;    public static final String EMAIL_ERROR=&quot;EMAIL_ERROR&quot;;    public static final String EMAIL_ERRORCODE=&quot;10004&quot;;    public static final String AUTH_TYPE=&quot;AUTH_TYPE&quot;;    public static final String AUTH_TYPE_ERROR=&quot;AUTH_TYPE_ERROR&quot;;    public static final String AUTH_TYPE_ERRORCODE=&quot;10020&quot;;    public static final String FIRM_CODE=&quot;FIRM_CODE&quot;;    public static final String FIRMCODE_NUM=&quot;FIRMCODE_NUM&quot;;    public static final String FIRM_CODE_ERROR=&quot;FIRM_CODE_ERROR&quot;;    public static final String FIRM_CODE_ERRORCODE=&quot;10009&quot;;    public static final String STARTTIME=&quot;STARTTIME&quot;;    public static final String STARTTIME_ERROR=&quot;STARTTIME_ERROR&quot;;    public static final String STARTTIME_ERRORCODE=&quot;10015&quot;;    public static final String ENDTIME=&quot;ENDTIME&quot;;    public static final String ENDTIME_ERROR=&quot;ENDTIME_ERROR&quot;;    public static final String ENDTIME_ERRORCODE=&quot;10016&quot;;    public static final String LOGINTIME=&quot;LOGINTIME&quot;;    public static final String LOGINTIME_ERROR=&quot;LOGINTIME_ERROR&quot;;    public static final String LOGINTIME_ERRORCODE=&quot;10017&quot;;    public static final String LOGOUTTIME=&quot;LOGOUTTIME&quot;;    public static final String LOGOUTTIME_ERROR=&quot;LOGOUTTIME_ERROR&quot;;    public static final String LOGOUTTIME_ERRORCODE=&quot;10018&quot;;    public static final String LONGITUDE=&quot;LONGITUDE&quot;;    public static final String LONGITUDE_ERROR=&quot;LONGITUDE_ERROR&quot;;    public static final String LONGITUDE_ERRORCODE=&quot;10012&quot;;    public static final String LATITUDE=&quot;LATITUDE&quot;;    public static final String LATITUDE_ERROR=&quot;LATITUDE_ERROR&quot;;    public static final String LATITUDE_ERRORCODE=&quot;10013&quot;;    //TODO 其他类型DATA_TYPE  记录    public static final String DATA_TYPE_OTHER=&quot;DATA_TYPE_OTHER&quot;;    public static final String DATA_TYPE_OTHER_ERROR=&quot;DATA_TYPE_OTHER_ERROR&quot;;    public static final String DATA_TYPE_OTHER_ERRORCODE=&quot;10022&quot;;    //TODO USERNAME 错误    public static final String USERNAME=&quot;USERNAME&quot;;    public static final String USERNAME_ERROR=&quot;USERNAME_ERROR&quot;;    public static final String USERNAME_ERRORCODE=&quot;10023&quot;;}</code></pre><p><strong>field/MapFields.java</strong></p><pre><code>package com.hsiehchou.flume.fields;public class MapFields {    public static final String ID=&quot;id&quot;;    public static final String SOURCE=&quot;source&quot;;    public static final String TYPE=&quot;TYPE&quot;;    public static final String TABLE=&quot;table&quot;;    public static final String FILENAME=&quot;filename&quot;;    public static final String RKSJ=&quot;rksj&quot;;    public static final String ABSOLUTE_FILENAME=&quot;absolute_filename&quot;;    public static final String BSSID=&quot;BSSID&quot;;    public static final String USERNAME=&quot;USERNAME&quot;;    public static final String DAYID=&quot;DAYID&quot;;    public static final String FIRMCODE_NUM=&quot;FIRMCODE_NUM&quot;;    public static final String FIRM_CODE=&quot;FIRM_CODE&quot;;    public static final String IMEI=&quot;IMEI&quot;;    public static final String IMSI=&quot;IMSI&quot;;    public static final String DATA_TYPE_NAME=&quot;DATA_TYPE_NAME&quot;;    public static final String AUTH_TYPE=&quot;AUTH_TYPE&quot;;    public static final String AUTH_ACCOUNT=&quot;AUTH_ACCOUNT&quot;;    //TODO 时间类参数    public static final String CAPTURETIME=&quot;CAPTURETIME&quot;;    public static final String LOGINTIME=&quot;LOGINTIME&quot;;    public static final String LOGOUTTIME=&quot;LOGOUTTIME&quot;;    public static final String STARTTIME=&quot;STARTTIME&quot;;    public static final String ENDTIME=&quot;ENDTIME&quot;;    public static final String FIRSTTIME=&quot;FIRSTTIME&quot;;    public static final String LASTTIME=&quot;LASTTIME&quot;;    //TODO 去重参数    public static final String COUNT=&quot;COUNT&quot;;    public static final String DATA_TYPE=&quot;DATA_TYPE&quot;;    public static final String VALUE=&quot;value&quot;;    public static final String SITECODE=&quot;SITECODE&quot;;    public static final String SITECODENEW=&quot;SITECODENEW&quot;;    public static final String DEVICENUM=&quot;DEVICENUM&quot;;    public static final String MAC=&quot;MAC&quot;;    public static final String CLIENTMAC=&quot;CLIENTMAC&quot;;    public static final String STATIONMAC=&quot;STATIONMAC&quot;;    public static final String BRAND=&quot;BRAND&quot;;    public static final String INDEX=&quot;INDEX&quot;;    public static final String ACTION_TYPE=&quot;ACTION_TYPE&quot;;    public static final String CITY_CODE=&quot;CITY_CODE&quot;;    /* public static final String FILENAME_FIELDS1=&quot;FILENAME_FIELDS1&quot;;    public static final String FILENAME_FIELDS1=&quot;FILENAME_FIELDS1&quot;;    public static final String FILENAME_FIELDS1=&quot;FILENAME_FIELDS1&quot;;    public static final String FILENAME_FIELDS1=&quot;FILENAME_FIELDS1&quot;;*/}</code></pre><h4 id="7、自定义sink"><a href="#7、自定义sink" class="headerlink" title="7、自定义sink"></a>7、自定义sink</h4><p><strong>sink/KafkaSink.java—将数据下沉到kafka</strong></p><pre><code>package com.hsiehchou.flume.sink;import com.google.common.base.Throwables;import com.hsiehchou.kafka.producer.StringProducer;import org.apache.flume.*;import org.apache.flume.conf.Configurable;import org.apache.flume.sink.AbstractSink;import org.apache.log4j.Logger;import java.util.ArrayList;import java.util.List;public class KafkaSink extends AbstractSink implements Configurable {    private final Logger logger = Logger.getLogger(KafkaSink.class);    private String[] kafkatopics = null;    //private List&lt;KeyedMessage&lt;String,String&gt;&gt; listKeyedMessage=null;    private List&lt;String&gt; listKeyedMessage=null;    private Long proTimestamp=System.currentTimeMillis();    /**     * 配置读取     * @param context     */    @Override    public void configure(Context context) {        //tier1.sinks.sink1.kafkatopic=chl_test7        //获取 推送kafkatopic参数        kafkatopics = context.getString(&quot;kafkatopics&quot;).split(&quot;,&quot;);        logger.info(&quot;获取kafka topic配置&quot; + context.getString(&quot;kafkatopics&quot;));        listKeyedMessage=new ArrayList&lt;&gt;();    }    @Override    public Status process() throws EventDeliveryException {        logger.info(&quot;sink开始执行&quot;);        Channel channel = getChannel();        Transaction transaction = channel.getTransaction();        transaction.begin();        try {            //从channel中拿到event            Event event = channel.take();            if (event == null) {                transaction.rollback();                return Status.BACKOFF;            }            // 解析记录 获取事件内容            String recourd = new String(event.getBody());            // 发送数据到kafka            try {                //调用kafka的消息推送，将数据推送到kafka                StringProducer.producer(kafkatopics[0],recourd);            /*    if(listKeyedMessage.size()&gt;1000){                    logger.info(&quot;数据大与10000,推送数据到kafka&quot;);                    sendListKeyedMessage();                    logger.info(&quot;数据大与10000,推送数据到kafka成功&quot;);                }else if(System.currentTimeMillis()-proTimestamp&gt;=60*1000){                    logger.info(&quot;时间间隔大与60,推送数据到kafka&quot;);                    sendListKeyedMessage();                    logger.info(&quot;时间间隔大与60,推送数据到kafka成功&quot;+listKeyedMessage.size());                }*/            } catch (Exception e) {                logger.error(&quot;推送数据到kafka失败&quot; , e);                throw Throwables.propagate(e);            }            transaction.commit();            return Status.READY;        } catch (ChannelException e) {            logger.error(e);            transaction.rollback();            return Status.BACKOFF;        } finally {            if(transaction != null){                transaction.close();            }        }    }    @Override    public synchronized void stop() {        super.stop();    }    /*private void sendListKeyedMessage(){        Producer&lt;String, String&gt; producer = new Producer&lt;&gt;(KafkaConfig.getInstance().getProducerConfig());        producer.send(listKeyedMessage);        listKeyedMessage.clear();        proTimestamp=System.currentTimeMillis();        producer.close();    }*/}</code></pre><h4 id="8、service"><a href="#8、service" class="headerlink" title="8、service"></a>8、service</h4><p><strong>DataCheck.java—数据校验</strong></p><pre><code>package com.hsiehchou.flume.service;import com.alibaba.fastjson.JSON;import com.hsiehchou.common.net.HttpRequest;import com.hsiehchou.common.project.datatype.DataTypeProperties;import com.hsiehchou.common.time.TimeTranstationUtils;import com.hsiehchou.flume.fields.ErrorMapFields;import com.hsiehchou.flume.fields.MapFields;import org.apache.log4j.Logger;import java.util.*;/** * 数据校验 */public class DataCheck {    private final static Logger LOG = Logger.getLogger(DataCheck.class);    /**     * 获取数据类型对应的字段  对应的文件     * 结构为 [ 数据类型1 = [字段1，字段2。。。。]，     * 数据类型2 = [字段1，字段2。。。。]]     */    private static Map&lt;String, ArrayList&lt;String&gt;&gt; dataMap = DataTypeProperties.dataTypeMap;    /**     * 数据解析     * @param line     * @param fileName     * @param absoluteFilename     * @return     */    public static Map&lt;String, String&gt; txtParse(String line, String fileName, String absoluteFilename) {        Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();        String[] fileNames = fileName.split(&quot;_&quot;);        String dataType = fileNames[0];        if (dataMap.containsKey(dataType)) {            List&lt;String&gt; fields = dataMap.get(dataType.toLowerCase());            String[] splits = line.split(&quot;\t&quot;);            //长度校验            if (fields.size() == splits.length) {                //添加公共字段                map.put(MapFields.ID, UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;));                map.put(MapFields.TABLE, dataType.toLowerCase());                map.put(MapFields.RKSJ, (System.currentTimeMillis() / 1000) + &quot;&quot;);                map.put(MapFields.FILENAME, fileName);                map.put(MapFields.ABSOLUTE_FILENAME, absoluteFilename);                for (int i = 0; i &lt; splits.length; i++) {                    map.put(fields.get(i), splits[i]);                }            } else {                map = null;                LOG.error(&quot;字段长度不匹配fields&quot;+fields.size()  + &quot;/t&quot; + splits.length);            }        } else {            map = null;            LOG.error(&quot;配置文件中不存在此数据类型&quot;);        }        return map;    }    /**     * 数据长度校验添加必要字段并转map，将长度不符合的插入ES数据库     * @param line     * @param fileName     * @param absoluteFilename     * @return     */    public static Map&lt;String, String&gt; txtParseAndalidation(String line, String fileName, String absoluteFilename) {        Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();        Map&lt;String, Object&gt; errorMap = new HashMap&lt;String, Object&gt;();        //文件名按&quot;_&quot;切分  wechat_source1_1111142.txt        //wechat 数据类型        //source1 数据来源        //1111142  不让文件名相同        String[] fileNames = fileName.split(&quot;_&quot;);        String dataType = fileNames[0];        String source = fileNames[1];        if (dataMap.containsKey(dataType)) {            //获取数据类型字段            // imei,imsi,longitude,latitude,phone_mac,device_mac,device_number,collect_time,username,phone,object_username,send_message,accept_message,message_time            //根据数据类型，获取改类型的字段            List&lt;String&gt; fields = dataMap.get(dataType.toLowerCase());            //line            String[] splits = line.split(&quot;\t&quot;);            //长度校验            if (fields.size() == splits.length) {                for (int i = 0; i &lt; splits.length; i++) {                    map.put(fields.get(i), splits[i]);                }                //添加公共字段                // map.put(SOURCE, source);                map.put(MapFields.ID, UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;));                map.put(MapFields.TABLE, dataType.toLowerCase());                map.put(MapFields.RKSJ, (System.currentTimeMillis() / 1000) + &quot;&quot;);                map.put(MapFields.FILENAME, fileName);                map.put(MapFields.ABSOLUTE_FILENAME, absoluteFilename);                //数据封装完成  开始进行数据校验                errorMap = DataValidation.dataValidation(map);            } else {                errorMap.put(ErrorMapFields.LENGTH, &quot;字段数不匹配 实际&quot; + fields.size() + &quot;\t&quot; + &quot;结果&quot; + splits.length);                errorMap.put(ErrorMapFields.LENGTH_ERROR, ErrorMapFields.LENGTH_ERROR_NUM);                LOG.info(&quot;字段数不匹配 实际&quot; + fields.size() + &quot;\t&quot; + &quot;结果&quot; + splits.length);                map = null;            }            //判断数据是否存在错误            if (null != errorMap &amp;&amp; errorMap.size() &gt; 0) {                LOG.info(&quot;errorMap===&quot; + errorMap);                if (&quot;1&quot;.equals(&quot;1&quot;)) {                    //addErrorMapES(errorMap, map, fileName, absoluteFilename);                    //验证没通过，将错误数据写到ES，并将map置空                    addErrorMapESByHTTP(errorMap, map, fileName, absoluteFilename);                }                map = null;            }        } else {            map = null;            LOG.error(&quot;配置文件中不存在此数据类型&quot;);        }        return map;    }    /**     *  将错误信息写入ES，方便查错     * @param errorMap     * @param map     * @param fileName     * @param absoluteFilename     */    public static void addErrorMapESByHTTP(Map&lt;String, Object&gt; errorMap, Map&lt;String, String&gt; map, String fileName, String absoluteFilename) {        String errorType = fileName.split(&quot;_&quot;)[0];        errorMap.put(MapFields.TABLE, errorType);        errorMap.put(MapFields.ID, UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;));        errorMap.put(ErrorMapFields.RECORD, map);        errorMap.put(ErrorMapFields.FILENAME, fileName);        errorMap.put(ErrorMapFields.ABSOLUTE_FILENAME, absoluteFilename);        errorMap.put(ErrorMapFields.RKSJ, TimeTranstationUtils.Date2yyyy_MM_dd_HH_mm_ss());        String url=&quot;http://192.168.116.201:9200/error_recourd/error_recourd/&quot;+ errorMap.get(MapFields.ID).toString();        String json = JSON.toJSONString(errorMap);        HttpRequest.sendPost(url,json);        //HttpRequest.sendPostMessage(url, errorMap);    }    /*    public static void addErrorMapES(Map&lt;String, Object&gt; errorMap, Map&lt;String, String&gt; map, String fileName, String absoluteFilename) {        String errorType = fileName.split(&quot;_&quot;)[0];        errorMap.put(MapFields.TABLE, errorType);        errorMap.put(MapFields.ID, UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;));        errorMap.put(ErrorMapFields.RECORD, map);        errorMap.put(ErrorMapFields.FILENAME, fileName);        errorMap.put(ErrorMapFields.ABSOLUTE_FILENAME, absoluteFilename);        errorMap.put(ErrorMapFields.RKSJ, TimeTranstationUtils.Date2yyyy_MM_dd_HH_mm_ss());        TransportClient client = null;        try {            LOG.info(&quot;开始获取客户端===============================&quot; + errorMap);            client = ESClientUtils.getClient();        } catch (Throwable t) {            if (t instanceof Error) {                throw (Error)t;            }            LOG.error(null,t);        }        //JestClient jestClient = JestService.getJestClient();        //boolean bool = JestService.indexOne(jestClient,TxtConstant.ERROR_INDEX, TxtConstant.ERROR_TYPE,errorMap.get(MapFields.ID).toString(),errorMap);        LOG.info(&quot;开始写入错误数据到ES===============================&quot; + errorMap);        boolean bool = IndexUtil.putIndexData(TxtConstant.ERROR_INDEX, TxtConstant.ERROR_TYPE, errorMap.get(MapFields.ID).toString(), errorMap,client);        if(bool){            LOG.info(&quot;写入错误数据到ES===============================&quot; + errorMap);        }else{            LOG.info(&quot;写入错误数据到ES===============================失败&quot;);        }    }*/    public static void main(String[] args) {    }}</code></pre><p><strong>DataValidation.java</strong></p><pre><code>package com.hsiehchou.flume.service;import com.hsiehchou.flume.fields.ErrorMapFields;import com.hsiehchou.flume.fields.MapFields;import com.hsiehchou.flume.utils.Validation;import org.apache.commons.lang.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.HashMap;import java.util.List;import java.util.Map;public class DataValidation {    private static final Logger LOG = LoggerFactory.getLogger(DataValidation.class);   //  private static final TxtConfigurationFileReader reader = TxtConfigurationFileReader.getInstance();   //  private static final DataTypeConfigurationFileReader datatypereader = DataTypeConfigurationFileReader.getInstance();   //  private static final ValidationConfigurationFileReader readerValidation = ValidationConfigurationFileReader.getInstance();    private static Map&lt;String,String&gt;  dataTypeMap;    private static List&lt;String&gt; listAuthType;    private static String isErrorES;    private static final String USERNAME=ErrorMapFields.USERNAME;    private static final String DATA_TYPE=ErrorMapFields.DATA_TYPE;    private static final String DATA_TYPE_ERROR=ErrorMapFields.DATA_TYPE_ERROR;    private static final String DATA_TYPE_ERRORCODE=ErrorMapFields.DATA_TYPE_ERRORCODE;    private static final String SJHM=ErrorMapFields.SJHM;    private static final String SJHM_ERROR=ErrorMapFields.SJHM_ERROR;    private static final String SJHM_ERRORCODE=ErrorMapFields.SJHM_ERRORCODE;    private static final String QQ=ErrorMapFields.QQ;    private static final String QQ_ERROR=ErrorMapFields.QQ_ERROR;    private static final String QQ_ERRORCODE=ErrorMapFields.QQ_ERRORCODE;    private static final String IMSI=ErrorMapFields.IMSI;    private static final String IMSI_ERROR=ErrorMapFields.IMSI_ERROR;    private static final String IMSI_ERRORCODE=ErrorMapFields.IMSI_ERRORCODE;    private static final String IMEI=ErrorMapFields.IMEI;    private static final String IMEI_ERROR=ErrorMapFields.IMEI_ERROR;    private static final String IMEI_ERRORCODE=ErrorMapFields.IMEI_ERRORCODE;    private static final String MAC=ErrorMapFields.MAC;    private static final String CLIENTMAC=ErrorMapFields.CLIENTMAC;    private static final String STATIONMAC=ErrorMapFields.STATIONMAC;    private static final String BSSID=ErrorMapFields.BSSID;    private static final String MAC_ERROR=ErrorMapFields.MAC_ERROR;    private static final String MAC_ERRORCODE=ErrorMapFields.MAC_ERRORCODE;    private static final String DEVICENUM=ErrorMapFields.DEVICENUM;    private static final String DEVICENUM_ERROR=ErrorMapFields.DEVICENUM_ERROR;    private static final String DEVICENUM_ERRORCODE=ErrorMapFields.DEVICENUM_ERRORCODE;    private static final String CAPTURETIME=ErrorMapFields.CAPTURETIME;    private static final String CAPTURETIME_ERROR=ErrorMapFields.CAPTURETIME_ERROR;    private static final String CAPTURETIME_ERRORCODE=ErrorMapFields.CAPTURETIME_ERRORCODE;    private static final String EMAIL=ErrorMapFields.EMAIL;    private static final String EMAIL_ERROR=ErrorMapFields.EMAIL_ERROR;    private static final String EMAIL_ERRORCODE=ErrorMapFields.EMAIL_ERRORCODE;    private static final String AUTH_TYPE=ErrorMapFields.AUTH_TYPE;    private static final String AUTH_TYPE_ERROR=ErrorMapFields.AUTH_TYPE_ERROR;    private static final String AUTH_TYPE_ERRORCODE=ErrorMapFields.AUTH_TYPE_ERRORCODE;    private static final String FIRM_CODE=ErrorMapFields.FIRM_CODE;    private static final String FIRM_CODE_ERROR=ErrorMapFields.FIRM_CODE_ERROR;    private static final String FIRM_CODE_ERRORCODE=ErrorMapFields.FIRM_CODE_ERRORCODE;    private static final String STARTTIME=ErrorMapFields.STARTTIME;    private static final String STARTTIME_ERROR=ErrorMapFields.STARTTIME_ERROR;    private static final String STARTTIME_ERRORCODE=ErrorMapFields.STARTTIME_ERRORCODE;    private static final String ENDTIME=ErrorMapFields.ENDTIME;    private static final String ENDTIME_ERROR=ErrorMapFields.ENDTIME_ERROR;    private static final String ENDTIME_ERRORCODE=ErrorMapFields.ENDTIME_ERRORCODE;    private static final String LOGINTIME=ErrorMapFields.LOGINTIME;    private static final String LOGINTIME_ERROR=ErrorMapFields.LOGINTIME_ERROR;    private static final String LOGINTIME_ERRORCODE=ErrorMapFields.LOGINTIME_ERRORCODE;    private static final String LOGOUTTIME=ErrorMapFields.LOGOUTTIME;    private static final String LOGOUTTIME_ERROR=ErrorMapFields.LOGOUTTIME_ERROR;    private static final String LOGOUTTIME_ERRORCODE=ErrorMapFields.LOGOUTTIME_ERRORCODE;    public static Map&lt;String, Object&gt; dataValidation( Map&lt;String, String&gt; map){        if(map == null){            return null;        }        Map&lt;String,Object&gt; errorMap = new HashMap&lt;String,Object&gt;();        //验证手机号码        sjhmValidation(map,errorMap);        //验证MAC        macValidation(map,errorMap);        //验证经纬度        longlaitValidation(map,errorMap);        //定义自己的清洗规则        //TODO 大小写统一        //TODO 时间类型统一        //TODO 数据字段统一        //TODO 业务字段转换        //TODO 数据矫正        //TODO 验证MAC不能为空        //TODO 验证IMSI不能为空        //TODO 验证 QQ IMSI IMEI        //TODO 验证DEVICENUM是否为空 为空返回错误        /*devicenumValidation(map,errorMap);        //TODO 验证CAPTURETIME是否为空 为空过滤   不为10，14位数字过滤        capturetimeValidation(map,errorMap);        //TODO 验证EMAIL        emailValidation(map,errorMap);        //TODO 验证STARTTIME ENDTIME LOGINTIME LOGOUTTIME        timeValidation(map,errorMap);        */        return errorMap;    }    /**     * 手机号码验证     * @param map     * @param errorMap     */    public static void sjhmValidation(Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){        if(map.containsKey(&quot;phone&quot;)){            String sjhm=map.get(&quot;phone&quot;);            //调用正则做手机号码验证，是否是正确的一个，检验            boolean ismobile = Validation.isMobile(sjhm);            if(!ismobile){                errorMap.put(SJHM,sjhm);                errorMap.put(SJHM_ERROR,SJHM_ERRORCODE);            }        }    }    //TODO QQ验证  10002  QQ编码 1030001    需要根据DATATYPE来判断数据类型的一起验证    public static void virtualValidation(String dataType, Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){        //TODO USERNAME验证  10023  长度》=2        if(map.containsKey(ErrorMapFields.USERNAME)){            String username=map.get(ErrorMapFields.USERNAME);            if(StringUtils.isNotBlank(username)){                if(username.length()&lt;2){                    errorMap.put(ErrorMapFields.USERNAME,username);                    errorMap.put(ErrorMapFields.USERNAME_ERROR,ErrorMapFields.USERNAME_ERRORCODE);                }            }        }        //TODO QQ验证  10002  QQ编码 1030001        if(&quot;1030001&quot;.equals(dataType)&amp;&amp; map.containsKey(USERNAME)){            String qqnum= map.get(USERNAME);            boolean bool = Validation.isQQ(qqnum);            if(!bool){                errorMap.put(QQ,qqnum);                errorMap.put(QQ_ERROR,QQ_ERRORCODE);            }        }        //TODO IMSI验证  10005  IMSI编码 1429997        if(&quot;1429997&quot;.equals(dataType)&amp;&amp; map.containsKey(IMSI)){            String imsi= map.get(IMSI);            boolean bool = Validation.isIMSI(imsi);            if(!bool){                errorMap.put(IMSI,imsi);                errorMap.put(IMSI_ERROR,IMSI_ERRORCODE);            }        }        //TODO IMEI验证  10006  IMEI编码 1429998        if(&quot;1429998&quot;.equals(dataType)&amp;&amp; map.containsKey(IMEI)){            String imei= map.get(IMEI);            boolean bool = Validation.isIMEI(imei);            if(!bool){                errorMap.put(IMEI,imei);                errorMap.put(IMEI_ERROR,IMEI_ERRORCODE);            }        }    }    //MAC验证  10003    public static void macValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){        if(map == null){            return ;        }        if(map.containsKey(&quot;phone_mac&quot;)){            String mac=map.get(&quot;phone_mac&quot;);            if(StringUtils.isNotBlank(mac)){                boolean bool = Validation.isMac(mac);                if(!bool){                    LOG.info(&quot;MAC验证失败&quot;);                    errorMap.put(MAC,mac);                    errorMap.put(MAC_ERROR,MAC_ERRORCODE);                }            }else{                LOG.info(&quot;MAC验证失败&quot;);                errorMap.put(MAC,mac);                errorMap.put(MAC_ERROR,MAC_ERRORCODE);            }        }    }    /**     * TODO DEVICENUM 验证 为空过滤     * @param map     * @param errorMap     */    public static void devicenumValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){        if(map == null){            return ;        }        if(map.containsKey(&quot;device_number&quot;)){            String devicenum=map.get(&quot;device_number&quot;);            if(StringUtils.isBlank(devicenum)){                errorMap.put(DEVICENUM,&quot;设备编码不能为空&quot;);                errorMap.put(DEVICENUM_ERROR,DEVICENUM_ERRORCODE);            }        }    }    /**     * TODO CAPTURETIME验证 为空过滤  10019  验证时间长度为10或14位     * @param map     * @param errorMap     */    public static void capturetimeValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){        if(map == null){            return ;        }        if(map.containsKey(CAPTURETIME)){            String capturetime=map.get(CAPTURETIME);            if(StringUtils.isBlank(capturetime)){                errorMap.put(CAPTURETIME,&quot;CAPTURETIME不能为空&quot;);                errorMap.put(CAPTURETIME_ERROR,CAPTURETIME_ERRORCODE);            }else{                boolean bool = Validation.isCAPTURETIME(capturetime);                if(!bool){                    errorMap.put(CAPTURETIME,capturetime);                    errorMap.put(CAPTURETIME_ERROR,CAPTURETIME_ERRORCODE);                }            }        }    }    //TODO EMAIL验证 为空过滤 为错误过滤  10004  通过TABLE取USERNAME验证    public static void emailValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){        if(map == null){            return ;        }        if(map.get(&quot;TABLE&quot;).equals(EMAIL)){            String email=map.get(USERNAME);            if(StringUtils.isNotBlank(email)){                boolean bool = Validation.isEmail(email);                if(!bool){                    errorMap.put(EMAIL,email);                    errorMap.put(EMAIL_ERROR,EMAIL_ERRORCODE);                }            }else{                errorMap.put(EMAIL,&quot;EMAIL不能为空&quot;);                errorMap.put(EMAIL_ERROR,EMAIL_ERRORCODE);            }        }    }    //TODO EMAIL验证 为空过滤 为错误过滤  10004  通过TABLE取USERNAME验证    public static void timeValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){        if(map == null){            return ;        }        if(map.containsKey(STARTTIME)&amp;&amp;map.containsKey(ENDTIME)){            String starttime=map.get(STARTTIME);            String endtime=map.get(ENDTIME);            if(StringUtils.isBlank(starttime)&amp;&amp;StringUtils.isBlank(endtime)){                errorMap.put(STARTTIME,&quot;STARTTIME和ENDTIME不能同时为空&quot;);                errorMap.put(STARTTIME_ERROR,STARTTIME_ERRORCODE);                errorMap.put(ENDTIME,&quot;STARTTIME和ENDTIME不能同时为空&quot;);                errorMap.put(ENDTIME_ERROR,ENDTIME_ERRORCODE);            }else{                Boolean bool1 = istime(starttime, STARTTIME, STARTTIME_ERROR, STARTTIME_ERRORCODE, errorMap);                Boolean bool2 = istime(endtime, ENDTIME, ENDTIME_ERROR, ENDTIME_ERRORCODE, errorMap);                if(bool1&amp;&amp;bool2&amp;&amp;(starttime.length()!=endtime.length())){                    errorMap.put(STARTTIME,&quot;STARTTIME和ENDTIME长度不等 STARTTIME：&quot;+starttime + &quot;\t&quot;+&quot;ENDTIME:&quot; + endtime);                    errorMap.put(STARTTIME_ERROR,STARTTIME_ERRORCODE);                    errorMap.put(ENDTIME,&quot;STARTTIME和ENDTIME长度不等 STARTTIME：&quot;+starttime + &quot;\t&quot;+&quot;ENDTIME:&quot; + endtime);                    errorMap.put(ENDTIME_ERROR,ENDTIME_ERRORCODE);                }                else if(bool1&amp;&amp;bool2&amp;&amp;(endtime.compareTo(starttime)&lt;0)){                    errorMap.put(STARTTIME,&quot;ENDTIME必须大于STARTTIME  STARTTIME:&quot;+starttime + &quot;\t&quot;+&quot;ENDTIME:&quot; + endtime);                    errorMap.put(STARTTIME_ERROR,STARTTIME_ERRORCODE);                    errorMap.put(ENDTIME,&quot;ENDTIME必须大于STARTTIME  STARTTIME:&quot;+starttime + &quot;\t&quot;+&quot;ENDTIME:&quot; + endtime);                    errorMap.put(ENDTIME_ERROR,ENDTIME_ERRORCODE);                }            }        }else if(map.containsKey(LOGINTIME)&amp;&amp;map.containsKey(LOGOUTTIME)){            String logintime=map.get(LOGINTIME);            String logouttime=map.get(LOGOUTTIME);            if(StringUtils.isBlank(logintime)&amp;&amp;StringUtils.isBlank(logouttime)){                errorMap.put(LOGINTIME,&quot;LOGINTIME和LOGOUTTIME不能同时为空&quot;);                errorMap.put(LOGINTIME_ERROR,LOGINTIME_ERRORCODE);                errorMap.put(LOGOUTTIME,&quot;LOGINTIME和LOGOUTTIME不能同时为空&quot;);                errorMap.put(LOGOUTTIME_ERROR,LOGOUTTIME_ERRORCODE);            }else{                Boolean bool1 = istime(logintime, LOGINTIME, LOGINTIME_ERROR, LOGINTIME_ERRORCODE, errorMap);                Boolean bool2 = istime(logouttime, LOGOUTTIME, LOGOUTTIME_ERROR, LOGOUTTIME_ERRORCODE, errorMap);                if(bool1&amp;&amp;bool2&amp;&amp;(logintime.length()!=logouttime.length())){                    errorMap.put(LOGINTIME,&quot;LOGOUTTIME LOGINTIME长度不等 LOGINTIME：&quot;+logintime + &quot;\t&quot;+&quot;LOGOUTTIME:&quot; + logouttime);                    errorMap.put(LOGINTIME_ERROR,LOGINTIME_ERRORCODE);                    errorMap.put(LOGOUTTIME,&quot;LOGOUTTIME LOGINTIME长度不等 LOGINTIME：&quot;+logintime + &quot;\t&quot;+&quot;LOGOUTTIME:&quot; + logouttime);                    errorMap.put(LOGOUTTIME_ERROR,LOGOUTTIME_ERRORCODE);                }                else if(bool1&amp;&amp;bool2&amp;&amp;(logouttime.compareTo(logintime)&lt;0)){                    errorMap.put(LOGINTIME,&quot;LOGOUTTIME必须大于LOGINTIME  LOGINTIME:&quot;+logintime + &quot;\t&quot;+&quot;LOGOUTTIME:&quot; + logouttime);                    errorMap.put(LOGINTIME_ERROR,LOGINTIME_ERRORCODE);                    errorMap.put(LOGOUTTIME,&quot;LOGOUTTIME必须大于LOGINTIME  LOGINTIME:&quot;+logintime + &quot;\t&quot;+&quot;LOGOUTTIME:&quot; + logouttime);                    errorMap.put(LOGOUTTIME_ERROR,LOGOUTTIME_ERRORCODE);                }            }        }    }    //TODO AUTH_TYPE验证  为空过滤 为错误过滤  10020    public static void authtypeValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){        if(map == null){            return ;        }        String fileName=map.get(MapFields.FILENAME);        if(fileName.split(&quot;_&quot;).length&lt;=2){            map = null;            return;        }        if(StringUtils.isNotBlank(fileName)){            if(&quot;bh&quot;.equals(fileName.split(&quot;_&quot;)[2])||&quot;wy&quot;.equals(fileName.split(&quot;_&quot;)[2])||&quot;yc&quot;.equals(fileName.split(&quot;_&quot;)[2])){                return ;            }else if(map.containsKey(AUTH_TYPE)){                String authtype=map.get(AUTH_TYPE);                if(StringUtils.isNotBlank(authtype)){                    if(listAuthType.contains(authtype)){                        if(&quot;1020004&quot;.equals(authtype)){                            String sjhm=map.get(MapFields.AUTH_ACCOUNT);                            boolean ismobile = Validation.isMobile(sjhm);                            if(!ismobile){                                errorMap.put(SJHM,sjhm);                                errorMap.put(SJHM_ERROR,SJHM_ERRORCODE);                            }                        }                        if(&quot;1020002&quot;.equals(authtype)){                            String mac=map.get(MapFields.AUTH_ACCOUNT);                            boolean ismac = Validation.isMac(mac);                            if(!ismac){                                errorMap.put(MAC,mac);                                errorMap.put(MAC_ERROR,MAC_ERRORCODE);                            }                        }                    }else{                        errorMap.put(AUTH_TYPE,&quot;AUTHTYPE_LIST 影射里没有&quot;+ &quot;\t&quot;+ &quot;[&quot;+ authtype+&quot;]&quot;);                        errorMap.put(AUTH_TYPE_ERROR,AUTH_TYPE_ERRORCODE);                    }                }else{                    errorMap.put(AUTH_TYPE,&quot;AUTH_TYPE 不能为空&quot;);                    errorMap.put(AUTH_TYPE_ERROR,AUTH_TYPE_ERRORCODE);                }            }        }    }    private static final String LONGITUDE = &quot;longitude&quot;;    private static final String LATITUDE = &quot;latitude&quot;;    private static final String LONGITUDE_ERROR=ErrorMapFields.LONGITUDE_ERROR;    private static final String LONGITUDE_ERRORCODE=ErrorMapFields.LONGITUDE_ERRORCODE;    private static final String LATITUDE_ERROR=ErrorMapFields.LATITUDE_ERROR;    private static final String LATITUDE_ERRORCODE=ErrorMapFields.LATITUDE_ERRORCODE;    /**     * 经纬度验证  错误过滤  10012  10013     * @param map     * @param errorMap     */    public static void longlaitValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){        if(map == null){            return ;        }        if(map.containsKey(LONGITUDE)&amp;&amp;map.containsKey(LATITUDE)){            String longitude=map.get(LONGITUDE);            String latitude=map.get(LATITUDE);            boolean bool1 = Validation.isLONGITUDE(longitude);            boolean bool2 = Validation.isLATITUDE(latitude);            if(!bool1){                errorMap.put(LONGITUDE,longitude);                errorMap.put(LONGITUDE_ERROR,LONGITUDE_ERRORCODE);            }            if(!bool2){                errorMap.put(LATITUDE,latitude);                errorMap.put(LATITUDE_ERROR,LATITUDE_ERRORCODE);            }        }    }    public static Boolean istime(String time,String str1,String str2,String str3,Map&lt;String,Object&gt; errorMap){        if(StringUtils.isNotBlank(time)){            boolean bool = Validation.isCAPTURETIME(time);            if(!bool){                errorMap.put(str1,time);                errorMap.put(str2,str3);                return false;            }            return true;        }        return false;    }}</code></pre><h4 id="9、配置CDH上的Agent文件—-跟FolderSource等里面读取配置文件相对应"><a href="#9、配置CDH上的Agent文件—-跟FolderSource等里面读取配置文件相对应" class="headerlink" title="9、配置CDH上的Agent文件—-跟FolderSource等里面读取配置文件相对应"></a>9、配置CDH上的Agent文件—-跟FolderSource等里面读取配置文件相对应</h4><p><img src="/medias/Flume%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6.PNG" alt="Flume配置文件"></p><p><strong>Flume配置：</strong></p><pre><code>tier1.sources= source1tier1.channels=channel1tier1.sinks=sink1#定义source1tier1.sources.source1.type = com.hsiehchou.flume.source.FolderSource#读取文件之后睡眠时间tier1.sources.source1.sleeptime=5tier1.sources.source1.filenum=3000tier1.sources.source1.dirs =/usr/chl/data/filedir/tier1.sources.source1.successfile=/usr/chl/data/filedir_successful/tier1.sources.source1.deserializer.outputCharset=UTF-8tier1.sources.source1.channels = channel1# 定义拦截器1tier1.sources.source1.interceptors=i1tier1.sources.source1.interceptors.i1.type=com.hsiehchou.flume.interceptor.DataCleanInterceptor$Builder#定义channeltier1.channels.channel1.type = memorytier1.channels.channel1.keep-alive= 300tier1.channels.channel1.capacity = 1000000tier1.channels.channel1.transactionCapacity = 5000tier1.channels.channel1.byteCapacityBufferPercentage = 200tier1.channels.channel1.byteCapacity = 80000#定义sink1tier1.sinks.sink1.type = com.hsiehchou.flume.sink.KafkaSinktier1.sinks.sink1.kafkatopics = chl_test7tier1.sinks.sink1.channel = channel1</code></pre><p><img src="/medias/ftp%E7%9B%91%E6%8E%A7%E6%96%87%E4%BB%B6.PNG" alt="ftp监控文件"></p><p><strong>flumesource 不断监控 ftp 文件目录，通过自定义拦截器拦截，然后推送到flumechannel，然后通过flumesink下沉到kafka</strong></p><h4 id="10、flume打包到服务器执行"><a href="#10、flume打包到服务器执行" class="headerlink" title="10、flume打包到服务器执行"></a>10、flume打包到服务器执行</h4><p><img src="/medias/flume%E6%8F%92%E4%BB%B6%E7%9B%AE%E5%BD%95.PNG" alt="flume插件目录"></p><p><strong>不能放在默认的/usr/lib/flume-ng/plugins.d下面</strong></p><p>mkdir -p /var/lib/flume-ng/plugins.d/chl/lib<br>mkdir -p /usr/chl/data/filedir/<br>mkdir -p /usr/chl/data/filedir_successful/</p><p><strong>要设置777，flume启动的时候是以flume权限启动的，所以要更改权限</strong><br><strong>chmod 777 /usr/chl/data/filedir/</strong></p><p>kafka-topics –zookeeper hadoop1:2181 –topic chl_test7 –create –replication-factor 1 –partitions 3</p><p>kafka-topics –zookeeper hadoop1:2181 –list</p><p>kafka-topics –zookeeper hadoop1:2181 –delete –topic chl_test7</p><p>kafka-console-consumer –bootstrap-server hadoop1:9092 –topic chl_test7 –from-beginning<br><img src="/medias/%E6%B6%88%E8%B4%B9kafka.PNG" alt="消费kafka"></p><h3 id="六、Kafka开发"><a href="#六、Kafka开发" class="headerlink" title="六、Kafka开发"></a>六、Kafka开发</h3><p><strong>xz_bigdata_kafka</strong></p><h4 id="1、pom-xml"><a href="#1、pom-xml" class="headerlink" title="1、pom.xml"></a>1、pom.xml</h4><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_kafka&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_kafka&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;        &lt;poi.version&gt;3.14&lt;/poi.version&gt;        &lt;kafka.version&gt;0.9.0-kafka-2.0.2&lt;/kafka.version&gt;        &lt;mysql.connector.version&gt;5.1.46&lt;/mysql.connector.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;            &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;            &lt;version&gt;${zookeeper.version}-${cdh.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;            &lt;artifactId&gt;kafka_2.10&lt;/artifactId&gt;            &lt;version&gt;${kafka.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.poi&lt;/groupId&gt;            &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt;            &lt;version&gt;${poi.version}&lt;/version&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;            &lt;artifactId&gt;scala-library&lt;/artifactId&gt;            &lt;version&gt;${scala.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;artifactId&gt;scala-reflect&lt;/artifactId&gt;            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;            &lt;version&gt;${scala.version}&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><h4 id="2、config-KafkaConfig-java—kafka配置文件-解析器"><a href="#2、config-KafkaConfig-java—kafka配置文件-解析器" class="headerlink" title="2、config/KafkaConfig.java—kafka配置文件 解析器"></a>2、config/KafkaConfig.java—kafka配置文件 解析器</h4><pre><code>package com.hsiehchou.kafka.config;import com.hsiehchou.common.config.ConfigUtil;import kafka.producer.ProducerConfig;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.text.SimpleDateFormat;import java.util.Date;import java.util.Properties;/** * kafka配置文件 解析器 */public class KafkaConfig {    private static final Logger LOG = LoggerFactory.getLogger(KafkaConfig.class);    private static final String DEFUALT_CONFIG_PATH = &quot;kafka/kafka-server-config.properties&quot;;    private volatile static KafkaConfig kafkaConfig = null;    private ProducerConfig config;    private Properties properties;    private KafkaConfig() throws IOException{        try {            properties = ConfigUtil.getInstance().getProperties(DEFUALT_CONFIG_PATH);        } catch (Exception e) {            IOException ioException = new IOException();            ioException.addSuppressed(e);            throw ioException;        }        config = new ProducerConfig(properties);    }    public static KafkaConfig getInstance(){        if(kafkaConfig == null){            synchronized (KafkaConfig.class) {                if(kafkaConfig == null){                    try {                        kafkaConfig = new KafkaConfig();                    } catch (IOException e) {                        LOG.error(&quot;实例化kafkaConfig失败&quot;, e);                    }                }            }        }        return kafkaConfig;    }    public ProducerConfig getProducerConfig(){        return config;    }    /**      * 获取当前时间的字符串       格式为：yyyy-MM-dd HH:mm:ss      * @return String     */    public static String nowStr(){        return new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;).format( new Date() );    }}</code></pre><h4 id="3、producer-StringProducer-java—生产者"><a href="#3、producer-StringProducer-java—生产者" class="headerlink" title="3、producer/StringProducer.java—生产者"></a>3、producer/StringProducer.java—生产者</h4><pre><code>package com.hsiehchou.kafka.producer;import com.hsiehchou.common.thread.ThreadPoolManager;import com.hsiehchou.kafka.config.KafkaConfig;import kafka.javaapi.producer.Producer;import kafka.producer.KeyedMessage;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.ArrayList;import java.util.List;import java.util.concurrent.CountDownLatch;import java.util.concurrent.ExecutorService;public class StringProducer {    private static final Logger LOG = LoggerFactory.getLogger(StringProducer.class);    public static void main(String[] args) {        StringProducer.producer(&quot;chl_test2&quot;,&quot;{\&quot;rksj\&quot;:\&quot;1558177156\&quot;,\&quot;latitude\&quot;:\&quot;24.000000\&quot;,\&quot;imsi\&quot;:\&quot;000000000000000\&quot;,\&quot;accept_message\&quot;:\&quot;\&quot;,\&quot;phone_mac\&quot;:\&quot;aa-aa-aa-aa-aa-aa\&quot;,\&quot;device_mac\&quot;:\&quot;bb-bb-bb-bb-bb-bb\&quot;,\&quot;message_time\&quot;:\&quot;1789098762\&quot;,\&quot;filename\&quot;:\&quot;wechat_source1_1111119.txt\&quot;,\&quot;absolute_filename\&quot;:\&quot;/usr/chl/data/filedir_successful/2019-05-18/data/filedir/wechat_source1_1111119.txt\&quot;,\&quot;phone\&quot;:\&quot;18609765432\&quot;,\&quot;device_number\&quot;:\&quot;32109231\&quot;,\&quot;imei\&quot;:\&quot;000000000000000\&quot;,\&quot;id\&quot;:\&quot;1792d6529e2143fa85717e706403c83c\&quot;,\&quot;collect_time\&quot;:\&quot;1557305988\&quot;,\&quot;send_message\&quot;:\&quot;\&quot;,\&quot;table\&quot;:\&quot;wechat\&quot;,\&quot;object_username\&quot;:\&quot;judy\&quot;,\&quot;longitude\&quot;:\&quot;23.000000\&quot;,\&quot;username\&quot;:\&quot;andiy\&quot;}&quot;);    }    private static int threadSize = 6;    /**     * 生产单条消息 单条推送     * @param topic     * @param recourd     */    public static void producer(String topic,String recourd){        Producer&lt;String, String&gt; producer = new Producer&lt;&gt;(KafkaConfig.getInstance().getProducerConfig());        KeyedMessage&lt;String, String&gt; keyedMessage = new KeyedMessage&lt;&gt;(topic, recourd);        producer.send(keyedMessage);        LOG.info(&quot;发送数据&quot;+recourd+&quot;到kafka成功&quot;);        producer.close();    }    /**     * 批量推送     * @param topic     * @param listRecourd     */    public static void producerList(String topic,List&lt;String&gt; listRecourd){        Producer&lt;String, String&gt; producer = new Producer&lt;&gt;(KafkaConfig.getInstance().getProducerConfig());        List&lt;KeyedMessage&lt;String, String&gt;&gt; listKeyedMessage= new ArrayList&lt;&gt;();        listRecourd.forEach(recourd-&gt;{            listKeyedMessage.add(new KeyedMessage&lt;&gt;(topic, recourd));        });        producer.send(listKeyedMessage);        producer.close();    }    /**     * 多线程推送     * @param topic  kafka  topic     * @param listMessage 消息     * @throws Exception     */    public void producer(String topic,List&lt;String&gt; listMessage) throws Exception{        //  int size = listMessage.size();        List&lt;List&lt;String&gt;&gt; lists = splitList(listMessage, 5);        int threadNum = lists.size();        long t1 = System.currentTimeMillis();        CountDownLatch cdl = new CountDownLatch(threadNum);        //使用线程池        ExecutorService executorService = ThreadPoolManager.getInstance().getExecutorService();        LOG.info(&quot;开启 &quot; + threadNum + &quot; 个线程来向  topic &quot; + topic + &quot; 生产数据 . &quot;);        for (int i = 0; i &lt; threadNum; i++) {            try {                executorService.execute(new ProducerTask(topic,lists.get(i)));            } catch (Exception e) {                LOG.error(&quot;&quot;, e);            }        }        cdl.await();        long t = System.currentTimeMillis() - t1;        LOG.info(  &quot; 一共耗时  ：&quot; + t + &quot;  毫秒 ... &quot; );        executorService.shutdown();    }    /**     * 拆分消息集合,计算使用多少个线程执行运算     * @param mtList     */    public static List&lt;List&lt;String&gt;&gt; splitList(List&lt;String&gt; mtList, int splitSize){        if(mtList == null || mtList.size()==0){            return null;        }        int length = mtList.size();        // 计算可以分成多少组        int num = ( length + splitSize - 1 )/splitSize ;        List&lt;List&lt;String&gt;&gt; spiltList = new ArrayList&lt;&gt;(num);        for (int i = 0; i &lt; num; i++) {            // 开始位置            int fromIndex = i * splitSize;            // 结束位置            int toIndex = (i+1) * splitSize &lt; length ? ( i+1 ) * splitSize : length ;            spiltList.add(mtList.subList(fromIndex,toIndex)) ;        }        return  spiltList;    }    class ProducerTask implements Runnable{        private String topic;        private List&lt;String&gt; listRecourd;        public ProducerTask( String topic, List&lt;String&gt; listRecourd){            this.topic = topic;            this.listRecourd = listRecourd;        }        public void run() {            producerList(topic,listRecourd);        }    }   /* public static void producer(String topic,List&lt;KeyedMessage&lt;String,String&gt;&gt; listMessage) throws Exception{        int size = listMessage.size();        int threads = ( ( size - 1  ) / threadSize ) + 1;        long t1 = System.currentTimeMillis();        CountDownLatch cdl = new CountDownLatch(threads);        //使用线程池        ExecutorService executorService = ThreadPoolManager.getInstance().getExecutorService();        LOG.info(&quot;开启 &quot; + threads + &quot; 个线程来向  topic &quot; + topic + &quot; 生产数据 . &quot;);      *//*  for( int i = 0 ; i &lt; threads ; i++ ){            executorService.execute( new StringProducer.ChildProducer( start , end ,  topic , id,  cdl ));        }*//*        cdl.await();        long t = System.currentTimeMillis() - t1;        LOG.info(  &quot; 一共耗时  ：&quot; + t + &quot;  毫秒 ... &quot; );        executorService.shutdown();    }    static class ChildProducer implements Runnable{        public ChildProducer( int start , int end ,  String topic , String id,  CountDownLatch cdl ){        }        public void run() {        }    }    */}</code></pre><h3 id="七、Spark—kafka2es开发"><a href="#七、Spark—kafka2es开发" class="headerlink" title="七、Spark—kafka2es开发"></a>七、Spark—kafka2es开发</h3><h4 id="Cloudera查找对应的maven依赖地址"><a href="#Cloudera查找对应的maven依赖地址" class="headerlink" title="Cloudera查找对应的maven依赖地址"></a>Cloudera查找对应的maven依赖地址</h4><p><a href="https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo_514x.html#concept_flv_nwn_yk" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo_514x.html#concept_flv_nwn_yk</a></p><p><strong>SparkStreaming+Kafka的两种模式receiver模式和Direct模式</strong></p><h4 id="Sparkstreming-kafka-receiver模式理解"><a href="#Sparkstreming-kafka-receiver模式理解" class="headerlink" title="Sparkstreming + kafka receiver模式理解"></a>Sparkstreming + kafka receiver模式理解</h4><p><img src="/medias/kafka%E7%9A%84receiver%E6%A8%A1%E5%BC%8F.PNG" alt="kafka的receiver模式"></p><p> <strong>receiver模式理解</strong><br>在SparkStreaming程序运行起来后，Executor中会有receiver tasks接收kafka推送过来的数据。数据会被持久化，默认级别为MEMORY_AND_DISK_SER_2,这个级别也可以修改。receiver task对接收过来的数据进行存储和备份，这个过程会有节点之间的数据传输。备份完成后去zookeeper中更新消费偏移量，然后向Driver中的receiver tracker汇报数据的位置。最后Driver根据数据本地化将task分发到不同节点上执行。</p><p><strong>receiver模式中存在的问题</strong><br>当Driver进程挂掉后，Driver下的Executor都会被杀掉，当更新完zookeeper消费偏移量的时候，Driver如果挂掉了，就会存在找不到数据的问题，相当于丢失数据。</p><p><strong>如何解决这个问题？</strong><br>开启WAL(write ahead log)预写日志机制,在接受过来数据备份到其他节点的时候，同时备份到HDFS上一份（我们需要将接收来的数据的持久化级别降级到MEMORY_AND_DISK），这样就能保证数据的安全性。不过，因为写HDFS比较消耗性能，要在备份完数据之后才能进行更新zookeeper以及汇报位置等，这样会增加job的执行时间，这样对于任务的执行提高了延迟度。</p><p><strong>注意</strong><br>1）开启WAL之后，接受数据级别要降级，有效率问题<br>2）开启WAL要checkpoint<br>3）开启WAL(write ahead log),往HDFS中备份一份数据</p><h4 id="Sparkstreming-kafka-receiver模式理解-1"><a href="#Sparkstreming-kafka-receiver模式理解-1" class="headerlink" title="Sparkstreming + kafka receiver模式理解"></a>Sparkstreming + kafka receiver模式理解</h4><p><img src="/medias/kafka%E7%9A%84direct%E6%A8%A1%E5%BC%8F.PNG" alt="kafka的direct模式"></p><ol><li>简化数据处理流程</li><li>自己定义offset存储，保证数据0丢失，但是会存在重复消费问题。（解决消费等幂问题）</li><li>不用接收数据，自己去kafka中拉取</li></ol><h4 id="1、spark下的pom-xml"><a href="#1、spark下的pom-xml" class="headerlink" title="1、spark下的pom.xml"></a>1、spark下的pom.xml</h4><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_spark&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_spark&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;        &lt;spark.version&gt;1.6.0&lt;/spark.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_es&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_redis&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_hbase&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;servlet-api&lt;/artifactId&gt;                    &lt;groupId&gt;javax.servlet&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;gson&lt;/artifactId&gt;                    &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;            &lt;version&gt;${spark.version}-${cdh.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-hive_2.10&lt;/artifactId&gt;            &lt;version&gt;${spark.version}-${cdh.version}&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;httpcore&lt;/artifactId&gt;                    &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;httpclient&lt;/artifactId&gt;                    &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;                    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;gson&lt;/artifactId&gt;                    &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt;            &lt;version&gt;${spark.version}-${cdh.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-streaming-kafka_2.10&lt;/artifactId&gt;            &lt;version&gt;${spark.version}-${cdh.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;            &lt;artifactId&gt;elasticsearch-spark-13_2.10&lt;/artifactId&gt;            &lt;version&gt;6.2.3&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;                &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;                &lt;version&gt;2.15.2&lt;/version&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;goals&gt;                            &lt;goal&gt;compile&lt;/goal&gt;                            &lt;goal&gt;testCompile&lt;/goal&gt;                        &lt;/goals&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;plugin&gt;&lt;!--打包依赖的jar包--&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;outputDirectory&gt;${project.build.directory}/lib&lt;/outputDirectory&gt;                    &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt; &lt;!-- 表示是否不包含间接依赖的包 --&gt;                    &lt;stripVersion&gt;false&lt;/stripVersion&gt; &lt;!-- 去除版本信息 --&gt;                &lt;/configuration&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;copy-dependencies&lt;/id&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;!-- 拷贝项目依赖包到lib/目录下 --&gt;                            &lt;outputDirectory&gt;${project.build.directory}/jars&lt;/outputDirectory&gt;                            &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt;                            &lt;stripVersion&gt;false&lt;/stripVersion&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><h4 id="2、spark中的文件结构"><a href="#2、spark中的文件结构" class="headerlink" title="2、spark中的文件结构"></a>2、spark中的文件结构</h4><p><img src="/medias/spark%E4%B8%AD%E7%9A%84%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84.PNG" alt="spark中的文件结构"></p><p><img src="/medias/%E8%AE%A9IDEA%E8%83%BD%E6%96%B0%E5%BB%BAscala.class.PNG" alt="让IDEA能新建scala.class"></p><p>点击”+”号，选择Scala SDK，点击Browse，选择本地下载的scala-sdk-2.10.4</p><h4 id="3、xz-bigdata-spark-spark-common"><a href="#3、xz-bigdata-spark-spark-common" class="headerlink" title="3、xz_bigdata_spark/spark/common"></a>3、xz_bigdata_spark/spark/common</h4><p><strong>SparkContextFactory.scala</strong></p><pre><code>package com.hsiehchou.spark.commonimport org.apache.spark.streaming.{Seconds, StreamingContext}import org.apache.spark.{Accumulator, SparkContext}object SparkContextFactory {  def newSparkBatchContext(appName:String = &quot;sparkBatch&quot;) : SparkContext = {    val sparkConf = SparkConfFactory.newSparkBatchConf(appName)    new SparkContext(sparkConf)  }  def newSparkLocalBatchContext(appName:String = &quot;sparkLocalBatch&quot; , threads : Int = 2) : SparkContext = {    val sparkConf = SparkConfFactory.newSparkLoalConf(appName, threads)    sparkConf.set(&quot;&quot;,&quot;&quot;)    new SparkContext(sparkConf)  }  def getAccumulator(appName:String = &quot;sparkBatch&quot;) : Accumulator[Int] = {    val sparkConf = SparkConfFactory.newSparkBatchConf(appName)    val accumulator: Accumulator[Int] = new SparkContext(sparkConf).accumulator(0,&quot;&quot;)    accumulator  }  /**    * 创建本地流streamingContext    * @param appName             appName    * @param batchInterval      多少秒读取一次    * @param threads            开启多少个线程    * @return    */  def newSparkLocalStreamingContext(appName:String = &quot;sparkStreaming&quot; ,                                    batchInterval:Long = 30L ,                                    threads : Int = 4) : StreamingContext = {    val sparkConf =  SparkConfFactory.newSparkLocalConf(appName, threads)    // sparkConf.set(&quot;spark.streaming.receiver.maxRate&quot;,&quot;10000&quot;)    sparkConf.set(&quot;spark.streaming.kafka.maxRatePerPartition&quot;,&quot;1&quot;)    new StreamingContext(sparkConf, Seconds(batchInterval))  }  /**    * 创建集群模式streamingContext    * 这里不设置线程数，在submit中指定    * @param appName    * @param batchInterval    * @return    */  def newSparkStreamingContext(appName:String = &quot;sparkStreaming&quot; , batchInterval:Long = 30L) : StreamingContext = {    val sparkConf = SparkConfFactory.newSparkStreamingConf(appName)    new StreamingContext(sparkConf, Seconds(batchInterval))  }  def startSparkStreaming(ssc:StreamingContext){    ssc.start()      ssc.awaitTermination()      ssc.stop()  }}</code></pre><p><strong>convert/DataConvert.scala</strong></p><pre><code>package com.hsiehchou.spark.common.convertimport java.utilimport com.hsiehchou.common.config.ConfigUtilimport org.apache.spark.Loggingimport scala.collection.JavaConversions._/**  * 数据类型转换  */object DataConvert extends Serializable with Logging {  val fieldMappingPath = &quot;es/mapping/fieldmapping.properties&quot;  private val typeFieldMap: util.HashMap[String, util.HashMap[String, String]] = getEsFieldtypeMap()  /**    * 将Map&lt;String,String&gt;转化为Map&lt;String,Object&gt;    */  def strMap2esObjectMap(map:util.Map[String,String]):util.Map[String,Object] ={    //获取配置文件中的数据类型    val dataType = map.get(&quot;table&quot;)    //获取配置文件中的数据类型的 字段类型    val fieldMap = typeFieldMap.get(dataType)    //获取数据类型的所有字段，配置文件里的字段    val keySet = fieldMap.keySet()    //var objectMap:util.HashMap[String,Object] = new util.HashMap[String,Object]()    var objectMap = new java.util.HashMap[String, Object]()    //数据里的字段    val set = map.keySet().iterator()    try {      //遍历真实数据的所有字段      while (set.hasNext()) {        val key = set.next()        var dataType:String = &quot;string&quot;        //如果在配置文件中的key包含真实数据的key        if (keySet.contains(key)) {          //则获取真实数据字段的数据类型          dataType = fieldMap.get(key)        }        dataType match {          case &quot;long&quot; =&gt; objectMap = BaseDataConvert.mapString2Long(map, key, objectMap)          case &quot;string&quot; =&gt; objectMap = BaseDataConvert.mapString2String(map, key, objectMap)          case &quot;double&quot; =&gt; objectMap = BaseDataConvert.mapString2Double(map, key, objectMap)          case _ =&gt; objectMap = BaseDataConvert.mapString2String(map, key, objectMap)        }      }    }catch {      case e: Exception =&gt; logInfo(&quot;转换异常&quot;, e)    }    println(&quot;转换后&quot; + objectMap)    objectMap  }  /**    * 读取 &quot;es/mapping/fieldmapping.properties 配置文件    * 主要作用是将 真实数据 根据配置来作数据类型转换 转换为和ES mapping结构保持一致    * @return    */  def getEsFieldtypeMap(): util.HashMap[String, util.HashMap[String, String]] = {    // [&quot;wechat&quot;:[&quot;phone_mac&quot;:&quot;string&quot;,&quot;latitude&quot;:&quot;long&quot;]]    //定义返回Map    val mapMap = new util.HashMap[String, util.HashMap[String, String]]    val properties = ConfigUtil.getInstance().getProperties(fieldMappingPath)    val tables = properties.get(&quot;tables&quot;).toString.split(&quot;,&quot;)    val tableFields = properties.keySet()    tables.foreach(table =&gt; {      val map = new util.HashMap[String, String]()      tableFields.foreach(tableField =&gt; {        if (tableField.toString.startsWith(table)) {          val key = tableField.toString.split(&quot;\\.&quot;)(1)          val value = properties.get(tableField).toString          map.put(key, value)        }      })      mapMap.put(table, map)    })    mapMap  }}</code></pre><p><img src="/medias/scala%E4%B8%AD%E7%9A%84scala%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84.PNG" alt="scala中的scala文件结构"></p><h4 id="4、org-apache-spark-streaming-kafka-KafkaManager-scala"><a href="#4、org-apache-spark-streaming-kafka-KafkaManager-scala" class="headerlink" title="4、org/apache/spark/streaming/kafka/KafkaManager.scala"></a>4、org/apache/spark/streaming/kafka/KafkaManager.scala</h4><p>构建Kafka时用到，KafkaCluster在org.apache.spark.streaming.kafka下面，而且只能在spark里面使用，这时候我们就可以新建相同的目录结构，就可以引用了，如下图所示：</p><p><img src="/medias/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%96%B0%E5%BB%BAorg.apache.spark.streaming.kafka.PNG" alt="为什么要新建org.apache.spark.streaming.kafka"></p><pre><code>package org.apache.spark.streaming.kafkaimport com.alibaba.fastjson.TypeReferenceimport kafka.common.TopicAndPartitionimport kafka.message.MessageAndMetadataimport kafka.serializer.{Decoder, StringDecoder}import org.apache.spark.Loggingimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.StreamingContextimport org.apache.spark.streaming.dstream.{DStream, InputDStream}import scala.reflect.ClassTag/**  * 包名说明 ：KafkaCluster是私有类，只能在spark包中使用，  *           所以包名保持和 KafkaCluster 一致才能调用  * @param kafkaParams  * @param autoUpdateoffset  */class KafkaManager(val kafkaParams:Map[String, String],                   val autoUpdateoffset:Boolean =true) extends Serializable with Logging {  //构造一个KafkaCluster  @transient  private var cluster = new KafkaCluster(kafkaParams)  //定义一个单例  def kc(): KafkaCluster = {    if (cluster == null) {      cluster = new KafkaCluster(kafkaParams)    }    cluster  }  /**    * 泛型流读取器    * @param ssc    * @param topics kafka topics,多个topic按&quot;,&quot;分割    * @tparam K  泛型 K    * @tparam V  泛型 V    * @tparam KD scala泛型 KD &lt;: Decoder[K] 说明KD 的类型必须是Decoder[K]的子类型  上下界    * @tparam VD scala泛型 VD &lt;: Decoder[V] 说明VD 的类型必须是Decoder[V]的子类型  上下界    * @return    */  def createDirectStream[K: ClassTag, V: ClassTag,  KD &lt;: Decoder[K] : ClassTag,  VD &lt;: Decoder[V] : ClassTag](ssc: StreamingContext, topics: Set[String]): InputDStream[(K, V)] = {    //获取消费者组ID    //val groupId = &quot;test&quot;    val groupId = kafkaParams.get(&quot;group.id&quot;).getOrElse(&quot;default&quot;)    // 在zookeeper上读取offsets前先根据实际情况更新offsets    setOrUpdateOffsets(topics, groupId)    //把所有的offsets处理完成，就可以从zookeeper上读取offset开始消费message    val messages = {      //获取kafka分区信息  为了打印信息      val partitionsE = kc.getPartitions(topics)      require(partitionsE.isRight, s&quot;获取 kafka topic ${topics}`s partition 失败。&quot;)      val partitions = partitionsE.right.get      println(&quot;打印分区信息&quot;)      partitions.foreach(println(_))      //获取分区的offset      val consumerOffsetsE = kc.getConsumerOffsets(groupId, partitions)      require(consumerOffsetsE.isRight, s&quot;获取 kafka topic ${topics}`s consumer offsets 失败。&quot;)      val consumerOffsets = consumerOffsetsE.right.get      println(&quot;打印消费者分区偏移信息&quot;)      consumerOffsets.foreach(println(_))      //读取数据      KafkaUtils.createDirectStream[K, V, KD, VD, (K, V)](        ssc, kafkaParams, consumerOffsets, (mmd: MessageAndMetadata[K, V]) =&gt; (mmd.key, mmd.message))    }    if (autoUpdateoffset) {      //更新offset      messages.foreachRDD(rdd =&gt; {        logInfo(&quot;RDD 消费成功，开始更新zookeeper上的偏移&quot;)        updateZKOffsets(rdd)      })    }    messages  }  /**    * 创建数据流前，根据实际消费情况更新消费offsets    * @param topics    * @param groupId    */  private def setOrUpdateOffsets(topics: Set[String], groupId: String): Unit = {    topics.foreach(topic =&gt; {      //先获取Kafka offset信息  Kafka partions的节点信息      //获取kafka本身的偏移量, Either类型可以认为就是封装了2种信息      val partitionsE = kc.getPartitions(Set(topic))      logInfo(partitionsE + &quot;&quot;)      //require(partitionsE.isRight, &quot;获取partition失败&quot;)      require(partitionsE.isRight, s&quot;获取 kafka topic ${topic}`s partition 失败。&quot;)      println(&quot;partitionsE=&quot; + partitionsE)      val partitions = partitionsE.right.get      println(&quot;打印分区信息&quot;)      partitions.foreach(println(_))      //获取kafka partions最早的offsets      val earliestLeader = kc.getEarliestLeaderOffsets(partitions)      require(earliestLeader.isRight, &quot;获取earliestLeader失败&quot;)      val earliestLeaderOffsets = earliestLeader.right.get      println(&quot;kafka最早的消息偏移量&quot;)      earliestLeaderOffsets.foreach(println(_))      //获取kafka最末尾的offsets      val latestLeader = kc.getLatestLeaderOffsets(partitions)      //require(latestLeader.isRight, &quot;获取latestLeader失败&quot;)      val latestLeaderOffsets = latestLeader.right.get      println(&quot;kafka最末尾的消息偏移量&quot;)      latestLeaderOffsets.foreach(println(_))      //获取消费者的offsets      val consumerOffsetsE = kc.getConsumerOffsets(groupId, partitions)      //判断消费者是否消费过,消费者offset存在      if (consumerOffsetsE.isRight) {        /**          * 如果zk上保存的offsets已经过时了，即kafka的定时清理策略已经将包含该offsets的文件删除。          * 针对这种情况，只要判断一下zk上的consumerOffsets和earliestLeaderOffsets的大小，          * 如果consumerOffsets比earliestLeaderOffsets还小的话，说明consumerOffsets已过时,          * 这时把consumerOffsets更新为earliestLeaderOffsets          */        //如果消费过，直接取过来的kafka消费，，earliestLeader 存在        if (earliestLeader.isRight) {          //获取到最早的offset  也就是最小的offset          require(earliestLeader.isRight, &quot;获取earliestLeader失败&quot;)          val earliestLeaderOffsets = earliestLeader.right.get          //获取消费者组的offset          val consumerOffsets = consumerOffsetsE.right.get          // 将 consumerOffsets 和 earliestLeaderOffsets 的offsets 做比较          // 可能只是存在部分分区consumerOffsets过时，所以只更新过时分区的consumerOffsets为earliestLeaderOffsets          var offsets: Map[TopicAndPartition, Long] = Map()          consumerOffsets.foreach({ case (tp, n) =&gt;            val earliestLeaderOffset = earliestLeaderOffsets(tp).offset            //如果消費者的偏移小于 kafka中最早的offset,那么，將最早的offset更新到zk            if (n &lt; earliestLeaderOffset) {              logWarning(&quot;consumer group:&quot; + groupId + &quot;,topic:&quot; + tp.topic + &quot;,partition:&quot; + tp.partition +                &quot; offsets已经过时，更新为&quot; + earliestLeaderOffset)              offsets += (tp -&gt; earliestLeaderOffset)            }          })          //设置offsets          setOffsets(groupId, offsets)        }      } else {        //如果没有消费过，那么就去取kafka获取earliestLeader写到zk中        // 消费者还没有消费过  也就是zookeeper中还没有消费者的信息        if (earliestLeader.isLeft)          logError(s&quot;${topic} hasConsumed but earliestLeaderOffsets is null。&quot;)        //看是从头消费还是从末开始消费  smallest表示从头开始消费        val reset = kafkaParams.get(&quot;auto.offset.reset&quot;).map(_.toLowerCase).getOrElse(&quot;smallest&quot;)        //往zk中去写，构建消费者 偏移        var leaderOffsets: Map[TopicAndPartition, Long] = Map.empty        //从头消费        if (reset.equals(&quot;smallest&quot;)) {          //分为 存在 和 不存在 最早的消费记录 两种情况          //如果kafka 最小偏移存在，则将消费者偏移设置为和kafka偏移一样          if (earliestLeader.isRight) {            leaderOffsets = earliestLeader.right.get.map {              case (tp, offset) =&gt; (tp, offset.offset)            }          } else {            //如果不存在，则从新构建偏移全部为0 offsets            leaderOffsets = partitions.map(tp =&gt; (tp, 0L)).toMap          }        } else {          //直接获取最新的offset          leaderOffsets = kc.getLatestLeaderOffsets(partitions).right.get.map {            case (tp, offset) =&gt; (tp, offset.offset)          }        }        //设置offsets 写到zk中        setOffsets(groupId, leaderOffsets)      }    })  }  /**    * 设置消费者组的offsets    * @param groupId    * @param offsets    */  private def setOffsets(groupId: String, offsets: Map[TopicAndPartition, Long]): Unit = {    if (offsets.nonEmpty) {      //更新offset      val o = kc.setConsumerOffsets(groupId, offsets)      logInfo(s&quot;更新zookeeper中消费组为：${groupId} 的 topic offset信息为： ${offsets}&quot;)      if (o.isLeft) {        logError(s&quot;Error updating the offset to Kafka cluster: ${o.left.get}&quot;)      }    }  }  /**    * 通过spark的RDD 更新zookeeper上的消费offsets    * @param rdd    */  def updateZKOffsets[K: ClassTag, V: ClassTag](rdd: RDD[(K, V)]) : Unit = {    //获取消费者组    val groupId = kafkaParams.get(&quot;group.id&quot;).getOrElse(&quot;default&quot;)    //spark使用kafka低阶API进行消费的时候,每个partion的offset是保存在 spark的RDD中，所以这里可以直接在    //RDD的 HasOffsetRanges 中获取倒offsets信息。因为这个信息spark不会把则个信息存储到zookeeper中，所以    //我们需要自己实现将这部分offsets信息存储到zookeeper中    val offsetsList = rdd.asInstanceOf[HasOffsetRanges].offsetRanges    //打印出spark中保存的offsets信息    offsetsList.foreach(x=&gt;{      println(&quot;获取spark 中的偏移信息&quot;+x)    })    for (offsets &lt;- offsetsList) {      //根据topic和partition 构建topicAndPartition      val topicAndPartition = TopicAndPartition(offsets.topic, offsets.partition)      logInfo(&quot;将SPARK中的 偏移信息 存到zookeeper中&quot;)      //将消费者组的offsets更新到zookeeper中      setOffsets(groupId, Map((topicAndPartition, offsets.untilOffset)))    }  }  //(null,{&quot;rksj&quot;:&quot;1558178497&quot;,&quot;latitude&quot;:&quot;24.000000&quot;,&quot;imsi&quot;:&quot;000000000000000&quot;})  //读取kafka流，并将json数据转为map  def createJsonToJMapObjectDirectStreamWithOffset(ssc:StreamingContext, topicsSet:Set[String]): DStream[java.util.Map[String,Object]] = {    //一个转换器    val converter = {json:String =&gt;      println(json)      var res : java.util.Map[String,Object] = null      try {        //JSON转map的操作        res = com.alibaba.fastjson.JSON.parseObject(json,          new TypeReference[java.util.Map[String, Object]]() {})      } catch {        case e: Exception =&gt; logError(s&quot;解析topic ${topicsSet}, 的记录 ${json} 失败。&quot;, e)      }      res    }    createDirectStreamWithOffset(ssc, topicsSet, converter).filter(_ != null)  }  /**    * 根据converter创建流数据    * @param ssc    * @param topicsSet    * @param converter    * @tparam T    * @return    */  def createDirectStreamWithOffset[T:ClassTag](ssc:StreamingContext,                                               topicsSet:Set[String], converter:String =&gt; T): DStream[T] = {    createDirectStream[String, String, StringDecoder, StringDecoder](ssc, topicsSet)      .map(pair =&gt;converter(pair._2))  }  def createJsonToJMapDirectStreamWithOffset(ssc:StreamingContext,                                             topicsSet:Set[String]): DStream[java.util.Map[String,String]] = {    val converter = {json:String =&gt;      var res : java.util.Map[String,String] = null      try {        res = com.alibaba.fastjson.JSON.parseObject(json, new TypeReference[java.util.Map[String, String]]() {})      } catch {        case e: Exception =&gt; logError(s&quot;解析topic ${topicsSet}, 的记录 ${json} 失败。&quot;, e)      }      res    }    createDirectStreamWithOffset(ssc, topicsSet, converter).filter(_ != null)  }  /*    /**      * @param ssc      * @param topicsSet      * @return      */    def createJsonToJavaBeanDirectStreamWithOffset(ssc:StreamingContext ,                                                   topicsSet:Set[String]): DStream[Object] = {      val converter = {json:String =&gt;        var res : Object = null        try {          res = com.alibaba.fastjson.JSON.parseObject(json, new TypeReference[Object]() {})        } catch {          case e: Exception =&gt; logError(s&quot;解析topic ${topicsSet}, 的记录 ${json} 失败。&quot;, e)        }        res      }      createDirectStreamWithOffset(ssc, topicsSet, converter).filter(_ != null)    }  */  /*    def createStringDirectStreamWithOffset(ssc:StreamingContext ,                                           topicsSet:Set[String]): DStream[String] = {      val converter = {json:String =&gt;        json      }      createDirectStreamWithOffset(ssc, topicsSet, converter).filter(_ != null)    }  */  /**    * 读取JSON的流 并将JSON流 转为MAP流  并且这个流支持RDD向zookeeper中记录消费信息    * @param ssc   spark ssc    * @param topicsSet topic 集合 支持从多个kafka topic同时读取数据    * @return  DStream[java.util.Map[String,String    */  def createJsonToJMapStringDirectStreamWithOffset(ssc:StreamingContext , topicsSet:Set[String]): DStream[java.util.Map[String,String]] = {    val converter = {json:String =&gt;      var res : java.util.Map[String,String] = null      try {        res = com.alibaba.fastjson.JSON.parseObject(json, new TypeReference[java.util.Map[String, String]]() {})      } catch {        case e: Exception =&gt; logError(s&quot;解析topic ${topicsSet}, 的记录 ${json} 失败。&quot;, e)      }      res    }    createDirectStreamWithOffset(ssc, topicsSet, converter).filter(_ != null)  }  /**    * 读取JSON的流 并将JSON流 转为MAP流  并且这个流支持RDD向zookeeper中记录消费信息    * @param ssc   spark ssc    * @param topicsSet topic 集合 支持从多个kafka topic同时读取数据    * @return  DStream[java.util.Map[String,String    */  def createJsonToJMapStringDirectStreamWithoutOffset(ssc:StreamingContext , topicsSet:Set[String]): DStream[java.util.Map[String,String]] = {    val converter = {json:String =&gt;      var res : java.util.Map[String,String] = null      try {        res = com.alibaba.fastjson.JSON.parseObject(json, new TypeReference[java.util.Map[String, String]]() {})      } catch {        case e: Exception =&gt; logError(s&quot;解析topic ${topicsSet}, 的记录 ${json} 失败。&quot;, e)      }      res    }    createDirectStreamWithOffset(ssc, topicsSet, converter).filter(_ != null)  }}object KafkaManager extends Logging{  def apply(broker:String, groupId:String = &quot;default&quot;,            numFetcher:Int = 1, offset:String = &quot;smallest&quot;,            autoUpdateoffset:Boolean = true): KafkaManager ={    new KafkaManager(      createKafkaParam(broker, groupId, numFetcher, offset),      autoUpdateoffset)  }  def createKafkaParam(broker:String, groupId:String = &quot;default&quot;,                       numFetcher:Int = 1, offset:String = &quot;smallest&quot;): Map[String, String] ={    //创建 stream 时使用的 topic 名字集合    Map[String, String](      &quot;metadata.broker.list&quot; -&gt; broker,      &quot;auto.offset.reset&quot; -&gt; offset,      &quot;group.id&quot; -&gt; groupId,      &quot;num.consumer.fetchers&quot; -&gt; numFetcher.toString)  }}</code></pre><h4 id="5、resources-log4j-properties"><a href="#5、resources-log4j-properties" class="headerlink" title="5、resources/log4j.properties"></a>5、resources/log4j.properties</h4><pre><code>### 设置###log4j.rootLogger = error,stdout,D,E### 输出信息到控制抬 ###log4j.appender.stdout = org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.Target = System.outlog4j.appender.stdout.layout = org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern = [%-5p] %d{yyyy-MM-dd HH:mm:ss,SSS} method:%l%n%m%n### 输出DEBUG 级别以上的日志到=E://logs/error.log ###log4j.appender.D = org.apache.log4j.DailyRollingFileAppenderlog4j.appender.D.File = E://logs/log.loglog4j.appender.D.Append = truelog4j.appender.D.Threshold = stdout log4j.appender.D.layout = org.apache.log4j.PatternLayoutlog4j.appender.D.layout.ConversionPattern = %-d{yyyy-MM-dd HH:mm:ss}  [ %t:%r ] - [ %p ]  %m%n###输出ERROR 级别以上的日志到=E://logs/error.log ###log4j.appender.E = org.apache.log4j.DailyRollingFileAppenderlog4j.appender.E.File =E://logs/error.log log4j.appender.E.Append = truelog4j.appender.E.Threshold = ERROR log4j.appender.E.layout = org.apache.log4j.PatternLayoutlog4j.appender.E.layout.ConversionPattern = %-d{yyyy-MM-dd HH:mm:ss}  [ %t:%r ] - [ %p ]  %m%n</code></pre><h4 id="6、xz-bigdata-spark-spark-streaming-kafka"><a href="#6、xz-bigdata-spark-spark-streaming-kafka" class="headerlink" title="6、xz_bigdata_spark/spark/streaming/kafka"></a>6、xz_bigdata_spark/spark/streaming/kafka</h4><p><strong>Spark_Es_ConfigUtil.scala</strong></p><pre><code>package com.hsiehchou.spark.streaming.kafkaimport org.apache.spark.Loggingobject Spark_Es_ConfigUtil extends Serializable with Logging{ // val ES_NODES = &quot;es.cluster.nodes&quot; // val ES_PORT = &quot;es.cluster.http.port&quot; // val ES_CLUSTERNAME = &quot;es.cluster.name&quot;  val ES_NODES = &quot;es.nodes&quot;  val ES_PORT = &quot;es.port&quot;  val ES_CLUSTERNAME = &quot;es.clustername&quot;  def getEsParam(id_field : String): Map[String,String] ={    Map[String ,String](&quot;es.mapping.id&quot; -&gt; id_field,      ES_NODES -&gt; &quot;hadoop1,hadoop2,hadoop3&quot;,      //ES_NODES -&gt; &quot;hadoop1&quot;,      ES_PORT -&gt; &quot;9200&quot;,      ES_CLUSTERNAME -&gt; &quot;xz_es&quot;,      &quot;es.batch.size.entries&quot;-&gt;&quot;6000&quot;,      /*   &quot;es.nodes.wan.only&quot;-&gt;&quot;true&quot;,*/      &quot;es.nodes.discovery&quot;-&gt;&quot;true&quot;,      &quot;es.batch.size.bytes&quot;-&gt;&quot;300000000&quot;,      &quot;es.batch.write.refresh&quot;-&gt;&quot;false&quot;    )  }}</code></pre><p><strong>Spark_Kafka_ConfigUtil.scala</strong></p><pre><code>package com.hsiehchou.spark.streaming.kafkaimport org.apache.spark.Loggingobject Spark_Kafka_ConfigUtil extends Serializable with Logging{  def getKafkaParam(brokerList:String,groupId : String): Map[String,String]={    val kafkaParam=Map[String,String](      &quot;metadata.broker.list&quot; -&gt; brokerList,      &quot;auto.offset.reset&quot; -&gt; &quot;smallest&quot;,      &quot;group.id&quot; -&gt; groupId,      &quot;refresh.leader.backoff.ms&quot; -&gt; &quot;1000&quot;,      &quot;num.consumer.fetchers&quot; -&gt; &quot;8&quot;)    kafkaParam  }}</code></pre><h4 id="7、kafka2es"><a href="#7、kafka2es" class="headerlink" title="7、kafka2es"></a>7、kafka2es</h4><p><strong>Kafka2esJob.scala</strong></p><pre><code>package com.hsiehchou.spark.streaming.kafka.kafka2esimport com.hsiehchou.es.admin.AdminUtilimport com.hsiehchou.es.client.ESClientUtilsimport com.hsiehchou.spark.common.convert.DataConvertimport com.hsiehchou.spark.streaming.kafka.Spark_Es_ConfigUtilimport org.apache.spark.Loggingimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.dstream.DStreamimport org.elasticsearch.client.transport.TransportClientimport org.elasticsearch.spark.rdd.EsSparkobject Kafka2esJob extends Serializable with Logging {  /**    * 按日期分组写入ES    * @param dataType    * @param typeDS    */  def insertData2EsBydate(dataType:String,typeDS:DStream[java.util.Map[String,String]]): Unit ={    //通过 dataType + 日期来动态创建 分索引。 日期格式为 yyyyMMdd    //主要就是时间混杂  通过时间分组就行了 groupby       filter    //index前缀  通过对日期进行过滤 避免shuffle操作    val index_prefix = dataType    val client: TransportClient = ESClientUtils.getClient    typeDS.foreachRDD(rdd=&gt;{      //如果时少量数据可以这样处理      //rdd.groupBy()      //吧所有的日期拿到      val days = getDays(dataType,rdd)      //我们使用日期对数据进行过滤  par时scala并发集合      days.par.foreach(day=&gt;{        //通过前缀+日期组成一个动态的索引   比例  qq + &quot;_&quot; + &quot;20190508&quot;        val index = index_prefix + &quot;_&quot; + day        //判断索引是否存在        val bool = AdminUtil.indexExists(client,index)        if(!bool){          //如果不存在，创建          val mappingPath = s&quot;es/mapping/${index_prefix}.json&quot;          AdminUtil.buildIndexAndTypes(index, index, mappingPath, 5, 1)        }        //构建RDD，数据类型 某一天的数据RDD        //返回一个map[String,obJECT] 的RDD   //就是一个单一类型  单一天数的RDD        val tableRDD = rdd.filter(map=&gt;{          day.equals(map.get(&quot;index_date&quot;))        }).map(x=&gt;{          //将map[String,String] 转为map[String,obJECT]          DataConvert.strMap2esObjectMap(x)        })        EsSpark.saveToEs(tableRDD,index+ &quot;/&quot;+index,Spark_Es_ConfigUtil.getEsParam(&quot;id&quot;))      })    })    //日期为后  }  /**    * 获取日期的集合    * @param dataType    * @param rdd    * @return    */  def getDays(dataType:String,rdd:RDD[java.util.Map[String,String]]): Array[String] ={    //对日期去重，然后集中到driver    return  rdd.map(x=&gt;{x.get(&quot;index_date&quot;)}).distinct().collect()  }  /**    * 将RDD转换之后写入ES    * @param dataType    * @param typeRDD    */  def insertData2Es(dataType:String,typeRDD:RDD[java.util.Map[String,String]]): Unit = {    val index = dataType    val esRDD =  typeRDD.map(x=&gt;{      DataConvert.strMap2esObjectMap(x)    })    EsSpark.saveToEs(esRDD,index+ &quot;/&quot;+index,Spark_Es_ConfigUtil.getEsParam(&quot;id&quot;))    println(&quot;写入ES&quot; + esRDD.count() + &quot;条数据成功&quot;)  }  /**    * 将RDD转换后写入ES    * @param dataType    * @param typeDS    */  def insertData2Es(dataType:String, typeDS:DStream[java.util.Map[String, String]]): Unit = {    val index = dataType    typeDS.foreachRDD(rdd=&gt;{      val esRDD = rdd.map(x=&gt;{        DataConvert.strMap2esObjectMap(x)      })      EsSpark.saveToEs(rdd, dataType+&quot;/&quot;+dataType, Spark_Es_ConfigUtil.getEsParam(&quot;id&quot;))      println(&quot;写入ES&quot; + esRDD.count() + &quot;条数据成功&quot;)    })  }}</code></pre><p><strong>Kafka2esStreaming.scala</strong></p><pre><code>package com.hsiehchou.spark.streaming.kafka.kafka2esimport java.utilimport java.util.Propertiesimport com.hsiehchou.common.config.ConfigUtilimport com.hsiehchou.common.project.datatype.DataTypePropertiesimport com.hsiehchou.common.time.TimeTranstationUtilsimport com.hsiehchou.spark.common.SparkContextFactoryimport com.hsiehchou.spark.streaming.kafka.Spark_Kafka_ConfigUtilimport org.apache.commons.lang3.StringUtilsimport org.apache.spark.Loggingimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.kafka.KafkaManagerimport scala.collection.JavaConversions._object Kafka2esStreaming extends Serializable with Logging {  //获取数据类型  private val dataTypes: util.Set[String] = DataTypeProperties.dataTypeMap.keySet()  val kafkaConfig: Properties = ConfigUtil.getInstance().getProperties(&quot;kafka/kafka-server-config.properties&quot;)  def main(args: Array[String]): Unit = {    //val topics = &quot;chl_test7&quot;.split(&quot;,&quot;)    val topics = args(1).split(&quot;,&quot;)    //   val ssc = SparkConfFactory.newSparkLocalStreamingContext(&quot;XZ_kafka2es&quot;, java.lang.Long.valueOf(10),1)    val ssc = SparkContextFactory.newSparkStreamingContext(&quot;Kafka2esStreaming&quot;, java.lang.Long.valueOf(10))    //构建kafkaManager    val kafkaManager = new KafkaManager(      Spark_Kafka_ConfigUtil.getKafkaParam(kafkaConfig.getProperty(&quot;metadata.broker.list&quot;), &quot;XZ3&quot;)    )    //使用kafkaManager创建DStreaming流    val kafkaDS = kafkaManager.createJsonToJMapStringDirectStreamWithOffset(ssc, topics.toSet)      //添加一个日期分组字段      //如果数据其他的转换，可以先在这里进行统一转换      .map(map=&gt;{      map.put(&quot;index_date&quot;,TimeTranstationUtils.Date2yyyyMMddHHmmss(java.lang.Long.valueOf(map.get(&quot;collect_time&quot;)+&quot;000&quot;)))      map    }).persist(StorageLevel.MEMORY_AND_DISK)    //使用par并发集合可以是任务并发执行。在资源充足的情况下    dataTypes.foreach(datatype=&gt;{      //过滤出单个类别的数据种类      val tableDS = kafkaDS.filter(x=&gt;{datatype.equals(x.get(&quot;table&quot;))})      Kafka2esJob.insertData2Es(datatype,tableDS)    })    ssc.start()    ssc.awaitTermination()  }  /**    * 启动参数检查    * @param args    */  def sparkParamCheck(args: Array[String]): Unit ={    if (args.length == 4) {      if (StringUtils.isBlank(args(1))) {        logInfo(&quot;kafka集群地址不能为空&quot;)        logInfo(&quot;kafka集群地址格式为     主机1名：9092,主机2名：9092,主机3名：9092...&quot;)        logInfo(&quot;格式为     主机1名：9092,主机2名：9092,主机3名：9092...&quot;)        System.exit(-1)      }      if (StringUtils.isBlank(args(2))) {        logInfo(&quot;kafka topic1不能为空&quot;)        System.exit(-1)      }      if (StringUtils.isBlank(args(3))) {        logInfo(&quot;kafka topic2不能为空&quot;)        System.exit(-1)      }    }else{      logError(&quot;启动参数个数错误&quot;)    }  }  def startJob(ds:DStream[String]): Unit ={  }}</code></pre><p><strong>java/com/hsiehchou/spark/common/convert/BaseDataConvert.java</strong></p><pre><code>package com.hsiehchou.spark.common.convert;import org.apache.commons.lang.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.HashMap;import java.util.Map;public class BaseDataConvert {    private static final Logger LOG = LoggerFactory.getLogger(BaseDataConvert.class);    public static HashMap&lt;String,Object&gt; mapString2Long(Map&lt;String,String&gt; map, String key, HashMap&lt;String,Object&gt; objectMap) {        String logouttime = map.get(key);        if (StringUtils.isNotBlank(logouttime)) {            objectMap.put(key, Long.valueOf(logouttime));        } else {            objectMap.put(key, 0L);        }        return objectMap;    }    public static HashMap&lt;String,Object&gt; mapString2Double(Map&lt;String,String&gt; map, String key, HashMap&lt;String,Object&gt; objectMap) {        String logouttime = map.get(key);        if (StringUtils.isNotBlank(logouttime)) {            objectMap.put(key, Double.valueOf(logouttime));        } else {            objectMap.put(key, 0.000000);        }        return objectMap;    }    public static HashMap&lt;String,Object&gt; mapString2String(Map&lt;String,String&gt; map, String key, HashMap&lt;String,Object&gt; objectMap) {        String logouttime = map.get(key);        if (StringUtils.isNotBlank(logouttime)) {            objectMap.put(key, logouttime);        } else {            objectMap.put(key, &quot;&quot;);        }        return objectMap;    }}</code></pre><h4 id="8、ES动态索引创建"><a href="#8、ES动态索引创建" class="headerlink" title="8、ES动态索引创建"></a>8、ES动态索引创建</h4><pre><code>/**    * 按日期分组写入ES    * @param dataType    * @param typeDS    */  def insertData2EsBydate(dataType:String,typeDS:DStream[java.util.Map[String,String]]): Unit ={    //通过 dataType + 日期来动态创建 分索引。 日期格式为 yyyyMMdd    //主要就是时间混杂  通过时间分组就行了 groupby       filter    //index前缀  通过对日期进行过滤 避免shuffle操作    val index_prefix = dataType    val client: TransportClient = ESClientUtils.getClient    typeDS.foreachRDD(rdd=&gt;{      //如果时少量数据可以这样处理      //rdd.groupBy()      //吧所有的日期拿到      val days = getDays(dataType,rdd)      //我们使用日期对数据进行过滤  par时scala并发集合      days.par.foreach(day=&gt;{        //通过前缀+日期组成一个动态的索引   比例  qq + &quot;_&quot; + &quot;20190508&quot;        val index = index_prefix + &quot;_&quot; + day        //判断索引是否存在        val bool = AdminUtil.indexExists(client,index)        if(!bool){          //如果不存在，创建          val mappingPath = s&quot;es/mapping/${index_prefix}.json&quot;          AdminUtil.buildIndexAndTypes(index, index, mappingPath, 5, 1)        }        //构建RDD，数据类型 某一天的数据RDD        //返回一个map[String,obJECT] 的RDD   //就是一个单一类型  单一天数的RDD        val tableRDD = rdd.filter(map=&gt;{          day.equals(map.get(&quot;index_date&quot;))        }).map(x=&gt;{          //将map[String,String] 转为map[String,obJECT]          DataConvert.strMap2esObjectMap(x)        })        EsSpark.saveToEs(tableRDD,index+ &quot;/&quot;+index,Spark_Es_ConfigUtil.getEsParam(&quot;id&quot;))      })    })    //日期为后  }</code></pre><p><strong>xz_bigdata_es下一节展示代码</strong><br><img src="/medias/%E5%85%A5ES%E4%BD%BF%E7%94%A8%E5%8A%A8%E6%80%81%E7%B4%A2%E5%BC%95.PNG" alt="入ES使用动态索引"></p><h4 id="9、CDH的java配置和Elasticsearch的配置"><a href="#9、CDH的java配置和Elasticsearch的配置" class="headerlink" title="9、CDH的java配置和Elasticsearch的配置"></a>9、CDH的java配置和Elasticsearch的配置</h4><p><strong>cdh的jdk设置</strong><br>/usr/local/jdk1.8</p><p><strong>kafka配置</strong></p><p>Default Number of Partitions：num.partitions 8</p><p>Offset Commit Topic Number of Partitions：180天</p><p>Log Compaction Delete Record Retention Time：log.cleaner.delete.retention.ms 30天</p><p>Data Log Roll Hours：log.retention.hours 30天  log.roll.hours 30天</p><p>Java Heap Size of Broker：broker_max_heap_size  1吉字节</p><p><strong>YARN</strong><br>容器内存 5g 5g 1g 10g</p><p><strong>这里的CDH安装另一篇文章介绍</strong></p><p><strong>前提安装好elasticsearch</strong></p><p>mkdir /opt/software/elasticsearch/data/</p><p>mkdir /opt/software/elasticsearch/logs/</p><p>chmod 777 /opt/software/elasticsearch/data/</p><p>useradd elasticsearch<br>passwd elasticsearch</p><p>chown -R elasticsearch elasticsearch/</p><p><strong>vim /etc/security/limits.conf</strong><br>添加如下内容:<br><code>*</code> <strong>soft nofile 65536</strong><br><code>*</code> <strong>hard nofile 131072</strong><br><code>*</code> <strong>soft nproc 2048</strong><br><code>*</code> <strong>hard nproc 4096</strong></p><p>进入limits.d目录下修改配置文件<br><strong>vim /etc/security/limits.d/90-nproc.conf</strong></p><p>修改如下内容：<br><strong>soft nproc 4096（修改为此参数，6版本的默认就是4096）</strong></p><p>修改配置sysctl.conf<br><strong>vim /etc/sysctl.conf</strong></p><p>添加下面配置：<br><strong>vm.max_map_count=655360</strong></p><p>并执行命令：<br><strong>sysctl -p</strong></p><p><strong>hadoop1的conf配置</strong><br><strong>elasticsearch.yml</strong></p><pre><code># ======================== Elasticsearch Configuration =========================## NOTE: Elasticsearch comes with reasonable defaults for most settings.#       Before you set out to tweak and tune the configuration, make sure you#       understand what are you trying to accomplish and the consequences.## The primary way of configuring a node is via this file. This template lists# the most important settings you may want to configure for a production cluster.## Please consult the documentation for further information on configuration options:# https://www.elastic.co/guide/en/elasticsearch/reference/index.html## ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:##cluster.name: my-applicationcluster.name: xz_es## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:##node.name: node-1#node.name: node-1node.master: truenode.data: true# Add custom attributes to the node:##node.attr.rack: r1## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):##path.data: /path/to/datapath.data: /opt/software/elasticsearch/data## Path to log files:##path.logs: /path/to/logspath.logs: /opt/software/elasticsearch/logs## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:##bootstrap.memory_lock: truebootstrap.memory_lock: falsebootstrap.system_call_filter: false## Make sure that the heap size is set to about half the memory available# on the system and that the owner of the process is allowed to use this# limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):##network.host: 192.168.0.1network.host: 192.168.116.201## Set a custom port for HTTP:##http.port: 9200## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when new node is started:# The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]##discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2&quot;]discovery.zen.ping.unicast.hosts: [&quot;hadoop1&quot;, &quot;hadoop2&quot;, &quot;hadoop3&quot;]## Prevent the &quot;split brain&quot; by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):##discovery.zen.minimum_master_nodes: ## For more information, consult the zen discovery module documentation.## ---------------------------------- Gateway -----------------------------------## Block initial recovery after a full cluster restart until N nodes are started:##gateway.recover_after_nodes: 3## For more information, consult the gateway module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: true</code></pre><p><strong>jvm.options</strong><br>修改下<br>-Xms64m<br>-Xmx64m</p><p><strong>hadoop2的conf配置</strong><br><strong>elasticsearch.yml</strong></p><pre><code># ======================== Elasticsearch Configuration =========================## NOTE: Elasticsearch comes with reasonable defaults for most settings.#       Before you set out to tweak and tune the configuration, make sure you#       understand what are you trying to accomplish and the consequences.## The primary way of configuring a node is via this file. This template lists# the most important settings you may want to configure for a production cluster.## Please consult the documentation for further information on configuration options:# https://www.elastic.co/guide/en/elasticsearch/reference/index.html## ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:##cluster.name: my-applicationcluster.name: xz_es## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:##node.name: node-1node.name: node-2node.master: falsenode.data: true## Add custom attributes to the node:##node.attr.rack: r1## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):##path.data: /path/to/datapath.data: /opt/software/elasticsearch/data## Path to log files:##path.logs: /path/to/logspath.logs: /opt/software/elasticsearch/logs## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:##bootstrap.memory_lock: truebootstrap.memory_lock: falsebootstrap.system_call_filter: false## Make sure that the heap size is set to about half the memory available# on the system and that the owner of the process is allowed to use this# limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):##network.host: 192.168.0.1network.host: 192.168.116.202## Set a custom port for HTTP:##http.port: 9200## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when new node is started:# The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]##discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2&quot;]discovery.zen.ping.unicast.hosts: [&quot;hadoop1&quot;, &quot;hadoop2&quot;, &quot;hadoop3&quot;]## Prevent the &quot;split brain&quot; by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):##discovery.zen.minimum_master_nodes: ## For more information, consult the zen discovery module documentation.## ---------------------------------- Gateway -----------------------------------## Block initial recovery after a full cluster restart until N nodes are started:##gateway.recover_after_nodes: 3## For more information, consult the gateway module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: true</code></pre><p><strong>jvm.options</strong><br>修改下<br>-Xms64m<br>-Xmx64m</p><p><strong>hadoop3的conf配置</strong><br><strong>elasticsearch.yml</strong></p><pre><code># ======================== Elasticsearch Configuration =========================## NOTE: Elasticsearch comes with reasonable defaults for most settings.#       Before you set out to tweak and tune the configuration, make sure you#       understand what are you trying to accomplish and the consequences.## The primary way of configuring a node is via this file. This template lists# the most important settings you may want to configure for a production cluster.## Please consult the documentation for further information on configuration options:# https://www.elastic.co/guide/en/elasticsearch/reference/index.html## ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:##cluster.name: my-applicationcluster.name: xz_es## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:##node.name: node-1node.name: node-3node.master: falsenode.data: true## Add custom attributes to the node:##node.attr.rack: r1## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):##path.data: /path/to/datapath.data: /opt/software/elasticsearch/data## Path to log files:##path.logs: /path/to/logspath.logs: /opt/software/elasticsearch/logs## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:##bootstrap.memory_lock: truebootstrap.memory_lock: falsebootstrap.system_call_filter: false## Make sure that the heap size is set to about half the memory available# on the system and that the owner of the process is allowed to use this# limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):##network.host: 192.168.0.1network.host: 192.168.116.203## Set a custom port for HTTP:##http.port: 9200## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when new node is started:# The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]##discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2&quot;]discovery.zen.ping.unicast.hosts: [&quot;hadoop1&quot;, &quot;hadoop2&quot;, &quot;hadoop3&quot;]## Prevent the &quot;split brain&quot; by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):##discovery.zen.minimum_master_nodes: ## For more information, consult the zen discovery module documentation.## ---------------------------------- Gateway -----------------------------------## Block initial recovery after a full cluster restart until N nodes are started:##gateway.recover_after_nodes: 3## For more information, consult the gateway module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: true</code></pre><p><strong>jvm.options</strong><br>修改下<br>-Xms64m<br>-Xmx64m</p><p><strong>Kibana的conf配置</strong></p><p><strong>kibana.yml</strong></p><pre><code># Kibana is served by a back end server. This setting specifies the port to use.server.port: 5601# Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values.# The default is &#39;localhost&#39;, which usually means remote machines will not be able to connect.# To allow connections from remote users, set this parameter to a non-loopback address.#server.host: &quot;localhost&quot;server.host: &quot;192.168.116.202&quot;# Enables you to specify a path to mount Kibana at if you are running behind a proxy. This only affects# the URLs generated by Kibana, your proxy is expected to remove the basePath value before forwarding requests# to Kibana. This setting cannot end in a slash.#server.basePath: &quot;&quot;# The maximum payload size in bytes for incoming server requests.#server.maxPayloadBytes: 1048576# The Kibana server&#39;s name.  This is used for display purposes.#server.name: &quot;your-hostname&quot;# The URL of the Elasticsearch instance to use for all your queries.#elasticsearch.url: &quot;http://localhost:9200&quot;elasticsearch.url: &quot;http://192.168.116.201:9200&quot;</code></pre><p><strong>运行Elasticsearch</strong><br>cd /opt/software/elasticsearch<br>su elasticsearch<br>bin/elasticsearch &amp;</p><p><strong>运行Kibana</strong><br>cd /opt/software/kibana/<br>bin/kibana &amp;</p><h4 id="10、kafka2es打包到集群执行"><a href="#10、kafka2es打包到集群执行" class="headerlink" title="10、kafka2es打包到集群执行"></a>10、kafka2es打包到集群执行</h4><p><strong>打包</strong><br>使用maven工具点击install</p><p><strong>放入集群</strong><br>将打包完成的jar文件和xz_bigdata_spark-1.0-SNAPSHOT.jar 一起放入/usr/chl/spark7/目录下面</p><p><strong>执行</strong><br>spark-submit <code>--</code>master yarn-cluster <code>--</code>num-executors 1 <code>--</code>driver-memory 500m <code>--</code>executor-memory 1g <code>--</code>executor-cores 1 <code>--</code>jars $(echo /usr/chl/spark7/jars/*.jar | tr ‘ ‘ ‘,’) <code>--</code>class com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar chl_test7 chl_test7</p><p>spark-submit<br><code>--</code>master yarn-cluster    //集群启动<br><code>--</code>num-executors 1        //分配多少个进程<br><code>--</code>driver-memory 500m  //driver内存<br><code>--</code>executor-memory 1g //进程内存<br><code>--</code>executor-cores 1       //开多少个核，线程<br><code>--</code>jars $(echo /usr/chl/spark8/jars/*.jar | tr ‘ ‘ ‘,’) //加载jar<br><code>--</code>class com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</p><h4 id="11、运行截图"><a href="#11、运行截图" class="headerlink" title="11、运行截图"></a>11、运行截图</h4><p><img src="/medias/kafka2esstreaming%E6%88%AA%E5%9B%BE.PNG" alt="kafka2esstreaming截图"></p><p><img src="/medias/Elasticsearch%E5%90%84%E4%B8%AA%E8%8A%82%E7%82%B9%E7%8A%B6%E5%86%B5.PNG" alt="Elasticsearch各个节点状况"></p><h4 id="12、冲突查找快捷键"><a href="#12、冲突查找快捷键" class="headerlink" title="12、冲突查找快捷键"></a>12、冲突查找快捷键</h4><p><strong>Ctrl+Alt+Shift+N</strong></p><h3 id="八、xz-bigdata-es开发"><a href="#八、xz-bigdata-es开发" class="headerlink" title="八、xz_bigdata_es开发"></a>八、xz_bigdata_es开发</h3><h4 id="1、pom-xml-1"><a href="#1、pom-xml-1" class="headerlink" title="1、pom.xml"></a>1、pom.xml</h4><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_es&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_es&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;            &lt;artifactId&gt;transport&lt;/artifactId&gt;            &lt;version&gt;6.2.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;io.searchbox&lt;/groupId&gt;            &lt;artifactId&gt;jest&lt;/artifactId&gt;            &lt;version&gt;6.3.1&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><h4 id="2、admin"><a href="#2、admin" class="headerlink" title="2、admin"></a>2、admin</h4><p><strong>AdminUtil.java</strong></p><pre><code>package com.hsiehchou.es.admin;import com.hsiehchou.common.file.FileCommon;import com.hsiehchou.es.client.ESClientUtils;import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;import org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsResponse;import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.common.settings.Settings;import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class AdminUtil {    private static Logger LOG = LoggerFactory.getLogger(AdminUtil.class);    public static void main(String[] args) throws Exception{        //创建索引核mapping        AdminUtil.buildIndexAndTypes(&quot;tanslator_test1111&quot;,&quot;tanslator_test1111&quot;, &quot;es/mapping/test.json&quot;,3,1);        //index = 类型+日期        //查找类  Ctrl+Shift+Alt+N    }    /**     * @param index     * @param type     * @param path     * @param shard     * @param replication     * @return     * @throws Exception     */    public static boolean buildIndexAndTypes(String index,String type,String path,int shard,int replication) throws Exception{        boolean flag ;        TransportClient client = ESClientUtils.getClient();        String mappingJson = FileCommon.getAbstractPath(path);        boolean indices = AdminUtil.createIndices(client, index, shard, replication);        if(indices){            LOG.info(&quot;创建索引&quot;+ index + &quot;成功&quot;);            flag = MappingUtil.addMapping(client, index, type, mappingJson);        }        else{            LOG.error(&quot;创建索引&quot;+ index + &quot;失败&quot;);            flag = false;        }        return flag;    }    /**     * @desc 判断需要创建的index是否存在     * */    public static boolean indexExists(TransportClient client,String index){        boolean ifExists = false;        try {            System.out.println(&quot;client===&quot; + client);            IndicesExistsResponse existsResponse = client.admin().indices().prepareExists(index).execute().actionGet();            ifExists = existsResponse.isExists();        } catch (Exception e) {            e.printStackTrace();            LOG.error(&quot;判断index是否存在失败...&quot;);            return ifExists;        }        return ifExists;    }    /**     * 创建索引     * @param client     * @param index     * @param shard     * @param replication     * @return     */    public static boolean createIndices(TransportClient client, String index, int shard , int replication){        if(!indexExists(client,index)) {            LOG.info(&quot;该index不存在，创建...&quot;);            CreateIndexResponse createIndexResponse =null;            try {                createIndexResponse = client.admin().indices().prepareCreate(index)                        .setSettings(Settings.builder()                                .put(&quot;index.number_of_shards&quot;, shard)                                .put(&quot;index.number_of_replicas&quot;, replication)                                .put(&quot;index.codec&quot;, &quot;best_compression&quot;)                                .put(&quot;refresh_interval&quot;, &quot;30s&quot;))                        .execute().actionGet();                return createIndexResponse.isAcknowledged();            } catch (Exception e) {                LOG.error(null, e);                return false;            }        }        LOG.warn(&quot;该index &quot; + index + &quot; 已经存在...&quot;);        return false;    }}</code></pre><p><strong>MappingUtil.java</strong></p><pre><code>package com.hsiehchou.es.admin;import com.alibaba.fastjson.JSON;import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;import org.elasticsearch.action.admin.indices.mapping.put.PutMappingResponse;import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.common.xcontent.XContentBuilder;import org.elasticsearch.common.xcontent.XContentFactory;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;public class MappingUtil {    private static Logger LOG = LoggerFactory.getLogger(MappingUtil.class);    //关闭自动添加字段，关闭后索引数据中如果有多余字段不会修改mapping,默认true    private boolean dynamic = true;    public static XContentBuilder buildMapping(String tableName) throws IOException {        XContentBuilder builder = null;        try {            builder = XContentFactory.jsonBuilder().startObject()                    .startObject(tableName)                    .startObject(&quot;_source&quot;).field(&quot;enabled&quot;, true).endObject()                    .startObject(&quot;properties&quot;)                    .startObject(&quot;id&quot;).field(&quot;type&quot;, &quot;long&quot;).endObject()                    .startObject(&quot;sn&quot;).field(&quot;type&quot;, &quot;text&quot;).endObject()                    .endObject()                  .endObject()                  .endObject();        } catch (IOException e) {            e.printStackTrace();        }        return builder;    }    public static boolean addMapping(TransportClient client, String index, String type, String jsonString){        PutMappingResponse putMappingResponse = null;        try {            PutMappingRequest mappingRequest = new PutMappingRequest(index)                    .type(type).source(JSON.parseObject(jsonString));            putMappingResponse = client.admin().indices().putMapping(mappingRequest).actionGet();        } catch (Exception e) {            LOG.error(null,e);            e.printStackTrace();            LOG.error(&quot;添加&quot; + type + &quot;的mapping失败....&quot;,e);            return false;        }        boolean success = putMappingResponse.isAcknowledged();        if (success){            LOG.info(&quot;创建&quot; + type + &quot;的mapping成功....&quot;);            return success;        }        return success;    }    public static void main(String[] args) throws Exception {        /*String singleConf = ConsulConfigUtil.getSingleConf(&quot;es6.1.0/mapping/http&quot;);        int i = singleConf.length() / 2;        System.out.println(i);*/    }}</code></pre><h4 id="3、client"><a href="#3、client" class="headerlink" title="3、client"></a>3、client</h4><p><strong>ESClientUtils.java</strong></p><pre><code>package com.hsiehchou.es.client;import com.hsiehchou.common.config.ConfigUtil;import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.common.settings.Settings;import org.elasticsearch.common.transport.TransportAddress;import org.elasticsearch.transport.client.PreBuiltTransportClient;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.Serializable;import java.net.InetAddress;import java.util.Properties;/** * ES 客户端获取 */public class ESClientUtils implements Serializable{    private static Logger LOG = LoggerFactory.getLogger(ESClientUtils.class);    private volatile static TransportClient esClusterClient;    private ESClientUtils(){}    private static Properties properties;    static {        properties = ConfigUtil.getInstance().getProperties(&quot;es/es_cluster.properties&quot;);    }    public static TransportClient getClient(){        System.setProperty(&quot;es.set.netty.runtime.available.processors&quot;, &quot;false&quot;);        String clusterName = properties.getProperty(&quot;es.cluster.name&quot;);        String clusterNodes1 = properties.getProperty(&quot;es.cluster.nodes1&quot;);        String clusterNodes2 = properties.getProperty(&quot;es.cluster.nodes2&quot;);        String clusterNodes3 = properties.getProperty(&quot;es.cluster.nodes3&quot;);        LOG.info(&quot;clusterName:&quot;+ clusterName);        LOG.info(&quot;clusterNodes:&quot;+ clusterNodes1);        LOG.info(&quot;clusterNodes:&quot;+ clusterNodes2);        LOG.info(&quot;clusterNodes:&quot;+ clusterNodes3);        if(esClusterClient==null){            synchronized (ESClientUtils.class){                if(esClusterClient==null){                    try{                        Settings settings = Settings.builder()                                .put(&quot;cluster.name&quot;, clusterName)                                //.put(&quot;searchguard.ssl.transport.enabled&quot;, false)                                //.put(&quot;xpack.security.user&quot;, &quot;sc_xy_mn_es:xy@66812.com&quot;)                               // .put(&quot;transport.type&quot;,&quot;netty3&quot;)                               // .put(&quot;http.type&quot;,&quot;netty3&quot;)                                .put(&quot;client.transport.sniff&quot;,true).build();//开启自动嗅探功能                        esClusterClient = new PreBuiltTransportClient(settings)                                .addTransportAddress(new TransportAddress(InetAddress.getByName(clusterNodes1), 9300))                                .addTransportAddress(new TransportAddress(InetAddress.getByName(clusterNodes2), 9300))                                .addTransportAddress(new TransportAddress(InetAddress.getByName(clusterNodes3), 9300));                        LOG.info(&quot;esClusterClient========&quot; + esClusterClient.listedNodes());                    }catch (Exception e){                        LOG.error(&quot;获取客户端失败&quot;,e);                    }finally {                    }                }            }        }        return esClusterClient;    }    public static void main(String[] args) {        TransportClient client = ESClientUtils.getClient();        System.out.println(client);    }}</code></pre><h4 id="4、jest-service"><a href="#4、jest-service" class="headerlink" title="4、jest/service"></a>4、jest/service</h4><p><strong>IndexTypeUtil.java</strong></p><pre><code>package com.hsiehchou.es.jest.service;import com.hsiehchou.common.config.JsonReader;import io.searchbox.client.JestClient;public class IndexTypeUtil {    public static void main(String[] args) {        IndexTypeUtil.createIndexAndType(&quot;tanslator&quot;,&quot;es/mapping/tanslator.json&quot;);       // IndexTypeUtil.createIndexAndType(&quot;task&quot;);      //  IndexTypeUtil.createIndexAndType(&quot;ability&quot;);       // IndexTypeUtil.createIndexAndType(&quot;paper&quot;);    }    public static void createIndexAndType(String index,String jsonPath){        try{            JestClient jestClient = JestService.getJestClient();            JestService.createIndex(jestClient, index);            JestService.createIndexMapping(jestClient,index,index,getSourceFromJson(jsonPath));        }catch (Exception e){            e.printStackTrace();            //LOG.error(&quot;创建索引失败&quot;,e);        }    }    public static String getSourceFromJson(String path){        return JsonReader.readJson(path);    }    public static String getSource(String index){        if(index.equals(&quot;task&quot;)){            return &quot;{\&quot;_source\&quot;: {\n&quot; +                    &quot;    \&quot;enabled\&quot;: true\n&quot; +                    &quot;  },\n&quot; +                    &quot;  \&quot;properties\&quot;: {\n&quot; +                    &quot;    \&quot;taskwordcount\&quot;: {\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +                    &quot;    },\n&quot; +                    &quot;    \&quot;taskprice\&quot;: {\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;float\&quot;\n&quot; +                    &quot;    }\n&quot; +                    &quot;  }\n&quot; +                    &quot;}&quot;;        }        if(index.equals(&quot;tanslator&quot;)){            return &quot;{\n&quot; +                    &quot;  \&quot;_source\&quot;: {\n&quot; +                    &quot;    \&quot;enabled\&quot;: true\n&quot; +                    &quot;  },\n&quot; +                    &quot;  \&quot;properties\&quot;: {\n&quot; +                    &quot;    \&quot;birthday\&quot;: {\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;text\&quot;,\n&quot; +                    &quot;      \&quot;fields\&quot;: {\n&quot; +                    &quot;        \&quot;keyword\&quot;: {\n&quot; +                    &quot;          \&quot;ignore_above\&quot;: 256,\n&quot; +                    &quot;          \&quot;type\&quot;: \&quot;keyword\&quot;\n&quot; +                    &quot;        }\n&quot; +                    &quot;      }\n&quot; +                    &quot;    },\n&quot; +                    &quot;    \&quot;createtime\&quot;:{\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +                    &quot;    },\n&quot; +                    &quot;    \&quot;updatetime\&quot;:{\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +                    &quot;    },\n&quot; +                    &quot;    \&quot;avgcooperation\&quot;:{\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +                    &quot;    },\n&quot; +                    &quot;    \&quot;cooperationwordcount\&quot;:{\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +                    &quot;    },\n&quot; +                    &quot;    \&quot;cooperation\&quot;:{\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +                    &quot;    },\n&quot; +                    &quot;    \&quot;cooperationtime\&quot;:{\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +                    &quot;    },\n&quot; +                    &quot;    \&quot;age\&quot;:{\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +                    &quot;    },\n&quot; +                    &quot;    \&quot;industry\&quot;: {\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;nested\&quot;,\n&quot; +                    &quot;      \&quot;properties\&quot;: {\n&quot; +                    &quot;        \&quot;industryname\&quot;: {\n&quot; +                    &quot;          \&quot;type\&quot;: \&quot;text\&quot;,\n&quot; +                    &quot;          \&quot;fields\&quot;: {\n&quot; +                    &quot;            \&quot;keyword\&quot;: {\n&quot; +                    &quot;              \&quot;ignore_above\&quot;: 256,\n&quot; +                    &quot;              \&quot;type\&quot;: \&quot;keyword\&quot;\n&quot; +                    &quot;            }\n&quot; +                    &quot;          }\n&quot; +                    &quot;        },\n&quot; +                    &quot;        \&quot;count\&quot;: {\n&quot; +                    &quot;          \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +                    &quot;        },\n&quot; +                    &quot;        \&quot;industryid\&quot;: {\n&quot; +                    &quot;          \&quot;type\&quot;: \&quot;text\&quot;,\n&quot; +                    &quot;          \&quot;fields\&quot;: {\n&quot; +                    &quot;            \&quot;keyword\&quot;: {\n&quot; +                    &quot;              \&quot;ignore_above\&quot;: 256,\n&quot; +                    &quot;              \&quot;type\&quot;: \&quot;keyword\&quot;\n&quot; +                    &quot;            }\n&quot; +                    &quot;          }\n&quot; +                    &quot;        }\n&quot; +                    &quot;      }\n&quot; +                    &quot;    }\n&quot; +                    &quot;\n&quot; +                    &quot;  }\n&quot; +                    &quot;}&quot;;        }        return &quot;&quot;;    }}</code></pre><p><strong>JestService.java</strong></p><pre><code>package com.hsiehchou.es.jest.service;import com.hsiehchou.common.file.FileCommon;import com.google.gson.GsonBuilder;import io.searchbox.action.Action;import io.searchbox.client.JestClient;import io.searchbox.client.JestClientFactory;import io.searchbox.client.JestResult;import io.searchbox.client.config.HttpClientConfig;import io.searchbox.core.*;import io.searchbox.indices.CreateIndex;import io.searchbox.indices.DeleteIndex;import io.searchbox.indices.IndicesExists;import io.searchbox.indices.mapping.GetMapping;import io.searchbox.indices.mapping.PutMapping;import org.apache.commons.lang.StringUtils;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.search.aggregations.AggregationBuilder;import org.elasticsearch.search.aggregations.AggregationBuilders;import org.elasticsearch.search.builder.SearchSourceBuilder;import org.elasticsearch.search.sort.SortOrder;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.util.List;import java.util.Map;public class JestService {    private static Logger LOG = LoggerFactory.getLogger(JestService.class);    /**     * 获取JestClient对象     *     * @return     */    public static JestClient getJestClient() {        JestClientFactory factory = new JestClientFactory();        factory.setHttpClientConfig(new HttpClientConfig                .Builder(&quot;http://hadoop1:9200&quot;)                //.defaultCredentials(&quot;sc_xy_mn_es&quot;,&quot;xy@66812.com&quot;)                .gson(new GsonBuilder().setDateFormat(&quot;yyyy-MM-dd&#39;T&#39;hh:mm:ss&quot;).create())                .connTimeout(1500)                .readTimeout(3000)                .multiThreaded(true)                .build());        return factory.getObject();    }    public static void main(String[] args) throws Exception {        JestClient jestClient = null;//        Map&lt;String, Long&gt; stringLongMap = null;        List&lt;Map&lt;String, Object&gt;&gt; maps = null;        try {            jestClient = JestService.getJestClient();           /* SearchResult aggregation = JestService.aggregation(jestClient,                    &quot;wechat&quot;,                    &quot;wechat&quot;,                    &quot;collect_time&quot;);            stringLongMap = ResultParse.parseAggregation(aggregation);*/           /* SearchResult search = search(jestClient,                    &quot;wechat&quot;,                    &quot;wechat&quot;,                    &quot;id&quot;,                    &quot;65a3d548bd3e42b1972191bc2bd2829b&quot;,                    &quot;collect_time&quot;,                    &quot;desc&quot;,                    1,                    2);*/            /*SearchResult search = search(jestClient,                    &quot;&quot;,                    &quot;&quot;,                    &quot;phone_mac&quot;,                    &quot;aa-aa-aa-aa-aa-aa&quot;,                    &quot;collect_time&quot;,                    &quot;asc&quot;,                    1,                    1000);*///            System.out.println(indexExists(jestClient,&quot;wechat&quot;));            System.out.println(&quot;wechat数据量：&quot;+count(jestClient,&quot;wechat&quot;,&quot;wechat&quot;));            System.out.println(aggregation(jestClient,&quot;wechat&quot;,&quot;wechat&quot;, &quot;phone&quot;));            String[] includes = new String[]{&quot;latitude&quot;,&quot;longitude&quot;,&quot;collect_time&quot;};//            try{            SearchResult search = JestService.search(jestClient,                        &quot;&quot;,                        &quot;&quot;,                        &quot;phone_mac.keyword&quot;,                        &quot;aa-aa-aa-aa-aa-aa&quot;,                        &quot;collect_time&quot;,                        &quot;asc&quot;,                        1,                        2000);                maps = ResultParse.parseSearchResultOnly(search);                System.out.println(maps.size());                System.out.println(maps);            } catch (Exception e) {                e.printStackTrace();            } finally {                JestService.closeJestClient(jestClient);            }        System.out.println(maps);//        } catch (Exception e) {//            e.printStackTrace();//        }finally {//            JestService.closeJestClient(jestClient);//        }//        System.out.println(stringLongMap);    }    /**     * 统计一个索引所有数据     * @param jestClient     * @param indexName     * @param typeName     * @return     * @throws Exception     */    public static Long count(JestClient jestClient,                             String indexName,                             String typeName) throws Exception {        Count count = new Count.Builder()                .addIndex(indexName)                .addType(typeName)                .build();        CountResult results = jestClient.execute(count);        return results.getCount().longValue();    }    /**     * 聚合分组查询     * @param jestClient     * @param indexName     * @param typeName     * @param field     * @return     * @throws Exception     */    public static SearchResult  aggregation(JestClient jestClient, String indexName, String typeName, String field) throws Exception {        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();        //分组聚合API        AggregationBuilder group1 = AggregationBuilders.terms(&quot;group1&quot;).field(field);        //group1.subAggregation(AggregationBuilders.terms(&quot;group2&quot;).field(query));        searchSourceBuilder.aggregation(group1);        searchSourceBuilder.size(0);        System.out.println(searchSourceBuilder.toString());        Search search = new Search.Builder(searchSourceBuilder.toString())                .addIndex(indexName)                .addType(typeName).build();        SearchResult result = jestClient.execute(search);        return result;    }    //基础封装    public static SearchResult search(            JestClient jestClient,            String indexName,            String typeName,            String field,            String fieldValue,            String sortField,            String sortValue,            int pageNumber,            int pageSize,            String[] includes) {        //构造一个查询体  封装的就是查询语句        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();        searchSourceBuilder.fetchSource(includes,new String[0]);        //查询构造器        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();        if(StringUtils.isEmpty(field)){            boolQueryBuilder = boolQueryBuilder.must(QueryBuilders.matchAllQuery());        }else{            boolQueryBuilder = boolQueryBuilder.must(QueryBuilders.termQuery(field,fieldValue));        }        searchSourceBuilder.query(boolQueryBuilder);        //定义分页        //从什么时候开始        searchSourceBuilder.from((pageNumber-1)*pageSize);        searchSourceBuilder.size(pageSize);        //设置排序        if(&quot;desc&quot;.equals(sortValue)){            searchSourceBuilder.sort(sortField,SortOrder.DESC);        }else{            searchSourceBuilder.sort(sortField,SortOrder.ASC);        }        System.out.println(&quot;sql =====&quot; + searchSourceBuilder.toString());        //构造一个查询执行器        Search.Builder builder = new Search.Builder(searchSourceBuilder.toString());        //设置indexName typeName        if(StringUtils.isNotBlank(indexName)){            builder.addIndex(indexName);        }        if(StringUtils.isNotBlank(typeName)){            builder.addType(typeName);        }        Search build = builder.build();        SearchResult searchResult = null;        try {            searchResult = jestClient.execute(build);        } catch (IOException e) {            LOG.error(&quot;查询失败&quot;,e);        }        return searchResult;    }    //基础封装    public static SearchResult search(            JestClient jestClient,            String indexName,            String typeName,            String field,            String fieldValue,            String sortField,            String sortValue,            int pageNumber,            int pageSize) {        //构造一个查询体  封装的就是查询语句        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();        //查询构造器        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();        if(StringUtils.isEmpty(field)){            boolQueryBuilder = boolQueryBuilder.must(QueryBuilders.matchAllQuery());        }else{            boolQueryBuilder = boolQueryBuilder.must(QueryBuilders.termQuery(field,fieldValue));        }        searchSourceBuilder.query(boolQueryBuilder);        //定义分页        //从什么时候开始        searchSourceBuilder.from((pageNumber-1)*pageSize);        searchSourceBuilder.size(pageSize);        //设置排序        if(&quot;desc&quot;.equals(sortValue)){            searchSourceBuilder.sort(sortField,SortOrder.DESC);        }else{            searchSourceBuilder.sort(sortField,SortOrder.ASC);        }        System.out.println(&quot;sql =====&quot; + searchSourceBuilder.toString());        //构造一个查询执行器        Search.Builder builder = new Search.Builder(searchSourceBuilder.toString());        //设置indexName typeName        if(StringUtils.isNotBlank(indexName)){            builder.addIndex(indexName);        }        if(StringUtils.isNotBlank(typeName)){            builder.addType(typeName);        }        Search build = builder.build();        SearchResult searchResult = null;        try {            searchResult = jestClient.execute(build);        } catch (IOException e) {            LOG.error(&quot;查询失败&quot;,e);        }        return searchResult;    }   /* //基础封装    public static SearchResult search(            JestClient jestClient,            String indexName,            String typeName,            String field,            String fieldValue,            String sortField,            String sortValue,            int pageNumber,            int pageSize) {        //构造一个查询体  封装的就是查询语句        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();        //查询构造器        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();        if(StringUtils.isEmpty(field)){            boolQueryBuilder = boolQueryBuilder.must(QueryBuilders.matchAllQuery());        }else{            boolQueryBuilder = boolQueryBuilder.must(QueryBuilders.termQuery(field,fieldValue));        }        searchSourceBuilder.query(boolQueryBuilder);        //定义分页        //从什么时候开始        searchSourceBuilder.from((pageNumber-1)*pageSize);        searchSourceBuilder.size(pageSize);        //设置排序        if(&quot;desc&quot;.equals(sortValue)){            searchSourceBuilder.sort(sortField,SortOrder.DESC);        }else{            searchSourceBuilder.sort(sortField,SortOrder.ASC);        }        System.out.println(&quot;sql =====&quot; + searchSourceBuilder.toString());        //构造一个查询执行器        Search.Builder builder = new Search.Builder(searchSourceBuilder.toString());        //设置indexName typeName        if(StringUtils.isNotBlank(indexName)){            builder.addIndex(indexName);        }        if(StringUtils.isNotBlank(typeName)){            builder.addType(typeName);        }        Search build = builder.build();        SearchResult searchResult = null;        try {            searchResult = jestClient.execute(build);        } catch (IOException e) {            LOG.error(&quot;查询失败&quot;,e);        }        return searchResult;    }*/    /**     * 判断索引是否存在     *     * @param jestClient     * @param indexName     * @return     * @throws Exception     */    public static boolean indexExists(JestClient jestClient, String indexName) {        JestResult result = null;        try {            Action action = new IndicesExists.Builder(indexName).build();            result = jestClient.execute(action);        } catch (IOException e) {            LOG.error(null, e);        }        return result.isSucceeded();    }    /**     * 创建索引     *     * @param jestClient     * @param indexName     * @return     * @throws Exception     */    public static boolean createIndex(JestClient jestClient, String indexName) throws Exception {        if (!JestService.indexExists(jestClient, indexName)) {            JestResult jr = jestClient.execute(new CreateIndex.Builder(indexName).build());            return jr.isSucceeded();        } else {            LOG.info(&quot;该索引已经存在&quot;);            return false;        }    }    public static boolean createIndexWithSettingsMapAndMappingsString(JestClient jestClient, String indexName, String type, String path) throws Exception {        // String mappingJson = &quot;{\&quot;type1\&quot;: {\&quot;_source\&quot;:{\&quot;enabled\&quot;:false},\&quot;properties\&quot;:{\&quot;field1\&quot;:{\&quot;type\&quot;:\&quot;keyword\&quot;}}}}&quot;;        String mappingJson = FileCommon.getAbstractPath(path);        String realMappingJson = &quot;{&quot; + type + &quot;:&quot; + mappingJson + &quot;}&quot;;        System.out.println(realMappingJson);        CreateIndex createIndex = new CreateIndex.Builder(indexName)                .mappings(realMappingJson)                .build();        JestResult jr = jestClient.execute(createIndex);        return jr.isSucceeded();    }    /**     * Put映射     *     * @param jestClient     * @param indexName     * @param typeName     * @param source     * @return     * @throws Exception     */    public static boolean createIndexMapping(JestClient jestClient, String indexName, String typeName, String source) throws Exception {        PutMapping putMapping = new PutMapping.Builder(indexName, typeName, source).build();        JestResult jr = jestClient.execute(putMapping);        return jr.isSucceeded();    }    /**     * Get映射     *     * @param jestClient     * @param indexName     * @param typeName     * @return     * @throws Exception     */    public static String getIndexMapping(JestClient jestClient, String indexName, String typeName) throws Exception {        GetMapping getMapping = new GetMapping.Builder().addIndex(indexName).addType(typeName).build();        JestResult jr = jestClient.execute(getMapping);        return jr.getJsonString();    }    /**     * 索引文档     *     * @param jestClient     * @param indexName     * @param typeName     * @return     * @throws Exception     */    public static boolean index(JestClient jestClient, String indexName, String typeName, String idField, List&lt;Map&lt;String, Object&gt;&gt; listMaps) throws Exception {        Bulk.Builder bulk = new Bulk.Builder().defaultIndex(indexName).defaultType(typeName);        for (Map&lt;String, Object&gt; map : listMaps) {            if (map != null &amp;&amp; map.containsKey(idField)) {                Object o = map.get(idField);                Index index = new Index.Builder(map).id(map.get(idField).toString()).build();                bulk.addAction(index);            }        }        BulkResult br = jestClient.execute(bulk.build());        return br.isSucceeded();    }    /**     * 索引文档     *     * @param jestClient     * @param indexName     * @param typeName     * @return     * @throws Exception     */    public static boolean indexString(JestClient jestClient, String indexName, String typeName, String idField, List&lt;Map&lt;String, String&gt;&gt; listMaps) throws Exception {        if (listMaps != null &amp;&amp; listMaps.size() &gt; 0) {            Bulk.Builder bulk = new Bulk.Builder().defaultIndex(indexName).defaultType(typeName);            for (Map&lt;String, String&gt; map : listMaps) {                if (map != null &amp;&amp; map.containsKey(idField)) {                    Index index = new Index.Builder(map).id(map.get(idField)).build();                    bulk.addAction(index);                }            }            BulkResult br = jestClient.execute(bulk.build());            return br.isSucceeded();        } else {            return false;        }    }    /**     * 索引文档     *     * @param jestClient     * @param indexName     * @param typeName     * @return     * @throws Exception     */    public static boolean indexOne(JestClient jestClient, String indexName, String typeName, String id, Map&lt;String, Object&gt; map) {        Index.Builder builder = new Index.Builder(map);        builder.id(id);        builder.refresh(true);        Index index = builder.index(indexName).type(typeName).build();        try {            JestResult result = jestClient.execute(index);            if (result != null &amp;&amp; !result.isSucceeded()) {                throw new RuntimeException(result.getErrorMessage() + &quot;插入更新索引失败!&quot;);            }        } catch (Exception e) {            e.printStackTrace();            return false;        }        return true;    }    /**     * 搜索文档     *     * @param jestClient     * @param indexName     * @param typeName     * @param query     * @return     * @throws Exception     */    public static SearchResult search(JestClient jestClient, String indexName, String typeName, String query) throws Exception {        Search search = new Search.Builder(query)                .addIndex(indexName)                .addType(typeName)                .build();        return jestClient.execute(search);    }    /**     * Get文档     *     * @param jestClient     * @param indexName     * @param typeName     * @param id     * @return     * @throws Exception     */    public static JestResult get(JestClient jestClient, String indexName, String typeName, String id) throws Exception {        Get get = new Get.Builder(indexName, id).type(typeName).build();        return jestClient.execute(get);    }    /**     * Delete索引     *     * @param jestClient     * @param indexName     * @return     * @throws Exception     */    public boolean delete(JestClient jestClient, String indexName) throws Exception {        JestResult jr = jestClient.execute(new DeleteIndex.Builder(indexName).build());        return jr.isSucceeded();    }    /**     * Delete文档     *     * @param jestClient     * @param indexName     * @param typeName     * @param id     * @return     * @throws Exception     */    public static boolean delete(JestClient jestClient, String indexName, String typeName, String id) throws Exception {        DocumentResult dr = jestClient.execute(new Delete.Builder(id).index(indexName).type(typeName).build());        return dr.isSucceeded();    }    /**     * 关闭JestClient客户端     *     * @param jestClient     * @throws Exception     */    public static void closeJestClient(JestClient jestClient) {        if (jestClient != null) {            jestClient.shutdownClient();        }    }    public static String query = &quot;{\n&quot; +            &quot;  \&quot;size\&quot;: 1,\n&quot; +            &quot;  \&quot;query\&quot;: {\n&quot; +            &quot;     \&quot;match\&quot;: {\n&quot; +            &quot;       \&quot;taskexcuteid\&quot;: \&quot;89899143\&quot;\n&quot; +            &quot;     }\n&quot; +            &quot;  },\n&quot; +            &quot;  \&quot;aggs\&quot;: {\n&quot; +            &quot;    \&quot;count\&quot;: {\n&quot; +            &quot;      \&quot;terms\&quot;: {\n&quot; +            &quot;        \&quot;field\&quot;: \&quot;source.keyword\&quot;\n&quot; +            &quot;      },\n&quot; +            &quot;      \&quot;aggs\&quot;: {\n&quot; +            &quot;        \&quot;sum_price\&quot;: {\n&quot; +            &quot;          \&quot;sum\&quot;: {\n&quot; +            &quot;            \&quot;field\&quot;: \&quot;taskprice\&quot;\n&quot; +            &quot;          }\n&quot; +            &quot;        },\n&quot; +            &quot;        \&quot;sum_wordcount\&quot;: {\n&quot; +            &quot;          \&quot;sum\&quot;: {\n&quot; +            &quot;            \&quot;field\&quot;: \&quot;taskwordcount\&quot;\n&quot; +            &quot;          }\n&quot; +            &quot;        },\n&quot; +            &quot;        \&quot;avg_taskprice\&quot;: {\n&quot; +            &quot;          \&quot;avg\&quot;: {\n&quot; +            &quot;            \&quot;field\&quot;: \&quot;taskprice\&quot;\n&quot; +            &quot;          }\n&quot; +            &quot;        }\n&quot; +            &quot;      }\n&quot; +            &quot;    }\n&quot; +            &quot;  }\n&quot; +            &quot;}&quot;;}</code></pre><p><strong>ResultParse.java</strong></p><pre><code>package com.hsiehchou.es.jest.service;import com.google.gson.Gson;import com.google.gson.JsonElement;import com.google.gson.JsonObject;import com.google.gson.JsonPrimitive;import io.searchbox.client.JestClient;import io.searchbox.client.JestResult;import io.searchbox.core.SearchResult;import io.searchbox.core.search.aggregation.MetricAggregation;import io.searchbox.core.search.aggregation.TermsAggregation;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.*;public class ResultParse {    private static Logger LOG = LoggerFactory.getLogger(ResultParse.class);    public static void main(String[] args) throws Exception {        JestClient jestClient = JestService.getJestClient();        /*long l = System.currentTimeMillis();        JestClient jestClient = JestClientUtil.getJestClient();        System.out.println(jestClient);        String json =&quot;{\n&quot; +                &quot;  \&quot;size\&quot;: 1, \n&quot; +                &quot;  \&quot;query\&quot;: {\n&quot; +                &quot;    \&quot;query_string\&quot;: {\n&quot; +                &quot;      \&quot;query\&quot;: \&quot;中文\&quot;\n&quot; +                &quot;    }\n&quot; +                &quot;  },\n&quot; +                &quot;  \&quot;highlight\&quot;: {\n&quot; +                &quot;    \&quot;pre_tags\&quot; : [ \&quot;&lt;red&gt;\&quot; ],\n&quot; +                &quot;    \&quot;post_tags\&quot; : [ \&quot;&lt;/red&gt;\&quot; ],\n&quot; +                &quot;    \&quot;fields\&quot;:{\n&quot; +                &quot;      \&quot;secondlanguage\&quot;: {}\n&quot; +                &quot;      ,\&quot;firstlanguage\&quot;: {}\n&quot; +                &quot;    }\n&quot; +                &quot;  }\n&quot; +                &quot;}&quot;;        SearchResult search = JestService.search(jestClient, ES_INDEX.TANSLATOR_TEST, ES_INDEX.TANSLATOR_TEST,json);        ResultParse.parseSearchResult(search);        jestClient.shutdownClient();        long l1 = System.currentTimeMillis();        System.out.println(l1-l);*/    }    public static Map&lt;String,Object&gt; parseGet(JestResult getResult){        Map&lt;String,Object&gt; map = null;        JsonObject jsonObject = getResult.getJsonObject().getAsJsonObject(&quot;_source&quot;);        if(jsonObject != null){            map = new HashMap&lt;String,Object&gt;();            //System.out.println(jsonObject);            Set&lt;Map.Entry&lt;String, JsonElement&gt;&gt; entries = jsonObject.entrySet();            for(Map.Entry&lt;String, JsonElement&gt; entry:entries){                JsonElement value = entry.getValue();                if(value.isJsonPrimitive()){                    JsonPrimitive value1 = (JsonPrimitive) value;                  //  LOG.error(&quot;转换前==========&quot; + value1);                    if( value1.isString() ){                       // LOG.error(&quot;转换后==========&quot; + value1.getAsString());                        map.put(entry.getKey(),value1.getAsString());                    }else{                        map.put(entry.getKey(),value1);                    }                }else{                    map.put(entry.getKey(),value);                }             }        }        return map;    }    public static Map&lt;String,Object&gt; parseGet2map(JestResult getResult){        JsonObject source = getResult.getJsonObject().getAsJsonObject(&quot;_source&quot;);        Gson gson = new Gson();        Map map = gson.fromJson(source, Map.class);        return map;    }    /**     * 解析listMap     * 结果格式为  {hits=0, total=0, data=[]}     * @param search     * @return     */    public static List&lt;Map&lt;String,Object&gt;&gt; parseSearchResultOnly(SearchResult search){        List&lt;Map&lt;String,Object&gt;&gt; list = new ArrayList&lt;Map&lt;String,Object&gt;&gt;();        List&lt;SearchResult.Hit&lt;Object, Void&gt;&gt; hits = search.getHits(Object.class);        for(SearchResult.Hit&lt;Object, Void&gt; hit : hits){            Map&lt;String,Object&gt; source = (Map&lt;String,Object&gt;)hit.source;            list.add(source);        }        return list;    }    /**     * 解析listMap     * 结果格式为  {hits=0, total=0, data=[]}     * @param search     * @return     */    public static Map&lt;String,Long&gt; parseAggregation(SearchResult search){        Map&lt;String,Long&gt; mapResult = new HashMap&lt;&gt;();        MetricAggregation aggregations = search.getAggregations();        TermsAggregation group1 = aggregations.getTermsAggregation(&quot;group1&quot;);        List&lt;TermsAggregation.Entry&gt; buckets = group1.getBuckets();        buckets.forEach(x-&gt;{            String key = x.getKey();            Long count = x.getCount();            mapResult.put(key,count);        });        return mapResult;    }    /**     * 解析listMap     * 结果格式为  {hits=0, total=0, data=[]}     * @param search     * @return     */    public static Map&lt;String,Object&gt; parseSearchResult(SearchResult search){        Map&lt;String,Object&gt; map = new HashMap&lt;String,Object&gt;();        List&lt;Map&lt;String,Object&gt;&gt; list = new ArrayList&lt;Map&lt;String,Object&gt;&gt;();        Long total = search.getTotal();        map.put(&quot;total&quot;,total);        List&lt;SearchResult.Hit&lt;Object, Void&gt;&gt; hits = search.getHits(Object.class);        map.put(&quot;hits&quot;,hits.size());        for(SearchResult.Hit&lt;Object, Void&gt; hit : hits){            Map&lt;String, List&lt;String&gt;&gt; highlight = hit.highlight;            Map&lt;String,Object&gt; source = (Map&lt;String,Object&gt;)hit.source;            source.put(&quot;highlight&quot;,highlight);            list.add(source);        }        map.put(&quot;data&quot;,list);        return map;    }}</code></pre><h4 id="5、search"><a href="#5、search" class="headerlink" title="5、search"></a>5、search</h4><p><strong>BuilderUtil.java</strong></p><pre><code>package com.hsiehchou.es.search;import org.apache.commons.lang.StringUtils;import org.elasticsearch.action.search.SearchRequestBuilder;import org.elasticsearch.client.transport.TransportClient;import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class BuilderUtil {    private static Logger LOG = LoggerFactory.getLogger(BuilderUtil.class);    public static SearchRequestBuilder getSearchBuilder(TransportClient client, String index, String type){        SearchRequestBuilder builder = null;        try {            if (StringUtils.isNotBlank(index)) {                builder = client.prepareSearch(index.split(&quot;,&quot;));            } else {                builder = client.prepareSearch();            }            if (StringUtils.isNotBlank(type)) {                builder.setTypes(type.split(&quot;,&quot;));            }        } catch (Exception e) {            LOG.error(null, e);        }        return builder;    }    public static SearchRequestBuilder getSearchBuilder(TransportClient client, String[] indexs, String type){        SearchRequestBuilder builder = null;        try {            if (indexs.length&gt;0) {                for(String index:indexs){                    builder = client.prepareSearch(index);                }            } else {                builder = client.prepareSearch();            }            if (StringUtils.isNotBlank(type)) {                builder.setTypes(type);            }        } catch (Exception e) {            LOG.error(null, e);        }        return builder;    }}</code></pre><p><strong>QueryUtil.java</strong></p><pre><code>package com.hsiehchou.es.search;import com.hsiehchou.es.utils.UnicodeUtil;import org.apache.lucene.queryparser.classic.QueryParser;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.index.query.QueryStringQueryBuilder;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.Map;public class QueryUtil {    private static Logger LOG = LoggerFactory.getLogger(QueryUtil.class);    /**     * EQ   等於     * NEQ  不等於     * GE   大于等于     * GT   大于     * LE   小于等于     * LT   小于     * RANGE 区间范围     */    public static enum OPREATOR {EQ, NEQ,WILDCARD, GE, LE, GT, LT, FUZZY, RANGE, IN, PREFIX}    /**     * @param paramMap     * @return     */    public static BoolQueryBuilder getSearchParam(Map&lt;OPREATOR, Map&lt;String, Object&gt;&gt; paramMap) {        BoolQueryBuilder qb = QueryBuilders.boolQuery();        if (null != paramMap &amp;&amp; !paramMap.isEmpty()) {            for (Map.Entry&lt;OPREATOR, Map&lt;String, Object&gt;&gt; paramEntry : paramMap.entrySet()) {                OPREATOR key = paramEntry.getKey();                Map&lt;String, Object&gt; fieldMap = paramEntry.getValue();                for (Map.Entry&lt;String, Object&gt; fieldEntry : fieldMap.entrySet()) {                    String field = fieldEntry.getKey();                    Object value = fieldEntry.getValue();                    switch (key) {                        case EQ:/**等於查詢 equale**/                            qb.must(QueryBuilders.matchPhraseQuery(field, value).slop(0));                            break;                        case NEQ:/**不等於查詢 not equale**/                            qb.mustNot(QueryBuilders.matchQuery(field, value));                            break;                        case GE: /**大于等于查詢  great than or equal to**/                            qb.must(QueryBuilders.rangeQuery(field).gte(value));                            break;                        case LE: /**小于等于查詢 less than or equal to**/                            qb.must(QueryBuilders.rangeQuery(field).lte(value));                            break;                        case GT: /**大于查詢**/                            qb.must(QueryBuilders.rangeQuery(field).gt(value));                            break;                        case LT: /**小于查詢**/                            qb.must(QueryBuilders.rangeQuery(field).lt(value));                            break;                        case FUZZY:                            String text = String.valueOf(value);                            if (!UnicodeUtil.hasChinese(text)) {                                text = &quot;*&quot; + text + &quot;*&quot;;                            }                            text = QueryParser.escape(text);                            qb.must(new QueryStringQueryBuilder(text).field(field));                            break;                        case RANGE: /**区间查詢**/                            String[] split = value.toString().split(&quot;,&quot;);                            if(split.length==2){                                qb.must(QueryBuilders.rangeQuery(field).from(Long.valueOf(split[0]))                                        .to(Long.valueOf(split[1])));                            }                             /*  if (value instanceof Map) {                                Map&lt;String, Object&gt; rangMap = (Map&lt;String, Object&gt;) value;                                qb.must(QueryBuilders.rangeQuery(field).from(rangMap.get(&quot;ge&quot;))                                        .to(rangMap.get(&quot;le&quot;)));                            }*/                            break;                        case PREFIX: /**前缀查詢**/                            qb.must(QueryBuilders.prefixQuery(field, String.valueOf(value)));                            break;                        case IN:                            qb.must(QueryBuilders.termsQuery(field, (Object[]) value));                            break;                        default:                            qb.must(QueryBuilders.matchQuery(field, value));                            break;                    }                }            }        }        return qb;    }}</code></pre><p><strong>ResponseParse.java</strong></p><pre><code>package com.hsiehchou.es.search;import org.elasticsearch.action.get.GetResponse;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.Map;public class ResponseParse {    private static Logger LOG = LoggerFactory.getLogger(BuilderUtil.class);    public static Map&lt;String, Object&gt; parseGetResponse(GetResponse getResponse){        Map&lt;String, Object&gt; source = null;        try {            source = getResponse.getSource();        } catch (Exception e) {            LOG.error(null,e);        }        return source;    }}</code></pre><p><strong>SearchUtil.java</strong></p><pre><code>package com.hsiehchou.es.search;import com.hsiehchou.es.client.ESClientUtils;import org.elasticsearch.action.get.GetRequestBuilder;import org.elasticsearch.action.get.GetResponse;import org.elasticsearch.action.search.SearchRequestBuilder;import org.elasticsearch.action.search.SearchResponse;import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.MatchQueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.search.SearchHit;import org.elasticsearch.search.SearchHits;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.ArrayList;import java.util.List;import java.util.Map;public class SearchUtil {    private static Logger LOG = LoggerFactory.getLogger(SearchUtil.class);    private static TransportClient client = ESClientUtils.getClient();    public static void main(String[] args) {        TransportClient client = ESClientUtils.getClient();        List&lt;Map&lt;String, Object&gt;&gt; maps = searchSingleData(client, &quot;wechat&quot;, &quot;wechat&quot;, &quot;phone_mac&quot;, &quot;aa-aa-aa-aa-aa-aa&quot;);        System.out.println(maps);        /* long l = System.currentTimeMillis();        searchSingleData(&quot;tanslator&quot;, &quot;tanslator&quot;,&quot;4e1117d7-c434-48a7-9134-45f7c90f94ee_TR1100397895_2&quot;);        System.out.println(&quot;消耗时间&quot; + (System.currentTimeMillis() - l));        long lll = System.currentTimeMillis();        searchSingleData(&quot;tanslator&quot;, &quot;tanslator&quot;,&quot;4e1117d7-c434-48a7-9134-45f7c90f94ee_TR1100397895_2&quot;);        System.out.println(&quot;消耗时间&quot; + (System.currentTimeMillis() - lll));        long ll = System.currentTimeMillis();        List&lt;Map&lt;String, Object&gt;&gt; maps = searchSingleData(client,&quot;tanslator&quot;, &quot;tanslator&quot;, &quot;iolid&quot;, &quot;TR1100397895&quot;);        System.out.println(&quot;消耗时间&quot; + (System.currentTimeMillis() - ll));        System.out.println(maps);*/    }    /**     * 查询单条数据     * @param index  索引     * @param type   表名     * @param id     字段     * @return     */    public static GetResponse searchSingleData(String index, String type, String id) {        GetResponse response = null;        try {            GetRequestBuilder builder = null;            builder = client.prepareGet(index, type, id);            response = builder.execute().actionGet();        } catch (Exception e) {            LOG.error(null, e);        }        return response;    }    /**     * @param index     * @param type     * @param field     * @param value     * @return     */    public static List&lt;Map&lt;String, Object&gt;&gt; searchSingleData(TransportClient client,String index, String type,String field, String value) {        List&lt;Map&lt;String, Object&gt;&gt; result = new ArrayList&lt;&gt;();        try {            SearchRequestBuilder builder = BuilderUtil.getSearchBuilder(client,index,type);            MatchQueryBuilder matchQueryBuilder = QueryBuilders.matchQuery(field, value);            builder.setQuery(matchQueryBuilder).setExplain(false);            SearchResponse searchResponse = builder.execute().actionGet();            SearchHits hits = searchResponse.getHits();            SearchHit[] searchHists = hits.getHits();            for (SearchHit sh : searchHists) {                result.add(sh.getSourceAsMap());            }        } catch (Exception e) {            e.printStackTrace();            LOG.error(null, e);        }        return result;    }    /**     * 多条件查詢     * @param index     * @param type     * @param paramMap 组合查询条件     * @return     */    public static SearchResponse searchListData(String index, String type,                                                Map&lt;QueryUtil.OPREATOR,Map&lt;String,Object&gt;&gt; paramMap) {        SearchRequestBuilder builder = BuilderUtil.getSearchBuilder(client,index,type);        builder.setQuery(QueryUtil.getSearchParam(paramMap)).setExplain(false);        SearchResponse searchResponse = builder.get();        return searchResponse;    }    /**     * 多条件查詢     * @param index     * @param type     * @param paramMap 组合查询条件     * @return     */    public static SearchResponse searchListData1(String index, String type, Map&lt;String,String&gt; paramMap) {        BoolQueryBuilder qb = QueryBuilders.boolQuery();        qb.must(QueryBuilders.matchQuery(&quot;&quot;, &quot;&quot;));        BoolQueryBuilder qb1 = QueryBuilders.boolQuery();        qb1.should(QueryBuilders.matchQuery(&quot;&quot;,&quot;&quot;));        qb1.should(QueryBuilders.matchQuery(&quot;&quot;,&quot;&quot;));        qb.must(qb1);        return null;    }}</code></pre><h4 id="6、utils"><a href="#6、utils" class="headerlink" title="6、utils"></a>6、utils</h4><p><strong>ESresultUtil.java</strong></p><pre><code>package com.hsiehchou.es.utils;import org.apache.commons.lang.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.Map;public class ESresultUtil {    private static Logger LOG = LoggerFactory.getLogger(ESresultUtil.class);    public static Long getLong(Map&lt;String,Object&gt; esMAp,String field){        Long valueLong = 0L;        if(esMAp!=null &amp;&amp; esMAp.size()&gt;0){            if(esMAp.containsKey(field)){                 Object value = esMAp.get(field);                 if(value!=null &amp;&amp; StringUtils.isNotBlank(value.toString())){                     valueLong = Long.valueOf(value.toString());                 }            }        }        return valueLong;    }}</code></pre><p><strong>UnicodeUtil.java</strong></p><pre><code>package com.hsiehchou.es.utils;import java.util.regex.Pattern;public class UnicodeUtil {    // 根据Unicode编码完美的判断中文汉字和符号    private static boolean isChinese(char c) {        Character.UnicodeBlock ub = Character.UnicodeBlock.of(c);        if (ub == Character.UnicodeBlock.CJK_UNIFIED_IDEOGRAPHS || ub == Character.UnicodeBlock.CJK_COMPATIBILITY_IDEOGRAPHS                || ub == Character.UnicodeBlock.CJK_UNIFIED_IDEOGRAPHS_EXTENSION_A || ub == Character.UnicodeBlock.CJK_UNIFIED_IDEOGRAPHS_EXTENSION_B                || ub == Character.UnicodeBlock.CJK_SYMBOLS_AND_PUNCTUATION || ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS                || ub == Character.UnicodeBlock.GENERAL_PUNCTUATION) {            return true;        }        return false;    }    // 完整的判断中文汉字和符号    public static boolean isChinese(String strName) {        char[] ch = strName.toCharArray();        for (int i = 0; i &lt; ch.length; i++) {            char c = ch[i];            if (isChinese(c)) {                return true;            }        }        return false;    }    // 完整的判断中文汉字和符号    public static boolean hasChinese(String strName) {        char[] ch = strName.toCharArray();        for (int i = 0; i &lt; ch.length; i++) {            char c = ch[i];            if (isChinese(c)) {                return true;            }        }        return false;    }    // 只能判断部分CJK字符（CJK统一汉字）    public static boolean isChineseByREG(String str) {        if (str == null) {            return false;        }        Pattern pattern = Pattern.compile(&quot;[\\u4E00-\\u9FBF]+&quot;);        return pattern.matcher(str.trim()).find();    }    // 只能判断部分CJK字符（CJK统一汉字）    /*    public static boolean isChineseByName(String str) {        if (str == null) {            return false;        }        // 大小写不同：\\p 表示包含，\\P 表示不包含        // \\p{Cn} 的意思为 Unicode 中未被定义字符的编码，\\P{Cn} 就表示 Unicode中已经被定义字符的编码        String reg = &quot;\\p{InCJK Unified Ideographs}&amp;&amp;\\P{Cn}&quot;;        Pattern pattern = Pattern.compile(reg);        return pattern.matcher(str.trim()).find();    }*/    public static void main(String[] args) {        System.out.println(hasChinese(&quot;aa表aa&quot;));    }}</code></pre><h4 id="7、V2"><a href="#7、V2" class="headerlink" title="7、V2"></a>7、V2</h4><p><strong>ElasticSearchService.java</strong></p><pre><code>package com.hsiehchou.es.V2;import com.hsiehchou.es.client.ESClientUtils;import org.apache.commons.collections.map.HashedMap;import org.apache.commons.lang.StringUtils;import org.elasticsearch.action.admin.indices.create.CreateIndexRequest;import org.elasticsearch.action.admin.indices.delete.DeleteIndexResponse;import org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsRequest;import org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsResponse;import org.elasticsearch.action.bulk.BulkRequestBuilder;import org.elasticsearch.action.search.SearchRequestBuilder;import org.elasticsearch.action.search.SearchResponse;import org.elasticsearch.action.update.UpdateRequest;import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.common.text.Text;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.search.SearchHit;import org.elasticsearch.search.SearchHits;import org.elasticsearch.search.aggregations.AggregationBuilder;import org.elasticsearch.search.aggregations.AggregationBuilders;import org.elasticsearch.search.aggregations.bucket.terms.Terms;import org.elasticsearch.search.fetch.subphase.highlight.HighlightBuilder;import org.elasticsearch.search.fetch.subphase.highlight.HighlightField;import org.elasticsearch.search.sort.SortBuilder;import org.elasticsearch.search.sort.SortOrder;import java.util.*;/** *  ES检索封装 */public class ElasticSearchService {    private final static int MAX = 10000;    private static TransportClient client = ESClientUtils.getClient();    /**     * 功能描述：新建索引     * @param indexName 索引名     */    public void createIndex(String indexName) {        client.admin().indices().create(new CreateIndexRequest(indexName))                .actionGet();    }    /**     * 功能描述：新建索引     * @param index 索引名     * @param type 类型     */    public void createIndex(String index, String type) {        client.prepareIndex(index, type).setSource().get();    }    /**     * 功能描述：删除索引     * @param index 索引名     */    public void deleteIndex(String index) {        if (indexExist(index)) {            DeleteIndexResponse dResponse = client.admin().indices().prepareDelete(index)                    .execute().actionGet();            if (!dResponse.isAcknowledged()) {            }        } else {        }    }    /**     * 功能描述：验证索引是否存在     * @param index 索引名     */    public boolean indexExist(String index) {        IndicesExistsRequest inExistsRequest = new IndicesExistsRequest(index);        IndicesExistsResponse inExistsResponse = client.admin().indices()                .exists(inExistsRequest).actionGet();        return inExistsResponse.isExists();    }    /**     * 功能描述：插入数据     * @param index 索引名     * @param type 类型     * @param json 数据     */    public void insertData(String index, String type, String json) {       client.prepareIndex(index, type)                .setSource(json)                .get();    }    /**     * 功能描述：插入数据     * @param index 索引名     * @param type 类型     * @param _id 数据id     * @param json 数据     */    public void insertData(String index, String type, String _id, String json) {        client.prepareIndex(index, type).setId(_id)                .setSource(json)                .get();    }    /**     * 功能描述：更新数据     * @param index 索引名     * @param type 类型     * @param _id 数据id     * @param json 数据     */    public void updateData(String index, String type, String _id, String json) throws Exception {        try {            UpdateRequest updateRequest = new UpdateRequest(index, type, _id)                    .doc(json);            client.update(updateRequest).get();        } catch (Exception e) {            //throw new MessageException(&quot;update data failed.&quot;, e);        }    }    /**     * 功能描述：删除数据     * @param index 索引名     * @param type 类型     * @param _id 数据id     */    public void deleteData(String index, String type, String _id) {        client.prepareDelete(index, type, _id)                .get();    }    /**     * 功能描述：批量插入数据     * @param index 索引名     * @param type 类型     * @param data (_id 主键, json 数据)     */    public void bulkInsertData(String index, String type, Map&lt;String, String&gt; data) {        BulkRequestBuilder bulkRequest = client.prepareBulk();        data.forEach((param1, param2) -&gt; {            bulkRequest.add(client.prepareIndex(index, type, param1)                    .setSource(param2)            );        });        bulkRequest.get();    }    /**     * 功能描述：批量插入数据     * @param index 索引名     * @param type 类型     * @param jsonList 批量数据     */    public void bulkInsertData(String index, String type, List&lt;String&gt; jsonList) {        BulkRequestBuilder bulkRequest = client.prepareBulk();        jsonList.forEach(item -&gt; {            bulkRequest.add(client.prepareIndex(index, type)                    .setSource(item)            );        });        bulkRequest.get();    }    /**     * 功能描述：查询     * @param index 索引名     * @param type 类型     * @param constructor 查询构造     */    public List&lt;Map&lt;String, Object&gt;&gt; search(String index, String type, ESQueryBuilderConstructor constructor) {        List&lt;Map&lt;String, Object&gt;&gt; list = new ArrayList&lt;&gt;();        SearchRequestBuilder searchRequestBuilder = client.prepareSearch(index).setTypes(type);        //排序        if (StringUtils.isNotEmpty(constructor.getAsc()))            searchRequestBuilder.addSort(constructor.getAsc(), SortOrder.ASC);        if (StringUtils.isNotEmpty(constructor.getDesc()))            searchRequestBuilder.addSort(constructor.getDesc(), SortOrder.DESC);        //设置查询体        searchRequestBuilder.setQuery(constructor.listBuilders());        //返回条目数        int size = constructor.getSize();        if (size &lt; 0) {            size = 0;        }        if (size &gt; MAX) {            size = MAX;        }        //返回条目数        searchRequestBuilder.setSize(size);        searchRequestBuilder.setFrom(constructor.getFrom() &lt; 0 ? 0 : constructor.getFrom());        SearchResponse searchResponse = searchRequestBuilder.execute().actionGet();        SearchHits hits = searchResponse.getHits();        SearchHit[] searchHists = hits.getHits();        for (SearchHit sh : searchHists) {            list.add(sh.getSourceAsMap());        }        return list;    }    /**     * 功能描述：查询     * @param index 索引名     * @param type 类型     * @param constructor 查询构造     */    public Map&lt;String,Object&gt; searchCountAndMessage(String index, String type, ESQueryBuilderConstructor constructor) {        Map&lt;String,Object&gt; map = new HashMap&lt;String,Object&gt;();        List&lt;Map&lt;String, Object&gt;&gt; list = new ArrayList&lt;&gt;();        SearchRequestBuilder searchRequestBuilder = client.prepareSearch(index).setTypes(type);        //排序        if (StringUtils.isNotEmpty(constructor.getAsc()))            searchRequestBuilder.addSort(constructor.getAsc(), SortOrder.ASC);        if (StringUtils.isNotEmpty(constructor.getDesc()))            searchRequestBuilder.addSort(constructor.getDesc(), SortOrder.DESC);        //设置查询体        searchRequestBuilder.setQuery(constructor.listBuilders());        //返回条目数        int size = constructor.getSize();        if (size &lt; 0) {            size = 0;        }        if (size &gt; MAX) {            size = MAX;        }        //返回条目数        searchRequestBuilder.setSize(size);        searchRequestBuilder.setFrom(constructor.getFrom() &lt; 0 ? 0 : constructor.getFrom());        SearchResponse searchResponse = searchRequestBuilder.execute().actionGet();        long totalHits = searchResponse.getHits().getTotalHits();        SearchHits hits = searchResponse.getHits();        SearchHit[] searchHists = hits.getHits();        for (SearchHit sh : searchHists) {            list.add(sh.getSourceAsMap());        }        map.put(&quot;total&quot;,(long)searchHists.length);        map.put(&quot;count&quot;,totalHits);        map.put(&quot;data&quot;,list);        return map;    }    /**     * 功能描述：查询     * @param index 索引名     * @param type 类型     * @param constructor 查询构造     */    public Map&lt;String,Object&gt; searchCountAndMessageNew(String index, String type, ESQueryBuilderConstructorNew constructor) {        Map&lt;String,Object&gt; map = new HashMap&lt;String,Object&gt;();        List&lt;Map&lt;String, Object&gt;&gt; list = new ArrayList&lt;&gt;();        SearchRequestBuilder searchRequestBuilder = client.prepareSearch(index).setTypes(type);        //排序        List&lt;SortBuilder&gt; sortBuilderList = constructor.getSortBuilderList();        if(sortBuilderList!=null &amp;&amp; sortBuilderList.size()&gt;0){            sortBuilderList.forEach(sortBuilder-&gt;{                searchRequestBuilder.addSort(sortBuilder);            });        }        //设置查询体        searchRequestBuilder.setQuery(constructor.listBuilders());        //返回条目数        int size = constructor.getSize();        if (size &lt; 0) {            size = 0;        }        if (size &gt; MAX) {            size = MAX;        }        //返回条目数        searchRequestBuilder.setSize(size);        searchRequestBuilder.setFrom(constructor.getFrom() &lt; 0 ? 0 : constructor.getFrom());        //设置高亮        HighlightBuilder highlightBuilder = new HighlightBuilder();        List&lt;String&gt; highLighterFields = constructor.getHighLighterFields();        if(highLighterFields.size()&gt;0){            highLighterFields.forEach(field -&gt; {                highlightBuilder.field(field);            });        }        highlightBuilder.preTags(&quot;&lt;font color=\&quot;red\&quot;&gt;&quot;);        highlightBuilder.postTags(&quot;&lt;/font&gt;&quot;);        SearchResponse searchResponse = searchRequestBuilder.highlighter(highlightBuilder).execute().actionGet();        long totalHits = searchResponse.getHits().getTotalHits();        SearchHits hits = searchResponse.getHits();        SearchHit[] searchHists = hits.getHits();        for (SearchHit hit : searchHists) {            Map&lt;String, Object&gt; sourceAsMap = hit.getSourceAsMap();            Map&lt;String, HighlightField&gt; highlightFields = hit.getHighlightFields();            //获取高亮结果            Set&lt;String&gt; set = highlightFields.keySet();            for (String str : set) {                Text[] fragments = highlightFields.get(str).getFragments();                String st1r=&quot;&quot;;                for(Text text:fragments){                    st1r = st1r + text.toString();                }                sourceAsMap.put(str,st1r);                System.out.println(&quot;str(==============&quot; + st1r);            }            list.add(sourceAsMap);        }        map.put(&quot;total&quot;,(long)searchHists.length);        map.put(&quot;count&quot;,totalHits);        map.put(&quot;data&quot;,list);        return map;    }    /**     * 功能描述：统计查询     * @param index 索引名     * @param type 类型     * @param constructor 查询构造     * @param groupBy 统计字段     */    public Map&lt;Object, Object&gt; statSearch(String index, String type, ESQueryBuilderConstructor constructor, String groupBy) {        Map&lt;Object, Object&gt; map = new HashedMap();        SearchRequestBuilder searchRequestBuilder = client.prepareSearch(index).setTypes(type);        //排序        if (StringUtils.isNotEmpty(constructor.getAsc()))            searchRequestBuilder.addSort(constructor.getAsc(), SortOrder.ASC);        if (StringUtils.isNotEmpty(constructor.getDesc()))            searchRequestBuilder.addSort(constructor.getDesc(), SortOrder.DESC);        //设置查询体        if (null != constructor) {            searchRequestBuilder.setQuery(constructor.listBuilders());        } else {            searchRequestBuilder.setQuery(QueryBuilders.matchAllQuery());        }        int size = constructor.getSize();        if (size &lt; 0) {            size = 0;        }        if (size &gt; MAX) {            size = MAX;        }        //返回条目数        searchRequestBuilder.setSize(size);        searchRequestBuilder.setFrom(constructor.getFrom() &lt; 0 ? 0 : constructor.getFrom());        SearchResponse sr = searchRequestBuilder.addAggregation(                AggregationBuilders.terms(&quot;agg&quot;).field(groupBy)        ).get();        Terms stateAgg = sr.getAggregations().get(&quot;agg&quot;);        Iterator&lt;? extends Terms.Bucket&gt; iter = stateAgg.getBuckets().iterator();        while (iter.hasNext()) {            Terms.Bucket gradeBucket = iter.next();            map.put(gradeBucket.getKey(), gradeBucket.getDocCount());        }        return map;    }    /**     * 功能描述：统计查询     * @param index 索引名     * @param type 类型     * @param constructor 查询构造     * @param agg 自定义计算     */    public Map&lt;Object, Object&gt; statSearch(String index, String type, ESQueryBuilderConstructor constructor, AggregationBuilder agg) {        if (agg == null) {            return null;        }        Map&lt;Object, Object&gt; map = new HashedMap();        SearchRequestBuilder searchRequestBuilder = client.prepareSearch(index).setTypes(type);        //排序        if (StringUtils.isNotEmpty(constructor.getAsc()))            searchRequestBuilder.addSort(constructor.getAsc(), SortOrder.ASC);        if (StringUtils.isNotEmpty(constructor.getDesc()))            searchRequestBuilder.addSort(constructor.getDesc(), SortOrder.DESC);        //设置查询体        if (null != constructor) {            searchRequestBuilder.setQuery(constructor.listBuilders());        } else {            searchRequestBuilder.setQuery(QueryBuilders.matchAllQuery());        }        int size = constructor.getSize();        if (size &lt; 0) {            size = 0;        }        if (size &gt; MAX) {            size = MAX;        }        //返回条目数        searchRequestBuilder.setSize(size);        searchRequestBuilder.setFrom(constructor.getFrom() &lt; 0 ? 0 : constructor.getFrom());        SearchResponse sr = searchRequestBuilder.addAggregation(                agg        ).get();        Terms stateAgg = sr.getAggregations().get(&quot;agg&quot;);        Iterator&lt;? extends Terms.Bucket&gt; iter = stateAgg.getBuckets().iterator();        while (iter.hasNext()) {            Terms.Bucket gradeBucket = iter.next();            map.put(gradeBucket.getKey(), gradeBucket.getDocCount());        }        return map;    }    /**     * 功能描述：关闭链接     */    public void close() {        client.close();    }    public static void test() {        try{            ElasticSearchService service = new ElasticSearchService();            ESQueryBuilderConstructorNew constructor = new ESQueryBuilderConstructorNew();            constructor.must(new ESQueryBuilders().bool(QueryBuilders.boolQuery()));            constructor.must(new ESQueryBuilders().match(&quot;secondlanguage&quot;, &quot;4&quot;));            constructor.must(new ESQueryBuilders().match(&quot;secondlanguage&quot;, &quot;4&quot;));            constructor.should(new ESQueryBuilders().match(&quot;source&quot;, &quot;5&quot;));            constructor.should(new ESQueryBuilders().match(&quot;source&quot;, &quot;5&quot;));            service.searchCountAndMessageNew(&quot;&quot;, &quot;&quot;, constructor);        }catch (Exception e){            e.printStackTrace();        }    }    public static void main(String[] args) {        try {            ElasticSearchService service = new ElasticSearchService();            ESQueryBuilderConstructor constructor = new ESQueryBuilderConstructor();         /*   constructor.must(new ESQueryBuilders().term(&quot;gender&quot;, &quot;f&quot;).range(&quot;age&quot;, 20, 50));            constructor.should(new ESQueryBuilders().term(&quot;gender&quot;, &quot;f&quot;).range(&quot;age&quot;, 20, 50).fuzzy(&quot;age&quot;, 20));            constructor.mustNot(new ESQueryBuilders().term(&quot;gender&quot;, &quot;m&quot;));            constructor.setSize(15);  //查询返回条数，最大 10000            constructor.setFrom(11);  //分页查询条目起始位置， 默认0            constructor.setAsc(&quot;age&quot;); //排序            List&lt;Map&lt;String, Object&gt;&gt; list = service.search(&quot;bank&quot;, &quot;account&quot;, constructor);            Map&lt;Object, Object&gt; map = service.statSearch(&quot;bank&quot;, &quot;account&quot;, constructor, &quot;state&quot;);*/            constructor.must(new ESQueryBuilders().match(&quot;id&quot;, &quot;WE16000190TR&quot;));            List&lt;Map&lt;String, Object&gt;&gt; list = service.search(&quot;test01&quot;, &quot;test01&quot;, constructor);             for(Map&lt;String, Object&gt; map : list){                 System.out.println(map);             }        } catch (Exception e) {            e.printStackTrace();        }    }}</code></pre><p><strong>ESCriterion.java</strong></p><pre><code>package com.hsiehchou.es.V2;import org.elasticsearch.index.query.QueryBuilder;import java.util.List;/** * 条件接口 */public interface ESCriterion {    public enum Operator {        PREFIX,             /**根据字段前缀查询**/        MATCH,              /**匹配查询**/        MATCH_PHRASE,       /**精确匹配**/        MULTI_MATCH,        /**多字段匹配**/        TERM,               /**term查询**/        TERMS,              /**term查询**/        RANGE,              /**范围查询**/        GTE,                 /**大于等于查询**/        LTE,        FUZZY,              /**根据字段前缀查询**/        QUERY_STRING,       /**根据字段前缀查询**/        MISSING ,           /**根据字段前缀查询**/        BOOL    }    public enum MatchMode {        START, END, ANYWHERE    }    public enum Projection {        MAX, MIN, AVG, LENGTH, SUM, COUNT    }    public List&lt;QueryBuilder&gt; listBuilders();}</code></pre><p><strong>ESQueryBuilderConstructor.java</strong></p><pre><code>package com.hsiehchou.es.V2;import org.apache.commons.collections.CollectionUtils;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.QueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import java.util.ArrayList;import java.util.List;/** * 查询条件容器 */public class ESQueryBuilderConstructor {    private int size = Integer.MAX_VALUE;    private int from = 0;    private String asc;    private String desc;    //查询条件容器    private List&lt;ESCriterion&gt; mustCriterions = new ArrayList&lt;ESCriterion&gt;();    private List&lt;ESCriterion&gt; shouldCriterions = new ArrayList&lt;ESCriterion&gt;();    private List&lt;ESCriterion&gt; mustNotCriterions = new ArrayList&lt;ESCriterion&gt;();    //构造builder    public QueryBuilder listBuilders() {        int count = mustCriterions.size() + shouldCriterions.size() + mustNotCriterions.size();        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();        QueryBuilder queryBuilder = null;        if (count &gt;= 1) {            //must容器            if (!CollectionUtils.isEmpty(mustCriterions)) {                for (ESCriterion criterion : mustCriterions) {                    for (QueryBuilder builder : criterion.listBuilders()) {                        queryBuilder = boolQueryBuilder.must(builder);                    }                }            }            //should容器            if (!CollectionUtils.isEmpty(shouldCriterions)) {                for (ESCriterion criterion : shouldCriterions) {                    for (QueryBuilder builder : criterion.listBuilders()) {                        queryBuilder = boolQueryBuilder.should(builder);                    }                }            }            //must not 容器            if (!CollectionUtils.isEmpty(mustNotCriterions)) {                for (ESCriterion criterion : mustNotCriterions) {                    for (QueryBuilder builder : criterion.listBuilders()) {                        queryBuilder = boolQueryBuilder.mustNot(builder);                    }                }            }            return queryBuilder;        } else {            return null;        }    }    /**     * 增加简单条件表达式     */    public ESQueryBuilderConstructor must(ESCriterion criterion){        if(criterion!=null){            mustCriterions.add(criterion);        }        return this;    }    /**     * 增加简单条件表达式     */    public ESQueryBuilderConstructor should(ESCriterion criterion){        if(criterion!=null){            shouldCriterions.add(criterion);        }        return this;    }    /**     * 增加简单条件表达式     */    public ESQueryBuilderConstructor mustNot(ESCriterion criterion){        if(criterion!=null){            mustNotCriterions.add(criterion);        }        return this;    }    public int getSize() {        return size;    }    public void setSize(int size) {        this.size = size;    }    public String getAsc() {        return asc;    }    public void setAsc(String asc) {        this.asc = asc;    }    public String getDesc() {        return desc;    }    public void setDesc(String desc) {        this.desc = desc;    }    public int getFrom() {        return from;    }    public void setFrom(int from) {        this.from = from;    }}</code></pre><p><strong>ESQueryBuilderConstructorNew.java</strong></p><pre><code>package com.hsiehchou.es.V2;import org.apache.commons.collections.CollectionUtils;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.QueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.search.sort.SortBuilder;import java.util.ArrayList;import java.util.List;import java.util.Map;/** * 查询条件容器 */public class ESQueryBuilderConstructorNew {    private List&lt;String&gt; highLighterFields = new ArrayList&lt;String&gt;();    private int size = Integer.MAX_VALUE;    private int from = 0;    private List&lt;SortBuilder&gt; sortBuilderList;    public List&lt;SortBuilder&gt; getSortBuilderList() {        return sortBuilderList;    }    public void setSortBuilderList(List&lt;SortBuilder&gt; sortBuilderList) {        this.sortBuilderList = sortBuilderList;    }    private Map&lt;String,List&lt;String&gt;&gt; sortMap;    //查询条件容器    private List&lt;ESCriterion&gt; mustCriterions = new ArrayList&lt;ESCriterion&gt;();    private List&lt;ESCriterion&gt; shouldCriterions = new ArrayList&lt;ESCriterion&gt;();    private List&lt;ESCriterion&gt; mustNotCriterions = new ArrayList&lt;ESCriterion&gt;();    //构造builder    public QueryBuilder listBuilders() {        int count = mustCriterions.size() + shouldCriterions.size() + mustNotCriterions.size();        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();        QueryBuilder queryBuilder = null;        if (count &gt;= 1) {            //must容器            if (!CollectionUtils.isEmpty(mustCriterions)) {                for (ESCriterion criterion : mustCriterions) {                    for (QueryBuilder builder : criterion.listBuilders()) {                        queryBuilder = boolQueryBuilder.must(builder);                    }                }            }            //should容器            if (!CollectionUtils.isEmpty(shouldCriterions)) {                for (ESCriterion criterion : shouldCriterions) {                    for (QueryBuilder builder : criterion.listBuilders()) {                        queryBuilder = boolQueryBuilder.should(builder);                    }                }            }            //must not 容器            if (!CollectionUtils.isEmpty(mustNotCriterions)) {                for (ESCriterion criterion : mustNotCriterions) {                    for (QueryBuilder builder : criterion.listBuilders()) {                        queryBuilder = boolQueryBuilder.mustNot(builder);                    }                }            }            return queryBuilder;        } else {            return null;        }    }    /**     * 增加简单条件表达式     */    public ESQueryBuilderConstructorNew must(ESCriterion criterion){        if(criterion!=null){            mustCriterions.add(criterion);        }        return this;    }    /**     * 增加简单条件表达式     */    public ESQueryBuilderConstructorNew should(ESCriterion criterion){        if(criterion!=null){            shouldCriterions.add(criterion);        }        return this;    }    /**     * 增加简单条件表达式     */    public ESQueryBuilderConstructorNew mustNot(ESCriterion criterion){        if(criterion!=null){            mustNotCriterions.add(criterion);        }        return this;    }    public List&lt;String&gt; getHighLighterFields() {        return highLighterFields;    }    public void setHighLighterFields(List&lt;String&gt; highLighterFields) {        this.highLighterFields = highLighterFields;    }    public int getSize() {        return size;    }    public void setSize(int size) {        this.size = size;    }    public Map&lt;String, List&lt;String&gt;&gt; getSortMap() {        return sortMap;    }    public void setSortMap(Map&lt;String, List&lt;String&gt;&gt; sortMap) {        this.sortMap = sortMap;    }    public int getFrom() {        return from;    }    public void setFrom(int from) {        this.from = from;    }}</code></pre><p><strong>ESQueryBuilders.java</strong></p><pre><code>package com.hsiehchou.es.V2;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.NestedQueryBuilder;import org.elasticsearch.index.query.QueryBuilder;import java.util.ArrayList;import java.util.Collection;import java.util.List;/** * 条件构造器 */public class ESQueryBuilders implements ESCriterion{    private List&lt;QueryBuilder&gt; list = new ArrayList&lt;QueryBuilder&gt;();    /**     * 功能描述：match 查询     * @param field 字段名     * @param value 值     */    public ESQueryBuilders match(String field, Object value) {        list.add(new ESSimpleExpression (field, value, Operator.MATCH).toBuilder());        return this;    }    /**     * 功能描述：match 查询     * @param field 字段名     * @param value 值     */    public ESQueryBuilders match_phrase(String field, Object value) {        list.add(new ESSimpleExpression (field, value, Operator.MATCH_PHRASE).toBuilder());        return this;    }    /**     * 功能描述：match 查询     * @param fieldNames 字段名     * @param value 值     */    public ESQueryBuilders multi_match(Object value , String... fieldNames ) {        String[] fields = fieldNames;        list.add(new ESSimpleExpression (value, Operator.MULTI_MATCH,fields).toBuilder());        return this;    }    /**     * 功能描述：Term 查询     * @param field 字段名     * @param value 值     */    public ESQueryBuilders term(String field, Object value) {        list.add(new ESSimpleExpression (field, value, Operator.TERM).toBuilder());        return this;    }    /**     * 功能描述：Terms 查询     * @param field 字段名     * @param values 集合值     */    public ESQueryBuilders terms(String field, Collection&lt;Object&gt; values) {        list.add(new ESSimpleExpression (field, values).toBuilder());        return this;    }    /**     * 功能描述：fuzzy 查询     * @param field 字段名     * @param value 值     */    public ESQueryBuilders fuzzy(String field, Object value) {        list.add(new ESSimpleExpression (field, value, Operator.FUZZY).toBuilder());        return this;    }    /**     * 功能描述：Range 查询     * @param from 起始值     * @param to 末尾值     */    public ESQueryBuilders range(String field, Object from, Object to) {        list.add(new ESSimpleExpression (field, from, to).toBuilder());        return this;    }    /**     * 功能描述：GTE 大于等于查询     * @param     */    public ESQueryBuilders gte(String field, Object num) {        list.add(new ESSimpleExpression (field, num,Operator.GTE).toBuilder());        return this;    }    /**     * 功能描述：LTE 小于等于查询     * @param     */    public ESQueryBuilders lte(String field, Object num) {        list.add(new ESSimpleExpression (field, num,Operator.LTE).toBuilder());        return this;    }    /**     * 功能描述：prefix 查询     * @param field 字段名     * @param value 值     */    public ESQueryBuilders prefix(String field, Object value) {        list.add(new ESSimpleExpression (field, value, Operator.PREFIX).toBuilder());        return this;    }    /**     * 功能描述：Range 查询     * @param queryString 查询语句     */    public ESQueryBuilders queryString(String queryString) {        list.add(new ESSimpleExpression (queryString, Operator.QUERY_STRING).toBuilder());        return this;    }    /**     * 功能描述：Range 查询     * @param     */    public ESQueryBuilders bool(BoolQueryBuilder boolQueryBuilder) {        list.add(boolQueryBuilder);        return this;    }    public ESQueryBuilders nested(NestedQueryBuilder nestedQueryBuilder) {        list.add(nestedQueryBuilder);        return this;    }    public List&lt;QueryBuilder&gt; listBuilders() {        return list;    }}</code></pre><p><strong>ESSimpleExpression.java</strong></p><pre><code>package com.hsiehchou.es.V2;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.QueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import java.util.Collection;import com.hsiehchou.es.V2.ESCriterion.Operator;import static org.elasticsearch.index.search.MatchQuery.Type.PHRASE;/** * 条件表达式 */public class ESSimpleExpression {    private String[] fieldNames;         //属性名    private String fieldName;         //属性名    private Object value;             //对应值    private Collection&lt;Object&gt; values;//对应值    private Operator operator;        //计算符    private Object from;    private Object to;    protected  ESSimpleExpression() {    }    protected  ESSimpleExpression(Object value, Operator operator,String... fieldNames) {        this.fieldNames = fieldNames;        this.value = value;        this.operator = operator;    }    protected  ESSimpleExpression(String fieldName, Object value, Operator operator) {        this.fieldName = fieldName;        this.value = value;        this.operator = operator;    }    protected  ESSimpleExpression(String value, Operator operator) {        this.value = value;        this.operator = operator;    }    protected ESSimpleExpression(String fieldName, Collection&lt;Object&gt; values) {        this.fieldName = fieldName;        this.values = values;        this.operator = Operator.TERMS;    }    protected ESSimpleExpression(String fieldName, Object from, Object to) {        this.fieldName = fieldName;        this.from = from;        this.to = to;        this.operator = Operator.RANGE;    }    public BoolQueryBuilder toBoolQueryBuilder(){        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();        boolQueryBuilder.mustNot(QueryBuilders.matchQuery(&quot;&quot;,&quot;&quot;));        boolQueryBuilder.mustNot(QueryBuilders.matchQuery(&quot;&quot;,&quot;&quot;));        return null;    }    public QueryBuilder toBuilder() {        QueryBuilder qb = null;        switch (operator) {            case MATCH:                qb = QueryBuilders.matchQuery(fieldName, value);                break;            case MATCH_PHRASE:                qb = QueryBuilders.matchPhraseQuery(fieldName, value);                break;            case MULTI_MATCH:                qb = QueryBuilders.multiMatchQuery(value,fieldNames).type(PHRASE);                break;            case TERM:                qb = QueryBuilders.termQuery(fieldName, value);                break;            case TERMS:                qb = QueryBuilders.termsQuery(fieldName, values);                break;            case RANGE:                qb = QueryBuilders.rangeQuery(fieldName).from(from).to(to).includeLower(true).includeUpper(true);                break;            case GTE:                qb = QueryBuilders.rangeQuery(fieldName).gte(value);                break;            case LTE:                qb = QueryBuilders.rangeQuery(fieldName).lte(value);                break;            case FUZZY:                qb = QueryBuilders.fuzzyQuery(fieldName, value);                break;            case PREFIX:                qb = QueryBuilders.prefixQuery(fieldName, value.toString());                break;            case QUERY_STRING:                qb = QueryBuilders.queryStringQuery(value.toString());                default:        }        return qb;    }}</code></pre><h3 id="九、预警"><a href="#九、预警" class="headerlink" title="九、预警"></a>九、预警</h3><p>通过后台或者界面设置规则，保存到mysql，然后同步到redis。</p><p>数据量大的话，用mysql是非常慢的，使用内存数据库redis进行规则缓存，使用时直接比对预警。</p><p><img src="/medias/%E9%A2%84%E8%AD%A6%E6%B5%81%E7%A8%8B.PNG" alt="预警流程"></p><p><img src="/medias/%E9%A2%84%E8%AD%A6%E8%BF%87%E7%A8%8B.PNG" alt="预警过程"></p><p>MySQL 需要2张表<br>一张是规则表   用来存储规则<br>一张是消息表   存储告警消息</p><h4 id="1、创建规则表（由界面控制规则发布）"><a href="#1、创建规则表（由界面控制规则发布）" class="headerlink" title="1、创建规则表（由界面控制规则发布）"></a>1、创建规则表（由界面控制规则发布）</h4><p>规则首先存放在mysql中，会使用一个定时任务将mysql中的规则同步到redis<br>       直接在test库中创建<br>       创建脚本<br><strong>xz_rule.sql</strong></p><pre><code>SET FOREIGN_KEY_CHECKS=0;DROP TABLE IF EXISTS `xz_rule`;CREATE TABLE `xz_rule` (  `id` int(11) NOT NULL AUTO_INCREMENT,  `warn_fieldname` varchar(20) DEFAULT NULL,  `warn_fieldvalue` varchar(255) DEFAULT NULL,  `publisher` varchar(255) DEFAULT NULL,  `send_type` varchar(255) CHARACTER SET utf8 DEFAULT NULL,  `send_mobile` varchar(255) DEFAULT NULL,  `send_mail` varchar(255) DEFAULT NULL,  `send_dingding` varchar(255) DEFAULT NULL,  `create_time` date DEFAULT NULL,  PRIMARY KEY (`id`)) ENGINE=MyISAM AUTO_INCREMENT=2 DEFAULT CHARSET=latin1;INSERT INTO `xz_rule` VALUES (&#39;1&#39;, &#39;phone&#39;, &#39;18609765432&#39;, &#39;?????1&#39;, &#39;2&#39;, &#39;13724536789&#39;, &#39;1782324@qq.com&#39;, &#39;32143243&#39;, &#39;2019-06-28&#39;);</code></pre><h4 id="2、创建消息表"><a href="#2、创建消息表" class="headerlink" title="2、创建消息表"></a>2、创建消息表</h4><ol><li>用于存放预警的消息，供界面定时刷新预警消息 或者是滚屏预警</li><li>预警消息统计</li></ol><p><strong>warn_message.sql</strong></p><pre><code>SET FOREIGN_KEY_CHECKS=0;DROP TABLE IF EXISTS `warn_message`;CREATE TABLE `warn_message` (  `id` int(11) NOT NULL AUTO_INCREMENT,  `alarmRuleid` varchar(255) DEFAULT NULL,  `alarmType` varchar(255) DEFAULT NULL,  `sendType` varchar(255) DEFAULT NULL,  `sendMobile` varchar(255) DEFAULT NULL,  `sendEmail` varchar(255) DEFAULT NULL,  `sendStatus` varchar(255) DEFAULT NULL,  `senfInfo` varchar(255) CHARACTER SET utf8 DEFAULT NULL,  `hitTime` datetime DEFAULT NULL,  `checkinTime` datetime DEFAULT NULL,  `isRead` varchar(255) DEFAULT NULL,  `readAccounts` varchar(255) DEFAULT NULL,  `alarmaccounts` varchar(255) DEFAULT NULL,  `accountid` varchar(11) DEFAULT NULL,  PRIMARY KEY (`id`)) ENGINE=MyISAM AUTO_INCREMENT=31 DEFAULT CHARSET=latin1;</code></pre><h4 id="3、创建数据库连接工具类"><a href="#3、创建数据库连接工具类" class="headerlink" title="3、创建数据库连接工具类"></a>3、创建数据库连接工具类</h4><p><strong>新建com.hsiehchou.common.netb.db包</strong><br><strong>创建DBCommon类</strong></p><p><strong>DBCommon.java</strong></p><pre><code>package com.hsiehchou.common.netb.db;import com.hsiehchou.common.config.ConfigUtil;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.sql.*;import java.util.Properties;public class DBCommon {    private static Logger LOG = LoggerFactory.getLogger(DBCommon.class);    private static String MYSQL_PATH = &quot;common/mysql.properties&quot;;    private static Properties properties = ConfigUtil.getInstance().getProperties(MYSQL_PATH);    private static Connection conn ;    private DBCommon(){}    public static void main(String[] args) {        System.out.println(properties);        Connection xz_bigdata = DBCommon.getConn(&quot;test&quot;);        System.out.println(xz_bigdata);    }    //TODO  配置文件    private static final String JDBC_DRIVER = &quot;com.mysql.jdbc.Driver&quot;;    private static final String USER_NAME = properties.getProperty(&quot;user&quot;);    private static final String PASSWORD = properties.getProperty(&quot;password&quot;);    private static final String IP = properties.getProperty(&quot;db_ip&quot;);    private static final String PORT = properties.getProperty(&quot;db_port&quot;);    private static final String DB_CONFIG = &quot;?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull&amp;autoReconnect=true&amp;failOverReadOnly=false&quot;;    static {        try {            Class.forName(JDBC_DRIVER);        } catch (ClassNotFoundException e) {            LOG.error(null, e);        }    }    /**     * 获取数据库连接     * @param dbName     * @return     */    public static Connection getConn(String dbName) {        Connection conn = null;        String  connstring = &quot;jdbc:mysql://&quot;+IP+&quot;:&quot;+PORT+&quot;/&quot;+dbName+DB_CONFIG;        try {            conn = DriverManager.getConnection(connstring, USER_NAME, PASSWORD);        } catch (SQLException e) {            e.printStackTrace();            LOG.error(null, e);        }        return conn;    }    /**     * @param url eg:&quot;jdbc:oracle:thin:@172.16.1.111:1521:d406&quot;     * @param driver eg:&quot;oracle.jdbc.driver.OracleDriver&quot;     * @param user eg:&quot;ucase&quot;     * @param password eg:&quot;ucase123&quot;     * @return     * @throws ClassNotFoundException     * @throws SQLException     */    public static Connection getConn(String url, String driver, String user,                                     String password) throws ClassNotFoundException, SQLException{        Class.forName(driver);        conn = DriverManager.getConnection(url, user, password);        return  conn;    }    public static void close(Connection conn){        try {            if( conn != null ){                conn.close();            }        } catch (SQLException e) {            LOG.error(null,e);        }    }    public static void close(Statement statement){        try {            if( statement != null ){                statement.close();            }        } catch (SQLException e) {            LOG.error(null,e);        }    }    public static void close(Connection conn,PreparedStatement statement){        try {            if( conn != null ){                conn.close();            }            if( statement != null ){                statement.close();            }        } catch (SQLException e) {            LOG.error(null,e);        }    }    public static void close(Connection conn,Statement statement,ResultSet resultSet) throws SQLException{        if( resultSet != null ){            resultSet.close();        }        if( statement != null ){            statement.close();        }        if( conn != null ){            conn.close();        }    }}</code></pre><p><strong>引入maven依赖</strong></p><pre><code>&lt;dependency&gt;    &lt;groupId&gt;commons-dbutils&lt;/groupId&gt;    &lt;artifactId&gt;commons-dbutils&lt;/artifactId&gt;    &lt;version&gt;${commons-dbutils.version}&lt;/version&gt;&lt;/dependency&gt;</code></pre><h4 id="4、创建实体类和dao"><a href="#4、创建实体类和dao" class="headerlink" title="4、创建实体类和dao"></a>4、创建实体类和dao</h4><p><strong>新建com.hsiehchou.spark.warn.domain包</strong><br><strong>新建 XZ_RuleDomain，WarningMessage</strong></p><p><strong>XZ_RuleDomain.java</strong></p><pre><code>package com.hsiehchou.spark.warn.domain;import java.sql.Date;public class XZ_RuleDomain {    private int id;    private String warn_fieldname;   //预警字段    private String warn_fieldvalue; //预警内容    private String publisher;       //发布者    private String send_type;       //消息接收方式    private String send_mobile;     //接收手机号    private String send_mail;       //接收邮箱    private String send_dingding;   //接收钉钉    private Date create_time;       //创建时间    public int getId() {        return id;    }    public void setId(int id) {        this.id = id;    }    public String getWarn_fieldname() {        return warn_fieldname;    }    public void setWarn_fieldname(String warn_fieldname) {        this.warn_fieldname = warn_fieldname;    }    public String getWarn_fieldvalue() {        return warn_fieldvalue;    }    public void setWarn_fieldvalue(String warn_fieldvalue) {        this.warn_fieldvalue = warn_fieldvalue;    }    public String getPublisher() {        return publisher;    }    public void setPublisher(String publisher) {        this.publisher = publisher;    }    public String getSend_type() {        return send_type;    }    public void setSend_type(String send_type) {        this.send_type = send_type;    }    public String getSend_mobile() {        return send_mobile;    }    public void setSend_mobile(String send_mobile) {        this.send_mobile = send_mobile;    }    public String getSend_mail() {        return send_mail;    }    public void setSend_mail(String send_mail) {        this.send_mail = send_mail;    }    public String getSend_dingding() {        return send_dingding;    }    public void setSend_dingding(String send_dingding) {        this.send_dingding = send_dingding;    }    public Date getCreate_time() {        return create_time;    }    public void setCreate_time(Date create_time) {        this.create_time = create_time;    }}</code></pre><p><strong>WarningMessage.java</strong></p><pre><code>package com.hsiehchou.spark.warn.domain;import java.sql.Date;public class WarningMessage {    private String id;            //主键id    private String alarmRuleid;   //规则id    private String alarmType;     //告警类型    private String sendType;      //发送方式    private String sendMobile;    //发送至手机    private String sendEmail;     //发送至邮箱    private String sendStatus;    //发送状态    private String senfInfo;      //发送内容    private Date hitTime;         //命中时间    private Date checkinTime;     //入库时间    private String isRead;        //是否已读    private String readAccounts;  //已读用户    private String alarmaccounts;    private String accountid;    public String getId() {        return id;    }    public void setId(String id) {        this.id = id;    }    public String getAlarmRuleid() {        return alarmRuleid;    }    public void setAlarmRuleid(String alarmRuleid) {        this.alarmRuleid = alarmRuleid;    }    public String getAlarmType() {        return alarmType;    }    public void setAlarmType(String alarmType) {        this.alarmType = alarmType;    }    public String getSendType() {        return sendType;    }    public void setSendType(String sendType) {        this.sendType = sendType;    }    public String getSendMobile() {        return sendMobile;    }    public void setSendMobile(String sendMobile) {        this.sendMobile = sendMobile;    }    public String getSendEmail() {        return sendEmail;    }    public void setSendEmail(String sendEmail) {        this.sendEmail = sendEmail;    }    public String getSendStatus() {        return sendStatus;    }    public void setSendStatus(String sendStatus) {        this.sendStatus = sendStatus;    }    public String getSenfInfo() {        return senfInfo;    }    public void setSenfInfo(String senfInfo) {        this.senfInfo = senfInfo;    }    public Date getHitTime() {        return hitTime;    }    public void setHitTime(Date hitTime) {        this.hitTime = hitTime;    }    public Date getCheckinTime() {        return checkinTime;    }    public void setCheckinTime(Date checkinTime) {        this.checkinTime = checkinTime;    }    public String getIsRead() {        return isRead;    }    public void setIsRead(String isRead) {        this.isRead = isRead;    }    public String getReadAccounts() {        return readAccounts;    }    public void setReadAccounts(String readAccounts) {        this.readAccounts = readAccounts;    }    public String getAlarmaccounts() {        return alarmaccounts;    }    public void setAlarmaccounts(String alarmaccounts) {        this.alarmaccounts = alarmaccounts;    }    public String getAccountid() {        return accountid;    }    public void setAccountid(String accountid) {        this.accountid = accountid;    }    @Override    public String toString() {        return &quot;WarningMessage{&quot; +                &quot;id=&#39;&quot; + id + &#39;\&#39;&#39; +                &quot;, alarmRuleid=&#39;&quot; + alarmRuleid + &#39;\&#39;&#39; +                &quot;, alarmType=&#39;&quot; + alarmType + &#39;\&#39;&#39; +                &quot;, sendType=&#39;&quot; + sendType + &#39;\&#39;&#39; +                &quot;, sendMobile=&#39;&quot; + sendMobile + &#39;\&#39;&#39; +                &quot;, sendEmail=&#39;&quot; + sendEmail + &#39;\&#39;&#39; +                &quot;, sendStatus=&#39;&quot; + sendStatus + &#39;\&#39;&#39; +                &quot;, senfInfo=&#39;&quot; + senfInfo + &#39;\&#39;&#39; +                &quot;, hitTime=&quot; + hitTime +                &quot;, checkinTime=&quot; + checkinTime +                &quot;, isRead=&#39;&quot; + isRead + &#39;\&#39;&#39; +                &quot;, readAccounts=&#39;&quot; + readAccounts + &#39;\&#39;&#39; +                &quot;, alarmaccounts=&#39;&quot; + alarmaccounts + &#39;\&#39;&#39; +                &quot;, accountid=&#39;&quot; + accountid + &#39;\&#39;&#39; +                &#39;}&#39;;    }}</code></pre><p><strong>新建com.hsiehchou.spark.warn.dao包</strong><br><strong>新建 XZ_RuleDao，WarningMessageDao</strong></p><p><strong>XZ_RuleDao.java</strong></p><pre><code>package com.hsiehchou.spark.warn.dao;import com.hsiehchou.common.netb.db.DBCommon;import com.hsiehchou.spark.warn.domain.XZ_RuleDomain;import org.apache.commons.dbutils.QueryRunner;import org.apache.commons.dbutils.handlers.BeanListHandler;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.sql.Connection;import java.sql.SQLException;import java.util.List;public class XZ_RuleDao {    private static final Logger LOG = LoggerFactory.getLogger(XZ_RuleDao.class);    /**     *  获取所有的规则     * @return     */    public static List&lt;XZ_RuleDomain&gt; getRuleList(){        List&lt;XZ_RuleDomain&gt; listRules = null;        //获取连接        Connection conn = DBCommon.getConn(&quot;test&quot;);        //执行器        QueryRunner query = new QueryRunner();        String sql = &quot;select * from xz_rule&quot;;        try {            listRules = query.query(conn,sql,new BeanListHandler&lt;&gt;(XZ_RuleDomain.class));        } catch (SQLException e) {            LOG.error(null,e);        }finally {            DBCommon.close(conn);        }        return listRules;    }    public static void main(String[] args) {        List&lt;XZ_RuleDomain&gt; ruleList = XZ_RuleDao.getRuleList();        System.out.println(ruleList.size());        ruleList.forEach(x-&gt;{            System.out.println(x);        });    }}</code></pre><p><strong>WarningMessageDao.java</strong></p><pre><code>package com.hsiehchou.spark.warn.dao;import com.hsiehchou.common.netb.db.DBCommon;import com.hsiehchou.spark.warn.domain.WarningMessage;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.sql.*;public class WarningMessageDao {    private static final Logger LOG = LoggerFactory.getLogger(WarningMessageDao.class);    /**     * 写入消息到mysql     * @param warningMessage     * @return     */    public static Integer insertWarningMessageReturnId(WarningMessage warningMessage) {        Connection conn= DBCommon.getConn(&quot;test&quot;);        String sql=&quot;insert into warn_message(alarmruleid,sendtype,senfinfo,hittime,sendmobile,alarmtype) &quot; +                &quot;values(?,?,?,?,?,?)&quot;;        PreparedStatement stmt=null;        ResultSet resultSet=null;        int id=-1;        try{            stmt = conn.prepareStatement(sql);            stmt.setString(1,warningMessage.getAlarmRuleid());            stmt.setInt(2,Integer.valueOf(warningMessage.getSendType()));            stmt.setString(3,warningMessage.getSenfInfo());            stmt.setTimestamp(4,new Timestamp(System.currentTimeMillis()));            stmt.setString(5,warningMessage.getSendMobile());            stmt.setInt(6,Integer.valueOf(warningMessage.getAlarmType()));            stmt.executeUpdate();        }catch(Exception e) {            LOG.error(null,e);        }finally {            try {                DBCommon.close(conn,stmt,resultSet);            } catch (SQLException e) {                e.printStackTrace();            }        }        return id;    }}</code></pre><h4 id="5、告警工具类"><a href="#5、告警工具类" class="headerlink" title="5、告警工具类"></a>5、告警工具类</h4><p><strong>新建com.hsiehchou.spark.warn.service包</strong><br><strong>新建 BlackRuleWarning，WarningMessageSendUtil</strong></p><p><strong>BlackRuleWarning.java</strong></p><pre><code>package com.hsiehchou.spark.warn.service;import com.hsiehchou.spark.warn.dao.WarningMessageDao;import com.hsiehchou.spark.warn.domain.WarningMessage;import org.apache.commons.lang3.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import redis.clients.jedis.Jedis;import java.text.SimpleDateFormat;import java.util.ArrayList;import java.util.List;import java.util.Map;public class BlackRuleWarning {    private static final Logger LOG = LoggerFactory.getLogger(BlackRuleWarning.class);    //可以通过数据库，配置文件加载    //为了遍历所有预警字段    private static List&lt;String&gt; listWarnFields = new ArrayList&lt;&gt;();    static {        listWarnFields.add(&quot;phone&quot;);        listWarnFields.add(&quot;mac&quot;);    }    /**     * 预警流程处理     * @param map     * @param jedis15     */    public static void blackWarning(Map&lt;String, Object&gt; map, Jedis jedis15) {        listWarnFields.forEach(warnField -&gt; {            if (map.containsKey(warnField) &amp;&amp; StringUtils.isNotBlank(map.get(warnField).toString())) {                //获取预警字段核预警值  相当于手机号                String warnFieldValue = map.get(warnField).toString();                //去redis中进行比对                //数据中  通过   &quot;字段&quot; + &quot;字段值&quot; 去拼接key                //            phone       :    186XXXXXX                String key = warnField + &quot;:&quot; + warnFieldValue;                //redis中的key是   phone:18609765435                System.out.println(&quot;拼接数据流中的key=======&quot; + key);                if (jedis15.exists(key)) {                    //对比命中之后 就可以发送消息提醒                    System.out.println(&quot;命中REDIS中的&quot; + key + &quot;===========开始预警&quot;);                    beginWarning(jedis15, key);                } else {                    //直接过                    System.out.println(&quot;未命中&quot; + key + &quot;===========不进行预警&quot;);                }            }        });    }    /**     * 规则已经命中，开始预警     * @param jedis15     * @param key     */    private static void beginWarning( Jedis jedis15, String key) {        System.out.println(&quot;============MESSAGE -1- =========&quot;);        //封装告警  信息及告警消息        WarningMessage warningMessage = getWarningMessage(jedis15, key);        System.out.println(&quot;============MESSAGE -4- =========&quot;);        if (warningMessage != null) {            //将预警信息写入预警信息表            WarningMessageDao.insertWarningMessageReturnId(warningMessage);            //String accountid = warningMessage.getAccountid();            //String readAccounts = warningMessage.getAlarmaccounts();            // WarnService.insertRead_status(messageId, accountid);            if (warningMessage.getSendType().equals(&quot;2&quot;)) {                //手机短信告警 默认告警方式                WarningMessageSendUtil.messageWarn(warningMessage);            }        }    }    /**     * 封装告警信息及告警消息     * @param jedis15     * @param key     * @return     */    private static WarningMessage getWarningMessage(Jedis jedis15, String key) {        System.out.println(&quot;============MESSAGE -2- =========&quot;);        //封装消息        String[] split = key.split(&quot;:&quot;);        if (split.length == 2) {            WarningMessage warningMessage = new WarningMessage();            String time = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;).toString();            String clew_type = split[0];//告警字段            String rulecontent = split[1];//告警字段值            //从redis中获取消息信息进行封装            Map&lt;String, String&gt; valueMap = jedis15.hgetAll(key);            //规则ID (是哪条规则命中的)            warningMessage.setAlarmRuleid(valueMap.get(&quot;id&quot;));            //预警方式            warningMessage.setSendType(valueMap.get(&quot;send_type&quot;));//告警方式，0：界面 1：邮件 2：短信 3：邮件+短信            //预警信息接收手机号            warningMessage.setSendMobile(valueMap.get(&quot;send_mobile&quot;));            //arningMessage.setSendEmail(valueMap.get(&quot;sendemail&quot;));            /*arningMessage.setAlarmaccounts(valueMap.get(&quot;alarmaccounts&quot;));*/            //规则发布人            warningMessage.setAccountid(valueMap.get(&quot;publisher&quot;));            warningMessage.setAlarmType(&quot;2&quot;);            StringBuffer warn_content = new StringBuffer();            //预警内容 信息   时间  地点  人物            //预警字段来进行设置  phone            //我们有手机号            //数据关联            // 手机  MAC  身份证， 车牌  人脸。。URL 姓名            // 全部设在推送消息里面            warn_content.append(&quot;【网络告警】：手机号为:&quot; + &quot;[&quot; + rulecontent + &quot;]在时间&quot; + time + &quot;出现在&quot; + &quot;&gt;附近,设备号&quot;            );            String content = warn_content.toString();            warningMessage.setSenfInfo(content);            System.out.println(&quot;============MESSAGE -3- =========&quot;);            return warningMessage;        } else {            return null;        }    }}</code></pre><p><strong>WarningMessageSendUtil.java</strong></p><pre><code>package com.hsiehchou.spark.warn.service;import com.hsiehchou.common.regex.Validation;import com.hsiehchou.spark.warn.domain.WarningMessage;import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class WarningMessageSendUtil {    private static final Logger LOG = LoggerFactory.getLogger(WarningMessageSendUtil.class);    public static void messageWarn(WarningMessage warningMessage) {        String[] mobiles = warningMessage.getSendMobile().split(&quot;,&quot;);        for(String phone:mobiles){            if(Validation.isMobile(phone)){                System.out.println(&quot;开始向手机号为&quot; + phone + &quot;发送告警消息====&quot; + warningMessage);                StringBuffer sb= new StringBuffer();                String content=warningMessage.getSenfInfo().toString();                //TODO  调用短信接口发送消息                //TODO  怎么通过短信发送  这个是需要公司开通接口                //TODO  DINGDING                // 专门的接口             /*   sb.append(ClusterProperties.https_url + &quot;username=&quot; + ClusterProperties.https_username +                        &quot;&amp;password=&quot; + ClusterProperties.https_password + &quot;&amp;mobile=&quot; + phone +                        &quot;&amp;apikey=&quot; + ClusterProperties.https_apikey+                        &quot;&amp;content=&quot; + URLEncoder.encode(content));*/               // sendMessage(sb.toString());            }        }    }}</code></pre><h4 id="6、创建redis子项目"><a href="#6、创建redis子项目" class="headerlink" title="6、创建redis子项目"></a>6、创建redis子项目</h4><p><strong>操作redis 使用</strong></p><p><strong>新建xz_bigdata_redis子模块</strong></p><p><strong>pom.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_redis&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_redis&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;        &lt;jedis.version&gt;2.7.0&lt;/jedis.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;redis.clients&lt;/groupId&gt;            &lt;artifactId&gt;jedis&lt;/artifactId&gt;            &lt;version&gt;${jedis.version}&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><p><strong>新建com.hsiehchou.redis.client包</strong><br> <strong>创建redis连接类—JedisSingle</strong></p><p><strong>JedisSingle.java</strong></p><pre><code>package com.hsiehchou.redis.client;import com.hsiehchou.common.config.ConfigUtil;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import redis.clients.jedis.Jedis;import redis.clients.jedis.exceptions.JedisConnectionException;import java.net.SocketTimeoutException;import java.util.Map;import java.util.Properties;public class JedisSingle {    private static final Logger LOG = LoggerFactory.getLogger(JedisSingle.class);    private static Properties redisConf;    /**     * 读取redis配置文件     * redis.hostname = 192.168.247.103     * redis.port  = 6379     */    static {        redisConf = ConfigUtil.getInstance().getProperties(&quot;redis/redis.properties&quot;);        System.out.println(redisConf);    }    public static Jedis getJedis(int db){        Jedis jedis = JedisSingle.getJedis();        if(jedis!=null){            jedis.select(db);        }        return jedis;    }    public static void main(String[] args) {        Jedis jedis = JedisSingle.getJedis(15);        Map&lt;String, String&gt; Map = jedis.hgetAll(&quot;phone:18609765435&quot;);        System.out.println(Map.toString());    }    public static Jedis getJedis(){        int timeoutCount = 0;        while (true) {// 如果是网络超时则多试几次            try            {                 Jedis jedis = new Jedis(redisConf.get(&quot;redis.hostname&quot;).toString(),                         Integer.valueOf(redisConf.get(&quot;redis.port&quot;).toString()));                return jedis;            } catch (Exception e)            {                if (e instanceof JedisConnectionException || e instanceof SocketTimeoutException)                {                    timeoutCount++;                    LOG.warn(&quot;获取jedis连接超时次数:&quot; +timeoutCount);                    if (timeoutCount &gt; 4)                    {                        LOG.error(&quot;获取jedis连接超时次数a:&quot; +timeoutCount);                        LOG.error(null,e);                        break;                    }                }else                {                    LOG.error(&quot;getJedis error&quot;, e);                    break;                }            }        }        return null;    }    public static void close(Jedis jedis){        if(jedis!=null){            jedis.close();        }    }}</code></pre><h4 id="7、创建定时任务，将规则同步到redis"><a href="#7、创建定时任务，将规则同步到redis" class="headerlink" title="7、创建定时任务，将规则同步到redis"></a>7、创建定时任务，将规则同步到redis</h4><p><strong>新建 com.hsiehchou.spark.warn.timer 包</strong><br><strong>新建 SyncRule2Redis，WarnHelper</strong></p><p><strong>SyncRule2Redis.java</strong></p><pre><code>package com.hsiehchou.spark.warn.timer;import java.util.TimerTask;public class SyncRule2Redis extends TimerTask {    @Override    public void run() {        //这里定义同步方法        //就是读取mysql的数据 然后写入到redis中        System.out.println(&quot;========开始同步MYSQL规则到redis=======&quot;);        WarnHelper.syncRuleFromMysql2Redis();        System.out.println(&quot;============开始同步规则成功===========&quot;);    }}</code></pre><p><strong>WarnHelper.java</strong></p><pre><code>package com.hsiehchou.spark.warn.timer;import com.hsiehchou.redis.client.JedisSingle;import com.hsiehchou.spark.warn.dao.XZ_RuleDao;import com.hsiehchou.spark.warn.domain.XZ_RuleDomain;import org.apache.commons.lang3.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import redis.clients.jedis.Jedis;import java.util.List;public class WarnHelper {    private static final Logger LOG = LoggerFactory.getLogger(WarnHelper.class);    /**     * 同步mysql规则数据到redis     */    public static void syncRuleFromMysql2Redis(){        //获取所有的规则        List&lt;XZ_RuleDomain&gt; ruleList = XZ_RuleDao.getRuleList();        Jedis jedis = null;        try {            //获取redis 客户端            jedis = JedisSingle.getJedis(15);            for (int i = 0; i &lt; ruleList.size(); i++) {                XZ_RuleDomain rule = ruleList.get(i);                String id = rule.getId()+&quot;&quot;;                String publisher = rule.getPublisher();                String warn_fieldname = rule.getWarn_fieldname();                String warn_fieldvalue = rule.getWarn_fieldvalue();                String send_mobile = rule.getSend_mobile();                String send_type = rule.getSend_type();                //拼接redis key值                String redisKey = warn_fieldname +&quot;:&quot; + warn_fieldvalue;                //通过redis hash结构   hashMap                jedis.hset(redisKey,&quot;id&quot;,StringUtils.isNoneBlank(id) ? id : &quot;&quot;);                jedis.hset(redisKey,&quot;publisher&quot;,StringUtils.isNoneBlank(publisher) ? publisher : &quot;&quot;);                jedis.hset(redisKey,&quot;warn_fieldname&quot;,StringUtils.isNoneBlank(warn_fieldname) ? warn_fieldname : &quot;&quot;);                jedis.hset(redisKey,&quot;warn_fieldvalue&quot;,StringUtils.isNoneBlank(warn_fieldvalue) ? warn_fieldvalue : &quot;&quot;);                jedis.hset(redisKey,&quot;send_mobile&quot;,StringUtils.isNoneBlank(send_mobile) ? send_mobile : &quot;&quot;);                jedis.hset(redisKey,&quot;send_type&quot;,StringUtils.isNoneBlank(send_type) ? send_type : &quot;&quot;);            }        } catch (Exception e) {           LOG.error(&quot;同步规则到es失败&quot;,e);        } finally {            JedisSingle.close(jedis);        }    }    public static void main(String[] args)    {        WarnHelper.syncRuleFromMysql2Redis();    }}</code></pre><h4 id="8、创建streaming流任务"><a href="#8、创建streaming流任务" class="headerlink" title="8、创建streaming流任务"></a>8、创建streaming流任务</h4><p><strong>scala/com/hsiehchou/spark/streaming/kafka/warn</strong><br><strong>WarningStreamingTask.scala</strong></p><pre><code>package com.hsiehchou.spark.streaming.kafka.warnimport java.util.Timerimport com.hsiehchou.redis.client.JedisSingleimport com.hsiehchou.spark.common.SparkContextFactoryimport com.hsiehchou.spark.streaming.kafka.Spark_Kafka_ConfigUtilimport com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming.kafkaConfigimport com.hsiehchou.spark.warn.service.BlackRuleWarningimport com.hsiehchou.spark.warn.timer.SyncRule2Redisimport org.apache.spark.Loggingimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.StreamingContextimport org.apache.spark.streaming.kafka.KafkaManagerimport redis.clients.jedis.Jedisobject WarningStreamingTask extends Serializable with Logging{  def main(args: Array[String]): Unit = {     //定义一个定时器去定时同步 MYSQL到REDIS     val timer : Timer = new Timer    //SyncRule2Redis 任务类    //0 第一次开始执行    //1*60*1000  隔多少时间执行一次    timer.schedule(new SyncRule2Redis,0,1*60*1000)     //从kafka中获取数据流     //val topics = &quot;chl_test7&quot;.split(&quot;,&quot;)     //kafka topic     val topics = &quot;chl_test7&quot;.split(&quot;,&quot;)     //val ssc = SparkContextFactory.newSparkLocalStreamingContext(&quot;WarningStreamingTask1&quot;, java.lang.Long.valueOf(10),1)     val ssc:StreamingContext = SparkContextFactory.newSparkStreamingContext(&quot;Kafka2esStreaming&quot;, java.lang.Long.valueOf(10))    //构建kafkaManager    val kafkaManager = new KafkaManager(      Spark_Kafka_ConfigUtil.getKafkaParam(kafkaConfig.getProperty(&quot;metadata.broker.list&quot;), &quot;WarningStreamingTask111&quot;)    )    //使用kafkaManager创建DStreaming流    val kafkaDS = kafkaManager.createJsonToJMapStringDirectStreamWithOffset(ssc, topics.toSet)      //添加一个日期分组字段      //如果数据其他的转换，可以先在这里进行统一转换       .persist(StorageLevel.MEMORY_AND_DISK)    kafkaDS.foreachRDD(rdd=&gt;{      //流量预警      //if(!rdd.isEmpty()){/*      val count_flow = rdd.map(x=&gt;{          val flow = java.lang.Long.valueOf(x.get(&quot;collect_time&quot;))          flow        }).reduce(_+_)      if(count_flow &gt; 1719179595L){        println(&quot;流量预警: 阈值[1719179595L] 实际值:&quot;+ count_flow)      }*/      //}      //客户端连接之类的 最好不要放在RDD外面，因为在处理partion时，数据需要分发到各个节点上去      //数据分发必须需要序列化才可以，如果不能序列化，分发会报错      //如果这个数据 包括他里面的内容 都可以序列化，那么可以直接放在RDD外面      var jedis:Jedis = null      try {        //jedis = JedisSingle.getJedis(15)        rdd.foreachPartition(partion =&gt; {          jedis = JedisSingle.getJedis(15)          while (partion.hasNext) {            val map = partion.next()            val table = map.get(&quot;table&quot;)            val mapObject = map.asInstanceOf[java.util.Map[String,Object]]            println(table)            //开始比对            BlackRuleWarning.blackWarning(mapObject,jedis)          }        })      } catch {        case e =&gt; e.printStackTrace()      } finally {        JedisSingle.close(jedis)      } /*       rdd.foreachPartition(partion =&gt; {          var jedis: Jedis = null          try {            jedis = JedisSingle.getJedis(15)            while (partion.hasNext) {              val map = partion.next()              val mapObject = map.asInstanceOf[java.util.Map[String, Object]]              //开始比对              BlackRuleWarning.blackWarning(mapObject, jedis)            }          } catch {            case e =&gt; logError(null,e)          }finally {            JedisSingle.close(jedis)          }        })*/    })    ssc.start()    ssc.awaitTermination()  }}</code></pre><h4 id="9、执行"><a href="#9、执行" class="headerlink" title="9、执行"></a>9、执行</h4><p>spark-submit <code>--</code>master local[1] <code>--</code>num-executors 1 <code>--</code>driver-memory 300m <code>--</code>executor-memory 500m <code>--</code>executor-cores 1 <code>--</code>jars $(echo /usr/chl/spark7/jars/*.jar | tr ‘ ‘ ‘,’) <code>--</code>class com.hsiehchou.spark.streaming.kafka.warn.WarningStreamingTask /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</p><h4 id="10、截图"><a href="#10、截图" class="headerlink" title="10、截图"></a>10、截图</h4><p><img src="/medias/redis%E5%AE%89%E8%A3%85%E6%88%90%E5%8A%9F.PNG" alt="redis安装成功"></p><p><img src="/medias/%E9%A2%84%E8%AD%A6.PNG" alt="预警"></p><p><img src="/medias/RedisManager.PNG" alt="RedisManager"></p><p><img src="/medias/mysql-xz_rule.PNG" alt="mysql-xz_rule"></p><p><img src="/medias/%E5%8F%91%E9%80%81%E9%A2%84%E8%AD%A6.PNG" alt="发送预警"></p><h4 id="11、redis安装"><a href="#11、redis安装" class="headerlink" title="11、redis安装"></a>11、redis安装</h4><p>解压：tar -zxvf redis-3.0.5.tar.gz<br>cd redis-3.0.5/<br>make<br>make PREFIX=/opt/software/redis install</p><p><strong>redis-benchmark</strong> ： Redis提供的压力测试工具。模拟产生客户端的压力<br><strong>redis-check-aof</strong> ： 检查aof日志文件<br><strong>redis-check-dump</strong> ： 检查rdb文件<br><strong>redis-cli</strong> ： Redis客户端脚本<br><strong>redis-sentinel</strong> ： 哨兵<br><strong>redis-server</strong> ： Redis服务器脚本</p><p><strong>核心配置文件:redis.conf</strong><br>[root@hsiehchou202 redis-3.0.5]# cp redis.conf /opt/software/redis<br>[root@hsiehchou202 redis]# mkdir conf<br>[root@hsiehchou202 redis]# mv redis.conf conf/<br>[root@hsiehchou202 conf]# vi redis.conf</p><p>42行 <strong>daemonize yes //后台方式运行</strong><br>50行 <strong>port 6379</strong></p><p>启动<strong>redis ./bin/redis-server conf/redis.conf</strong></p><p><strong>检测是否启动好</strong><br>[root@hsiehchou202 redis]# <strong>bin/redis-server conf/redis.conf</strong></p><h3 id="十、Spark—kafka2hive"><a href="#十、Spark—kafka2hive" class="headerlink" title="十、Spark—kafka2hive"></a>十、Spark—kafka2hive</h3><h4 id="1、CDH启用Hive-on-spark"><a href="#1、CDH启用Hive-on-spark" class="headerlink" title="1、CDH启用Hive on spark"></a>1、CDH启用Hive on spark</h4><p><strong>设置 hive on spark 参数</strong><br>原来的HIVE执行引擎使用的hadoop的mapreduce，Hive on Spark 就是讲执行引擎换为spark 引擎</p><h4 id="2、hive配置文件"><a href="#2、hive配置文件" class="headerlink" title="2、hive配置文件"></a>2、hive配置文件</h4><p><strong>scala/com/hsiehchou/spark/streaming/kafka/kafka2hdfs/</strong></p><p><strong>HiveConfig.scala</strong></p><pre><code>package com.hsiehchou.spark.streaming.kafka.kafka2hdfsimport java.utilimport org.apache.commons.configuration.{CompositeConfiguration, ConfigurationException, PropertiesConfiguration}import org.apache.spark.Loggingimport org.apache.spark.sql.types.{StringType, StructField, StructType}import scala.collection.mutable.ArrayBufferimport scala.collection.JavaConversions._object HiveConfig extends Serializable with Logging {  //HIVE 文件根目录  var hive_root_path = &quot;/apps/hive/warehouse/external/&quot;  var hiveFieldPath = &quot;es/mapping/fieldmapping.properties&quot;  var config: CompositeConfiguration = null  //所有的表  var tables: util.List[_] = null  //表对应所有的字段映射,可以通过table名获取 这个table的所有字段  var tableFieldsMap: util.Map[String, util.HashMap[String, String]] = null  //StructType  var mapSchema: util.Map[String, StructType] = null  //建表语句  var hiveTableSQL: util.Map[String, String] = null  /**    * 主要就是创建mapSchema  和  hiveTableSQL    */  initParams()  def main(args: Array[String]): Unit = {  }  /**    * 初始化HIVE参数    */  def initParams(): Unit = {    //加载es/mapping/fieldmapping.properties 配置文件    config = HiveConfig.readCompositeConfiguration(hiveFieldPath)    println(&quot;==========================config====================================&quot;)    config.getKeys.foreach(key =&gt; {      println(key + &quot;:&quot; + config.getProperty(key.toString))    })    println(&quot;==========================tables====================================&quot;)    //wechat,mail,qq    tables = config.getList(&quot;tables&quot;)    tables.foreach(table =&gt; {      println(table)    })    var tables1 = config.getProperty(&quot;tables&quot;)    println(&quot;======================tableFieldsMap================================&quot;)    //(qq,{qq.imsi=string, qq.id=string, qq.send_message=string, qq.filename=string})    tableFieldsMap = HiveConfig.getKeysByType()    tableFieldsMap.foreach(x =&gt; {      println(x)    })    println(&quot;=========================mapSchema===================================&quot;)    mapSchema = HiveConfig.createSchema()    mapSchema.foreach(x =&gt; {//      val structType = x._2//      println(&quot;-----------&quot;)//      println(structType)//////      val names = structType.fieldNames//      names.foreach(field =&gt; {//        println(field)//      })      println(x)    })    println(&quot;=========================hiveTableSQL===================================&quot;)    hiveTableSQL = HiveConfig.getHiveTables()    hiveTableSQL.foreach(x =&gt; {      println(x)    })  }  /**    * 读取hive 字段配置文件    * @param path    * @return    */  def readCompositeConfiguration(path: String): CompositeConfiguration = {    logInfo(&quot;加载配置文件 &quot; + path)    //多配置工具    val compositeConfiguration = new CompositeConfiguration    try {      val configuration = new PropertiesConfiguration(path)      compositeConfiguration.addConfiguration(configuration)    } catch {      case e: ConfigurationException =&gt; {        logError(&quot;加载配置文件 &quot; + path + &quot;失败&quot;, e)      }    }    logInfo(&quot;加载配置文件&quot; + path + &quot;成功。 &quot;)    compositeConfiguration  }  /**    * 获取table-字段 对应关系    * 使用 util.Map[String,util.HashMap[String, String结构保存    * @return    */  def getKeysByType(): util.Map[String, util.HashMap[String, String]] = {    val map = new util.HashMap[String, util.HashMap[String, String]]()    println(&quot;__________________tables_____________________&quot;+tables)    //wechat, mail, qq    val iteratorTable = tables.iterator()    //对每个表进行遍历    while (iteratorTable.hasNext) {      //使用一个MAP保存一种对应关系      val fieldMap = new util.HashMap[String, String]()      //获取一个表      val table: String = iteratorTable.next().toString      //获取这个表的所有字段      val fields = config.getKeys(table)      //获取通用字段  这里暂时没有      val commonKeys: util.Iterator[String] = config.getKeys(&quot;common&quot;).asInstanceOf[util.Iterator[String]]      //将通用字段放到map结构中去      while (commonKeys.hasNext) {        val key = commonKeys.next()        fieldMap.put(key.replace(&quot;common&quot;, table), config.getString(key))      }      //将每种表的私有字段放到map中去      while (fields.hasNext) {        val field = fields.next().toString        fieldMap.put(field, config.getString(field))        println(&quot;__________________field_____________________&quot;+&quot;\n&quot;+field)      }      map.put(table, fieldMap)    }    map  }  /**    * 构建建表语句    * 例如CREATE external TABLE IF NOT EXISTS qq (imei string,imsi string,longitude string,latitude string,phone_mac string,device_mac string,device_number string,collect_time string,username string,phone string,object_username string,send_message string,accept_message string,message_time string,id string,table string,filename string,absolute_filename string)    * @return    */  def getHiveTables(): util.Map[String, String] = {    val hiveTableSqlMap: util.Map[String, String] = new util.HashMap[String, String]()    //获取没中数据的建表语句    tables.foreach(table =&gt; {      var sql: String = s&quot;CREATE external TABLE IF NOT EXISTS ${table} (&quot;      val tableFields = config.getKeys(table.toString)      tableFields.foreach(tableField =&gt; {        //qq.imsi=string, qq.id=string, qq.send_message=string        val fieldType = config.getProperty(tableField.toString)        val field = tableField.toString.split(&quot;\\.&quot;)(1)        sql = sql + field        fieldType match {          //就是将配置中的类型映射为HIVE 建表语句中的类型          case &quot;string&quot; =&gt; sql = sql + &quot; string,&quot;          case &quot;long&quot; =&gt; sql = sql + &quot; string,&quot;          case &quot;double&quot; =&gt; sql = sql + &quot; string,&quot;          case _ =&gt; println(&quot;Nothing Matched!!&quot; + fieldType)        }      })      sql = sql.substring(0, sql.length - 1)      //sql = sql + s&quot;)STORED AS PARQUET location &#39;${hive_root_path}${table}&#39;&quot;      sql = sql + s&quot;) partitioned by(year string,month string,day string) STORED AS PARQUET &quot; + s&quot;location &#39;${hive_root_path}${table}&#39;&quot;      hiveTableSqlMap.put(table.toString, sql)    })    hiveTableSqlMap  }  /**    * 使用tableFieldsMap    * 对每种类型数据创建对应的Schema    * @return    */  def createSchema(): util.Map[String, StructType] = {    // schema  表结构    /*   CREATE TABLE `warn_message` (         //arrayStructType         `id` int(11) NOT NULL AUTO_INCREMENT,         `alarmRuleid` varchar(255) DEFAULT NULL,         `alarmType` varchar(255) DEFAULT NULL,         `sendType` varchar(255) DEFAULT NULL,         `sendMobile` varchar(255) DEFAULT NULL,         `sendEmail` varchar(255) DEFAULT NULL,         `sendStatus` varchar(255) DEFAULT NULL,         `senfInfo` varchar(255) CHARACTER SET utf8 DEFAULT NULL,         `hitTime` datetime DEFAULT NULL,         `checkinTime` datetime DEFAULT NULL,         `isRead` varchar(255) DEFAULT NULL,         `readAccounts` varchar(255) DEFAULT NULL,         `alarmaccounts` varchar(255) DEFAULT NULL,         `accountid` varchar(11) DEFAULT NULL,         PRIMARY KEY (`id`)       ) ENGINE=MyISAM AUTO_INCREMENT=528 DEFAULT CHARSET=latin1;*/    val mapStructType: util.Map[String, StructType] = new util.HashMap[String, StructType]()    for (table &lt;- tables) {      //通过tableFieldsMap 拿到这个表的所有字段      val tableFields = tableFieldsMap.get(table)      //对这个字段进行遍历      val keyIterator = tableFields.keySet().iterator()      //创建ArrayBuffer      var arrayStructType = ArrayBuffer[StructField]()      while (keyIterator.hasNext) {        val key = keyIterator.next()        val value = tableFields.get(key)        //将key拆分 获取 &quot;.&quot;后面的部分作为数据字段        val field = key.split(&quot;\\.&quot;)(1)        value match {          /* case &quot;string&quot; =&gt; arrayStructType += StructField(field, StringType, true)           case &quot;long&quot;   =&gt; arrayStructType += StructField(field, LongType, true)           case &quot;double&quot;   =&gt; arrayStructType += StructField(field, DoubleType, true)*/          case &quot;string&quot; =&gt; arrayStructType += StructField(field, StringType, true)          case &quot;long&quot; =&gt; arrayStructType += StructField(field, StringType, true)          case &quot;double&quot; =&gt; arrayStructType += StructField(field, StringType, true)          case _ =&gt; println(&quot;Nothing Matched!!&quot; + value)        }      }      val schema = StructType(arrayStructType)      mapStructType.put(table.toString, schema)    }    mapStructType  }}</code></pre><h4 id="3、kafka写hdfs和创建hive表"><a href="#3、kafka写hdfs和创建hive表" class="headerlink" title="3、kafka写hdfs和创建hive表"></a>3、kafka写hdfs和创建hive表</h4><p><strong>Kafka2HiveTest.scala</strong> </p><pre><code>package com.hsiehchou.spark.streaming.kafka.kafka2hdfsimport java.utilimport com.hsiehchou.hdfs.HdfsAdminimport com.hsiehchou.hive.HiveConfimport com.hsiehchou.spark.common.{SparkContextFactory}import com.hsiehchou.spark.streaming.kafka.Spark_Kafka_ConfigUtilimport com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming.kafkaConfigimport org.apache.hadoop.fs.Pathimport org.apache.spark.{Logging}import org.apache.spark.rdd.RDDimport org.apache.spark.sql.hive.HiveContextimport org.apache.spark.sql.{DataFrame, Row, SaveMode}import org.apache.spark.sql.types.StructTypeimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.kafka.KafkaManagerimport scala.collection.JavaConversions._object Kafka2HiveTest extends Serializable with Logging{  val topics = &quot;chl_test7&quot;.split(&quot;,&quot;)  //获取所有数据类型  //获取所有数据的Schema  def main(args: Array[String]): Unit = {    //val ssc = SparkContextFactory.newSparkLocalStreamingContext(&quot;XZ_kafka2es&quot;, java.lang.Long.valueOf(10),1)    val ssc = SparkContextFactory.newSparkStreamingContext(&quot;Kafka2HiveTest&quot;, java.lang.Long.valueOf(10))    //1.创建HIVE表  hiveSQL已經創建好了    val sc = ssc.sparkContext    val hiveContext: HiveContext = HiveConf.getHiveContext(sc)    hiveContext.setConf(&quot;spark.sql.parquet.mergeSchema&quot;, &quot;true&quot;)    createHiveTable(hiveContext)    //kafka拿到流数据    val kafkaDS = new KafkaManager(Spark_Kafka_ConfigUtil                                    .getKafkaParam(kafkaConfig.getProperty(&quot;metadata.broker.list&quot;),                                      &quot;Kafka2HiveTest&quot;))                                    .createJsonToJMapStringDirectStreamWithOffset(ssc, topics.toSet)                                    .persist(StorageLevel.MEMORY_AND_DISK)    HiveConfig.tables.foreach(table=&gt;{      //过滤出单一数据类型(获取和table相同类型的所有数据)       val tableDS = kafkaDS.filter(x =&gt; {table.equals(x.get(&quot;table&quot;))})      //获取数据类型的schema 表结构      val schema = HiveConfig.mapSchema.get(table)      //获取这个表的所有字段      val schemaFields: Array[String] = schema.fieldNames      tableDS.foreachRDD(rdd=&gt;{        //TODO 数据写入HDFS        /* val sc = rdd.sparkContext        val hiveContext = HiveConf.getHiveContext(sc)        hiveContext.sql(s&quot;USE DEFAULT&quot;)*/        //将RDD转为DF   原因：要加字段描述，写比较方便        val tableDF = rdd2DF(rdd,schemaFields,hiveContext,schema)        //多种数据一起处理        val path_all = s&quot;hdfs://hadoop1:8020${HiveConfig.hive_root_path}${table}&quot;        val exists = HdfsAdmin.get().getFs.exists(new Path(path_all))        //2.写到HDFS   不管存不存在我们都要把数据写入进去 通过追加的方式        //每10秒写一次，写一次会生成一个文件        tableDF.write.mode(SaveMode.Append).parquet(path_all)        //3.加载数据到HIVE        if (!exists) {          //如果不存在 进行首次加载          System.out.println(&quot;===================开始加载数据到分区=============&quot;)          hiveContext.sql(s&quot;ALTER TABLE ${table} LOCATION &#39;${path_all}&#39;&quot;)        }      })    })    ssc.start()    ssc.awaitTermination()  }  /**    * 创建HIVE表    * @param hiveContext    */  def createHiveTable(hiveContext: HiveContext): Unit ={    val keys = HiveConfig.hiveTableSQL.keySet()    keys.foreach(key=&gt;{      val sql = HiveConfig.hiveTableSQL.get(key)      //通过hiveContext 和已经创建好的SQL语句去创建HIVE表      hiveContext.sql(sql)      println(s&quot;创建表${key}成功&quot;)    })  }  /**    * 将RDD转为DF    * @param rdd    * @param schemaFields    * @param hiveContext    * @param schema    * @return    */  def rdd2DF(rdd:RDD[util.Map[String,String]],             schemaFields: Array[String],             hiveContext:HiveContext,             schema:StructType): DataFrame ={      //将RDD[Map[String,String]]转为RDD[ROW]      val rddRow = rdd.map(recourd =&gt; {        val listRow: util.ArrayList[Object] = new util.ArrayList[Object]()          for (schemaField &lt;- schemaFields) {            listRow.add(recourd.get(schemaField))          }          Row.fromSeq(listRow)          //所有分区合并成一个      }).repartition(1)    //构建DF    //def createDataFrame(rowRDD: RDD[Row], schema: StructType)    val typeDF = hiveContext.createDataFrame(rddRow, schema)    typeDF  }}</code></pre><h4 id="4、Kafka2HiveTest-执行"><a href="#4、Kafka2HiveTest-执行" class="headerlink" title="4、Kafka2HiveTest 执行"></a>4、Kafka2HiveTest 执行</h4><p>spark-submit <code>--</code>master local[1] <code>--</code>num-executors 1 <code>--</code>driver-memory 300m <code>--</code>executor-memory 500m <code>--</code>executor-cores 1 <code>--</code>jars $(echo /usr/chl/spark7/jars/*.jar | tr ‘ ‘ ‘,’) <code>--</code>class com.hsiehchou.spark.streaming.kafka.kafka2hdfs.Kafka2HiveTest /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</p><p><img src="/medias/%E5%AD%98%E5%88%B0hdfs%E4%B8%AD.PNG" alt="存到hdfs中"></p><p><img src="/medias/hive%E6%9F%A5%E8%AF%A21.PNG" alt="hive查询1"></p><h4 id="5、xz-bigdata-spark-src-java"><a href="#5、xz-bigdata-spark-src-java" class="headerlink" title="5、xz_bigdata_spark/src/java/"></a>5、xz_bigdata_spark/src/java/</h4><p><strong>com/hsiehchou/hdfs</strong><br><strong>HdfsAdmin.java—HDFS 文件操作类</strong></p><pre><code>package com.hsiehchou.hdfs;import com.hsiehchou.common.adjuster.StringAdjuster;import com.hsiehchou.common.file.FileCommon;import com.google.common.base.Preconditions;import com.google.common.collect.Lists;import org.apache.commons.io.IOUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.*;import org.apache.log4j.Logger;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStream;import java.io.InputStreamReader;import java.lang.reflect.Array;import java.util.Collection;import java.util.List;/** * HDFS 文件操作类 */public class HdfsAdmin {    private static Logger LOG;    private static final String HDFS_SITE = &quot;/hadoop/hdfs-site.xml&quot;;    private static final String CORE_SITE = &quot;/hadoop/core-site.xml&quot;;    private volatile static HdfsAdmin hdfsAdmin;    private  FileSystem fs;    private HdfsAdmin(Configuration conf, Logger logger){        try {            if(conf == null) conf = newConf();            conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://hadoop1:8020&quot;);            fs = FileSystem.get(conf);        } catch (IOException e) {            LOG.error(&quot;获取 hdfs的FileSystem出现异常。&quot;, e);        }        Preconditions.checkNotNull(fs, &quot;没有获取到可用的Hdfs的FileSystem&quot;);        this.LOG = logger;        if(this.LOG == null)            this.LOG = Logger.getLogger(HdfsAdmin.class);    }    private Configuration newConf(){        Configuration conf = new Configuration();        if(FileCommon.exist(HDFS_SITE)) conf.addResource(HDFS_SITE);        if(FileCommon.exist(CORE_SITE)) conf.addResource(CORE_SITE);        return conf;    }    public static HdfsAdmin get(){        return get(null);    }    /**     * 获取hdfsAdmin     * @param logger     * @return     */    public static HdfsAdmin get(Logger logger){        if(hdfsAdmin == null){            synchronized (HdfsAdmin.class){                if(hdfsAdmin == null) hdfsAdmin = new HdfsAdmin(null, logger);            }        }        return hdfsAdmin;    }    public static HdfsAdmin get(Configuration conf, Logger logger){        if(hdfsAdmin == null){            synchronized (HdfsAdmin.class){                if(hdfsAdmin == null) hdfsAdmin = new HdfsAdmin(conf, logger);            }        }        return hdfsAdmin;    }    public FileStatus getFileStatus(String dir) {        FileStatus fileStatus = null;        try {            fileStatus = fs.getFileStatus(new Path(dir));        } catch (IOException e) {            LOG.error(String.format(&quot;获取文件 %s信息失败。&quot;, dir), e);        }        return fileStatus;    }    public void createFile(String dst , byte[] contents){        //目标路径        Path dstPath = new Path(dst);        //打开一个输出流        FSDataOutputStream outputStream;        try {            outputStream = fs.create(dstPath);            outputStream.write(contents);            outputStream.flush();            outputStream.close();        } catch (IOException e) {            LOG.error(String.format(&quot;创建文件 %s 失败。&quot;, dst), e);        }        LOG.info(String.format(&quot;文件: %s 创建成功！&quot;, dst));    }    //上传本地文件    public void uploadFile(String src,String dst){        //原路径        Path srcPath = new Path(src);        //目标路径        Path dstPath = new Path(dst);        //调用文件系统的文件复制函数,前面参数是指是否删除原文件，true为删除，默认为false        try {            fs.copyFromLocalFile(false,srcPath, dstPath);        } catch (IOException e) {            LOG.error(String.format(&quot;上传文件 %s 到 %s 失败。&quot;, src, dst), e);        }        //打印文件路径        LOG.info(String.format(&quot;上传文件 %s 到 %s 完成。&quot;, src, dst));    }    public void downloadFile(String src , String dst){        Path dstPath = new Path(dst) ;        try {            fs.copyToLocalFile(false, new Path(src), dstPath);        } catch (IOException e) {            LOG.error(String.format(&quot;下载文件 %s 到 %s 失败。&quot;, src, dst), e);        }        LOG.info(String.format(&quot;下载文件 %s 到 %s 完成&quot;, src, dst));    }    //文件重命名    public void rename(String oldName,String newName){        Path oldPath = new Path(oldName);        Path newPath = new Path(newName);        boolean isok = false;        try {            isok = fs.rename(oldPath, newPath);        } catch (IOException e) {            LOG.error(String.format(&quot;重命名文件 %s 为 %s 失败。&quot;, oldName, newName), e);        }        if(isok){            LOG.info(String.format(&quot;重命名文件 %s 为 %s 完成。&quot;, oldName, newName));        }else{            LOG.error(String.format(&quot;重命名文件 %s 为 %s 失败。&quot;, oldName, newName));        }    }    public void delete(String path){        delete(path, true);    }    //删除文件    public void delete(String path, boolean recursive){        Path deletePath = new Path(path);        boolean isok = false;        try {            isok = fs.delete(deletePath, recursive);        } catch (IOException e) {            LOG.error(String.format(&quot;删除文件 %s 失败。&quot;, path), e);        }        if(isok){            LOG.info(String.format(&quot;删除文件 %s 完成。&quot;, path));        }else{            LOG.error(String.format(&quot;删除文件 %s 失败。&quot;, path));        }    }    //创建目录    public void mkdir(String path){        Path srcPath = new Path(path);        boolean isok = false;        try {            isok = fs.mkdirs(srcPath);        } catch (IOException e) {            LOG.error(String.format(&quot;创建目录 %s 失败。&quot;, path), e);        }        if(isok){            LOG.info(String.format(&quot;创建目录 %s 完成。&quot;, path));        }else{            LOG.error(String.format(&quot;创建目录 %s 失败。&quot;, path));        }    }    //读取文件的内容    public InputStream readFile(String filePath){        Path srcPath = new Path(filePath);        InputStream in = null;        try {           in = fs.open(srcPath);        } catch (IOException e) {            LOG.error(String.format(&quot;读取文件  %s 失败。&quot;, filePath), e);        }        return in;    }    public &lt;T&gt; void readFile(String filePath, StringAdjuster&lt;T&gt; adjuster, Collection&lt;T&gt; result){        InputStream inputStream = readFile(filePath);        if(inputStream != null){            InputStreamReader reader = new InputStreamReader(inputStream);            BufferedReader bufferedReader = new BufferedReader(reader);            String line;            try {                T t;                while((line = bufferedReader.readLine()) != null){                    t = adjuster.doAdjust(line);                    if(t != null)result.add(t);                }            } catch (IOException e) {                LOG.error(String.format(&quot;利用缓冲流读取文件  %s 失败。&quot;, filePath), e);            }finally {                IOUtils.closeQuietly(bufferedReader);                IOUtils.closeQuietly(reader);                IOUtils.closeQuietly(inputStream);            }        }    }    public List&lt;String&gt; readLines(String filePath){        return readLines(filePath, &quot;UTF-8&quot;);    }    public  List&lt;String&gt; readLines(String filePath, String encoding){        InputStream inputStream = readFile(filePath);        List&lt;String&gt; lines = null;        if(inputStream != null) {            try {                lines = IOUtils.readLines(inputStream, encoding);            } catch (IOException e) {                LOG.error(String.format(&quot;按行读取文件 %s 失败。&quot;, filePath), e);            }finally {                IOUtils.closeQuietly(inputStream);            }        }        return lines;    }    public List&lt;FileStatus&gt; findNewFileOrDirInDir(String dir, HdfsFileFilter filter,                                                final boolean onlyFile, final boolean onlyDir){       return findNewFileOrDirInDir(dir, filter, onlyFile, onlyDir, false);    }    public List&lt;FileStatus&gt; findNewFileOrDirInDir(String dir, HdfsFileFilter filter,                          final boolean onlyFile, final boolean onlyDir, boolean recursive){        if(onlyFile &amp;&amp; onlyDir){            FileStatus fileStatus = getFileStatus(dir);            if(fileStatus == null)return Lists.newArrayList();            if(isAccepted(fileStatus,filter)){                return Lists.newArrayList(fileStatus);            }            return Lists.newArrayList();        }       if(onlyFile){           return findNewFileInDir(dir, filter, recursive);       }       if(onlyDir){           return findNewDirInDir(dir, filter, recursive);       }       return Lists.newArrayList();    }    /**     * 查找一个文件夹中 新建的目录     * @param dir     * @param filter     * @return     */    public List&lt;FileStatus&gt; findNewDirInDir(String dir, HdfsFileFilter filter){        return findNewDirInDir(new Path(dir), filter, false);    }    public List&lt;FileStatus&gt; findNewDirInDir(Path path, HdfsFileFilter filter){        return findNewDirInDir(path, filter, false);    }    public List&lt;FileStatus&gt; findNewDirInDir(String dir, HdfsFileFilter filter, boolean recursive){        return findNewDirInDir(new Path(dir), filter, recursive);    }    public List&lt;FileStatus&gt; findNewDirInDir(Path path, HdfsFileFilter filter, boolean recursive){        FileStatus[] files = null;        try {            files = fs.listStatus(path);        } catch (IOException e) {            LOG.error(String.format(&quot;获取目录 %s下的文件列表失败。&quot;, path), e);        }        if(files == null)return Lists.newArrayList();        List&lt;FileStatus&gt; paths = Lists.newArrayList();        List&lt;String&gt; res = Lists.newArrayList();        for(FileStatus fileStatus : files){            if (fileStatus.isDirectory()) {                if (isAccepted(fileStatus, filter)) {                    paths.add(fileStatus);                    res.add(fileStatus.getPath().toString());                }else if(recursive){                    paths.addAll(findNewDirInDir(fileStatus.getPath(), filter, recursive));                }            }        }        LOG.info(String.format(&quot;从目录%s 找到满足条件%s 有如下 %s 个文件： %s&quot;,                path, filter,res.size(), res));        return paths;    }    /**     * 查找一个文件夹中 新建的文件     * @param dir     * @param filter     * @return     */    public List&lt;FileStatus&gt; findNewFileInDir(String dir, HdfsFileFilter filter){        return  findNewFileInDir(new Path(dir), filter, false);    }    public List&lt;FileStatus&gt; findNewFileInDir(String dir, HdfsFileFilter filter, boolean recursive){        return  findNewFileInDir(new Path(dir), filter, recursive);    }    public List&lt;FileStatus&gt; findNewFileInDir(Path path, HdfsFileFilter filter){        return  findNewFileInDir(path, filter, false);    }    public List&lt;FileStatus&gt; findNewFileInDir(Path path, HdfsFileFilter filter, boolean recursive){        FileStatus[] files = null;        try {            files = fs.listStatus(path);        } catch (IOException e) {            LOG.error(String.format(&quot;获取目录 %s下的文件列表失败。&quot;, path), e);        }        if(files == null)return Lists.newArrayList();        List&lt;FileStatus&gt; paths = Lists.newArrayList();        List&lt;String&gt; res = Lists.newArrayList();        for(FileStatus fileStatus : files){            if (fileStatus.isFile()) {                if (isAccepted(fileStatus, filter)) {                    paths.add(fileStatus);                    res.add(fileStatus.getPath().toString());                }            }else if(recursive){                paths.addAll(findNewFileInDir(fileStatus.getPath(), filter, recursive));            }        }        LOG.info(String.format(&quot;从目录%s 找到满足条件%s 有如下 %s 个文件： %s&quot;, path, filter,res.size(), res));        return paths;    }    private boolean isAccepted(String file, HdfsFileFilter filter) {        if(filter == null) return true;        FileStatus fileStatus = getFileStatus(file);        if(fileStatus == null)return false;        return isAccepted(fileStatus, filter);    }    private boolean isAccepted(FileStatus fileStatus, HdfsFileFilter filter) {        return  filter == null ? true : filter.filter(fileStatus);    }    public long getModificationTime(Path path){        try {            FileStatus status = fs.getFileStatus(path);            return status.getModificationTime();        } catch (IOException e) {            LOG.error(String.format(&quot;获取路径 %s信息失败。&quot;, path), e);        }        return -1L;    }    public FileSystem getFs() {        return fs;    }    public static void main(String[] args) throws Exception {        // HdfsAdmin hdfsAdmin = HdfsAdmin.get();       // hdfsAdmin.mkdir(&quot;hdfs://hdp04.ultiwill.com:8020/test1111&quot;);        //System.out.println(hdfsAdmin.getFs().exists(new Path(&quot;hdfs://hdp04.ultiwill.com:8020/test&quot;)));        //hdfsAdmin.delete(&quot;hdfs://hdp04.ultiwill.com:8020/test1111&quot;);        //System.out.println(&quot;hdfsAdmin = &quot; + );       // List&lt;FileStatus&gt; status = hdfsAdmin.findNewDirInDir(&quot;hdfs://hdp04.ultiwill.com:50070/hdp&quot;, null);        //System.out.println(&quot;status = &quot; + status.size());    }}</code></pre><p><strong>HdfsFileFilter.java</strong></p><pre><code>package com.hsiehchou.hdfs;import com.hsiehchou.common.filter.Filter;import org.apache.hadoop.fs.FileStatus;public abstract class HdfsFileFilter implements Filter&lt;FileStatus&gt; {}</code></pre><p><strong>com/hsiehchou/hive</strong><br><strong>HiveConf.java</strong></p><pre><code>package com.hsiehchou.hive;import org.apache.hadoop.conf.Configuration;import org.apache.spark.SparkContext;import org.apache.spark.sql.hive.HiveContext;import java.util.Iterator;import java.util.Map;public class HiveConf {    //private static String DEFUALT_CONFIG = &quot;spark/hive/hive-server-config&quot;;    private static HiveConf hiveConf;    private static HiveContext hiveContext;    private HiveConf(){    }    public static HiveConf getHiveConf(){        if(hiveConf==null){            synchronized (HiveConf.class){                if(hiveConf==null){                    hiveConf=new  HiveConf();                }            }        }        return hiveConf;    }    public static HiveContext getHiveContext(SparkContext sparkContext){        if(hiveContext==null){            synchronized (HiveConf.class){                if(hiveContext==null){                    hiveContext = new  HiveContext(sparkContext);                    Configuration conf = new Configuration();                    conf.addResource(&quot;spark/hive/hive-site.xml&quot;);                    Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator = conf.iterator();                    while (iterator.hasNext()) {                        Map.Entry&lt;String, String&gt; next = iterator.next();                        hiveContext.setConf(next.getKey(), next.getValue());                    }                    hiveContext.setConf(&quot;spark.sql.parquet.mergeSchema&quot;, &quot;true&quot;);                }            }        }        return hiveContext;    }}</code></pre><h4 id="6、小文件合并"><a href="#6、小文件合并" class="headerlink" title="6、小文件合并"></a>6、小文件合并</h4><p><strong>scala/com/hsiehchou/spark/streaming/kafka/kafka2hdfs</strong></p><p><strong>CombineHdfs.scala—合并HDFS小文件任务</strong></p><pre><code>package com.hsiehchou.spark.streaming.kafka.kafka2hdfsimport com.hsiehchou.hdfs.HdfsAdminimport com.hsiehchou.spark.common.SparkContextFactoryimport org.apache.hadoop.fs.{FileSystem, FileUtil, Path}import org.apache.spark.Loggingimport org.apache.spark.sql.{SQLContext, SaveMode}import scala.collection.JavaConversions._/**  * 合并HDFS小文件任务  */object CombineHdfs extends Serializable with Logging{  def main(args: Array[String]): Unit = {    //  val sparkContext = SparkContextFactory.newSparkBatchContext(&quot;CombineHdfs&quot;)    val sparkContext = SparkContextFactory.newSparkLocalBatchContext(&quot;CombineHdfs&quot;)    //创建一个 sparkSQL    val sqlContext: SQLContext = new SQLContext(sparkContext)    //遍历表 就是遍历HIVE表    HiveConfig.tables.foreach(table=&gt;{      //获取HDFS文件目录      //apps/hive/warehouse/external/mail类似      //apps/hive/warehouse/external/mail      val table_path =s&quot;${HiveConfig.hive_root_path}$table&quot;       //通过sparkSQL 加载 这些目录的文件      val tableDF = sqlContext.read.load(table_path)      //先获取原来数据种的所有文件  HDFS文件 API      val fileSystem:FileSystem = HdfsAdmin.get().getFs      //通过globStatus 获取目录下的正则匹配文件      //fileSystem.listFiles()      val arrayFileStatus = fileSystem.globStatus(new Path(table_path+&quot;/part*&quot;))      //stat2Paths将文件状态转为文件路径   这个文件路径是用来删除的      val paths = FileUtil.stat2Paths(arrayFileStatus)      //写入合并文件   //repartition 需要根据生产中实际情况去定义      tableDF.repartition(1).write.mode(SaveMode.Append).parquet(table_path)      println(&quot;写入&quot; + table_path +&quot;成功&quot;)      //删除小文件      paths.foreach(path =&gt;{        HdfsAdmin.get().getFs.delete(path)        println(&quot;删除文件&quot; + path + &quot;成功&quot;)      })    })  }}</code></pre><h4 id="7、定时任务"><a href="#7、定时任务" class="headerlink" title="7、定时任务"></a>7、定时任务</h4><p><strong>命令行输入：crontab -e</strong></p><p><strong>内容：</strong><br><code>0 1 * * *</code> spark-submit <code>--</code>master local[1] <code>--</code>num-executors 1 <code>--</code>driver-memory 300m <code>--</code>executor-memory 500m <code>--</code>executor-cores 1 <code>--</code>jars $(echo /usr/chl/spark7/jars/*.jar | tr ‘ ‘ ‘,’) <code>--</code>class com.hsiehchou.spark.streaming.kafka.kafka2hdfs.CombineHdfs /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</p><p><strong>说明：</strong><br><code>* * * * *</code> 执行的任务</p><table><thead><tr><th align="center">项目</th><th align="center">含义</th><th align="center">范围</th></tr></thead><tbody><tr><td align="center">第一个“*”</td><td align="center">一小时当中的第几分钟（分）</td><td align="center">0-59</td></tr><tr><td align="center">第二个“*”</td><td align="center">一天当中的第几小时（时）</td><td align="center">0-23</td></tr><tr><td align="center">第三个“*”</td><td align="center">一个月当中的第几天（天）</td><td align="center">1-31</td></tr><tr><td align="center">第四个“*”</td><td align="center">一年当中的第几月（月）</td><td align="center">1-12</td></tr><tr><td align="center">第五个“*”</td><td align="center">一周当中的星期几（周）</td><td align="center">0-7（0和7都代表星期日）</td></tr></tbody></table><h4 id="8、合并小文件截图"><a href="#8、合并小文件截图" class="headerlink" title="8、合并小文件截图"></a>8、合并小文件截图</h4><p><img src="/medias/%E5%90%88%E5%B9%B6%E5%B0%8F%E6%96%87%E4%BB%B6.PNG" alt="合并小文件">        </p><h4 id="9、hive命令"><a href="#9、hive命令" class="headerlink" title="9、hive命令"></a>9、hive命令</h4><p>show tales;</p><p>hdfs dfs -ls /apps/hive/warehouse/external</p><p>hdfs dfs -rm -r /apps/hive/warehouse/external/mail</p><p>drop table mail;</p><p>desc qq;</p><p>select * from qq limit 1;<br>select count(*) from qq;</p><p>/usr/bin下面的启动zookeeper客户端<br>zookeeper-client</p><p>删除zookeeper里面的消费者数据<br>rmr /consumers/WarningStreamingTask2/offsets</p><p>rmr /consumers/Kafka2HiveTest/offsets</p><p>rmr /consumers/DataRelationStreaming1/offsets</p><h3 id="十一、Spark—Kafka2Hbase"><a href="#十一、Spark—Kafka2Hbase" class="headerlink" title="十一、Spark—Kafka2Hbase"></a>十一、Spark—Kafka2Hbase</h3><h4 id="1、数据关联"><a href="#1、数据关联" class="headerlink" title="1、数据关联"></a>1、数据关联</h4><p><strong>（1）为什么需要关联</strong><br><strong>问题</strong>：我们不能充分了解数据之间的关联关系。</p><p><strong>公司中应用的非常多</strong><br><strong>离线关联</strong>，传通数据 mysql 通过关联字段去关联。<br>但是，如果数据量非常大，关联表非常多。处理不了。</p><p>数据零散，只能从单一维度去看数据，看的面比较窄。<br>如果需要从多个维度分析，关联成本比较大。</p><p>建立数据之间的关联关系，实现<strong>关联查询</strong>的<strong>毫秒级响应</strong>；<br>另一个方面，可以为数据挖掘，机器学习<strong>提供训练数据</strong>。</p><p>后面进行机器学习的时候，都需要从<strong>多维度</strong>对数据进行<strong>分析和建模</strong>。</p><p><strong>（2）HBASE 只要rowkey一样，那么他们就是一条数据</strong><br>QQ<br>aa-aa-aa-aa-aa-aa 666666</p><p>微信<br>aa-aa-aa-aa-aa-aa weixin</p><p>邮箱<br>aa-aa-aa-aa-aa-aa <a href="mailto:666666@qq.com">666666@qq.com</a></p><p><strong>（3）如何关联</strong><br>一对一的情况 :<br><a href="https://blog.csdn.net/shujuelin/article/details/83657485" target="_blank" rel="noopener">https://blog.csdn.net/shujuelin/article/details/83657485</a></p><p><strong>使用HBASE写入特性</strong><br>比如 MAC1  1789932321<br> MAC1  <a href="mailto:88888@qq.com">88888@qq.com</a><br> MAC1  88888 </p><p>一对多的情况怎么处理<br> <strong>使用多版本</strong><br>aa-aa-aa-aa-aa-aa 666666<br>aa-aa-aa-aa-aa-aa 777777</p><p><strong>（4）一对多</strong><br>使用多版本存一堆多的关系<br>多版本 插入了一个777777 一个版本<br>再插入一个777777   一个版本</p><p>所以需要自定义版本号 确定版本唯一<br>通过 “888888”.hashCode() &amp; Integer.MAX_VALUE</p><p><strong>（5）如果实现hbase多字段查询</strong><br>往主关联表 test:relation 里面写入数据  rowkey=&gt;aa-aa-aa-aa-aa-aa version=&gt;1637094383 类型phone_mac value=&gt;aa-aa-aa-aa-aa-aa<br>往二级索表 test:phone_mac里面写入数据  rowkey=&gt;aa-aa-aa-aa-aa-aa version=&gt;1736188717 value=&gt;aa-aa-aa-aa-aa-aa</p><p><img src="/medias/Hbase%E5%85%B3%E8%81%94.PNG" alt="Hbase关联"></p><p>查询不直接查主关联表，因为查询字段不在主键里面，没办法查或者性能非常低下。</p><p>查询是分2步rowkey查询<br>第一步， 通过查询字段取对应的二级索引表里面去找主关联表的ROWKEY<br>第二步， 通过主关联表的ROWKEY 获取HBASE中的全量数据</p><p> WIFI 已经入库的情况下，手机号也必须已经入库了，才能找到<br> 加入WIFI的手机号还没有入库</p><p>如果是基础数据先过来   没有mac 没有主键</p><table><thead><tr><th align="center">Card</th><th align="center">phone</th></tr></thead><tbody><tr><td align="center">400000000000000</td><td align="center">18612345678</td></tr></tbody></table><p>关联</p><table><thead><tr><th align="center">Phone</th><th align="center">value （识别这个字段是身份证才可以）</th></tr></thead><tbody><tr><td align="center">18612345678</td><td align="center">400000000000000</td></tr></tbody></table><p>1）因为检索的时候都是通过索引表直接找MAC，混入了身份证<br>2）要进行一个合并</p><p><strong>（6）关联及二级索引示意</strong></p><p><img src="/medias/%E5%85%B3%E8%81%94%E5%8F%8A%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95%E7%A4%BA%E6%84%8F.PNG" alt="关联及二级索引示意"></p><p><img src="/medias/Hbase%E5%85%B3%E8%81%94%E8%A1%A8%E7%A4%BA%E6%84%8F%E5%9B%BE.PNG" alt="Hbase关联表示意图"></p><p><strong>（7）如果使用ES建立二级索引</strong></p><p><img src="/medias/%E4%BD%BF%E7%94%A8ES%E5%BB%BA%E7%AB%8B%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95.PNG" alt="使用ES建立二级索引"></p><p>如果hbase 里面有100个字段，存放的是全量信息，但是只有20个字段参与查询、检索，那么我们可以把这个20个字段单独提出来存放到es中，因为ES是对对字段，多条件查询非常灵活。所以我们可以先在ES中对条件进行检索，根据检索的结果拿到hbaSe的rowkey，然后再通过rowkey到hbase里面获取全量信息。      </p><p><strong>（8）Hbase 预分区</strong><br>主要是根据rowkey分布来进行预分区</p><p>分区主要是为了防止热点问题</p><p>relation表为例<br>这个表的rowkey 是不是就是 mac</p><p>phone_mac 都是以0-9  a-f开头的<br>device_mac 都是以0-9  a-z开头的<br>Hbase 是按字典序排序</p><p><strong>（9）自定义版本号</strong><br>通过这样的一个转换我们可以精确定位数据的多版本号，，然后可以根据版本号对数据进行多版本删除。<br>156511 aaaaaaaa</p><h4 id="2、DataRelationStreaming—数据关联"><a href="#2、DataRelationStreaming—数据关联" class="headerlink" title="2、DataRelationStreaming—数据关联"></a>2、DataRelationStreaming—数据关联</h4><p><strong>DataRelationStreaming.scala</strong></p><pre><code>package com.hsiehchou.spark.streaming.kafka.kafka2hbaseimport java.util.Propertiesimport com.hsiehchou.common.config.ConfigUtilimport com.hsiehchou.hbase.config.HBaseTableUtilimport com.hsiehchou.hbase.insert.HBaseInsertHelperimport com.hsiehchou.hbase.spilt.SpiltRegionUtilimport com.hsiehchou.spark.common.SparkContextFactoryimport com.hsiehchou.spark.streaming.kafka.Spark_Kafka_ConfigUtilimport org.apache.hadoop.hbase.client.Putimport org.apache.hadoop.hbase.util.Bytesimport org.apache.spark.Loggingimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.kafka.KafkaManagerobject DataRelationStreaming extends Serializable with Logging{  // 读取需要关联的配置文件字段  // phone_mac,phone,username,send_mail,imei,imsi  val relationFields = ConfigUtil.getInstance()    .getProperties(&quot;spark/relation.properties&quot;)    .get(&quot;relationfield&quot;)    .toString    .split(&quot;,&quot;)  def main(args: Array[String]): Unit = {    //初始化hbase表    //initRelationHbaseTable(relationFields)    val ssc = SparkContextFactory.newSparkLocalStreamingContext(&quot;DataRelationStreaming&quot;, java.lang.Long.valueOf(10),1)    //  val ssc = SparkContextFactory.newSparkStreamingContext(&quot;DataRelationStreaming&quot;, java.lang.Long.valueOf(10))    val kafkaConfig: Properties = ConfigUtil.getInstance().getProperties(&quot;kafka/kafka-server-config.properties&quot;)    val topics = &quot;chl_test7&quot;.split(&quot;,&quot;)    val kafkaDS = new KafkaManager(Spark_Kafka_ConfigUtil      .getKafkaParam(kafkaConfig.getProperty(&quot;metadata.broker.list&quot;),        &quot;DataRelationStreaming2&quot;))      .createJsonToJMapStringDirectStreamWithOffset(ssc, topics.toSet)      .persist(StorageLevel.MEMORY_AND_DISK)    kafkaDS.foreachRDD(rdd=&gt;{      rdd.foreachPartition(partion=&gt;{        //对partion进行遍历        while (partion.hasNext){          //获取每一条流数据          val map = partion.next()          //获取mac 主键          var phone_mac:String = map.get(&quot;phone_mac&quot;)          //获取所有关联字段 //phone_mac,phone,username,send_mail,imei,imsi          relationFields.foreach(relationFeild =&gt;{            //relationFields 是关联字段，需要进行关联处理的，所有判断            //map中是不是包含这个字段，如果包含的话，取出来进行处理            if(map.containsKey(relationFeild)){              //创建主关联，并遍历关联字段进行关联              val put = new Put(phone_mac.getBytes())              //取关联字段的值              //TODO  到这里  主关联表的 主键和值都有了  然后封装成PUT写入hbase主关联表就行了              val value = map.get(relationFeild)              //自定义版本号  通过 (表字段名 + 字段值 取hashCOde)              //因为值有可能是字符串，但是版本号必须是long类型，所以这里我们需要              //将字符串影射唯一数字，而且必须是正整数              val versionNum = (relationFeild+value).hashCode() &amp; Integer.MAX_VALUE              put.addColumn(&quot;cf&quot;.getBytes(), Bytes.toBytes(relationFeild),versionNum ,Bytes.toBytes(value.toString))              HBaseInsertHelper.put(&quot;test:relation&quot;,put)              println(s&quot;往主关联表 test:relation 里面写入数据  rowkey=&gt;${phone_mac} version=&gt;${versionNum} 类型${relationFeild} value=&gt;${value}&quot;)              // 建立二级索引              // 使用关联字段的值最为二级索引的rowkey              // 二级索引就是把这个字段的值作为索引表rowkey              // 把这个字段的mac做为索引表的值              val put_2 = new Put(value.getBytes())//把这个字段的值作为索引表rowkey              val table_name = s&quot;test:${relationFeild}&quot;//往索引表里面取写              //使用主表的rowkey  就是 取hash作为二级索引的版本号              val versionNum_2 = phone_mac.hashCode() &amp; Integer.MAX_VALUE              put_2.addColumn(&quot;cf&quot;.getBytes(), Bytes.toBytes(&quot;phone_mac&quot;),versionNum_2 ,Bytes.toBytes(phone_mac.toString))              HBaseInsertHelper.put(table_name,put_2)              println(s&quot;往二级索表 ${table_name}里面写入数据  rowkey=&gt;${value} version=&gt;${versionNum_2} value=&gt;${phone_mac}&quot;)            }          })        }      })    })    ssc.start()    ssc.awaitTermination()  }  def initRelationHbaseTable(relationFields:Array[String]): Unit ={    //初始化总关联表    val relation_table = &quot;test:relation&quot;    HBaseTableUtil.createTable(relation_table,      &quot;cf&quot;,      true,      -1,      100,      SpiltRegionUtil.getSplitKeysBydinct)    //HBaseTableUtil.deleteTable(relation_table)    //遍历所有关联字段，根据字段创建二级索引表    relationFields.foreach(field=&gt;{      val hbase_table = s&quot;test:${field}&quot;      HBaseTableUtil.createTable(hbase_table, &quot;cf&quot;, true, -1, 100, SpiltRegionUtil.getSplitKeysBydinct)      // HBaseTableUtil.deleteTable(hbase_table)    })  }}</code></pre><h4 id="3、com-hsiehchou-spark-streaming"><a href="#3、com-hsiehchou-spark-streaming" class="headerlink" title="3、com.hsiehchou.spark.streaming"></a>3、com.hsiehchou.spark.streaming</h4><p><strong>common/SparkContextFactory.scala</strong></p><pre><code>package com.hsiehchou.spark.commonimport org.apache.spark.streaming.{Seconds, StreamingContext}import org.apache.spark.{Accumulator, SparkContext}object SparkContextFactory {  def newSparkBatchContext(appName:String = &quot;sparkBatch&quot;) : SparkContext = {    val sparkConf = SparkConfFactory.newSparkBatchConf(appName)    new SparkContext(sparkConf)  }  def newSparkLocalBatchContext(appName:String = &quot;sparkLocalBatch&quot; , threads : Int = 2) : SparkContext = {    val sparkConf = SparkConfFactory.newSparkLoalConf(appName, threads)    sparkConf.set(&quot;&quot;,&quot;&quot;)    new SparkContext(sparkConf)  }  def getAccumulator(appName:String = &quot;sparkBatch&quot;) : Accumulator[Int] = {    val sparkConf = SparkConfFactory.newSparkBatchConf(appName)    val accumulator: Accumulator[Int] = new SparkContext(sparkConf).accumulator(0,&quot;&quot;)    accumulator  }  /**    * 创建本地流streamingContext    * @param appName             appName    * @param batchInterval      多少秒读取一次    * @param threads            开启多少个线程    * @return    */  def newSparkLocalStreamingContext(appName:String = &quot;sparkStreaming&quot; ,                                    batchInterval:Long = 30L ,                                    threads : Int = 4) : StreamingContext = {    val sparkConf =  SparkConfFactory.newSparkLocalConf(appName, threads)    // sparkConf.set(&quot;spark.streaming.receiver.maxRate&quot;,&quot;10000&quot;)    sparkConf.set(&quot;spark.streaming.kafka.maxRatePerPartition&quot;,&quot;1&quot;)    new StreamingContext(sparkConf, Seconds(batchInterval))  }  /**    * 创建集群模式streamingContext    * 这里不设置线程数，在submit中指定    * @param appName    * @param batchInterval    * @return    */  def newSparkStreamingContext(appName:String = &quot;sparkStreaming&quot; , batchInterval:Long = 30L) : StreamingContext = {    val sparkConf = SparkConfFactory.newSparkStreamingConf(appName)    new StreamingContext(sparkConf, Seconds(batchInterval))  }  def startSparkStreaming(ssc:StreamingContext){    ssc.start()      ssc.awaitTermination()      ssc.stop()  }}</code></pre><p><strong>streaming/kafka/Spark_Kafka_ConfigUtil.scala</strong></p><pre><code>package com.hsiehchou.spark.streaming.kafkaimport org.apache.spark.Loggingobject Spark_Kafka_ConfigUtil extends Serializable with Logging{  def getKafkaParam(brokerList:String,groupId : String): Map[String,String]={    val kafkaParam=Map[String,String](      &quot;metadata.broker.list&quot; -&gt; brokerList,      &quot;auto.offset.reset&quot; -&gt; &quot;smallest&quot;,      &quot;group.id&quot; -&gt; groupId,      &quot;refresh.leader.backoff.ms&quot; -&gt; &quot;1000&quot;,      &quot;num.consumer.fetchers&quot; -&gt; &quot;8&quot;)    kafkaParam  }}</code></pre><h4 id="4、com-hsiehchou-common-config-ConfigUtil"><a href="#4、com-hsiehchou-common-config-ConfigUtil" class="headerlink" title="4、com/hsiehchou/common/config/ConfigUtil"></a>4、com/hsiehchou/common/config/ConfigUtil</h4><p><strong>ConfigUtil.java</strong></p><pre><code>package com.hsiehchou.common.config;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.io.InputStream;import java.util.Properties;public class ConfigUtil {    private static Logger LOG = LoggerFactory.getLogger(ConfigUtil.class);    private static ConfigUtil configUtil;    public static ConfigUtil getInstance(){        if(configUtil == null){            configUtil = new ConfigUtil();        }        return configUtil;    }    public Properties getProperties(String path){        Properties properties = new Properties();        try {            LOG.info(&quot;开始加载配置文件&quot; + path);            InputStream insss = this.getClass().getClassLoader().getResourceAsStream(path);            properties = new Properties();            properties.load(insss);        } catch (IOException e) {            LOG.info(&quot;加载配置文件&quot; + path + &quot;失败&quot;);            LOG.error(null,e);        }        LOG.info(&quot;加载配置文件&quot; + path + &quot;成功&quot;);        System.out.println(&quot;文件内容：&quot;+properties);        return properties;    }    public static void main(String[] args) {        ConfigUtil instance = ConfigUtil.getInstance();        Properties properties = instance.getProperties(&quot;common/datatype.properties&quot;);        //Properties properties = instance.getProperties(&quot;spark/relation.properties&quot;);       // properties.get(&quot;relationfield&quot;);        System.out.println(properties);    }}</code></pre><h4 id="5、构建模块—xz-bigdata-hbase"><a href="#5、构建模块—xz-bigdata-hbase" class="headerlink" title="5、构建模块—xz_bigdata_hbase"></a>5、构建模块—xz_bigdata_hbase</h4><p><strong>pom.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_hbase&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_hbase&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;        &lt;hbase.version&gt;1.2.0&lt;/hbase.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;            &lt;version&gt;${hbase.version}-${cdh.version}&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;guava&lt;/artifactId&gt;                    &lt;groupId&gt;com.google.guava&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;                    &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;            &lt;artifactId&gt;hbase-server&lt;/artifactId&gt;            &lt;version&gt;${hbase.version}-${cdh.version}&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;servlet-api-2.5&lt;/artifactId&gt;                    &lt;groupId&gt;org.mortbay.jetty&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><p><strong>com/hsiehchou/hbase/config/HBaseConf.java</strong></p><pre><code>package com.hsiehchou.hbase.config;import com.hsiehchou.hbase.spilt.SpiltRegionUtil;import org.apache.commons.configuration.CompositeConfiguration;import org.apache.commons.configuration.ConfigurationException;import org.apache.commons.configuration.PropertiesConfiguration;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.BufferedMutator;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import org.apache.log4j.Logger;import java.io.IOException;import java.io.Serializable;public class HBaseConf implements Serializable {    private static final long serialVersionUID = 1L;    private static final Logger LOG = Logger.getLogger(HBaseConf.class);    private static final String HBASE_SERVER_CONFIG = &quot;hbase/hbase-server-config.properties&quot;;    private static final String HBASE_SITE = &quot;hbase/hbase-site.xml&quot;;    private volatile static HBaseConf hbaseConf;    private CompositeConfiguration hbase_server_config;    public CompositeConfiguration getHbase_server_config() {        return hbase_server_config;    }    public void setHbase_server_config(CompositeConfiguration hbase_server_config) {        this.hbase_server_config = hbase_server_config;    }    //hbase 配置文件    private  Configuration configuration;    //hbase 连接    private volatile transient Connection conn;    /**     * 初始化HBaseConf的时候加载配置文件     */    private HBaseConf() {        hbase_server_config = new CompositeConfiguration();        //加载配置文件        loadConfig(HBASE_SERVER_CONFIG,hbase_server_config);        //初始化连接        getHconnection();    }    //获取连接    public Configuration getConfiguration(){        if(configuration==null){            configuration = HBaseConfiguration.create();            configuration.addResource(HBASE_SITE);            LOG.info(&quot;加载配置文件&quot; + HBASE_SITE + &quot;成功&quot;);        }        return configuration;    }    public BufferedMutator getBufferedMutator(String tableName) throws IOException {        return getHconnection().getBufferedMutator(TableName.valueOf(tableName));    }    public Connection getHconnection(){        if(conn==null){            //获取配置文件            getConfiguration();            synchronized (HBaseConf.class) {                if (conn == null) {                    try {                        conn = ConnectionFactory.createConnection(configuration);                    } catch (IOException e) {                        LOG.error(String.format(&quot;获取hbase的连接失败  参数为： %s&quot;, toString()), e);                    }                }            }        }        return conn;    }    /**     * 加载配置文件     * @param path     * @param configuration     */    private void loadConfig(String path,CompositeConfiguration configuration) {        try {            LOG.info(&quot;加载配置文件 &quot; + path);            configuration.addConfiguration(new PropertiesConfiguration(path));            LOG.info(&quot;加载配置文件&quot; + path +&quot;成功。 &quot;);        } catch (ConfigurationException e) {            LOG.error(&quot;加载配置文件 &quot; + path + &quot;失败&quot;, e);        }    }    /**     * 单例 初始化HBaseConf     * @return     */    public static HBaseConf getInstance() {        if (hbaseConf == null) {            synchronized (HBaseConf.class) {                if (hbaseConf == null) {                    hbaseConf = new HBaseConf();                }            }        }        return hbaseConf;    }    public static void main(String[] args) {        String hbase_table = &quot;test:chl_test2&quot;;        HBaseTableUtil.createTable(hbase_table, &quot;cf&quot;, true, -1, 1, SpiltRegionUtil.getSplitKeysBydinct());      /*  Connection hconnection = HBaseConf.getInstance().getHconnection();        Connection hconnection1 = HBaseConf.getInstance().getHconnection();        System.out.println(hconnection);        System.out.println(hconnection1);*/    }}</code></pre><p><strong>com/hsiehchou/hbase/config/HBaseTableFactory.java</strong></p><pre><code>package com.hsiehchou.hbase.config;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.BufferedMutator;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.HTable;import org.apache.hadoop.hbase.client.Table;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.io.Serializable;public class HBaseTableFactory implements Serializable {    private static final long serialVersionUID = -1071596337076137201L;    private static final Logger LOG = LoggerFactory.getLogger(HBaseTableFactory.class);    private HBaseConf conf;    private transient Connection conn  ;    private boolean isReady = true;    public HBaseTableFactory(){        conf = HBaseConf.getInstance();        if(true){            conn = conf.getHconnection();        }else{            isReady = false;            LOG.warn(&quot;HBase 连接没有启动。&quot;);        }    }    public HBaseTableFactory(Connection conn){        this.conn = conn;    }    /**      * 根据表名创建 表的实例      * @param tableName      * @return      * @throws IOException      * HTableInterface     */    public Table getHBaseTableInstance(String tableName) throws IOException{        if(conn == null){            if(conf == null){                conf = HBaseConf.getInstance();                isReady = true;                LOG.warn(&quot;HBaseConf为空，重新初始化。&quot;);            }            synchronized (HBaseTableFactory.class) {                if(conn == null) {                    conn = conf.getHconnection();                    LOG.warn(&quot;初始 hbase Connection 为空 ， 获取  Connection成功。&quot;);                }            }        }        return  isReady ? conn.getTable(TableName.valueOf(tableName)) : null;    }    public HTable getHTable(String tableName) throws IOException{        return  (HTable) getHBaseTableInstance(tableName);    }    public BufferedMutator getBufferedMutator(String tableName) throws IOException {        return getConf().getBufferedMutator(tableName);    }    public boolean isReady() {        return isReady;    }    private HBaseConf getConf(){        if(conf == null){            conf = HBaseConf.getInstance();        }        return conf;    }    public void close() throws IOException{        conn.close();        conn = null;    }}</code></pre><p><strong>com/hsiehchou/hbase/config/HBaseTableUtil</strong></p><pre><code>package com.hsiehchou.hbase.config;import com.google.common.collect.Sets;import org.apache.hadoop.fs.Path;import org.apache.hadoop.hbase.*;import org.apache.hadoop.hbase.client.Admin;import org.apache.hadoop.hbase.client.Table;import org.apache.hadoop.hbase.io.compress.Compression;import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;import org.apache.hadoop.hbase.regionserver.BloomType;import org.apache.hadoop.hbase.util.Bytes;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.util.*;import static com.google.common.base.Preconditions.checkArgument;public class HBaseTableUtil {    private static final Logger LOG = LoggerFactory.getLogger(HBaseTableUtil.class);    private static final String COPROCESSORCLASSNAME =  &quot;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&quot;;    private static HBaseConf conf = HBaseConf.getInstance() ;    private HBaseTableUtil(){}    /**     * 获取hbase 表连接     * @param tableName     * @return     */    public static Table getTable(String tableName){        Table table =null;        if(tableExists(tableName)){            try {                table = conf.getHconnection().getTable(TableName.valueOf(tableName));            } catch (IOException e) {                LOG.error(null,e);            }        }        return table;    }    public static void close(Table table){        if(table != null) {            try {                table.close();            } catch (IOException e) {                e.printStackTrace();            }        }    }    /**     * 判断   HBase中是否存在  名为  tableName 的表     * @param tableName     * @return  boolean     */    public static boolean tableExists(String tableName){        boolean  isExists = false;        try {            isExists = conf.getHconnection().getAdmin().tableExists(TableName.valueOf(tableName));        } catch (MasterNotRunningException e) {            LOG.error(&quot;HBase  master  未运行 。 &quot;, e);        } catch (ZooKeeperConnectionException e) {            LOG.error(&quot;zooKeeper 连接异常。 &quot;, e);        } catch (IOException e) {            LOG.error(&quot;&quot;, e);        }        return isExists;    }    /**     * 删除表     * @param tableName     * @return     */    public static boolean deleteTable(String tableName){        boolean status = false;        TableName name = TableName.valueOf(tableName);        try {            Admin admin = conf.getHconnection().getAdmin();            if(admin.tableExists(name)){                if(!admin.isTableDisabled(name)){                    admin.disableTable(name);                }                admin.deleteTable(name);            }else{                LOG.warn(&quot; HBase中不存在 表 &quot; + tableName);            }            admin.close();            status = true;        } catch (MasterNotRunningException e) {            LOG.error(&quot;HBase  master  未运行 。 &quot;, e);        } catch (ZooKeeperConnectionException e) {            LOG.error(&quot;zooKeeper 连接异常。 &quot;, e);        } catch (IOException e) {            LOG.error(&quot;&quot;, e);        }        return status;    }    /**     * 清空表     * @param tableName     * @return     */    public static boolean truncateTable(String tableName){        boolean status = false;        TableName name = TableName.valueOf(tableName);        try {            Admin admin = conf.getHconnection().getAdmin();            if(admin.tableExists(name)){                if(admin.isTableAvailable(name)){                    admin.disableTable(name);                }                admin.truncateTable(name, true);            }else{                LOG.warn(&quot; HBase中不存在 表 &quot; + tableName);            }            admin.close();            status = true;        } catch (MasterNotRunningException e) {            LOG.error(&quot;HBase  master  未运行 。 &quot;, e);        } catch (ZooKeeperConnectionException e) {            LOG.error(&quot;zooKeeper 连接异常。 &quot;, e);        } catch (IOException e) {            LOG.error(&quot;&quot;, e);        }        return status;    }    /**     * 创建HBase表     * @param tableName     * @param cf       列族名     * @param inMemory     * @param ttl    ttl &lt; 0     则为永久保存     */    public static boolean createTable(String tableName, String cf, boolean inMemory, int ttl, int maxVersion){        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, COPROCESSORCLASSNAME);        return createTable(htd);    }    public static boolean createTable(String tableName, String cf, boolean inMemory, int ttl, int maxVersion,  boolean useSNAPPY){        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, useSNAPPY , COPROCESSORCLASSNAME);        return createTable(htd);    }    public static boolean createTable(String tableName, String cf, boolean inMemory, int ttl, int maxVersion,  boolean useSNAPPY, byte[][] splits){        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, useSNAPPY, COPROCESSORCLASSNAME);        return createTable(htd , splits);    }    /**     * @param tableName    表名     * @param cf           列簇     * @param inMemory     是否存在内存     * @param ttl          数据过期时间     * @param maxVersion   最大版本     * @param splits       分区     * @return     */    public static boolean createTable(String tableName,                                      String cf,                                      boolean inMemory,                                      int ttl,                                      int maxVersion,                                      byte[][] splits){        //返回表说明        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, COPROCESSORCLASSNAME);        //通过HTableDescriptor 和 splits 分区策略来定义表        return createTable(htd , splits);    }    public static List&lt;String&gt; listTables(){        List&lt;String&gt; list = new ArrayList&lt;String&gt;();        Admin admin = null;        try {            admin = conf.getHconnection().getAdmin();            TableName[] listTableNames = admin.listTableNames();            for( TableName t :  listTableNames ){                list.add( t.getNameAsString() );            }        } catch(IOException e )  {            LOG.error(&quot;创建HBase表失败。&quot;, e);        }finally{            try {                if(admin!=null){                    admin.close();                }            } catch (IOException e) {                LOG.error(&quot;&quot;, e);            }        }        return list;    }    /**     * 列出所有表     * @param reg     * @return     */    public static List&lt;String&gt; listTables(String reg){        List&lt;String&gt; list = new ArrayList&lt;String&gt;();        Admin admin = null;        try {            admin = conf.getHconnection().getAdmin();            TableName[] listTableNames = admin.listTableNames(reg);            for(TableName t :  listTableNames){                list.add(t.getNameAsString());            }        } catch(IOException e)  {            LOG.error(&quot;创建HBase表失败。&quot;, e);        }finally{            try {                if(admin!=null){                    admin.close();                }            } catch (IOException e) {                LOG.error(&quot;&quot;, e);            }        }        return list;    }    /**     * 创建HBase表     * @param tableName     * @param cf       列族名     * @param inMemory     * @param ttl      ttl &lt; 0     则为永久保存     */    public static boolean  createTable(String tableName, String cf, boolean inMemory, int ttl , int maxVersion, String ... coprocessorClassNames){        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, coprocessorClassNames);        return createTable(htd);    }    public static boolean  createTable( String tableName, String cf, boolean inMemory, int ttl, int maxVersion, boolean useSNAPPY, String ... coprocessorClassNames){        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, useSNAPPY, coprocessorClassNames);        return createTable(htd);    }    public static boolean  createTable( String tableName,String cf,boolean inMemory, int ttl ,int maxVersion ,  boolean useSNAPPY ,byte[][] splits, String ... coprocessorClassNames){        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, useSNAPPY ,coprocessorClassNames);        return createTable(htd,splits );    }    public static boolean  createTable(String tableName, String cf, boolean inMemory, int ttl, int maxVersion, byte[][] splits, String ... coprocessorClassNames){        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, coprocessorClassNames);        return createTable(htd,splits );    }    /**     * 通过HTableDescriptor 和 分区 来构建hbase     * @param htd     * @param splits     * @return     */    public static boolean createTable(HTableDescriptor htd, byte[][] splits){        Admin admin = null;        try {            admin = conf.getHconnection().getAdmin();            TableName tableName = htd.getTableName();            boolean exist = admin.tableExists(tableName);            if(exist){                LOG.error(&quot;表&quot;+tableName.getNameAsString() + &quot;已经存在&quot;);            }else{                //使用Admin进行创建表                admin.createTable(htd, splits);            }        } catch(IOException e )  {            LOG.error(&quot;创建HBase表失败。&quot;, e);            return false;        }finally{            try {                if(admin!=null){                    admin.close();                }            } catch (IOException e) {                LOG.error(&quot;&quot;, e);            }        }        return true;    }    public static boolean createTable(HTableDescriptor htd){        Admin admin = null;        try {            admin = conf.getHconnection().getAdmin();            if(admin.tableExists(htd.getTableName())){                LOG.info(&quot;表&quot; + htd.getTableName() + &quot;已经存在&quot;);            }else{                admin.createTable(htd);            }        } catch(IOException e )  {            LOG.error(&quot;创建HBase表失败。&quot;, e);            return false;        }finally{            try {                if(admin!=null){                    admin.close();                }            } catch (IOException e) {                LOG.error(&quot;&quot;, e);            }        }        return true;    }    /**     * 创建命名空间     * @param nameSpace     * @return     */    public static boolean createNameSpace(String nameSpace){        Admin admin = null;        try {            admin = conf.getHconnection().getAdmin();            NamespaceDescriptor[] listNamespaceDescriptors = admin.listNamespaceDescriptors();            boolean exist = false;            for(NamespaceDescriptor namespaceDescriptor : listNamespaceDescriptors){                if(namespaceDescriptor.getName().equals(nameSpace)){                    exist = true;                }            }            if(!exist) admin.createNamespace(NamespaceDescriptor.create(nameSpace).build());        } catch(IOException e )  {            LOG.error(&quot;创建HBase命名空间失败。&quot;, e);            return false;        }finally{            try {                if(admin!=null){                    admin.close();                }            } catch (IOException e) {                LOG.error(&quot;&quot;, e);            }        }        return true;    }    /**     * 为 HBase中的表  tableName添加 协处理器  coprocessorClassName     * @param tableName     * @param coprocessorClassName    必须是已经存在与HBase集群中     * @return  boolean     */    public static boolean addCoprocessorClassForTable(String tableName,String coprocessorClassName){        boolean status = false;        TableName name = TableName.valueOf(tableName);        Admin admin = null;        try {            admin = conf.getHconnection().getAdmin();            HTableDescriptor htd = admin.getTableDescriptor(name);            if(!htd.hasCoprocessor(coprocessorClassName)){                htd.addCoprocessor(coprocessorClassName);                admin.disableTable(name);                admin.modifyTable(name, htd);                admin.enableTable(name);            }else{                LOG.warn(String.format(&quot;表 %s中已经存在协处理器%s&quot;, tableName, coprocessorClassName));            }            status = true;        } catch (MasterNotRunningException e) {            LOG.error(&quot;HBase  master  未运行 。 &quot;, e);        } catch (ZooKeeperConnectionException e) {            LOG.error(&quot;zooKeeper 连接异常。 &quot;, e);        } catch (IOException e) {            LOG.error(&quot;&quot;, e);        }finally{            try {                if(admin!=null){                    admin.close();                }            } catch (IOException e) {                LOG.error(&quot;&quot;, e);            }        }        return status;    }    /**     * 为HBase中的表 tableName添加指定位置的 协处理器 jar     * @param tableName     * @param coprocessorClassName   jar中的具体的协处理器     * @param jarPath     hdfs的路径     * @param level       执行级别     * @param kvs         运行参数    可以为 null     * @return   boolean     */    public static boolean addCoprocessorJarForTable(String  tableName, String coprocessorClassName,String jarPath,int level ,Map&lt;String, String&gt; kvs ){        boolean status = false;        TableName name = TableName.valueOf(tableName);        Admin admin = null;        try {            admin = conf.getHconnection().getAdmin();            HTableDescriptor htd = admin.getTableDescriptor(name);            if(!htd.hasCoprocessor(coprocessorClassName)){                admin.disableTable(name);                htd.addCoprocessor(coprocessorClassName, new Path(jarPath), level, kvs);                admin.modifyTable(name, htd);                admin.enableTable(name);            }else{                LOG.warn(String.format(&quot;表 %s中已经存在协处理器%s&quot;, tableName, coprocessorClassName));            }            status = true;        } catch (MasterNotRunningException e) {            LOG.error(&quot;HBase  master  未运行 。 &quot;, e);        } catch (ZooKeeperConnectionException e) {            LOG.error(&quot;zooKeeper 连接异常。 &quot;, e);        } catch (IOException e) {            LOG.error(&quot;&quot;, e);        }finally{            try {                if(admin!=null){                    admin.close();                }            } catch (IOException e) {                LOG.error(&quot;&quot;, e);            }        }        return status;    }    /**     * @param tableName     * @param cf     * @param inMemory     * @param ttl     * @param maxVersion     * @param coprocessorClassNames     * @return     */    public static HTableDescriptor createHTableDescriptor( String tableName,String cf,boolean inMemory, int ttl ,int maxVersion ,String ... coprocessorClassNames ){        return createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, true , COPROCESSORCLASSNAME);    }    /**     * @param tableName     * @param cf     * @param inMemory     * @param ttl     * @param maxVersion     * @param useSNAPPY     * @param coprocessorClassNames     * @return     */    public static HTableDescriptor createHTableDescriptor( String tableName,String cf,boolean inMemory, int ttl ,int maxVersion , boolean useSNAPPY , String ... coprocessorClassNames ){        // 1.创建命名空间        String[] split = tableName.split(&quot;:&quot;);        if(split.length==2){            createNameSpace(split[0]);        }        // 2.添加协处理器        HTableDescriptor htd = new HTableDescriptor(TableName.valueOf(tableName));        for( String coprocessorClassName : coprocessorClassNames ){            try {                htd.addCoprocessor(coprocessorClassName);            } catch (IOException e1) {                LOG.error(&quot;为表&quot; + tableName + &quot; 添加协处理器失败。 &quot;, e1);            }        }        // 创建HColumnDescriptor        HColumnDescriptor hcd = new HColumnDescriptor(cf);        if( maxVersion &gt; 0 )            //定义最大版本号            hcd.setMaxVersions(maxVersion);        /**         * 设置布隆过滤器         * 默认是NONE 是否使用布隆过虑及使用何种方式         * 布隆过滤可以每列族单独启用         * Default = ROW 对行进行布隆过滤。         * 对 ROW，行键的哈希在每次插入行时将被添加到布隆。         * 对 ROWCOL，行键 + 列族 + 列族修饰的哈希将在每次插入行时添加到布隆         * 使用方法: create ‘table’,{BLOOMFILTER =&gt;’ROW’}         * 启用布隆过滤可以节省读磁盘过程，可以有助于降低读取延迟         * */        hcd.setBloomFilterType(BloomType.ROWCOL);        /**         * hbase在LRU缓存基础之上采用了分层设计，整个blockcache分成了三个部分，分别是single、multi和inMemory。三者区别如下：         * single：如果一个block第一次被访问，放在该优先队列中；         * multi：如果一个block被多次访问，则从single队列转移到multi队列         * inMemory：优先级最高，常驻cache，因此一般只有hbase系统的元数据，如meta表之类的才会放到inMemory队列中。普通的hbase列族也可以指定IN_MEMORY属性，方法如下：         * create &#39;table&#39;, {NAME =&gt; &#39;f&#39;, IN_MEMORY =&gt; true}         * 修改上表的inmemory属性，方法如下：         * alter &#39;table&#39;,{NAME=&gt;&#39;f&#39;,IN_MEMORY=&gt;true}         * */        hcd.setInMemory(inMemory);        hcd.setScope(1);        /**         * 数据量大，边压边写也会提升性能的，毕竟IO是大数据的最严重的瓶颈，         * 哪怕使用了SSD也是一样。众多的压缩方式中，推荐使用SNAPPY。从压缩率和压缩速度来看，         * 性价比最高。         **/        if(useSNAPPY)hcd.setCompressionType(Compression.Algorithm.SNAPPY);        //默认为NONE        //如果数据存储时设置了编码， 在缓存到内存中的时候是不会解码的，这样和不编码的情况相比，相同的数据块，编码后占用的内存更小， 即提高了内存的使用率        //如果设置了编码，用户必须在取数据的时候进行解码， 因此在内存充足的情况下会降低读写性能。        //在任何情况下开启PREFIX_TREE编码都是安全的        //不要同时开启PREFIX_TREE和SNAPPY        //通常情况下 SNAPPY并不能比 PREFIX_TREE取得更好的优化效果        //hcd.setDataBlockEncoding(DataBlockEncoding.PREFIX_TREE);        //默认为64k     65536        //随着blocksize的增大， 系统随机读的吞吐量不断的降低，延迟也不断的增大，        //64k大小比16k大小的吞吐量大约下降13%，延迟增大13%        //128k大小比64k大小的吞吐量大约下降22%，延迟增大27%        //对于随机读取为主的业务，可以考虑调低blocksize的大小        //随着blocksize的增大， scan的吞吐量不断的增大，延迟也不断降低，        //64k大小比16k大小的吞吐量大约增加33%，延迟降低24%        //128k大小比64k大小的吞吐量大约增加7%，延迟降低7%        //对于scan为主的业务，可以考虑调大blocksize的大小        //如果业务请求以Get为主，则可以适当的减小blocksize的大小        //如果业务是以scan请求为主，则可以适当的增大blocksize的大小        //系统默认为64k, 是一个scan和get之间取的平衡值        //hcd.setBlocksize(s)        //设置表中数据的存储生命期，过期数据将自动被删除，        // 例如如果只需要存储最近两天的数据，        // 那么可以设置setTimeToLive(2 * 24 * 60 * 60)        if( ttl &lt; 0 ) ttl = HConstants.FOREVER;        hcd.setTimeToLive(ttl);        htd.addFamily( hcd);        return htd;    }    public static boolean createTable(HBaseTableParam param){        String nameSpace = param.getNameSpace();        if(!&quot;default&quot;.equalsIgnoreCase(nameSpace)){            checkArgument(createNameSpace(nameSpace), String.format(&quot;创建命名空间%s失败。&quot;, nameSpace));        }        HTableDescriptor desc = createHTableDescriptor(param);        byte[][] splits = param.getSplits();        if(splits == null){            return createTable(desc);        }else{            return createTable(desc, splits);        }    }    public static HTableDescriptor createHTableDescriptor(HBaseTableParam param){        String tableName = String.format(&quot;%s:%s&quot;, param.getNameSpace(), param.getTableName());        HTableDescriptor htd = new HTableDescriptor(TableName.valueOf(tableName));        for(String coprocessorClassName : param.getCoprocessorClazz()){            try {                htd.addCoprocessor(coprocessorClassName);            } catch (IOException e) {                LOG.error(String.format(&quot;为表  %s 添加协处理器失败。&quot;, tableName), e);            }        }        HColumnDescriptor hcd = new HColumnDescriptor(param.getCf());        hcd.setBloomFilterType(param.getBloomType());        hcd.setMaxVersions(param.getMaxVersions());        hcd.setScope(param.getReplicationScope());        hcd.setBlocksize(param.getBlocksize());        hcd.setInMemory(param.isInMemory());        hcd.setTimeToLive(param.getTtl());        /* 数据量大，边压边写也会提升性能的，毕竟IO是大数据的最严重的瓶颈，哪怕使用了SSD也是一样。众多的压缩方式中，推荐使用SNAPPY。从压缩率和压缩速度来看，性价比最高。  */        if(param.isUsePrefix_tree())hcd.setDataBlockEncoding(DataBlockEncoding.PREFIX_TREE);        if(param.isUseSnappy())hcd.setCompressionType(Compression.Algorithm.SNAPPY);        htd.addFamily( hcd);        return htd;    }    public static void closeTable( Table table ){        if( table != null ){            try {                table.close();            } catch (IOException e) {                LOG.error(&quot; &quot;, e);            }            table = null;        }    }    public static byte[][] getSplitKeys() {        //String[] keys = new String[]{&quot;50|&quot;};        //String[] keys = new String[]{&quot;25|&quot;,&quot;50|&quot;,&quot;75|&quot;};        //String[] keys = new String[]{&quot;13|&quot;,&quot;26|&quot;,&quot;39|&quot;, &quot;52|&quot;,&quot;65|&quot;,&quot;78|&quot;,&quot;90|&quot;};        String[] keys = new String[]{ &quot;06|&quot;,&quot;13|&quot;,&quot;20|&quot;, &quot;26|&quot;,&quot;33|&quot;, &quot;39|&quot;,&quot;46|&quot;, &quot;52|&quot;,&quot;58|&quot;, &quot;65|&quot;,&quot;72|&quot;,&quot;78|&quot;, &quot;84|&quot;,&quot;90|&quot;,&quot;95|&quot;};        //String[] keys = new String[]{&quot;10|&quot;, &quot;20|&quot;, &quot;30|&quot;, &quot;40|&quot;, &quot;50|&quot;, &quot;60|&quot;, &quot;70|&quot;, &quot;80|&quot;, &quot;90|&quot;};        byte[][] splitKeys = new byte[keys.length][];        TreeSet&lt;byte[]&gt; rows = new TreeSet&lt;byte[]&gt;(Bytes.BYTES_COMPARATOR);//升序排序        for (int i = 0; i &lt; keys.length; i++) {            rows.add(Bytes.toBytes(keys[i]));        }        Iterator&lt;byte[]&gt; rowKeyIter = rows.iterator();        int i = 0;        while (rowKeyIter.hasNext()) {            byte[] tempRow = rowKeyIter.next();            rowKeyIter.remove();            splitKeys[i] = tempRow;            i++;        }        return splitKeys;    }    public static class HBaseTableParam{        private final String nameSpace; //命名空间        private final String tableName; //表名        private final String cf;        //列簇        private Set&lt;String&gt;  coprocessorClazz = Sets.newHashSet(&quot;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&quot;);        private int maxVersions = 1;    //版本号 默认为1        private BloomType bloomType = BloomType.ROWCOL;        private boolean inMemory = false;        private int replicationScope = 1;        private boolean useSnappy = false; //默认不使用压缩        private boolean usePrefix_tree = false;        private int blocksize = 65536;        private int ttl = HConstants.FOREVER;        private byte[][] splits;        public HBaseTableParam(String nameSpace, String tableName, String cf) {            super();            this.nameSpace = nameSpace == null ? &quot;default&quot; : nameSpace;            this.tableName = tableName;            this.cf = cf;        }        public String getNameSpace() {            return nameSpace;        }        public String getTableName() {            return tableName;        }        public String getCf() {            return cf;        }        public Set&lt;String&gt; getCoprocessorClazz() {            return coprocessorClazz;        }        public void clearCoprocessor(){            coprocessorClazz.clear();        }        public void addCoprocessorClazz(String clazz) {            this.coprocessorClazz.add(clazz);        }        public void addCoprocessorClazz(String ... clazz) {            addCoprocessorClazz(Arrays.asList(clazz));        }        public void addCoprocessorClazz(Collection&lt;String&gt;  clazz) {            this.coprocessorClazz.addAll(clazz);        }        public int getMaxVersions() {            return maxVersions;        }        public void setMaxVersions(int maxVersions) {            this.maxVersions = maxVersions &lt;= 0 ? 1 : maxVersions;        }        public BloomType getBloomType() {            return bloomType;        }        public void setBloomType(BloomType bloomType) {            this.bloomType = bloomType == null ? BloomType.ROWCOL : bloomType;        }        public boolean isInMemory() {            return inMemory;        }        public void setInMemory(boolean inMemory) {            this.inMemory = inMemory;        }        public int getReplicationScope() {            return replicationScope;        }        public void setReplicationScope(int replicationScope) {            this.replicationScope = replicationScope &lt; 0 ? 1 : replicationScope;        }        public boolean isUseSnappy() {            return useSnappy;        }        /**         * 控制是否使用 snappy 压缩数据， 默认是不启用         * @param useSnappy         */        public void setUseSnappy(boolean useSnappy) {            this.useSnappy = useSnappy;        }        public boolean isUsePrefix_tree() {            return usePrefix_tree;        }        /**         * 控制是否使用数据编码，默认是不使用         *         * 如果数据存储时设置了编码， 在缓存到内存中的时候是不会解码的，这样和不编码的情况相比，相同的数据块，编码后占用的内存更小， 即提高了内存的使用率         * 如果设置了编码，用户必须在取数据的时候进行解码， 因此在内存充足的情况下会降低读写性能。         * 在任何情况下开启PREFIX_TREE编码都是安全的         * 不要同时开启PREFIX_TREE和SNAPPY         * 通常情况下 SNAPPY并不能比 PREFIX_TREE取得更好的优化效果         */        public void setUsePrefix_tree(boolean usePrefix_tree) {            this.usePrefix_tree = usePrefix_tree;        }        public int getBlocksize() {            return blocksize;        }        /**         *默认为64k     65536         *随着blocksize的增大， 系统随机读的吞吐量不断的降低，延迟也不断的增大，         *64k大小比16k大小的吞吐量大约下降13%，延迟增大13%         *128k大小比64k大小的吞吐量大约下降22%，延迟增大27%         *对于随机读取为主的业务，可以考虑调低blocksize的大小         *         *随着blocksize的增大， scan的吞吐量不断的增大，延迟也不断降低，         *64k大小比16k大小的吞吐量大约增加33%，延迟降低24%         *128k大小比64k大小的吞吐量大约增加7%，延迟降低7%         *对于scan为主的业务，可以考虑调大blocksize的大小         *         *如果业务请求以Get为主，则可以适当的减小blocksize的大小         *如果业务是以scan请求为主，则可以适当的增大blocksize的大小         *系统默认为64k, 是一个scan和get之间取的平衡值         *         */        public void setBlocksize(int blocksize) {            this.blocksize = blocksize &lt;= 0 ? 65536 : blocksize;        }        public int getTtl() {            return ttl;        }        /**         * 默认是永久保存         * @param ttl  大于 零的整数，  &lt;= 0 ? tt 为  永久保存         */        public void setTtl(int ttl) {            this.ttl = ttl &lt;= 0 ? HConstants.FOREVER : ttl;        }        public byte[][] getSplits() {            return splits;        }        /*         * 预分区的rowKey范围配置         * @param splits         */        /*        public void setSplits(byte[][] splits) {            this.splits = splits;        }*/    }    public static void main(String[] args) throws Exception{        Admin admin = conf.getHconnection().getAdmin();        System.out.println(admin);        //deleteTable(&quot;test:user&quot;);        // HBaseTableUtil.createTable(&quot;aaaaa&quot;,&quot;info1&quot;,true,-1,1);        //  HBaseTableUtil.truncateTable(&quot;aaaaa&quot;);     /*   boolean b = tableExists(&quot;test:user2&quot;);        Table table = getTable(&quot;test:user2&quot;);        System.out.println(&quot;==================&quot;+table);        System.out.println(&quot;==================&quot;+table.getName());*/        //HBaseTableUtil.deleteTable(&quot;aaaaa&quot;);       /* Table table = HBaseTableUtil.getTable(&quot;countform:typecount&quot;);        System.out.println(table);*//*        boolean b = HBaseTableUtil.tableExists(&quot;countform:typecount&quot;);        System.out.println(b);*/        HBaseTableUtil.deleteTable(&quot;tanslator&quot;);        HBaseTableUtil.deleteTable(&quot;ability&quot;);        HBaseTableUtil.deleteTable(&quot;task&quot;);        HBaseTableUtil.deleteTable(&quot;paper&quot;);        //  HbaseSearchService hbaseSearchService=new HbaseSearchService();        //  Map&lt;String, String&gt; stringStringMap = hbaseSearchService.get(&quot;countform:bsid&quot;,&quot;&quot;, new BaseMapRowExtrator());        // Map&lt;String, String&gt; aaaaa = hbaseSearchService.get(&quot;countform:bsid&quot;, &quot;aaaaa&quot;, new BaseMapRowExtrator());        // System.out.println(aaaaa);    }}</code></pre><p><strong>com/hsiehchou/hbase/entity/AbstractRow.java</strong></p><pre><code>package com.hsiehchou.hbase.entity;import com.google.common.collect.HashMultimap;import com.google.common.collect.Sets;import java.util.Collection;import java.util.Map;import java.util.Set;public abstract class AbstractRow&lt;T extends HBaseCell&gt; {    protected String rowKey;    protected HashMultimap&lt;String, T&gt; cells;    protected Set&lt;String&gt; fields;    protected long maxCapTime;    public AbstractRow(String rowKey){        this.rowKey = rowKey;        cells = HashMultimap.create();        fields = Sets.newHashSet();    }    public boolean addCell(String field, String value, long capTime){        return addCell(field, createCell(field, value, capTime));    }    public boolean addCell(String field, T cell){        fields.add(cell.getField());        if(cell.getCapTime() &gt; maxCapTime)            maxCapTime = cell.getCapTime();        return cells.put(field, cell);    }    public boolean[] addCell(String field, Collection&lt;T&gt; cells){        boolean[] status = new boolean[cells.size()];        int n = 0;        for(T cell : cells){            status[n] = addCell(field, cell);            n++;        }        return status;    }    public String getRowKey() {        return rowKey;    }    protected abstract T createCell(String field, String value, long capTime);    public Map&lt;String, Collection&lt;T&gt;&gt; getCell() {        return cells.asMap();    }    public Collection&lt;T&gt; getCellByField(String field){        return cells.get(field);    }    public Set&lt;Map.Entry&lt;String, T&gt;&gt; entries(){        return  cells.entries();    }    @Override    public String toString() {        return &quot;AbstractRow [rowKey=&quot; + rowKey + &quot;, cells=&quot; + cells + &quot;]&quot;;    }    public boolean equals(Object obj) {       if(this == obj)return true ;       if(!(obj instanceof AbstractRow))return false ;       @SuppressWarnings(&quot;unchecked&quot;)       AbstractRow&lt;T&gt; row = (AbstractRow&lt;T&gt;) obj;       if(rowKey.equals(row.getRowKey()))return true;       return false;    }    public int hashCode(){        return this.rowKey.hashCode();    }    public long getMaxCapTime() {        return maxCapTime;    }    public Set&lt;String&gt; getFields() {        return Sets.newHashSet(fields);    }}</code></pre><p><strong>com/hsiehchou/hbase/entity/HBaseCell.java</strong></p><pre><code>package com.hsiehchou.hbase.entity;public class HBaseCell implements Comparable&lt;HBaseCell&gt;{    protected String field;               protected String value;    protected Long capTime;    public HBaseCell(String field, String value, long capTime){        this.field = field;        this.capTime = capTime;        this.value = value;    }    public String getField(){        return field;    }    public String getValue(){        return value;    }    public void setCapTime(long capTime) {        this.capTime = capTime;    }    public Long getCapTime() {        return capTime;    }    public String toString(){        return String.format(&quot;%s_[%s]_%s&quot;, field, capTime, value);    }    public int compareTo(HBaseCell o) {        return o.getCapTime().compareTo(this.capTime);    }    public boolean equals(Object obj) {       if(this == obj)return true ;       if(!(obj instanceof HBaseCell))return false ;       HBaseCell cell = (HBaseCell)obj;       if(field.equals(cell.getField()) &amp;&amp; value.equals(cell.getValue())){           if(cell.getCapTime() &lt; capTime){               cell.setCapTime(this.capTime);           }           return true;       }       return false;    }    public int hashCode(){        return this.field.hashCode() +  31*this.value.hashCode();    }}</code></pre><p><strong>com/hsiehchou/hbase/entity/HBaseRow.java</strong></p><pre><code>package com.hsiehchou.hbase.entity;public class HBaseRow extends AbstractRow&lt;HBaseCell&gt; {    public HBaseRow(String rowKey){        super(rowKey);    }    public boolean[] addCell(String field, HBaseCell ... cells){        boolean[] status = new boolean[cells.length];        for(int i = 0; i &lt; cells.length; i++){            status[i] = addCell(field, cells[i]);        }        return status;    }    protected HBaseCell createCell(String field, String value, long capTime) {        return new HBaseCell(field, value, capTime);    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/BaseListRowExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.util.ArrayList;import java.util.List;public class BaseListRowExtrator implements RowExtractor&lt;List&lt;String&gt;&gt;{    private List&lt;String&gt; row;    public Long lastcjtime = 0l;    public Long firstcjtime = 0l;    @Override    public List&lt;String&gt; extractRowData(Result result, int rowNum)            throws IOException {        row = new ArrayList&lt;String&gt;();        for(Cell cell :  result.listCells()) {            String column = Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength());            String value = Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());            if(column.equalsIgnoreCase(&quot;cjtime&quot;)) {                Long v = Long.parseLong(value);                if(lastcjtime &lt; v) {                    lastcjtime = v;                }else if(firstcjtime &gt; v) {                    firstcjtime = v;                }            }            row.add(value);        }        return row;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/BaseMapRowExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.text.SimpleDateFormat;import java.util.Date;import java.util.HashMap;import java.util.List;import java.util.Map;public class BaseMapRowExtrator implements RowExtractor&lt;Map&lt;String,String&gt;&gt; {    private Map&lt;String,String&gt; row;    private List&lt;byte[]&gt; rows;    private String longTimeField;    private SimpleDateFormat format;    private String field;    private String value;    private long time;    public BaseMapRowExtrator(){}    /**     * @param rows   需要提取 所有的 rowKey  , null 则不提取     */    public BaseMapRowExtrator(List&lt;byte[]&gt; rows){        this.rows = rows;    }    /**     * @param rows             需要提取 所有的 rowKey  , null 则不提取     * @param longTimeField    long类型的时间字段   表示需要将其转换称 String 类型     */    public BaseMapRowExtrator(List&lt;byte[]&gt; rows,String longTimeField){        this.rows = rows;        this.longTimeField = longTimeField;    }    /**     * @param rows                  需要提取 所有的 rowKey  , null 则不提取     * @param longTimeField         long类型的时间字段     * @param timePattern           表示需要已该指定的格式  将时间字段的值转换成字符串     */    public BaseMapRowExtrator(List&lt;byte[]&gt; rows,String longTimeField,String timePattern){        this.rows = rows;        this.longTimeField = longTimeField;        if(StringUtils.isNotBlank(timePattern)){            format = new SimpleDateFormat(timePattern);        }    }    public Map&lt;String, String&gt; extractRowData(Result result, int rowNum) throws IOException {            row = new HashMap&lt;String,String&gt;();            List&lt;Cell&gt; cells = result.listCells();            for(Cell cell :  cells) {                field = Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength());                if( field.equals(longTimeField)  ){                    time = Bytes.toLong(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());                    if( format != null ){                        value = format.format(new Date(time));                    }else{                        value = String.valueOf(time);                    }                }else{                    value = Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());                }                row.put(field,value);            }            if( rows != null ){                rows.add(result.getRow());            }        return row;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/BaseMapWithRowKeyExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.util.HashMap;import java.util.Map;public class BaseMapWithRowKeyExtrator implements RowExtractor&lt;Map&lt;String,String&gt;&gt; {    private Map&lt;String,String&gt; row;    /* (non-Javadoc)     * @see com.bh.d406.bigdata.hbase.extractor.RowExtractor#extractRowData(org.apache.hadoop.hbase.client.Result, int)     */    @Override    public Map&lt;String, String&gt; extractRowData(Result result, int rowNum)            throws IOException {        row = new HashMap&lt;String,String&gt;();        row.put(&quot;rowKey&quot;, Bytes.toString( result.getRow() ));        for(Cell cell :  result.listCells()) {            row.put(Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()),Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));        }        return row;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/BeanRowExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import com.google.common.collect.Maps;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.lang.reflect.Field;import java.util.Map;public class BeanRowExtrator&lt;T&gt; implements RowExtractor&lt;T&gt; {    private static final Logger LOG = LoggerFactory.getLogger(BeanRowExtrator.class);    private Class&lt;T&gt; clazz;    private Map&lt;String,Field&gt; fieldMap;    public BeanRowExtrator(Class&lt;T&gt; clazz){        this.clazz = clazz;        this.fieldMap = getDeclaredFields(clazz);    }    public T extractRowData(Result result, int rowNum) throws IOException {        return resultReflectToClass(result, rowNum);    }    private T resultReflectToClass(Result result, int rowNum){        String column = null;        Field field = null;        T obj = null;        try {            obj = clazz.newInstance();            for(Cell cell : result.listCells()){                column = Bytes.toString(cell.getQualifierArray(),                        cell.getQualifierOffset(), cell.getQualifierLength());                /*检查该列是否在实体类中存在对应的属性,若存在则 为其赋值*/                if((field = fieldMap.get(column.toLowerCase())) != null){                    field.set(obj, Bytes.toString(cell.getValueArray(),                            cell.getValueOffset(), cell.getValueLength()));                }            }        } catch (InstantiationException e) {            LOG.error(String.format(&quot;解析第%个满足条件的记录%s失败。&quot;, rowNum, result), e);        } catch (IllegalAccessException e) {            LOG.error(String.format(&quot;解析第%s个满足条件的记录%s失败。&quot;, rowNum, result), e);        }        return obj;    }    private  Map&lt;String,Field&gt;  getDeclaredFields(Class&lt;?&gt; clazz){        Field[] fields = clazz.getDeclaredFields();        Field field = null;        Map&lt;String,Field&gt; fieldMap = Maps.newHashMapWithExpectedSize(fields.length);        for(int i = 0; i &lt; fields.length; i++){            field = fields[i];            if(field.getModifiers() == 2){                field.setAccessible(true);                fieldMap.put(field.getName().toLowerCase(), field);            }        }        fields = null;        return fieldMap;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/CellNumExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.client.Result;import java.io.IOException;public class CellNumExtrator implements RowExtractor&lt;Integer&gt; {    public Integer extractRowData(Result result, int rowNum) throws IOException {        return  result.listCells().size();    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/MapLongRowExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.util.HashMap;import java.util.Map;public class MapLongRowExtrator implements RowExtractor&lt;Map&lt;String,Long&gt;&gt; {    private Map&lt;String,Long&gt; row;    @Override    public Map&lt;String, Long&gt; extractRowData(Result result, int rowNum) throws IOException {        row = new HashMap&lt;String,Long&gt;();        for(Cell cell :  result.listCells()) {            row.put(Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()),Bytes.toLong(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));        }        return row;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/MapRowExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.io.Serializable;import java.util.HashMap;import java.util.Map;public class MapRowExtrator implements RowExtractor&lt;Map&lt;String,String&gt;&gt;,Serializable {    private static final long serialVersionUID = 1543027485077396235L;    private Map&lt;String,String&gt; row;    /* (non-Javadoc)     * @see com.bh.d406.bigdata.hbase.extractor.RowExtractor#extractRowData(org.apache.hadoop.hbase.client.Result, int)     */    @Override    public Map&lt;String, String&gt; extractRowData(Result result, int rowNum) throws IOException {        row = new HashMap&lt;String,String&gt;();        for(Cell cell :  result.listCells()) {            row.put(Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()),Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));        }        return row;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/MultiVersionRowExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import com.hsiehchou.hbase.entity.HBaseRow;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;public class MultiVersionRowExtrator implements RowExtractor&lt;HBaseRow&gt;{    private HBaseRow row;    public HBaseRow extractRowData(Result result, int rowNum) throws IOException {        row = new HBaseRow(Bytes.toString(result.getRow()));        String field = null;        String value = null;        long capTime = 0L;        for(Cell cell : result.listCells()){            field = Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength());            value = Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());            capTime = cell.getTimestamp();            row.addCell(field, value, capTime);        }        return  row ;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/OneColumnRowByteExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.client.Result;import java.io.IOException;import java.io.Serializable;public class OneColumnRowByteExtrator implements RowExtractor&lt;byte[]&gt; ,Serializable{    private static final long serialVersionUID = -3420092335124240222L;    private byte[] cf;    private byte[] cl;    public OneColumnRowByteExtrator( byte[] cf,byte[] cl ){        this.cf = cf;        this.cl = cl;    }    public byte[] extractRowData(Result result, int rowNum) throws IOException {        return result.getValue(cf, cl);    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/OneColumnRowStringExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.io.Serializable;public class OneColumnRowStringExtrator implements RowExtractor&lt;String&gt;  , Serializable{    private static final long serialVersionUID = -8585637277902568648L;    private byte[] cf ;    private byte[] cl ;    public OneColumnRowStringExtrator( byte[] cf , byte[] cl ){        this.cf = cf;        this.cl = cl;    }    /* (non-Javadoc)     * @see com.bh.d406.bigdata.hbase.extractor.RowExtractor#extractRowData(org.apache.hadoop.hbase.client.Result, int)     */    @Override    public String extractRowData(Result result, int rowNum) throws IOException {        byte[] value = result.getValue(cf, cl);        if( value == null ) return null;        return  Bytes.toString( value ) ;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/OnlyRowKeyExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.client.Result;import java.io.IOException;public class OnlyRowKeyExtrator implements RowExtractor&lt;byte[]&gt; {    @Override    public byte[] extractRowData(Result result, int rowNum) throws IOException {        // TODO Auto-generated method stub        return result.getRow();    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/OnlyRowKeyStringExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;public class OnlyRowKeyStringExtrator implements RowExtractor&lt;String&gt; {    public String extractRowData(Result result, int rowNum) throws IOException {        return Bytes.toString( result.getRow() );    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/RowExtractor.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.client.Result;import java.io.IOException;public interface RowExtractor&lt;T&gt;  {    /**      * description:      * @param result  result解析器      * @param rowNum        * @return      * @throws Exception      * T     */    T extractRowData(Result result, int rowNum) throws IOException;}</code></pre><p><strong>com/hsiehchou/hbase/extractor/SingleColumnMultiVersionRowExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.util.Set;public class SingleColumnMultiVersionRowExtrator implements RowExtractor&lt;Set&lt;String&gt;&gt;{    private Set&lt;String&gt; values;    private byte[] cf;    private byte[] cl;    /**     * 单列解析器  获取hbase 单列多版本数据     * @param cf     列簇     * @param cl     列     * @param values 返回值     */    public SingleColumnMultiVersionRowExtrator(byte[] cf, byte[] cl, Set&lt;String&gt; values){        this.cf = cf;        this.cl = cl;        this.values = values;    }    public Set&lt;String&gt; extractRowData(Result result, int rowNum) throws IOException {        for(Cell cell : result.getColumnCells(cf, cl)){            values.add(Bytes.toString(cell.getValueArray(),cell.getValueOffset(), cell.getValueLength()));        }        return values;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/StrToByteExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.io.Serializable;import java.util.HashMap;import java.util.Map;public class StrToByteExtrator implements RowExtractor&lt;Map&lt;String,byte[]&gt;&gt; ,Serializable {    private static final long serialVersionUID = 4633698173362569711L;    private Map&lt;String,byte[]&gt; row;    @Override    public Map&lt;String, byte[]&gt; extractRowData(Result result, int rowNum) throws IOException {        row = new HashMap&lt;String,byte[]&gt;();        for(Cell cell :  result.listCells()) {            row.put(Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()),                    Bytes.copy(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));        }        return row;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/ToRowList.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.CellUtil;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.util.HashMap;import java.util.LinkedList;import java.util.List;import java.util.Map;/** * Hbase数据库中数据提取接口实现： * 提取result的rowKey，和每个cell的值作为一行数据， * 一个cell=(row, family:qualifier:value, version) * * &lt;p&gt; * 每行数据的格式为：{rowKey column${separator}value column${separator}value ...} * 其中，不同的列之间用空格分隔，同样列元素的描述符与值之间用${separator}分隔 */public class ToRowList implements RowExtractor&lt;List&lt;String&gt;&gt; {    private Boolean currentVersion; //currentVersion为true:只取当前最新版本，false:取所有版本    private char separator; //不同元素之间拼接时的分隔符，默认为`#`    private ToRowList(Boolean currentVersion, char separator) {        this.separator = separator;        this.currentVersion = currentVersion;    }    public ToRowList(Boolean currentVersion) {        this(currentVersion, &#39;#&#39;);    }    public ToRowList() {        this(true, &#39;#&#39;);    }    /**      * 对{当前版本}存放在list[0] = {rowKey` `column`#`value` `column`#`value ...}      * 多版本的时候list({rowKey`#`version1` `column`#`value` `column`#`value ...},      * {rowKey`#`version2` `column`#`value` `column`#`value ...})      */    @Override    public List&lt;String&gt; extractRowData(Result result, int rowNum) throws IOException {        if(result == null || result.isEmpty()) return null;        final char SPACE = &#39; &#39;;        List&lt;String&gt; rows = new LinkedList&lt;&gt;();        //一个result是同一个rowKey的所有cells集合        String rowKey = Bytes.toString(result.getRow());        //build rowKey` `column`#`value` `column`#`value ...        StringBuilder row = new StringBuilder();        row.append(rowKey).append(SPACE);        //用于处理不同版本的映射        Map&lt;Long, String&gt; version2qualifiersAndValues = new HashMap&lt;&gt;();        List&lt;Cell&gt; cells = result.listCells();        for (Cell cell : cells) {            String value = Bytes.toString(cell.getValueArray(),                    cell.getValueOffset(), cell.getValueLength());            String qualifier = Bytes.toString(CellUtil.cloneQualifier(cell));            if (currentVersion) {                row.append(qualifier).append(separator).append(value).append(SPACE);            } else {                Long version = cell.getTimestamp();                String tmp = version2qualifiersAndValues.get(version);                version2qualifiersAndValues.put(version,                        StringUtils.isNotBlank(tmp) ? tmp + &quot; &quot; + qualifier + separator + value                                : rowKey + separator + version + &quot; &quot; + qualifier + separator + value);            }        }        if (currentVersion) {            rows.add(row.toString());        } else {            for (String v : version2qualifiersAndValues.values()) {                rows.add(v);            }        }        return rows;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/ToRowMap.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.CellUtil;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.util.HashMap;import java.util.Map;/** * currentVersion 标识是否取多版本的数据，默认取当前版本 * 对当前版本，返回row`#`qualifier-&gt;value的映射 * 对多个版本，返回row`#`version`#`qualifier-&gt;value的映射 */public class ToRowMap implements RowExtractor&lt;Map&lt;String, String&gt;&gt; {    private Boolean currentVersion;    public ToRowMap() {        this(true);    }    private ToRowMap(Boolean currentVersion) {        this.currentVersion = currentVersion;    }    @Override    public Map&lt;String, String&gt; extractRowData(Result result, int rowNum)            throws IOException {        if(result == null || result.isEmpty()) return null;        final char HashTag = &#39;#&#39;;        HashMap&lt;String, String&gt; col2value = new HashMap&lt;&gt;();        String rowKey = Bytes.toString(result.getRow());        for (Cell cell : result.listCells()) {            String value = Bytes.toString(cell.getValueArray(),                    cell.getValueOffset(), cell.getValueLength());            String qualifier = Bytes.toString(CellUtil.cloneQualifier(cell));            if (currentVersion)                col2value.put(rowKey + HashTag + qualifier, value);            else {                long version = cell.getTimestamp();                col2value.put(rowKey + HashTag + version + HashTag + qualifier, value);            }        }        return col2value;    }}</code></pre><p><strong>com/hsiehchou/hbase/insert/HBaseInsertException.java</strong></p><pre><code>package com.hsiehchou.hbase.insert;import java.util.Iterator;public class HBaseInsertException extends Exception{    public HBaseInsertException(String message) {        super(message);    }    public final synchronized void addSuppresseds(Iterable&lt;Exception&gt; exceptions){        if(exceptions != null){            Iterator&lt;Exception&gt; iterator = exceptions.iterator();            while (iterator.hasNext()){                addSuppressed(iterator.next());            }        }    }}</code></pre><p><strong>com/hsiehchou/hbase/insert/HBaseInsertHelper.java</strong></p><pre><code>package com.hsiehchou.hbase.insert;import com.hsiehchou.hbase.config.HBaseTableUtil;import com.google.common.collect.Lists;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Table;import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;import java.io.Serializable;import java.util.ArrayList;import java.util.Collections;import java.util.List;import java.util.concurrent.CountDownLatch;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;/** * 添加HBASE 插入数据类 */public class HBaseInsertHelper implements Serializable{    private HBaseInsertHelper(){}    public static void put(String tableName, Put put) throws Exception {        put(tableName, Lists.newArrayList(put));    }    public static void put(String tableName, List&lt;Put&gt; puts) throws Exception {        if(!puts.isEmpty()){            Table table = HBaseTableUtil.getTable(tableName);            try {                table.put(puts);            }catch (Exception e){                e.printStackTrace();            }finally {                HBaseTableUtil.close(table);            }        }     }    public static void put(final String tableName, List&lt;Put&gt; puts, int perThreadPutSize) throws Exception {        int size = puts.size();        if(size &gt; perThreadPutSize){            int threadNum = (int)Math.ceil(size / (double)perThreadPutSize);            ExecutorService executorService = Executors.newFixedThreadPool(threadNum);            final CountDownLatch  cdl = new CountDownLatch(threadNum);            final List&lt;Exception&gt;  es = Collections.synchronizedList(new ArrayList&lt;Exception&gt;());            try {                for(int i = 0; i &lt; threadNum; i++){                    final List&lt;Put&gt; tmp;                    if(i == (threadNum - 1)){                        tmp = puts.subList(perThreadPutSize*i, size);                    }else{                        tmp = puts.subList(perThreadPutSize*i, perThreadPutSize*(i + 1));                    }                    executorService.execute(new Runnable() {                        public void run() {                            try {                                if(es.isEmpty()) put(tableName, tmp);                            } catch (Exception e) {                                es.add(e);                            }finally {                                cdl.countDown();                            }                        }                    });                }                cdl.await();            }finally {                executorService.shutdown();            }            if(es.size() &gt; 0){                HBaseInsertException insertException = new HBaseInsertException(String.format(&quot;put数据到表%s失败。&quot;));                insertException.addSuppresseds(es);                throw insertException;            }        }else {            put(tableName, puts);        }    }    public static void checkAndPut(String tableName, byte[] row, byte[] family, byte[] qualifier,                                   byte[] value, Put put) throws Exception {        checkAndPut(tableName, row, family, qualifier, null, value, put);    }    public static void checkAndPut(String tableName, byte[] row, byte[] family, byte[] qualifier,                                   CompareOp compareOp, byte[] value, Put put) throws Exception {        if(!put.isEmpty() ){            Table table = HBaseTableUtil.getTable(tableName);            try {                if(compareOp == null){                    table.checkAndPut(row, family, qualifier, value, put);                }else{                    table.checkAndPut(row, family, qualifier, compareOp, value, put);                }            }finally{                HBaseTableUtil.close(table);            }        }    }}</code></pre><p><strong>com/hsiehchou/hbase/search/HBaseSearchService.java</strong></p><pre><code>package com.hsiehchou.hbase.search;import com.hsiehchou.hbase.extractor.RowExtractor;import org.apache.hadoop.hbase.client.Get;import org.apache.hadoop.hbase.client.Scan;import java.io.IOException;import java.util.List;import java.util.Map;public interface HBaseSearchService {    /**      *  根据  用户 给定的解析类  解析  查询结果      * @param tableName      * @param scan      * @param extractor  用户自定义的 结果解析 类      * @return      * @throws IOException      * List&lt;T&gt;     */    &lt;T&gt; List&lt;T&gt; search(String tableName, Scan scan, RowExtractor&lt;T&gt; extractor) throws IOException;    /**      * 当存在多个  scan时  采用多线程查询      * @param tableName      * @param scans      * @param extractor  用户自定义的 结果解析 类      * @return      * @throws IOException      * List&lt;T&gt;     */    &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Scan&gt; scans, RowExtractor&lt;T&gt; extractor) throws IOException;    /**      * 采用多线程  同时查询多个表      * @param more      * @return      * @throws IOException      * List&lt;T&gt;     */    &lt;T&gt; Map&lt;String,List&lt;T&gt;&gt; searchMore(List&lt;SearchMoreTable&lt;T&gt;&gt; more) throws IOException;    /**      * 利用反射  自动封装实体类      * @param tableName      * @param scan          * @param cls   HBase表对应的实体类，属性只包含对应表的 列 ， 不区分大小写      * @return      * @throws IOException      * @throws InstantiationException      * @throws IllegalAccessException      * List&lt;T&gt;     */    &lt;T&gt; List&lt;T&gt; search(String tableName, Scan scan, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException;    /**      * 当存在多个 scan 时  采用多线程查询      * @param tableName      * @param scans      * @param cls   HBase表对应的实体类，属性只包含对应表的 列 ， 不区分大小写      * @return      * @throws IOException      * @throws InstantiationException      * @throws IllegalAccessException      * List&lt;T&gt;     */    &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Scan&gt; scans, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException;    /**      * 批量 get 查询  并按自定义的方式解析结果集      * @param tableName      * @param gets      * @param extractor  用户自定义的 结果解析 类      * @return      * @throws IOException      * List&lt;T&gt;     */    &lt;T&gt; List&lt;T&gt; search(String tableName, List&lt;Get&gt; gets, RowExtractor&lt;T&gt; extractor) throws IOException;    /**      * 多线程批量get, 并按自定义的方式解析结果集      * 建议 : perThreadExtractorGetNum &gt;= 100      * @param tableName      * @param gets      * @param perThreadExtractorGetNum    每个线程处理的 get的个数       * @param extractor  用户自定义的 结果解析 类      * @return      * @throws IOException      * List&lt;T&gt;     */    &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Get&gt; gets, int perThreadExtractorGetNum, RowExtractor&lt;T&gt; extractor) throws IOException;    /**      * 批量 get 查询  并利用反射 封装到指定的实体类中      * @param tableName      * @param gets      * @param  cls   HBase表对应的实体类，属性只包含对应表的 列 ， 不区分大小写      * @return            * @throws IOException      * @throws InstantiationException      * List&lt;T&gt;     */    &lt;T&gt; List&lt;T&gt; search(String tableName, List&lt;Get&gt; gets, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException;    /**      * 多线程批量 get 查询  并利用反射 封装到指定的实体类中      * 建议 : perThreadExtractorGetNum &gt;= 100      * @param tableName      * @param gets      * @param perThreadExtractorGetNum  每个线程处理的 get的个数       * @param  cls   HBase表对应的实体类，属性只包含对应表的 列 ， 不区分大小写      * @return      * @throws IOException      * @throws InstantiationException      * @throws IllegalAccessException      * List&lt;T&gt;     */    &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Get&gt; gets, int perThreadExtractorGetNum, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException;    /**      * get 查询  并按自定义的方式解析结果集      * @param tableName      * @param extractor   用户自定义的 结果解析 类      * @return     如果 查询不到  则 返回  null      * @throws IOException      * List&lt;T&gt;     */    &lt;T&gt; T search(String tableName, Get get, RowExtractor&lt;T&gt; extractor) throws IOException;    /**      * get 查询  并利用反射 封装到指定的实体类中      * @param tableName      * @param  cls   HBase表对应的实体类，属性只包含对应表的 列 ， 不区分大小写      * @return     如果 查询不到  则 返回  null      * @throws IOException      * @throws InstantiationException      * List&lt;T&gt;     */    &lt;T&gt; T search(String tableName, Get get, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException;}</code></pre><p><strong>com/hsiehchou/hbase/search/HBaseSearchServiceImpl.java</strong></p><pre><code>package com.hsiehchou.hbase.search;import com.hsiehchou.hbase.config.HBaseTableFactory;import com.hsiehchou.hbase.extractor.RowExtractor;import org.apache.hadoop.hbase.client.*;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.io.Serializable;import java.util.ArrayList;import java.util.Arrays;import java.util.List;import java.util.Map;public class HBaseSearchServiceImpl implements HBaseSearchService,Serializable{    private static final long serialVersionUID = -8657479861137115645L;    private static final Logger LOG = LoggerFactory.getLogger(HBaseSearchServiceImpl.class);    private HBaseTableFactory factory = new HBaseTableFactory();    private int poolCapacity = 6;    @Override    public &lt;T&gt; List&lt;T&gt; search(String tableName, Scan scan, RowExtractor&lt;T&gt; extractor) throws IOException {        return null;    }    @Override    public &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Scan&gt; scans, RowExtractor&lt;T&gt; extractor) throws IOException {        return null;    }    @Override    public &lt;T&gt; Map&lt;String, List&lt;T&gt;&gt; searchMore(List&lt;SearchMoreTable&lt;T&gt;&gt; more) throws IOException {        return null;    }    @Override    public &lt;T&gt; List&lt;T&gt; search(String tableName, Scan scan, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException {        return null;    }    @Override    public &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Scan&gt; scans, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException {        return null;    }    @Override    public &lt;T&gt; List&lt;T&gt; search(String tableName, List&lt;Get&gt; gets, RowExtractor&lt;T&gt; extractor) throws IOException {        List&lt;T&gt; data = new ArrayList&lt;T&gt;();        search(tableName, gets, extractor,data);        return data;    }    @Override    public &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Get&gt; gets, int perThreadExtractorGetNum, RowExtractor&lt;T&gt; extractor) throws IOException {        return null;    }    @Override    public &lt;T&gt; List&lt;T&gt; search(String tableName, List&lt;Get&gt; gets, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException {        return null;    }    @Override    public &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Get&gt; gets, int perThreadExtractorGetNum, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException {        return null;    }    @Override    public &lt;T&gt; T search(String tableName, Get get, RowExtractor&lt;T&gt; extractor) throws IOException {        T obj = null;        List&lt;T&gt; res = search(tableName,Arrays.asList(get),extractor);        if( !res.isEmpty()){            obj = res.get(0);        }        return obj;    }    @Override    public &lt;T&gt; T search(String tableName, Get get, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException {        return null;    }    private &lt;T&gt; void search(String tableName, List&lt;Get&gt; gets,                            RowExtractor&lt;T&gt; extractor , List&lt;T&gt; data ) throws IOException {        //根据table名获取表连接        Table table = factory.getHBaseTableInstance(tableName);        if(table != null ){            Result[] results = table.get(gets);            int n = 0;            T row = null;            for( Result result : results){                if( !result.isEmpty() ){                    row = extractor.extractRowData(result, n);                    if(row != null )data.add(row);                    n++;                }            }            close( table, null);        }else{            throw new IOException(&quot; table  &quot; + tableName + &quot; is not exists ..&quot;);        }    }    public static boolean  existsRowkey( Table table, String rowkey){        boolean exists =true;        try {            exists = table.exists(new Get(rowkey.getBytes()));        } catch (IOException e) {            LOG.error(&quot;失败。&quot;, e );        }        return exists;    }    public static void  close( Table table, ResultScanner scanner ){        try {            if( table != null ){                table.close();                table = null;            }            if( scanner != null ){                scanner.close();                scanner = null;            }        } catch (IOException e) {            LOG.error(&quot;关闭 HBase的表  &quot; + table.getName().toString() + &quot; 失败。&quot;, e );        }    }}</code></pre><p><strong>com/hsiehchou/hbase/search/SearchMoreTable.java</strong></p><pre><code>package com.hsiehchou.hbase.search;import com.hsiehchou.hbase.extractor.RowExtractor;import org.apache.hadoop.hbase.client.Scan;public class SearchMoreTable&lt;T&gt; {    private String tableName;    private Scan scan;    private RowExtractor&lt;T&gt; extractor;    public SearchMoreTable() {        super();    }    public SearchMoreTable(String tableName, Scan scan,            RowExtractor&lt;T&gt; extractor) {        super();        this.tableName = tableName;        this.scan = scan;        this.extractor = extractor;    }    public String getTableName() {        return tableName;    }    public void setTableName(String tableName) {        this.tableName = tableName;    }    public Scan getScan() {        return scan;    }    public void setScan(Scan scan) {        this.scan = scan;    }    public RowExtractor&lt;T&gt; getExtractor() {        return extractor;    }    public void setExtractor(RowExtractor&lt;T&gt; extractor) {        this.extractor = extractor;    }}</code></pre><p><strong>com/hsiehchou/hbase/spilt/SpiltRegionUtil.java</strong></p><pre><code>package com.hsiehchou.hbase.spilt;import org.apache.hadoop.hbase.util.Bytes;import java.util.Iterator;import java.util.TreeSet;/** * hbase 预分区 */public class SpiltRegionUtil {    /**     * 定义分区     * @return     */    public static byte[][] getSplitKeysBydinct() {        String[] keys = new String[]{&quot;1&quot;,&quot;2&quot;, &quot;3&quot;,&quot;4&quot;, &quot;5&quot;,&quot;6&quot;, &quot;7&quot;,&quot;8&quot;, &quot;9&quot;,&quot;a&quot;,&quot;b&quot;, &quot;c&quot;,&quot;d&quot;,&quot;e&quot;,&quot;f&quot;};        //String[] keys = new String[]{&quot;10|&quot;, &quot;20|&quot;, &quot;30|&quot;, &quot;40|&quot;, &quot;50|&quot;, &quot;60|&quot;, &quot;70|&quot;, &quot;80|&quot;, &quot;90|&quot;};        byte[][] splitKeys = new byte[keys.length][];        //通过treeset排序        TreeSet&lt;byte[]&gt; rows = new TreeSet&lt;byte[]&gt;(Bytes.BYTES_COMPARATOR);//升序排序        for (int i = 0; i &lt; keys.length; i++) {            rows.add(Bytes.toBytes(keys[i]));        }        Iterator&lt;byte[]&gt; rowKeyIter = rows.iterator();        int i = 0;        while (rowKeyIter.hasNext()) {            byte[] tempRow = rowKeyIter.next();            rowKeyIter.remove();            splitKeys[i] = tempRow;            i++;        }        return splitKeys;    }}</code></pre><h4 id="6、执行"><a href="#6、执行" class="headerlink" title="6、执行"></a>6、执行</h4><p>spark-submit <code>--</code>master local[1] <code>--</code>num-executors 1 <code>--</code>driver-memory 300m <code>--</code>executor-memory 500m <code>--</code>executor-cores 1 <code>--</code>jars $(echo /usr/chl/spark7/jars/*.jar | tr ‘ ‘ ‘,’) <code>--</code>class com.hsiehchou.spark.streaming.kafka.kafka2hbase.DataRelationStreaming /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</p><h4 id="7、执行截图"><a href="#7、执行截图" class="headerlink" title="7、执行截图"></a>7、执行截图</h4><p><img src="/medias/hbase_list.PNG" alt="hbase_list"></p><p><img src="/medias/hbase_scan.PNG" alt="hbase_scan"></p><p><img src="/medias/hbase%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE.PNG" alt="hbase写入数据"></p><h3 id="十二、SpringCloud-项目构建"><a href="#十二、SpringCloud-项目构建" class="headerlink" title="十二、SpringCloud 项目构建"></a>十二、SpringCloud 项目构建</h3><p><img src="/medias/SpringCloud%E5%BE%AE%E6%9C%8D%E5%8A%A1.PNG" alt="SpringCloud微服务"></p><p><img src="/medias/%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C.PNG" alt="服务注册"></p><p><strong>解决IntelliJ IDEA 创建Maven项目速度慢问题</strong><br>add Maven Property<br>Name:archetypeCatalog<br>Value:internal</p><h4 id="1、构建SpringCloud父项目"><a href="#1、构建SpringCloud父项目" class="headerlink" title="1、构建SpringCloud父项目"></a>1、构建SpringCloud父项目</h4><p>在原项目下新建 xz_bigdata_springcloud_dir目录</p><p><img src="/medias/%E6%96%B0%E5%BB%BA%20xz_bigdata_springcloud_dir%E7%9B%AE%E5%BD%95.PNG" alt="新建 xz_bigdata_springcloud_dir目录"></p><h4 id="2、在此目录下新建-xz-bigdata-springclod-root项目"><a href="#2、在此目录下新建-xz-bigdata-springclod-root项目" class="headerlink" title="2、在此目录下新建 xz_bigdata_springclod_root项目"></a>2、在此目录下新建 xz_bigdata_springclod_root项目</h4><p><img src="/medias/%E6%96%B0%E5%BB%BA%20xz_bigdata_springcloud_root%E9%A1%B9%E7%9B%AE.PNG" alt="新建 xz_bigdata_springcloud_root项目"></p><h4 id="3、-引入SpringCloud依赖"><a href="#3、-引入SpringCloud依赖" class="headerlink" title="3、    引入SpringCloud依赖"></a>3、    引入SpringCloud依赖</h4><p><strong>父pom.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;  &lt;modules&gt;    &lt;module&gt;xz_bigdata_springcloud_common&lt;/module&gt;    &lt;module&gt;xz_bigdata_springcloud_esquery&lt;/module&gt;    &lt;module&gt;xz_bigdata_springcloud_eureka&lt;/module&gt;    &lt;module&gt;xz_bigdata_springcloud_hbasequery&lt;/module&gt;  &lt;/modules&gt;  &lt;parent&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;    &lt;version&gt;2.0.9.RELEASE&lt;/version&gt;  &lt;/parent&gt;  &lt;groupId&gt;com.hsiehchou.springcloud&lt;/groupId&gt;  &lt;artifactId&gt;xz_bigdata_springcloud_root&lt;/artifactId&gt;  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;  &lt;packaging&gt;pom&lt;/packaging&gt;  &lt;name&gt;xz_bigdata_springcloud_root&lt;/name&gt;  &lt;properties&gt;    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;    &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;    &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;  &lt;/properties&gt;  &lt;!--CDH源--&gt;  &lt;repositories&gt;    &lt;repository&gt;      &lt;id&gt;cloudera&lt;/id&gt;      &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;    &lt;/repository&gt;  &lt;/repositories&gt;  &lt;!--依赖管理，用于管理spring-cloud的依赖--&gt;  &lt;dependencyManagement&gt;    &lt;dependencies&gt;      &lt;!--spring-cloud-dependencies--&gt;      &lt;dependency&gt;        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;        &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;        &lt;version&gt;Finchley.SR1&lt;/version&gt;        &lt;type&gt;pom&lt;/type&gt;        &lt;scope&gt;import&lt;/scope&gt;      &lt;/dependency&gt;    &lt;/dependencies&gt;  &lt;/dependencyManagement&gt;  &lt;!--打包插件--&gt;  &lt;build&gt;    &lt;plugins&gt;      &lt;plugin&gt;        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;        &lt;version&gt;3.1&lt;/version&gt;        &lt;configuration&gt;          &lt;source&gt;1.8&lt;/source&gt;          &lt;target&gt;1.8&lt;/target&gt;          &lt;encoding&gt;UTF-8&lt;/encoding&gt;        &lt;/configuration&gt;      &lt;/plugin&gt;    &lt;/plugins&gt;  &lt;/build&gt;&lt;/project&gt;</code></pre><p><strong>删除父项目src目录。因为这个项目主要是管理子项目不做任何逻辑业务</strong></p><h4 id="4、构建SpringCloud-Common子项目"><a href="#4、构建SpringCloud-Common子项目" class="headerlink" title="4、构建SpringCloud Common子项目"></a>4、构建SpringCloud Common子项目</h4><p><strong>新建子模块</strong><br>xz_bigdata_springcloud_common</p><p><strong>引入依赖</strong></p><p><strong>pom.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata_springcloud_root&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_springcloud_common&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_springcloud_common&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!--eureka-server--&gt;        &lt;!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-eureka-server --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt;                    &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;            &lt;version&gt;1.2.24&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><h4 id="5、构建Eureka服务注册中心"><a href="#5、构建Eureka服务注册中心" class="headerlink" title="5、构建Eureka服务注册中心"></a>5、构建Eureka服务注册中心</h4><p><strong>新建xz_bigdata_springcloud_eureka子模块</strong></p><p><img src="/medias/%E6%96%B0%E5%BB%BAxz_bigdata_springcloud_eureka%E5%AD%90%E6%A8%A1%E5%9D%97.PNG" alt="新建xz_bigdata_springcloud_eureka子模块"></p><p><strong>引入依赖</strong></p><p><strong>pom.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata_springcloud_root&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_springcloud_eureka&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_springcloud_eureka&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou.springcloud&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_springcloud_common&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;!--用户验证--&gt;  &lt;!--      &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;            &lt;version&gt;1.4.1.RELEASE&lt;/version&gt;        &lt;/dependency&gt;--&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;&lt;!--打包依赖的jar包--&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;outputDirectory&gt;${project.build.directory}/lib&lt;/outputDirectory&gt;                    &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt; &lt;!-- 表示是否不包含间接依赖的包 --&gt;                    &lt;stripVersion&gt;false&lt;/stripVersion&gt; &lt;!-- 去除版本信息 --&gt;                &lt;/configuration&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;copy-dependencies&lt;/id&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;!-- 拷贝项目依赖包到lib/目录下 --&gt;                            &lt;outputDirectory&gt;${project.build.directory}/jars&lt;/outputDirectory&gt;                            &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt;                            &lt;stripVersion&gt;false&lt;/stripVersion&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;!-- 打成jar包插件 --&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;                &lt;version&gt;2.5&lt;/version&gt;                &lt;configuration&gt;                    &lt;archive&gt;                        &lt;!--                        生成的jar中，不要包含pom.xml和pom.properties这两个文件                    --&gt;                        &lt;addMavenDescriptor&gt;false&lt;/addMavenDescriptor&gt;                        &lt;!-- 生成MANIFEST.MF的设置 --&gt;                        &lt;manifest&gt;                            &lt;!-- 为依赖包添加路径, 这些路径会写在MANIFEST文件的Class-Path下 --&gt;                            &lt;addClasspath&gt;true&lt;/addClasspath&gt;                            &lt;classpathPrefix&gt;jars/&lt;/classpathPrefix&gt;                            &lt;!-- jar启动入口类--&gt;                            &lt;mainClass&gt;com.cn.hbase.mr.HbaseMr&lt;/mainClass&gt;                        &lt;/manifest&gt;                        &lt;!--       &lt;manifestEntries&gt;                                   &amp;lt;!&amp;ndash; 在Class-Path下添加配置文件的路径 &amp;ndash;&amp;gt;                                   &lt;Class-Path&gt;&lt;/Class-Path&gt;                               &lt;/manifestEntries&gt;--&gt;                    &lt;/archive&gt;                    &lt;outputDirectory&gt;${project.build.directory}/&lt;/outputDirectory&gt;                    &lt;includes&gt;                        &lt;!-- 打jar包时，只打包class文件 --&gt;                        &lt;include&gt;**/*.class&lt;/include&gt;                        &lt;include&gt;**/*.properties&lt;/include&gt;                        &lt;include&gt;**/*.yml&lt;/include&gt;                    &lt;/includes&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><p>新建resources配置文件目录，添加application.yml配置文件或者 application.properties</p><p><strong>application.yml</strong></p><pre><code>server:  port: 8761eureka:  client:    register-with-eureka: false    fetch-registry: false    service-url:      defaultZone: http://root:root@hadoop3:8761/eureka/</code></pre><p><img src="/medias/xz_bigdata_springcloud_eureka%E7%BB%93%E6%9E%84.PNG" alt="xz_bigdata_springcloud_eureka结构"></p><p><strong>新建EurekaApplication 启动类</strong></p><p><strong>EurekaApplication.java</strong></p><pre><code>package com.hsiehchou.springcloud.eureka;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;/** * 注册中心 */@SpringBootApplication@EnableEurekaServerpublic class EurekaApplication{    public static void main( String[] args )    {        SpringApplication.run(EurekaApplication.class, args);    }}</code></pre><p><strong>执行EurekaApplication 启动</strong></p><p><strong>访问localhost:8761</strong></p><p><img src="/medias/%E8%AE%BF%E9%97%AEhadoop38761.PNG" alt="访问hadoop3:8761"></p><h4 id="6、构建HBase查询服务模块"><a href="#6、构建HBase查询服务模块" class="headerlink" title="6、构建HBase查询服务模块"></a>6、构建HBase查询服务模块</h4><p><strong>新建xz_bigdata_springcloud_root子模块</strong></p><p><img src="/medias/%E6%96%B0%E5%BB%BAxz_bigdata_springcloud_root%E5%AD%90%E6%A8%A1%E5%9D%97.PNG" alt="新建xz_bigdata_springcloud_root子模块"></p><p><strong>添加依赖</strong></p><p><strong>pom.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata_springcloud_root&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_springcloud_hbasequery&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_springcloud_hbasequery&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!--spring common依赖--&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou.springcloud&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_springcloud_common&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt;                    &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;!--基础服务hbase依赖--&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_hbase&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;fastjson&lt;/artifactId&gt;                    &lt;groupId&gt;com.alibaba&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;&lt;!--打包依赖的jar包--&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;outputDirectory&gt;${project.build.directory}/lib&lt;/outputDirectory&gt;                    &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt; &lt;!-- 表示是否不包含间接依赖的包 --&gt;                    &lt;stripVersion&gt;false&lt;/stripVersion&gt; &lt;!-- 去除版本信息 --&gt;                &lt;/configuration&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;copy-dependencies&lt;/id&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;!-- 拷贝项目依赖包到lib/目录下 --&gt;                            &lt;outputDirectory&gt;${project.build.directory}/jars&lt;/outputDirectory&gt;                            &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt;                            &lt;stripVersion&gt;false&lt;/stripVersion&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;!-- 打成jar包插件 --&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;                &lt;version&gt;2.5&lt;/version&gt;                &lt;configuration&gt;                    &lt;archive&gt;                        &lt;!--                        生成的jar中，不要包含pom.xml和pom.properties这两个文件                    --&gt;                        &lt;addMavenDescriptor&gt;false&lt;/addMavenDescriptor&gt;                        &lt;!-- 生成MANIFEST.MF的设置 --&gt;                        &lt;manifest&gt;                            &lt;!-- 为依赖包添加路径, 这些路径会写在MANIFEST文件的Class-Path下 --&gt;                            &lt;addClasspath&gt;true&lt;/addClasspath&gt;                            &lt;classpathPrefix&gt;jars/&lt;/classpathPrefix&gt;                            &lt;!-- jar启动入口类--&gt;                            &lt;mainClass&gt;com.cn.hbase.mr.HbaseMr&lt;/mainClass&gt;                        &lt;/manifest&gt;                        &lt;!--       &lt;manifestEntries&gt;                                   &amp;lt;!&amp;ndash; 在Class-Path下添加配置文件的路径 &amp;ndash;&amp;gt;                                   &lt;Class-Path&gt;&lt;/Class-Path&gt;                               &lt;/manifestEntries&gt;--&gt;                    &lt;/archive&gt;                    &lt;outputDirectory&gt;${project.build.directory}/&lt;/outputDirectory&gt;                    &lt;includes&gt;                        &lt;!-- 打jar包时，只打包class文件 --&gt;                        &lt;include&gt;**/*.class&lt;/include&gt;                        &lt;include&gt;**/*.properties&lt;/include&gt;                        &lt;include&gt;**/*.yml&lt;/include&gt;                    &lt;/includes&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><p><strong>添加配置文件</strong></p><p><strong>新建 resources 目录</strong><br>添加 <strong>application.properties</strong> 文件</p><pre><code>server.port=8002logging.level.root=INFOlogging.level.org.hibernate=INFOlogging.level.org.hibernate.type.descriptor.sql.BasicBinder=TRACElogging.level.org.hibernate.type.descriptor.sql.BasicExtractor= TRACElogging.level.com.itmuch=DEBUGspring.http.encoding.charset=UTF-8spring.http.encoding.enable=truespring.http.encoding.force=trueeureka.client.serviceUrl.defaultZone=http://root:root@hadoop3:8761/eureka/spring.application.name=xz-bigdata-springcloud-hbasequeryeureka.instance.prefer-ip-address=true</code></pre><p><strong>构建启动类</strong></p><p>新建 <strong>com.hsiehchou.springcloud.hbase</strong>包<br>构建 <strong>HbaseApplication</strong> 启动类</p><pre><code>package com.hsiehchou.springcloud;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;@SpringBootApplication@EnableEurekaServerpublic class HbaseQueryApplication{    public static void main( String[] args )    {        SpringApplication.run(HbaseQueryApplication.class, args);    }}</code></pre><p><img src="/medias/%E6%B3%A8%E5%86%8C%E6%88%90%E5%8A%9F.PNG" alt="注册成功"><br>说明注册成功</p><p><strong>构建服务</strong></p><p><img src="/medias/%E6%9E%84%E5%BB%BAHbase%E6%9C%8D%E5%8A%A1.PNG" alt="构建Hbase服务"></p><p>构建 <strong>com.hsiehchou.springcloud.hbase.controller</strong></p><p>创建 <strong>HbaseBaseController</strong></p><p><strong>HbaseBaseController.java</strong></p><pre><code>package com.hsiehchou.springcloud.hbase.controller;import com.hsiehchou.hbase.extractor.SingleColumnMultiVersionRowExtrator;import com.hsiehchou.hbase.search.HBaseSearchService;import com.hsiehchou.hbase.search.HBaseSearchServiceImpl;import com.hsiehchou.springcloud.hbase.service.HbaseBaseService;import org.apache.hadoop.hbase.client.Get;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.*;import javax.annotation.Resource;import java.io.IOException;import java.util.HashSet;import java.util.List;import java.util.Map;import java.util.Set;@Controller@RequestMapping(value=&quot;/hbase&quot;)public class HbaseBaseController {    private static Logger LOG = LoggerFactory.getLogger(HbaseBaseController.class);    //注入 通过这个注解可以直接拿到HbaseBaseService这个的实例    @Resource    private HbaseBaseService hbaseBaseService;    @ResponseBody    @RequestMapping(value=&quot;/search/{table}/{rowkey}&quot;, method={RequestMethod.GET,RequestMethod.POST})    public Set&lt;String&gt; search(@PathVariable(value = &quot;table&quot;) String table,                              @PathVariable(value = &quot;rowkey&quot;) String rowkey){        return hbaseBaseService.getSingleColumn(table,rowkey);    }    @ResponseBody    @RequestMapping(value=&quot;/search1&quot;, method={RequestMethod.GET,RequestMethod.POST})    public Set&lt;String&gt; search1( @RequestParam(name = &quot;table&quot;) String table,                                @RequestParam(name = &quot;rowkey&quot;) String rowkey){        //通过二级索引去找主关联表的rowkey 这个rowkey就是MAC        return hbaseBaseService.getSingleColumn(table,rowkey);    }    @ResponseBody    @RequestMapping(value = &quot;/getHbase&quot;,method = {RequestMethod.GET,RequestMethod.POST})    public Set&lt;String&gt; getHbase(@RequestParam(name=&quot;table&quot;) String table,                                @RequestParam(name=&quot;rowkey&quot;) String rowkey){        return hbaseBaseService.getSingleColumn(table, rowkey);    }    @ResponseBody    @RequestMapping(value = &quot;/getRelation&quot;,method = {RequestMethod.GET,RequestMethod.POST})    public Map&lt;String,List&lt;String&gt;&gt; getRelation(@RequestParam(name = &quot;field&quot;) String field,                                                @RequestParam(name = &quot;fieldValue&quot;) String fieldValue){        return hbaseBaseService.getRealtion(field,fieldValue);    }    public static void main(String[] args) {        HbaseBaseController hbaseBaseController = new HbaseBaseController();        hbaseBaseController.getHbase(&quot;send_mail&quot;, &quot;65497873@qq.com&quot;);    }}</code></pre><p>构建 <strong>com.hsiehchou.springcloud.hbase.service</strong></p><p>创建 <strong>HbaseBaseService</strong></p><p><strong>HbaseBaseService.java</strong></p><pre><code>package com.hsiehchou.springcloud.hbase.service;import com.hsiehchou.hbase.entity.HBaseCell;import com.hsiehchou.hbase.entity.HBaseRow;import com.hsiehchou.hbase.extractor.MultiVersionRowExtrator;import com.hsiehchou.hbase.extractor.SingleColumnMultiVersionRowExtrator;import com.hsiehchou.hbase.search.HBaseSearchService;import com.hsiehchou.hbase.search.HBaseSearchServiceImpl;import org.apache.hadoop.hbase.client.Get;import org.apache.hadoop.hbase.client.Put;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.stereotype.Service;import javax.annotation.Resource;import java.io.IOException;import java.util.*;@Servicepublic class HbaseBaseService {    private static Logger LOG = LoggerFactory.getLogger(HbaseBaseService.class);    @Resource    private HbaseBaseService hbaseBaseService;    /**     * 获取hbase单列数据的多版本信息     * @param field     * @param rowkey     * @return     */    public Set&lt;String&gt; getSingleColumn(String field,String rowkey){        //从索引表中获取总关联表的rowkey,获取phone对应的多版本MAC        Set&lt;String&gt; search = null;        HBaseSearchService hBaseSearchService = new HBaseSearchServiceImpl();        String table = &quot;test:&quot;+field;        Get get = new Get(rowkey.getBytes());        try {            get.setMaxVersions(100);        } catch (IOException e) {            e.printStackTrace();        }        Set set = new HashSet&lt;String&gt;();        SingleColumnMultiVersionRowExtrator singleColumnMultiVersionRowExtrator = new SingleColumnMultiVersionRowExtrator(&quot;cf&quot;.getBytes(), &quot;phone_mac&quot;.getBytes(), set);        try {            search = hBaseSearchService.search(table, get, singleColumnMultiVersionRowExtrator);            System.out.println(search.toString());        } catch (IOException e) {            e.printStackTrace();        }        return search;    }    /**     *  获取单列多版本     * @param table     * @param rowkey     * @param versions     * @return     */    public Set&lt;String&gt; getSingleColumn(String table,String rowkey,int versions){        Set&lt;String&gt; search = null;        try {            HBaseSearchService baseSearchService = new HBaseSearchServiceImpl();            Get get = new Get(rowkey.getBytes());            get.setMaxVersions(versions);            Set set = new HashSet&lt;String&gt;();            SingleColumnMultiVersionRowExtrator singleColumnMultiVersionRowExtrator = new SingleColumnMultiVersionRowExtrator(&quot;cf&quot;.getBytes(), &quot;phone_mac&quot;.getBytes(), set);            search = baseSearchService.search(table, get, singleColumnMultiVersionRowExtrator);        } catch (IOException e) {            LOG.error(null,e);        }        System.out.println(search);        return search;    }    /**     * 直接通过关联表字段值获取整条记录     * hbase 二级查找     * @param field     * @param fieldValue     * @return     */    public Map&lt;String,List&lt;String&gt;&gt; getRealtion(String field,String fieldValue){        //第一步 从二级索引表中找到多版本的rowkey        Map&lt;String,List&lt;String&gt;&gt; map = new HashMap&lt;&gt;();        //首先查找索引表        //查找的表名        String table = &quot;test:&quot; + field;        String indexRowkey = fieldValue;        HbaseBaseService hbaseBaseService = new HbaseBaseService();        Set&lt;String&gt; relationRowkeys = hbaseBaseService.getSingleColumn(table, indexRowkey, 100);        //第二步 拿到二级索引表中得到的 主关联表的rowkey        //对这些rowkey进行遍历 获取主关联表中rowkey对应的所有多版本数据        //遍历relationRowkeys，将其封装成List&lt;Get&gt;        List&lt;Get&gt; list = new ArrayList&lt;&gt;();        relationRowkeys.forEach(relationRowkey-&gt;{            //通过relationRowkey去找relation表中的所有信息            Get get = new Get(relationRowkey.getBytes());            try {                get.setMaxVersions(100);            } catch (IOException e) {                e.printStackTrace();            }            list.add(get);        });        MultiVersionRowExtrator multiVersionRowExtrator = new MultiVersionRowExtrator();        HBaseSearchService hBaseSearchService = new HBaseSearchServiceImpl();        try {            //&lt;T&gt; List&lt;T&gt; search(String tableName, List&lt;Get&gt; gets, RowExtractor&lt;T&gt; extractor) throws IOException;            List&lt;HBaseRow&gt; search = hBaseSearchService.search(&quot;test:relation&quot;, list, multiVersionRowExtrator);            search.forEach(hbaseRow-&gt;{                Map&lt;String, Collection&lt;HBaseCell&gt;&gt; cellMap = hbaseRow.getCell();                cellMap.forEach((key,value)-&gt;{                    //把Map&lt;String,Collection&lt;HBaseCell&gt;&gt;转为Map&lt;String,List&lt;String&gt;&gt;                    List&lt;String&gt; listValue = new ArrayList&lt;&gt;();                    value.forEach(x-&gt;{                        listValue.add(x.toString());                    });                    map.put(key,listValue);                });            });        } catch (IOException e) {            e.printStackTrace();        }        System.out.println(map.toString());     return map;    }    public static void main(String[] args) {        HbaseBaseService hbaseBaseService = new HbaseBaseService();//        hbaseBaseService.getRealtion(&quot;send_mail&quot;,&quot;65494533@qq.com&quot;);        hbaseBaseService.getSingleColumn(&quot;phone&quot;,&quot;18609765012&quot;);    }}</code></pre><h4 id="7、构建ES查询服务"><a href="#7、构建ES查询服务" class="headerlink" title="7、构建ES查询服务"></a>7、构建ES查询服务</h4><p>使用jest API 是走的 <strong>HTTP 请求</strong>  <strong>9200端口</strong><br>依赖如下:</p><pre><code>&lt;dependency&gt;    &lt;groupId&gt;io.searchbox&lt;/groupId&gt;    &lt;artifactId&gt;jest&lt;/artifactId&gt;    &lt;version&gt;6.3.1&lt;/version&gt;&lt;/dependency&gt;</code></pre><p>9200作为Http协议，<strong>主要用于外部通讯</strong></p><p>9300作为Tcp协议，jar之间就是通过 <strong>tcp协议通讯</strong></p><p><strong>ES集群之间是通过9300进行通讯</strong></p><p><strong>新建xz_bigdata_springcloud_esquery</strong></p><p><strong>新建xz_bigdata_springcloud_esquery子项目</strong></p><p><strong>准备</strong></p><p>新建 <strong>resources</strong> 配置文件目录</p><p><strong>增加配置文件</strong></p><p><strong>application.properties</strong></p><pre><code>server.port=8003logging.level.root=INFOlogging.level.org.hibernate=INFOlogging.level.org.hibernate.type.descriptor.sql.BasicBinder=TRACElogging.level.org.hibernate.type.descriptor.sql.BasicExtractor= TRACElogging.level.com.itmuch=DEBUGspring.http.encoding.charset=UTF-8spring.http.encoding.enable=truespring.http.encoding.force=trueeureka.client.serviceUrl.defaultZone=http://root:root@hadoop3:8761/eureka/spring.application.name=xz-bigdata-springcloud-esqueryeureka.instance.prefer-ip-address=true#关闭EDES检测management.health.elasticsearch.enabled=falsespring.elasticsearch.jest.uris=[&quot;http://192.168.116.201:9200&quot;]#全部索引esIndexs=wechat,mail,qq</code></pre><p><strong>新建ES微服务启动类</strong></p><p><strong>ESqueryApplication.java</strong></p><pre><code>package com.hsiehchou.springcloud.es;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;import org.springframework.cloud.openfeign.EnableFeignClients;@SpringBootApplication@EnableEurekaServer@EnableDiscoveryClient@EnableFeignClientspublic class ESqueryApplication {    public static void main(String[] args) {        SpringApplication.run(ESqueryApplication.class,args);    }}</code></pre><p><strong>启动 Eureka  ES 微服务</strong></p><p><img src="/medias/%E6%B3%A8%E5%86%8C%E6%88%90%E5%8A%9F.PNG" alt="注册成功"><br>说明注册成功</p><p><img src="/medias/ES%E8%B0%83%E7%94%A8Hbase.PNG" alt="ES调用Hbase"></p><p>构建 <strong>com.hsiehchou.springcloud.es.controller</strong></p><p>创建 <strong>EsBaseController</strong></p><pre><code>package com.hsiehchou.springcloud.es.controller;import com.hsiehchou.springcloud.es.feign.HbaseFeign;import com.hsiehchou.springcloud.es.service.EsBaseService;import org.springframework.beans.factory.annotation.Value;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.ResponseBody;import javax.annotation.Resource;import java.util.List;import java.util.Map;import java.util.Set;@Controller@RequestMapping(value = &quot;/es&quot;)public class EsBaseController {    @Value(&quot;${esIndexs}&quot;)    private String esIndexs;    @Resource    private EsBaseService esBaseService;    @Resource    private HbaseFeign hbaseFeign;    /**     * 基础查询     * @param indexName     * @param typeName     * @param sortField     * @param sortValue     * @param pageNumber     * @param pageSize     * @return     */    @ResponseBody    @RequestMapping(value = &quot;/getBaseInfo&quot;, method = {RequestMethod.GET, RequestMethod.POST})    public List&lt;Map&lt;String, Object&gt;&gt; getBaseInfo(@RequestParam(name = &quot;indexName&quot;) String indexName,                                                 @RequestParam(name = &quot;typeName&quot;) String typeName,                                                 @RequestParam(name = &quot;sortField&quot;) String sortField,                                                 @RequestParam(name = &quot;sortValue&quot;) String sortValue,                                                 @RequestParam(name = &quot;pageNumber&quot;) int pageNumber,                                                 @RequestParam(name = &quot;pageSize&quot;) int pageSize) {        // 根据数据类型, 排序，分页        // indexName typeName        // sortField sortValue        // pageNumber  pageSize        return  esBaseService.getBaseInfo(indexName,typeName,sortField,sortValue,pageNumber,pageSize);    }    /**     * 根据任意条件查找轨迹数据     * @param field     * @param fieldValue     * @return     */    @ResponseBody    @RequestMapping(value = &quot;/getLocus&quot;, method = {RequestMethod.GET, RequestMethod.POST})    public List&lt;Map&lt;String, Object&gt;&gt; getLocus(@RequestParam(name = &quot;field&quot;) String field,                                                 @RequestParam(name = &quot;fieldValue&quot;) String fieldValue) {        Set&lt;String&gt; macs = hbaseFeign.search1(field, fieldValue);        System.out.println(macs.toString());        // 根据数据类型, 排序，分页        // indexName typeName        // sortField sortValue        // pageNumber  pageSize        String mac = macs.iterator().next();        return  esBaseService.getLocus(mac);    }    /**     * 所有表数据总量     * @return     */    @ResponseBody    @RequestMapping(value=&quot;/getAllCount&quot;, method={RequestMethod.GET,RequestMethod.POST})    public Map&lt;String,Long&gt; getAllCount(){        Map&lt;String, Long&gt; allCount = esBaseService.getAllCount(esIndexs);        System.out.println(allCount);        return allCount;    }    @ResponseBody    @RequestMapping(value=&quot;/group&quot;, method={RequestMethod.GET,RequestMethod.POST})    public Map&lt;String,Long&gt; group(@RequestParam(name = &quot;indexName&quot;) String indexName,                                  @RequestParam(name = &quot;typeName&quot;) String typeName,                                  @RequestParam(name = &quot;field&quot;) String field){        return esBaseService.aggregation(indexName,typeName,field);    }    public static void main(String[] args){        EsBaseController esBaseController = new EsBaseController();        esBaseController.getLocus(&quot;phone&quot;,&quot;18609765432&quot;);    }}</code></pre><p>构建 <strong>com.hsiehchou.springcloud.es.service</strong></p><p>创建 <strong>EsBaseService</strong></p><pre><code>package com.hsiehchou.springcloud.es.service;import com.hsiehchou.es.jest.service.JestService;import com.hsiehchou.es.jest.service.ResultParse;import io.searchbox.client.JestClient;import io.searchbox.core.SearchResult;import org.springframework.stereotype.Service;import java.util.HashMap;import java.util.List;import java.util.Map;@Servicepublic class EsBaseService {    // 根据数据类型, 排序，分页    // indexName typeName    // sortField sortValue    // pageNumber  pageSize    public List&lt;Map&lt;String, Object&gt;&gt; getBaseInfo(String indexName,                                                 String typeName,                                                 String sortField,                                                 String sortValue,                                                 int pageNumber,                                                 int pageSize) {        //实现查询        JestClient jestClient = null;        List&lt;Map&lt;String, Object&gt;&gt; maps = null;        try {            jestClient = JestService.getJestClient();            SearchResult search = JestService.search(jestClient,                    indexName,                    typeName,                    &quot;&quot;,                    &quot;&quot;,                    sortField,                    sortValue,                    pageNumber,                    pageSize);            maps = ResultParse.parseSearchResultOnly(search);        } catch (Exception e) {            e.printStackTrace();        } finally {            JestService.closeJestClient(jestClient);        }        return maps;    }    // 传时间范围   比如你要查3天之内的轨迹    // es中text的类型的可以直接查询，而keyword类型的必须带.keyword，例如，phone_mac.keyword    public List&lt;Map&lt;String, Object&gt;&gt; getLocus(String mac){        //实现查询        JestClient jestClient = null;        List&lt;Map&lt;String, Object&gt;&gt; maps = null;        String[] includes = new String[]{&quot;latitude&quot;,&quot;longitude&quot;,&quot;collect_time&quot;};        try {            jestClient = JestService.getJestClient();            SearchResult search = JestService.search(jestClient,                    &quot;&quot;,                    &quot;&quot;,                    &quot;phone_mac.keyword&quot;,                    mac,                    &quot;collect_time&quot;,                    &quot;asc&quot;,                    1,                    2000,                    includes);            maps = ResultParse.parseSearchResultOnly(search);        } catch (Exception e) {            e.printStackTrace();        } finally {            JestService.closeJestClient(jestClient);        }        return maps;    }     public Map&lt;String,Long&gt; getAllCount(String esIndexs){        Map&lt;String,Long&gt; countMap = new HashMap&lt;&gt;();        JestClient jestClient = null;        try {            jestClient = JestService.getJestClient();            String[] split = esIndexs.split(&quot;,&quot;);            for (int i = 0; i &lt; split.length; i++) {                String index = split[i];                Long count = JestService.count(jestClient, index, index);                countMap.put(index,count);            }        } catch (Exception e) {            e.printStackTrace();        }finally {            JestService.closeJestClient(jestClient);        }        return countMap;    }    public Map&lt;String,Long&gt; aggregation(String indexName,String typeName,String field){        JestClient jestClient = null;        Map&lt;String, Long&gt; stringLongMap = null;        try {            jestClient = JestService.getJestClient();            SearchResult aggregation = JestService.aggregation(jestClient, indexName, typeName, field);            stringLongMap = ResultParse.parseAggregation(aggregation);        } catch (Exception e) {            e.printStackTrace();        }finally {            JestService.closeJestClient(jestClient);        }        return stringLongMap;    }}</code></pre><p><strong>这里用到了ES的大数据基础服务</strong></p><p><strong>轨迹查询</strong></p><p>用到了 <strong>HBase</strong> 的服务，使用 <strong>Fegin</strong><br><strong>SpringCloud Feign</strong></p><p><strong>Feign</strong> 是一个声明式的伪Http客户端，它使得写Http客户端变得更简单。使用 <strong>Feign</strong> ，只需要创建一个接口并用注解的方式来配置它，即可完成对服务提供方的接口绑定服务调用客户端的开发量。</p><p>构建 <strong>com.hsiehchou.springcloud.es.fegin</strong></p><p>创建 <strong>HbaseFeign</strong></p><pre><code>package com.hsiehchou.springcloud.es.feign;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.ResponseBody;import java.util.Set;@FeignClient(name = &quot;xz-bigdata-springcloud-hbasequery&quot;)public interface HbaseFeign {    @ResponseBody    @RequestMapping(value=&quot;/hbase/search1&quot;, method=RequestMethod.GET)    public Set&lt;String&gt; search1(@RequestParam(name = &quot;table&quot;) String table,                               @RequestParam(name = &quot;rowkey&quot;) String rowkey);}</code></pre><h4 id="8、微服务手动部署"><a href="#8、微服务手动部署" class="headerlink" title="8、微服务手动部署"></a>8、微服务手动部署</h4><p><strong>Maven添加打包插件</strong></p><pre><code> &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;&lt;!--打包依赖的jar包--&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;outputDirectory&gt;${project.build.directory}/lib&lt;/outputDirectory&gt;                    &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt; &lt;!-- 表示是否不包含间接依赖的包 --&gt;                    &lt;stripVersion&gt;false&lt;/stripVersion&gt; &lt;!-- 去除版本信息 --&gt;                &lt;/configuration&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;copy-dependencies&lt;/id&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;!-- 拷贝项目依赖包到lib/目录下 --&gt;                            &lt;outputDirectory&gt;${project.build.directory}/jars&lt;/outputDirectory&gt;                            &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt;                            &lt;stripVersion&gt;false&lt;/stripVersion&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;!-- 打成jar包插件 --&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;                &lt;version&gt;2.4&lt;/version&gt;                &lt;configuration&gt;                    &lt;archive&gt;                        &lt;!--                        生成的jar中，不要包含pom.xml和pom.properties这两个文件                    --&gt;                        &lt;addMavenDescriptor&gt;false&lt;/addMavenDescriptor&gt;                        &lt;!-- 生成MANIFEST.MF的设置 --&gt;                        &lt;manifest&gt;                            &lt;!-- 为依赖包添加路径, 这些路径会写在MANIFEST文件的Class-Path下 --&gt;                            &lt;addClasspath&gt;true&lt;/addClasspath&gt;                            &lt;classpathPrefix&gt;jars/&lt;/classpathPrefix&gt;                            &lt;!-- jar启动入口类--&gt;                            &lt;mainClass&gt;com.cn.hbase.mr.HbaseMr&lt;/mainClass&gt;                        &lt;/manifest&gt;                        &lt;!--       &lt;manifestEntries&gt;                                   &amp;lt;!&amp;ndash; 在Class-Path下添加配置文件的路径 &amp;ndash;&amp;gt;                                   &lt;Class-Path&gt;&lt;/Class-Path&gt;                               &lt;/manifestEntries&gt;--&gt;                    &lt;/archive&gt;                    &lt;outputDirectory&gt;${project.build.directory}/&lt;/outputDirectory&gt;                    &lt;includes&gt;                        &lt;!-- 打jar包时，只打包class文件 --&gt;                        &lt;include&gt;**/*.class&lt;/include&gt;                        &lt;include&gt;**/*.properties&lt;/include&gt;                        &lt;include&gt;**/*.yml&lt;/include&gt;                    &lt;/includes&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;</code></pre><p>因为微服务<strong>依赖 xz_bigdata2</strong> 所以<strong>先打包xz_bigdata2</strong> </p><p><strong>修改配置文件</strong></p><pre><code>defaultZone: http://root:root@hadoop3:8761/eureka/</code></pre><p>将注册中心 IP 改为部署服务器的IP<br>微服务同理</p><p>上面给出的配置文件已经修改好了</p><p><strong>部署</strong></p><ol><li><strong>先部署Erueka服务中心</strong><br>新建<strong>/usr/chl/springcloud/eureka</strong></li></ol><p><img src="/medias/%E9%83%A8%E7%BD%B2%E5%9C%B0%E6%96%B9.PNG" alt="部署地方"></p><p>上传jars 和jar</p><p><img src="/medias/eureka.PNG" alt="eureka"></p><ol start="2"><li><strong>启动服务中心</strong><br>eureka服务注册中心启动</li></ol><pre><code>nohup java -cp /usr/chl/springcloud/eureka/xz_bigdata_springcloud_eureka-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.eureka.EurekaApplication &amp;</code></pre><p>查看日志</p><pre><code>tail -f nohup.out</code></pre><ol start="3"><li><strong>部署esquery</strong><br>esquery微服务启动</li></ol><pre><code>nohup java -cp /usr/chl/springcloud/esquery/xz_bigdata_springcloud_esquery-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.es.ESqueryApplication &amp;</code></pre><ol start="4"><li><strong>部署hbasequery</strong><br>hbasequery微服务启动</li></ol><pre><code>nohup java -cp /usr/chl/springcloud/hbasequery/xz_bigdata_springcloud_hbasequery-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.HbaseQueryApplication &amp;</code></pre><h4 id="9、执行-1"><a href="#9、执行-1" class="headerlink" title="9、执行"></a>9、执行</h4><ol><li><p>hadoop3:8002/hbase/getRelation?field=phone&amp;fieldValue=18609765012<br><img src="/medias/10.PNG" alt="1"></p></li><li><p>hadoop3:8002/hbase/search1?table=phone&amp;rowkey=18609765012<br><img src="/medias/20.PNG" alt="2"></p></li><li><p>hadoop3:8002/hbase/getHbase?table=send_mail&amp;rowkey=65497873@qq.com<br><img src="/medias/30.PNG" alt="3"></p></li><li><p>hadoop3:8002/hbase/getHbase?table=phone&amp;rowkey=18609765012<br><img src="/medias/40.PNG" alt="4"></p></li><li><p>hadoop3:8002/hbase/search/phone/18609765012<br><img src="/medias/5.PNG" alt="5"></p></li><li><p>hadoop3:8003/es/getAllCount<br><img src="/medias/6.PNG" alt="6"></p></li><li><p>hadoop3:8003/es/getBaseInfo<br><img src="/medias/7.PNG" alt="7"></p></li><li><p>hadoop3:8003/es/getLocus<br><img src="/medias/8.PNG" alt="8"></p></li><li><p>hadoop3:8003/es/group<br><img src="/medias/9.PNG" alt="9"></p></li></ol><h3 id="十三、附录"><a href="#十三、附录" class="headerlink" title="十三、附录"></a>十三、附录</h3><h4 id="1、测试数据"><a href="#1、测试数据" class="headerlink" title="1、测试数据"></a>1、测试数据</h4><p><strong>mail_source1_1111101.txt</strong></p><pre><code>000000000000011    000000000000011    23.000011    24.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300088    65497873@qq.com    1789090763    11111111@qq.com    1789097863    今天出去打球吗    send000000000000011    000000000000011    24.000011    25.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300085    65497873@qq.com    1789090764    22222222@qq.com    1789097864    今天出去打球吗    send000000000000011    000000000000011    23.000011    24.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300088    65497873@qq.com    1789090763    33333333@qq.com    1789097863    今天出去打球吗    send000000000000011    000000000000011    24.000011    25.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300085    65497873@qq.com    1789090764    44444444@qq.com    1789097864    今天出去打球吗    send000000000000000    000000000000000    23.000001    24.000001    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305988    1323243@qq.com    1789098763    43432543@qq.com    1789098863    今天出去打球吗    send000000000000000    000000000000000    24.000001    25.000001    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    1323243@qq.com    1789098764    43432543@qq.com    1789098864    今天出去打球吗    send000000000000000    000000000000000    23.000001    24.000001    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305988    1323243@qq.com    1789098763    43432543@qq.com    1789098863    今天出去打球吗    send000000000000000    000000000000000    24.000001    25.000001    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    1323243@qq.com    1789098764    43432543@qq.com    1789098864    今天出去打球吗    send</code></pre><p><strong>qq_source1_1111101.txt</strong></p><pre><code>000000000000000    000000000000000    23.000000    24.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305988    andiy    18609765432    judy            1789098762000000000000000    000000000000000    24.000000    25.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    andiy    18609765432    judy            1789098763000000000000000    000000000000000    23.000000    24.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305988    andiy    18609765432    judy            1789098762000000000000000    000000000000000    24.000000    25.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    andiy    18609765432    judy            1789098763000000000000011    000000000000011    23.000011    24.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300388    xz    18609765012    ls            1789000653000000000000011    000000000000011    24.000011    25.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300545    xz    18609765012    ls            1789000343000000000000011    000000000000011    23.000011    24.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300658    xz    18609765012    ls            1789000542000000000000011    000000000000011    24.000011    25.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300835    xz    18609765012    ls            1789000263000000000000011    000000000000011    23.000021    24.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557300388    xz    18609765016    ls            1789001653000000000000011    000000000000011    24.000021    25.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557302235    xz    18609765016    ls            1789001343000000000000011    000000000000011    23.000021    24.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557303658    xz    18609765016    ls            1789001542000000000000011    000000000000011    24.000021    25.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557303835    xz    18609765016    ls            1789001263000000000000011    000000000000011    23.000031    24.000041    4c-6f-c7-3d-a4-3d    9g-gd-3h-3k-ld-3f    32109246    1557300001    xz    18609765014    ls            1789050653000000000000011    000000000000011    24.000031    25.000051    7c-8e-d4-a6-3d-5c    54-hg-gi-yx-ef-ge    32109246    1557300005    xz    18609765015    ls            1789070343000000000000011    000000000000011    23.000031    24.000061    8c-g1-ed-7b-5f-1b    47-fy-vv-hs-ue-fd    32109246    1557300008    xz    18609765017    ls            1789080542000000000000011    000000000000011    24.000031    25.000071    0c-76-2a-b1-3c-1a    f5-nw-hf-ud-ht-ea    32109246    1557300115    xz    18609765010    ls            1789082263</code></pre><p><strong>wechat_source1_1111101.txt</strong></p><pre><code>000000000000000    000000000000000    23.000000    24.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305988    andiy    18609765432    judy            1789098762000000000000000    000000000000000    24.000000    25.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    andiy    18609765432    judy            1789098763000000000000000    000000000000000    23.000000    24.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305988    andiy    18609765432    judy            1789098762000000000000000    000000000000000    24.000000    25.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    andiy    18609765432    judy            1789098763000000000000011    000000000000011    23.000011    24.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300388    xz    18609765012    ls            1789000653000000000000011    000000000000011    24.000011    25.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300545    xz    18609765012    ls            1789000343000000000000011    000000000000011    23.000011    24.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300658    xz    18609765012    ls            1789000542000000000000011    000000000000011    24.000011    25.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300835    xz    18609765012    ls            1789000263000000000000011    000000000000011    23.000021    24.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557300388    xz    18609765016    ls            1789001653000000000000011    000000000000011    24.000021    25.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557302235    xz    18609765016    ls            1789001343000000000000011    000000000000011    23.000021    24.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557303658    xz    18609765016    ls            1789001542000000000000011    000000000000011    24.000021    25.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557303835    xz    18609765016    ls            1789001263000000000000011    000000000000011    23.000031    24.000041    4c-6f-c7-3d-a4-3d    9g-gd-3h-3k-ld-3f    32109246    1557300001    xz    18609765014    ls            1789050653000000000000011    000000000000011    24.000031    25.000051    7c-8e-d4-a6-3d-5c    54-hg-gi-yx-ef-ge    32109246    1557300005    xz    18609765015    ls            1789070343000000000000011    000000000000011    23.000031    24.000061    8c-g1-ed-7b-5f-1b    47-fy-vv-hs-ue-fd    32109246    1557300008    xz    18609765017    ls            1789080542000000000000011    000000000000011    24.000031    25.000071    0c-76-2a-b1-3c-1a    f5-nw-hf-ud-ht-ea    32109246    1557300115    xz    18609765010    ls            1789082263</code></pre><h4 id="2、Kafka"><a href="#2、Kafka" class="headerlink" title="2、Kafka"></a>2、Kafka</h4><p>创建topic，1个副本3个分区<br>kafka-topics –zookeeper hadoop1:2181 –topic chl_test7 –create –replication-factor 1 –partitions 3</p><p><strong>删除topic</strong><br>kafka-topics –zookeeper hadoop1:2181 –delete –topic chl_test7</p><p><strong>列出所有的topic</strong><br>kafka-topics –zookeeper hadoop1:2181 –list</p><p><strong>消费</strong><br>kafka-console-consumer –bootstrap-server hadoop1:9092 –topic chl_test7 –from-beginning</p><h4 id="3、kafka2es"><a href="#3、kafka2es" class="headerlink" title="3、kafka2es"></a>3、kafka2es</h4><p><strong>启动sparkstreaming任务</strong></p><pre><code>spark-submit --master yarn-cluster --num-executors 1 --driver-memory 500m --executor-memory 1g --executor-cores 1 --jars $(echo /usr/chl/spark7/jars/*.jar | tr &#39; &#39; &#39;,&#39;) --class com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar chl_test7 chl_test7</code></pre><pre><code>spark-submit --master yarn-cluster    //集群启动--num-executors 1        //分配多少个进程--driver-memory 500m  //driver内存--executor-memory 1g //进程内存--executor-cores 1       //开多少个核，线程--jars $(echo /usr/chl/spark8/jars/*.jar | tr &#39; &#39; &#39;,&#39;) //加载jar--class com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming //执行类 /usr/chl/spark8/xz_bigdata_spark-1.0-SNAPSHOT.jar //包的位置</code></pre><h4 id="4、Yarn"><a href="#4、Yarn" class="headerlink" title="4、Yarn"></a>4、Yarn</h4><p><strong>将yarn的执行日志输出</strong><br>yarn logs -applicationId application_1561627166793_0002 &gt; log.log</p><p><strong>查看日志</strong><br>more log.log</p><p>cat log.log</p><h4 id="5、CDH的7180打不开"><a href="#5、CDH的7180打不开" class="headerlink" title="5、CDH的7180打不开"></a>5、CDH的7180打不开</h4><p><strong>查看cloudera-scm-server状态</strong><br>service cloudera-scm-server status</p><p><strong>查看cloudera-scm-server 日志</strong><br>cat /var/log/cloudera-scm-server/cloudera-scm-server.log</p><p><strong>重启cloudera-scm-server</strong><br>service cloudera-scm-server restart</p><h4 id="6、CDH的jdk设置—重要"><a href="#6、CDH的jdk设置—重要" class="headerlink" title="6、CDH的jdk设置—重要"></a>6、CDH的jdk设置—重要</h4><p><strong>/usr/local/jdk1.8</strong></p><h4 id="7、预警"><a href="#7、预警" class="headerlink" title="7、预警"></a>7、预警</h4><pre><code>spark-submit --master local[1] --num-executors 1 --driver-memory 300m --executor-memory 500m --executor-cores 1 --jars $(echo /usr/chl/spark7/jars/*.jar | tr &#39; &#39; &#39;,&#39;) --class com.hsiehchou.spark.streaming.kafka.warn.WarningStreamingTask /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</code></pre><h4 id="8、Kibana的DEV-Tools"><a href="#8、Kibana的DEV-Tools" class="headerlink" title="8、Kibana的DEV Tools"></a>8、Kibana的DEV Tools</h4><pre><code>GET _search{  &quot;query&quot;: {    &quot;match_all&quot;: {}  }}GET  _cat/indicesDELETE tanslator_test1111DELETE qqDELETE wechatDELETE mailGET wechatGET mailGET _searchGET mail/_searchGET mail/_mappingPUT mailPUT mail/mail/_mapping{  &quot;_source&quot;: {    &quot;enabled&quot;: true  },  &quot;properties&quot;: {    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;imsi&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;longitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;latitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;phone_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_number&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;collect_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;send_mail&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;send_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;accept_mail&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;accept_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;mail_content&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;mail_type&quot;:{&quot;type&quot;: &quot;keyword&quot;},     &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;table&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;absolute_filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}  }}GET qq/_searchGET qq/_mappingPUT qqPUT qq/qq/_mapping{  &quot;_source&quot;: {    &quot;enabled&quot;: true  },  &quot;properties&quot;: {    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;imsi&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;longitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;latitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;phone_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_number&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;collect_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;phone&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;object_username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;send_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;accept_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;message_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;table&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;absolute_filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}  }}GET wechat/_searchGET wechat/_mappingPUT wechatPUT wechat/wechat/_mapping{  &quot;_source&quot;: {    &quot;enabled&quot;: true  },  &quot;properties&quot;: {    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;imsi&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;longitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;latitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;phone_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_number&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;collect_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;phone&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;object_username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;send_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;accept_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;message_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;table&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;absolute_filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}  }}</code></pre><h4 id="9、Hive"><a href="#9、Hive" class="headerlink" title="9、Hive"></a>9、Hive</h4><p><strong>kafka写入hive</strong></p><pre><code>spark-submit --master local[1] --num-executors 1 --driver-memory 300m --executor-memory 500m --executor-cores 1 --jars $(echo /usr/chl/spark7/jars/*.jar | tr &#39; &#39; &#39;,&#39;) --class com.hsiehchou.spark.streaming.kafka.kafka2hdfs.Kafka2HiveTest /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</code></pre><pre><code>show tables;hdfs dfs -ls /apps/hive/warehouse/externalhdfs dfs -rm -r /apps/hive/warehouse/external/maildrop table mail;desc qq;select * from qq limit 1;注意了：cdh的hive版本跟其对应的spark版本不一致的话此处执行不了select count(*) from qq;</code></pre><p><strong>合并小文件</strong></p><pre><code>crontab -e0 1 * * * spark-submit --master local[1] --num-executors 1 --driver-memory 300m --executor-memory 500m --executor-cores 1 --jars $(echo /usr/chl/spark7/jars/*.jar | tr &#39; &#39; &#39;,&#39;) --class com.hsiehchou.spark.streaming.kafka.kafka2hdfs.CombineHdfs /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</code></pre><p><img src="/medias/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1crontab.PNG" alt="定时任务crontab"></p><h4 id="10、Zookeeper"><a href="#10、Zookeeper" class="headerlink" title="10、Zookeeper"></a>10、Zookeeper</h4><p><strong>启动zookeeper客户端</strong><br>zookeeper-client</p><p><strong>清除消费者</strong><br>rmr /consumers/WarningStreamingTask2/offsets</p><p>rmr /consumers/Kafka2HiveTest/offsets</p><p>rmr /consumers/DataRelationStreaming1/offsets</p><h4 id="11、Hbase"><a href="#11、Hbase" class="headerlink" title="11、Hbase"></a>11、Hbase</h4><pre><code>spark-submit --master local[1] --num-executors 1 --driver-memory 500m --executor-memory 1g --executor-cores 1 --jars $(echo /usr/chl/spark7/jars/*.jar | tr &#39; &#39; &#39;,&#39;) --class com.hsiehchou.spark.streaming.kafka.kafka2hbase.DataRelationStreaming /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</code></pre><pre><code>hbase shelllistcreate &#39;t1&#39;,&#39;cf&#39;desc &#39;t1&#39;put &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,&#39;cf:qq&#39;,&#39;66666666&#39;put &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,&#39;cf:weixin&#39;,&#39;weixin1&#39;put &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,&#39;cf:mail&#39;,&#39;66666@qq.com&#39;scan &#39;t1&#39;将表变成多版本alter &#39;t1&#39;,{NAME=&gt;&#39;cf&#39;,VERSIONS=&gt;50}put &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,&#39;cf:qq&#39;,&#39;77777777&#39;get &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,{COLUMN=&gt;&#39;cf&#39;,VERSIONS=&gt;10}put &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,&#39;cf:qq&#39;,&#39;55555555&#39;put &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,&#39;cf:qq&#39;,&#39;88888888&#39;,1290300544执行DataRelationStreamingscan &#39;test:relation&#39;get &#39;test:username&#39;,&#39;andiy&#39;scan &#39;test:relation&#39;mail 改mac 邮箱get  &#39;test:relation&#39;,&#39;&#39;,{COLUMN=&gt;&#39;cf&#39;,VERSIONS=&gt;10}disable &#39;test:imei&#39;drop &#39;test:imei&#39;disable &#39;test:imsi&#39;drop &#39;test:imsi&#39;disable &#39;test:phone&#39;drop &#39;test:phone&#39;disable &#39;test:phone_mac&#39;drop &#39;test:phone_mac&#39;disable &#39;test:relation&#39;drop &#39;test:relation&#39;disable &#39;test:send_mail&#39;drop &#39;test:send_mail&#39;disable &#39;test:username&#39;drop &#39;test:username&#39;</code></pre><h4 id="12、SpringCloud"><a href="#12、SpringCloud" class="headerlink" title="12、SpringCloud"></a>12、SpringCloud</h4><p><strong>eureka服务注册中心启动</strong></p><pre><code>nohup java -cp /usr/chl/springcloud/eureka/xz_bigdata_springcloud_eureka-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.eureka.EurekaApplication &amp;</code></pre><p><strong>查看日志</strong></p><pre><code>tail -f nohup.out</code></pre><p><strong>esquery微服务启动</strong></p><pre><code>nohup java -cp /usr/chl/springcloud/esquery/xz_bigdata_springcloud_esquery-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.es.ESqueryApplication &amp;</code></pre><p><strong>hbasequery微服务启动</strong></p><pre><code>nohup java -cp /usr/chl/springcloud/hbasequery/xz_bigdata_springcloud_hbasequery-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.HbaseQueryApplication &amp;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据项目 </tag>
            
            <tag> 网络日志分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Eureka服务注册中心启动</title>
      <link href="/2019/07/25/eureka-fu-wu-zhu-ce-zhong-xin-qi-dong/"/>
      <url>/2019/07/25/eureka-fu-wu-zhu-ce-zhong-xin-qi-dong/</url>
      
        <content type="html"><![CDATA[<p><strong>nohup启动（nohup不间断运行）</strong></p><pre><code>nohup java -cp /usr/chl/springcloud/eureka/xz_bigdata_springcloud_eureka-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.eureka.EurekaApplication &amp;</code></pre><p><strong>查看日志</strong></p><pre><code>tail -f nohup.out</code></pre>]]></content>
      
      
      <categories>
          
          <category> SpringCloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringCloud </tag>
            
            <tag> Eureka </tag>
            
            <tag> nohup </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ZooKeeper清除已有的消费的offsets</title>
      <link href="/2019/07/23/zookeeper-qing-chu-yi-you-de-xiao-fei-de-offsets/"/>
      <url>/2019/07/23/zookeeper-qing-chu-yi-you-de-xiao-fei-de-offsets/</url>
      
        <content type="html"><![CDATA[<p><strong>CDH集群中的ZooKeeper</strong></p><p><strong>启动ZooKeeper客户端</strong><br>zookeeper-client</p><p><strong>清除消费者</strong></p><p><strong>例如</strong><br>rmr /consumers/WarningStreamingTask/offsets</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> ZooKeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkStreaming+Kafka的两种模式Receiver模式和Direct模式</title>
      <link href="/2019/07/19/sparkstreaming-kafka-de-liang-chong-mo-shi-receiver-mo-shi-he-direct-mo-shi/"/>
      <url>/2019/07/19/sparkstreaming-kafka-de-liang-chong-mo-shi-receiver-mo-shi-he-direct-mo-shi/</url>
      
        <content type="html"><![CDATA[<h3 id="SparkStreming-Kafka-Receiver模式理解"><a href="#SparkStreming-Kafka-Receiver模式理解" class="headerlink" title="SparkStreming + Kafka Receiver模式理解"></a>SparkStreming + Kafka Receiver模式理解</h3><p><img src="/medias/kafka%E7%9A%84receiver%E6%A8%A1%E5%BC%8F.PNG" alt="Kafka的Receiver模式"></p><h4 id="Receiver模式理解"><a href="#Receiver模式理解" class="headerlink" title="Receiver模式理解"></a>Receiver模式理解</h4><p>在SparkStreaming程序运行起来后，Executor中会有Receiver Tasks接收Kafka推送过来的数据。数据会被持久化，默认级别为MEMORY_AND_DISK_SER_2,这个级别也可以修改。Receiver Task对接收过来的数据进行存储和备份，这个过程会有节点之间的数据传输。备份完成后去ZooKeeper中更新消费偏移量，然后向Driver中的Receiver Tracker汇报数据的位置。最后Driver根据数据本地化将Task分发到不同节点上执行。</p><h4 id="Receiver模式中存在的问题"><a href="#Receiver模式中存在的问题" class="headerlink" title="Receiver模式中存在的问题"></a>Receiver模式中存在的问题</h4><p>当Driver进程挂掉后，Driver下的Executor都会被杀掉，当更新完ZooKeeper消费偏移量的时候，Driver如果挂掉了，就会存在找不到数据的问题，相当于丢失数据。</p><h4 id="如何解决这个问题？"><a href="#如何解决这个问题？" class="headerlink" title="如何解决这个问题？"></a>如何解决这个问题？</h4><p>开启WAL(write ahead log)预写日志机制,在接受过来数据备份到其他节点的时候，同时备份到HDFS上一份（我们需要将接收来的数据的持久化级别降级到MEMORY_AND_DISK），这样就能保证数据的安全性。不过，因为写HDFS比较消耗性能，要在备份完数据之后才能进行更新ZooKeeper以及汇报位置等，这样会增加job的执行时间，这样对于任务的执行提高了延迟度。</p><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><ol><li>开启WAL之后，接受数据级别要降级，有效率问题</li><li>开启WAL要checkpoint</li><li>开启WAL(write ahead log),往HDFS中备份一份数据</li></ol><h3 id="SparkStreming-Kafka-Receiver模式理解-1"><a href="#SparkStreming-Kafka-Receiver模式理解-1" class="headerlink" title="SparkStreming + Kafka Receiver模式理解"></a>SparkStreming + Kafka Receiver模式理解</h3><p><img src="/medias/kafka%E7%9A%84direct%E6%A8%A1%E5%BC%8F.PNG" alt="Kafka的Direct模式"></p><ol><li>简化数据处理流程</li><li>自己定义offset存储，保证数据0丢失，但是会存在重复消费问题。（解决消费等幂问题）</li><li>不用接收数据，自己去Kafka中拉取</li></ol>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Kafka </tag>
            
            <tag> SparkStreaming </tag>
            
            <tag> Receiver </tag>
            
            <tag> Direct </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch的9200端口和9300端口的区别</title>
      <link href="/2019/07/18/elasticsearch-de-9200-duan-kou-he-9300-duan-kou-de-qu-bie/"/>
      <url>/2019/07/18/elasticsearch-de-9200-duan-kou-he-9300-duan-kou-de-qu-bie/</url>
      
        <content type="html"><![CDATA[<p>9200端口作为HTTP协议，<strong>主要用于外部通讯</strong></p><p>9300端口作为TCP协议，jar之间就是通过 <strong>TCP协议通讯</strong></p><p><strong>ES集群之间是通过9300端口进行通讯</strong></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> ElasticSearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch集群配置</title>
      <link href="/2019/07/06/elasticsearch-ji-qun-an-zhuang-pei-zhi/"/>
      <url>/2019/07/06/elasticsearch-ji-qun-an-zhuang-pei-zhi/</url>
      
        <content type="html"><![CDATA[<p><strong>安装Elasticsearch</strong><br>mkdir /opt/software/elasticsearch/data/</p><p>mkdir /opt/software/elasticsearch/logs/</p><p>chmod 777 /opt/software/elasticsearch/data/</p><p>useradd elasticsearch<br>passwd elasticsearch</p><p>chown -R elasticsearch elasticsearch/</p><p><strong>vim /etc/security/limits.conf</strong><br>添加如下内容:<br><code>*</code> <strong>soft nofile 65536</strong><br><code>*</code> <strong>hard nofile 131072</strong><br><code>*</code> <strong>soft nproc 2048</strong><br><code>*</code> <strong>hard nproc 4096</strong></p><p>进入limits.d目录下修改配置文件<br><strong>vim /etc/security/limits.d/90-nproc.conf</strong></p><p>修改如下内容：<br><strong>soft nproc 4096（修改为此参数，6版本的默认就是4096）</strong></p><p>修改配置sysctl.conf<br><strong>vim /etc/sysctl.conf</strong></p><p>添加下面配置：<br><strong>vm.max_map_count=655360</strong></p><p>并执行命令：<br><strong>sysctl -p</strong></p><p><strong>hadoop1的conf配置</strong><br><strong>elasticsearch.yml</strong></p><pre><code>cluster.name: xz_esnode.name: node-1node.master: truenode.data: truepath.data: /opt/software/elasticsearch/datapath.logs: /opt/software/elasticsearch/logsbootstrap.memory_lock: falsebootstrap.system_call_filter: falsenetwork.host: 192.168.116.201discovery.zen.ping.unicast.hosts: [&quot;hadoop1&quot;, &quot;hadoop2&quot;, &quot;hadoop3&quot;]</code></pre><p><strong>jvm.options</strong><br>修改下<br>-Xms64m<br>-Xmx64m</p><p><strong>hadoop2的conf配置</strong><br><strong>elasticsearch.yml</strong></p><pre><code>cluster.name: xz_esnode.name: node-2node.master: falsenode.data: truepath.data: /opt/software/elasticsearch/datapath.logs: /opt/software/elasticsearch/logsbootstrap.memory_lock: falsebootstrap.system_call_filter: falsenetwork.host: 192.168.116.202discovery.zen.ping.unicast.hosts: [&quot;hadoop1&quot;, &quot;hadoop2&quot;, &quot;hadoop3&quot;]</code></pre><p><strong>jvm.options</strong><br>修改下<br>-Xms64m<br>-Xmx64m</p><p><strong>hadoop3的conf配置</strong><br><strong>elasticsearch.yml</strong></p><pre><code>cluster.name: xz_esnode.name: node-3node.master: falsenode.data: truepath.data: /opt/software/elasticsearch/datapath.logs: /opt/software/elasticsearch/logsbootstrap.memory_lock: falsebootstrap.system_call_filter: falsenetwork.host: 192.168.116.203discovery.zen.ping.unicast.hosts: [&quot;hadoop1&quot;, &quot;hadoop2&quot;, &quot;hadoop3&quot;]</code></pre><p><strong>jvm.options</strong><br>修改下<br>-Xms64m<br>-Xmx64m</p><p><strong>Kibana的conf配置</strong></p><p><strong>kibana.yml</strong></p><pre><code>server.port: 5601server.host: &quot;192.168.116.202&quot;elasticsearch.url: &quot;http://192.168.116.201:9200&quot;</code></pre><p><strong>运行Elasticsearch</strong><br>cd /opt/software/elasticsearch<br>su elasticsearch<br>bin/elasticsearch &amp;</p><p><strong>运行Kibana</strong><br>cd /opt/software/kibana/<br>bin/kibana &amp;</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> ElasticSearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一万小时天才理论（阅读笔记）</title>
      <link href="/2019/06/19/yi-wan-xiao-shi-tian-cai-li-lun/"/>
      <url>/2019/06/19/yi-wan-xiao-shi-tian-cai-li-lun/</url>
      
        <content type="html"><![CDATA[<p><strong>一万小时天才理论（阅读笔记）</strong></p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a><strong>前言</strong></h3><p>上大一时，我就听人讲过一个人只要在一个领域专攻1万小时，那么这个人便会成为这个领域的专家，把时间分配到每天3小时，需要10年，每天8小时（当然这很不现实，不可能工作的所有时间都花在学习上，还有周末休息啥的），原则上4年就够了。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>1、中国目前发展飞速，想要在这个大的环境中生存，必须靠自己，靠别人是永远靠不住的，因为你永远不知道别人办事的进度，可能什么都没做，也可能做的差不多了。总之，相信自己，依靠自己，才能生存。</p><p>2、遗传因素对我们对影响不大，所以，还是需要靠我们后天的学习和发展才能壮大自己。</p><p>3、犯错不用怕，怕的是犯错了不去找解决的方法，这样永远不会进步的。我们可以从犯错中总结有效的解决方法，发现问题，解决问题。</p><p>4、学会去爱做一件事或者很多事，当自己爱上了做这些事，便会发现用的这些时间并不是浪费，花得值得，养成一种习惯，成为这个领域的专家。6分钟的时间可以学到很多，只要自己坚持去做，自己会在不知不觉中受益。</p><h3 id="第一部分-精深"><a href="#第一部分-精深" class="headerlink" title="第一部分 精深"></a><strong>第一部分 精深</strong></h3><h3 id="01-冒牌哈佛"><a href="#01-冒牌哈佛" class="headerlink" title="01 冒牌哈佛"></a><strong>01 冒牌哈佛</strong></h3><p><strong>谁也不能随随便便成功，它来自彻底的自我管理和毅力</strong> 。 ——哈佛图书馆训言</p><p><strong>犯错让你更聪明</strong> 。                                                               ——德国寓言</p><p>我们需要去完成我们和别人都认为不可能完成的任务，突破自己的极限。</p><p>在不断精深练习中培养自己的技能，让这种动作成为自己的习惯，不需要通过大脑的深度思考，当遇到这种情况，我们不需要考虑就能自动的进行相应的操作。这跟人类的潜意识的养成相似，也可以说一样的（我是这么认为的）。</p><p>需要 <strong>不断地进行正确的精深练习</strong> ，学习到独特的体会和技巧，让其成为自己的一种能力，潜意识就可以办到。</p><h3 id="02-才能细胞"><a href="#02-才能细胞" class="headerlink" title="02 才能细胞"></a><strong>02 才能细胞</strong></h3><p>髓鞘质是在我们不断地进行精深练习后不断地增加厚度、变厚。改善自己的神经回路，优化自己的神经回路。</p><p>需要 <strong>有明确的目标</strong> 、 <strong>重视错误的练习</strong> ，这样我们便可以 <strong>不断地去发现问题</strong> 、 <strong>处理问题</strong> 。在这个过程中我们需要 <strong>保持激情和坚持</strong> ，只有这样才能 <strong>让自己的髓鞘质进化到巅峰水平</strong> 。</p><p>人的各种行动都是通过大脑发出的指令信号来进行的，进行各种预判，对可能发生的情况进行预估，做出对应的对策。当自己的经验越多，自己会不知不觉的完成很多的事情，自己都没有察觉自己做了什么。髓鞘质对于大脑的神经信息处理非常的关键，它能把握住时间点， <strong>该出手时就出手</strong> ，控制大脑传输速度，让各种信号准时到达。</p><p>髓鞘质无法逆转，一旦技能回路包裹上了绝缘体，那将无法去除。髓鞘质只在乎我们自己做了什么，它把这些技能更好的发扬光大，让我们充分利用。</p><p>任何领域的任何专家都要经过10000小时专心致志地学习。</p><p>我们通过不断地精深练习，达到1万小时后，便能成为这个领域的专家，人才的培养最好从小时候就开始，这个幼时的不断地练习会让孩子一生受益（我们人人都可以这样，但必须投入足够的时间去做）。</p><h3 id="03-天降人才"><a href="#03-天降人才" class="headerlink" title="03 天降人才"></a><strong>03 天降人才</strong></h3><p>我们做任何事情都需要坚持到最后，不能快到终点了自己就放弃了，只要熬过最后的这一分钟，我们便能做成这件事，实现自己的梦想。</p><p>一个人的成功离不开他 <strong>自身的努力</strong> ，我们往往看到的是那些成功人士眼前的辉煌，但是没法看到他们背后付出的辛酸和泪水。 <strong>一个人的成功离不开自己默默地付出</strong> ，需要 <strong>自己一个人慢慢地前行</strong> ，探索未知的领域，当自己慢慢摸透了这个未知的领域，自己便可以翱翔其中。</p><p> 我们的髓鞘质不在乎我们是谁，只关心我们做了什么，把这些做的东西经过我们的日积月累慢慢地形成我们的技能。</p><p><strong>文艺复兴时期伟大的艺术家的特点：</strong></p><p>每个人都把大部分的青春岁月投入在精深练习上，锤炼和优化技能回路，纠错，竞争，然后进步。每个人都参与了这副任何人都能创作的最伟大的艺术作品：构建自己的才能。</p><p><strong>造成青少年做出错误的决定的原因：</strong></p><p>冲动行为发生时，神经回路不会马上去阻止这个冲动行为，尽管它是可以的，青少年需要自己花时间去体会。</p><p>我们需要不断地学习，学习经验，因为髓鞘质也是会消亡的，我们需要生成新的髓鞘质来对已经消亡的髓鞘质做个补充。</p><p>读了本章，深刻体会到了髓鞘质对我们人类潜力的巨大作用，它在更新，也在消亡，但是我们需要不断地学习、不断地强化我们的髓鞘质（就是不断地进行精深练习），那我们的能力便会飞快的提升，从而成为一个领域的专家。</p><h3 id="04-三大秘技"><a href="#04-三大秘技" class="headerlink" title="04 三大秘技"></a><strong>04 三大秘技</strong></h3><p><strong>屡败屡战。屡战屡败。败了更好</strong> 。   ——塞缪尔·贝克特(Samuel Beckett)</p><p><strong>哇塞效应</strong></p><p><strong>组块化：</strong></p><p>整体吸收：花时间观察或者倾听自己想学习的技能，将技能具体化。</p><p>学会最高效地去模仿别人，把握住技巧，将学习的东西拆分成一个个的小模块，慢慢练习，找到自己的感觉，渐渐地就变成了自己的技能了。</p><p><strong>重复练习：</strong></p><p>我们不管干什么事，只要长时间不练习，我们便会忘记，这是我们人类的记忆规律。所以，想要掌握某一技能，就必须不间断地练习，每天练习2小时也好，只要不长时间地隔1个月不练习就行了。</p><p><strong>尝试体会：</strong></p><p>我们需要 <strong>自己切身去体会做</strong> ，不能看着别人感觉自己完成不了，我们需要去挑战自己，自己去体会，可能别人做不了的任务，而你却可以。有了这种尝试，我们就不再害怕，便对这些事感觉很平常。</p><p>除了亲自体会外，我们还需要自己去不断地练习，找到那种感觉，就是将那种要完成的任务牢牢地掌握在自己手心里。</p><p>注意力、连接、建立、完整的、警觉、关注、错误、重复、疲劳、边缘、唤醒</p><h3 id="第二部分-激情"><a href="#第二部分-激情" class="headerlink" title="第二部分 激情"></a><strong>第二部分 激情</strong></h3><h3 id="05-信号"><a href="#05-信号" class="headerlink" title="05 信号"></a><strong>05 信号</strong></h3><p>1、技能学习需要精深练习。</p><p>2、精深练习需要精力、激情和投入。</p><pre><code> 激情的存在就像为我们提供动力火箭的原料，维持我们不停地重新开始，锻炼技巧，不断进步。</code></pre><p>3、一次突破性的胜利，接着就会涌现出大规模的&quot;人才井喷&quot;。</p><p>4、总之，精深练习需要时间（1万小时的练习）。</p><p>我们意识到&quot;我就想成为那样的人&quot;的时刻，这就是激情工作的原理。只有激情才能刺激自己不断地突破极限。</p><p><strong>小小念头，影响深远</strong></p><p>短期承诺、中期承诺、长期承诺</p><p>研究表明，长期承诺的孩子，哪怕每周只花20分钟进行练习，也比那些花1个半小时的孩子进步神速。长期承诺的孩子充分的练习，技能已经出神入化了。</p><p>我想成为那样的人，可能就因为这个小小的念头，让我们改变一生，让我们走向成功。</p><p><strong>启动信号</strong></p><p>我们如果能够放弃眼前的舒适，去熬那些艰难的时光的话，就是让我们变成一个自己想要变成的人，自己为之不断地奋斗。</p><p>每个强烈的刺激人大脑神经的能让人立马行动信号指引人去做超级有意义的事情。这针对未来的归属感。</p><p>书上讲缺乏安全感会激发和引导自己的大脑去解决危险，处理生命中的可能性。</p><p>生活的不再安全让那些人开启了古老的自我保护的进化开关，从而让他们倾入时间和精力去耕耘事业，渐渐地，他们完成了一万小时的精深练习，养成了各自的才能。</p><p><strong>统治所有人的普遍原则：</strong></p><p>1、才能需要精深练习</p><p>2、精深练习需要充分的能量</p><p>3、某些信号会触发巨大能量的迸发</p><p>研究表明，往往家中老四是成就最高的。造成这种状况的原因是，家中老四，生得较晚，因为有兄长在前面作为榜样，往往会不断地去追赶自己的兄长们，长时间的练习形成了一种习惯，从而让自己最终走向人生的高处。</p><p><strong>好运临门</strong></p><p>稀缺感、归属感、安全感，通过激发人类自己的欲望，不断地针对这个去刺激自己。激情需要长久， <strong>长期的激情</strong> ， <strong>不断地去向自己地目标奋斗</strong> 。</p><h3 id="06-疯狂地海岛"><a href="#06-疯狂地海岛" class="headerlink" title="06 疯狂地海岛"></a><strong>06 疯狂地海岛</strong></h3><p><strong>打破保守，走出去，让激情之火不灭，永远保持</strong> ，这在开始的时候是非常困难的，但是一旦自己习惯了这种激情，便永远浇不灭了。</p><p>&quot;嘿！你也可以&quot;</p><p>相信自己，不停地去练习，一定会有很大收获的。</p><p><strong>激情的语言</strong></p><p>那些 <strong>鼓励的语言是我们前进的最大的动力</strong> 。（番外，这里让我想起了我高二的那个英语老师，我们英语差的从来不鼓励，破骂一顿，好像我们欠了她一个亿，那些英语好的基础好，表扬得不得了，醉了，反正我周围的同学都讨厌她；反之，我高二的物理老师，不断地鼓励我们，我高中物理总拿A（高三还进入了省级的物理竞赛，当然了省考特别难，什么都不会，写了点字也拿了个江苏省物理竞赛三等奖），而且当时我们班物理在整个理科都是佼佼者，有时候平均分超其他班20-30分，是不是有点过，这样的好的情况到了高三换了老师就没了，而且我们当时物理竞赛去省赛的5个人，其中4个都是我们高二物理老师教过的），我们这位物理老师，只要考好了，就有电影看，极大地刺激了我们的学习的动力。</p><p>激励性的语言是鼓励人们争取不断进步的语言，让我们向希望、梦想不断的前进。</p><p>精深练习需要深入认真的工作以及热情的劲头，集中我们的精力，然后慢慢进步。</p><p>赞人们勤奋的语句之所以有效，是因为它直达学习的核心，而想要点燃激情，没有比这更强大的了。</p><h3 id="07-点燃明灯"><a href="#07-点燃明灯" class="headerlink" title="07 点燃明灯"></a><strong>07 点燃明灯</strong></h3><p><strong>教育不是在填坑，而是点燃照明之火。</strong> ——叶芝</p><p><strong>X一代的荒谬念头</strong></p><p>KIPP这所学校的诞生也是原来两个创始人的一个奇特的想法，不再与教育系统抗衡，而是打算开办一所自己的学校。刚开始的一两年学校办的整体效果并不怎么好，但是，突然间，学生都变得非常优秀，我个人认为这是哲学上的量变向质变的转化，学生长时间的培养好的习惯，培养他们的思维，让他们能有自己的独立的想法，能安心做任何事情，学生成绩的提高就是KIPP学校使用这种模式的见证。</p><p>努力培养学生的整体素质，而不单纯是为了提高学生的学习成绩。学生做的任何一件事都会与其它事情有关联，这便让学生养成随时思考的好习惯，当学生独立生活时，他们便日子过得很滋润。</p><p>培养学生，一旦学生懂了，基本不需要老师去教了，自己会去学习的，所以KIPP就是这样的，为的是让学生终生受益，而不是为的只是学习成绩的提高。</p><h3 id="第三部分-伯乐"><a href="#第三部分-伯乐" class="headerlink" title="第三部分 伯乐"></a><strong>第三部分 伯乐</strong></h3><h3 id="08-伯乐的武器"><a href="#08-伯乐的武器" class="headerlink" title="08 伯乐的武器"></a><strong>08 伯乐的武器</strong></h3><p>伯乐，自古以来就流传很久，古传是伯乐相马，对马的研究非常出色，优质的马在他眼中是逃不掉的。古代就已经将伯乐相马比喻善于识别人才，爱惜人才。</p><p>一个好的伯乐是能找出自己要看的人的任何优点包括缺点，一眼就能看出来。长处、短处都能识别，优秀的人才在这里能够展现雄姿。</p><p>该怎么做，怎么做，何时强化。这样，不是那样。</p><p>上面那句红色的是著名的约翰·伍顿教练教他的学员的原则，先示范正确的动作，后示范错误的动作，最后再示范正确的动作。教学员该做什么，哪怕穿袜子这种简单的他都教学员，让其不容易起水泡。事实证明，他的教得非常得成功。</p><p>那些KEEP项目的人将其用在了阅读上，效果非常明显，还获得了格威文美尔奖。</p><p><strong>点燃热爱的火花</strong></p><p>一个人爱上做一件事是很难的，如果让一个人爱上一个事业是更难的，在打拼的过程中会遇到各种的磨难，克服它们是对我们的一种考验。</p><p>约翰·伍顿使用了人才理论中精深练习的部分，提供知识，纠正错误，加强技能回路。而玛丽对付的是激情部分，利用情绪开关，在邮箱中加满爱和动机。他们都取得了成功，因为髓鞘质回路不仅需要精深练习，也需要激情；他们的成功真实反应了天才理论的本身。</p><h3 id="09-伯乐的一万小时"><a href="#09-伯乐的一万小时" class="headerlink" title="09 伯乐的一万小时"></a><strong>09 伯乐的一万小时</strong></h3><p><strong>&quot;我可不是为了钱，我只是在做自己喜欢的事情。当我还是个孩子的时候我就梦想成为奥运冠军。&quot;</strong>                                               ——迈克尔·菲尔普斯</p><p><strong>教师的影响是永恒的；他永远无法知道自己的影响有多深远。</strong>      ——亨利·亚当斯</p><p><strong>教师的四大优势</strong></p><p>一个好的老师是关心学生的一言一行的，而且通过这种关心，可以利用他们老师自己对该课题已有的深刻的理解，捕捉到学生在技能学习道路上碰到的障碍，以及摸索过程中难以形容的状态，然后按照已经设定的目标与学生进行沟通。</p><p>老师是学生的学习导师，也是学生的人生导师，对学生前进的路起着向导的作用。这便要求导师有非常优秀的洞察力，能够找出学生目前的困难的解决方案的突破口，给出正确的信号，帮助学生达到真正的目标，并反复这个过程。</p><p><strong>优势一：知识矩阵 —— 伯乐的杀手锏</strong></p><p>知识矩阵是教练老师多年的技术上的知识、策略、经验等，是他们多年来知识等的总结，他们对自己的领域是非常的熟悉的，因为他们几乎全部遇到过，肯定都找到了解决方案了。他们对学生所遇到的几乎所有问题都是非常清楚的，可以不时的点拨下自己的学生。教练跟老师的知识矩阵并不是与生俱来的，而是通过激情和精深练习逐渐掌握的。</p><p>作为一个人，不可能做任何事情都很符合标准，多听听别人的建议，对自己进行相应的改进，成为一个优秀的人。不断地去尝试，不要放弃，增加自己的经验，然后通过精深练习，不断地掌握自己这个领域。</p><p><strong>优势二：洞察力 —— 鹰的视力</strong></p><p>教练跟老师都需要有敏锐的洞察力，对自己的学生要时刻进行观察，从观察中可以发现问题，从而解决问题，提高学生的整体素养。</p><p><strong>优势三：简明的指示 —— 神奇的教鞭</strong></p><p>作为一名导师，需要对他的学生进行指导，优秀的指导是直接指出错误，并给出一些专业的建议，话不需要多，言简意赅。导师不用说太多的废话，因为废话，学生听了没多大用，有用的信息不多，所以只要让学生知道自己的哪里有错并去改正就行了。</p><p><strong>优势四：气质与诚信 —— 不可阻挡的魅力</strong></p><p>导师需要有自己气质，有自己的个性。道德跟诚信是导师最需要的，因为作为一名导师，不能一直给自己的学生讲他已经知道的东西，这样学生不会成长，需要指出一个方向，按照导师自己规划的方向便可以走向自己能够达到的高度。</p><p><strong>足球训练与小提琴练习</strong></p><p>首先，足球训练与小提琴练习是大不相同的，这两样老师培养的性质是不一样的。</p><p>一个需要亲身体会技巧，自己琢磨技巧，老师是不能亲身感受到的，而另一个没有正确的技法便不会有好的结果，所以培养学生需要因行业而异。</p><p>实战中感悟技法是比老师传授的更加的好，因为这自己的实际情况是一致的，自己判断问题，解决问题。</p><h3 id="10-伯乐的赌注"><a href="#10-伯乐的赌注" class="headerlink" title="10 伯乐的赌注"></a><strong>10 伯乐的赌注</strong></h3><p>*<em>教师就是为了逐步淡出。 *</em>           ——托马斯·卡拉瑟斯 (Thomas Carruthers)</p><p>一名导师教了很多学生，培养人才，当学生远航的时候，自己留在原地仰望自己的学生，学生的成功自己便满足了。</p><p>导师对学生的教导也是赌注，可能失败也可能成功，毕竟不可能一种方法适合各种人，所以需要导师来对不同的学生选择不同的教导方法，这就考验导师的眼力了。</p><h3 id="后记"><a href="#后记" class="headerlink" title="后记"></a><strong>后记</strong></h3><p>如果出现问题，问5次为什么。先自己去看看能不能解决问题，去查找资料，再解决不了的话才去请教别人。</p><p>我们干什么事都 <strong>不要害羞</strong> ， <strong>克服自己的害怕的心理</strong> ，对自己害怕的领域不断地进行磨练，让自己变得娴熟，不再害怕。</p><p><strong>人脑越用越灵活</strong> 。</p>]]></content>
      
      
      <categories>
          
          <category> 读书 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书 </tag>
            
            <tag> 个人感悟 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>系统定时任务</title>
      <link href="/2019/06/04/xi-tong-ding-shi-ren-wu/"/>
      <url>/2019/06/04/xi-tong-ding-shi-ren-wu/</url>
      
        <content type="html"><![CDATA[<h3 id="crond系统定时任务"><a href="#crond系统定时任务" class="headerlink" title="crond系统定时任务"></a>crond系统定时任务</h3><h4 id="1、crond服务管理"><a href="#1、crond服务管理" class="headerlink" title="1、crond服务管理"></a>1、crond服务管理</h4><p>service crond restart             （重新启动服务）</p><h4 id="2、crontab定时任务设置"><a href="#2、crontab定时任务设置" class="headerlink" title="2、crontab定时任务设置"></a>2、crontab定时任务设置</h4><p><strong>1）基本语法</strong><br>crontab [选项]<br>选项：<br>  -e：  编辑crontab定时任务<br>  -l：  查询crontab任务<br>  -r：  删除当前用户所有的crontab任务</p><p><strong>2）参数说明</strong><br>crontab -e<br>（1）进入crontab编辑界面。会打开vim编辑你的工作<br><code>* * * * *</code> 执行的任务</p><table><thead><tr><th align="center">项目</th><th align="center">含义</th><th align="center">范围</th></tr></thead><tbody><tr><td align="center">第一个“*”</td><td align="center">一小时当中的第几分钟（分）</td><td align="center">0-59</td></tr><tr><td align="center">第二个“*”</td><td align="center">一天当中的第几小时（时）</td><td align="center">0-23</td></tr><tr><td align="center">第三个“*”</td><td align="center">一个月当中的第几天（天）</td><td align="center">1-31</td></tr><tr><td align="center">第四个“*”</td><td align="center">一年当中的第几月（月）</td><td align="center">1-12</td></tr><tr><td align="center">第五个“*”</td><td align="center">一周当中的星期几（周）</td><td align="center">0-7（0和7都代表星期日）</td></tr></tbody></table><p>（2）特殊符号</p><table><thead><tr><th align="center">特殊符号</th><th align="center">含义</th></tr></thead><tbody><tr><td align="center"><code>*</code></td><td align="center">代表任何时间。比如第一个“*”就代表一小时中每分钟都执行一次的意思</td></tr><tr><td align="center"><code>，</code></td><td align="center">代表不连续的时间。比如“0 8,12,16 * * * 命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令</td></tr><tr><td align="center"><code>-</code></td><td align="center">代表连续的时间范围。比如“0 5  *  *  1-6命令”，代表在周一到周六的凌晨5点0分执行命令</td></tr><tr><td align="center"><code>*/n</code></td><td align="center">代表每隔多久执行一次。比如“*/10  *  *  *  *  命令”，代表每隔10分钟就执行一遍命令</td></tr></tbody></table><p>（3）特定时间执行命令</p><table><thead><tr><th align="center">时间</th><th align="center">含义</th></tr></thead><tbody><tr><td align="center">45 22 * * *  命令</td><td align="center">在22点45分执行命令</td></tr><tr><td align="center">0  17 * * 1  命令</td><td align="center">每周1 的17点0分执行命令</td></tr><tr><td align="center">0 5 1,15 * * 命令</td><td align="center">每月1号和15号的凌晨5点0分执行命令</td></tr><tr><td align="center">40 4 * * 1-5 命令</td><td align="center">每周一到周五的凌晨4点40分执行命令</td></tr><tr><td align="center"><code>*/10</code> 4 * * * 命令</td><td align="center">每天的凌晨4点，每隔10分钟执行一次命令</td></tr><tr><td align="center">0 0 1,15 * 1 命令</td><td align="center">每月1号和15号，每周1的0点0分都会执行命令</td></tr></tbody></table><p>*<em>注意: *</em>星期几和几号最好不要同时出现，因为他们定义的都是天，非常容易让管理员混乱</p><p><strong>3）案例</strong><br><code>*/5</code> * * * * /bin/echo ”11” &gt;&gt; /tmp/test</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 定时脚本 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>电信大数据</title>
      <link href="/2019/05/19/dian-xin-da-shu-ju/"/>
      <url>/2019/05/19/dian-xin-da-shu-ju/</url>
      
        <content type="html"><![CDATA[<h3 id="一、项目背景"><a href="#一、项目背景" class="headerlink" title="一、项目背景"></a>一、项目背景</h3><p>通信运营商每时每刻会产生大量的通信数据，例如通话记录，短信记录，彩信记录，第三方服务资费等等繁多信息。数据量如此巨大，除了要满足用户的实时查询和展示之外，还需要定时定期的对已有数据进行离线的分析处理。例如，当日话单，月度话单，季度话单，年度话单，通话详情，通话记录等等+。我们以此为背景，寻找一个切入点，学习其中的方法论</p><h3 id="二、项目架构"><a href="#二、项目架构" class="headerlink" title="二、项目架构"></a>二、项目架构</h3><p><img src="/medias/%E7%94%B5%E4%BF%A1%E9%A1%B9%E7%9B%AE%E6%9E%B6%E6%9E%84.PNG" alt="电信项目架构"> </p><h3 id="三、项目实现"><a href="#三、项目实现" class="headerlink" title="三、项目实现"></a>三、项目实现</h3><p><strong>系统环境</strong></p><table><thead><tr><th align="center">系统</th><th align="center">版本</th></tr></thead><tbody><tr><td align="center">windows</td><td align="center">10 专业版</td></tr><tr><td align="center">linux</td><td align="center">CentOS7.2 1611内核</td></tr></tbody></table><p><strong>开发工具</strong></p><table><thead><tr><th align="center">工具</th><th align="center">版本</th></tr></thead><tbody><tr><td align="center">idea</td><td align="center">2018.2.5旗舰版</td></tr><tr><td align="center">maven</td><td align="center">3.3.9</td></tr><tr><td align="center">JDK</td><td align="center">1.8+</td></tr></tbody></table><p><strong>尖叫提示</strong>：idea2018.2.5必须使用Maven3.3.9，不要使用Maven3.5，有部分兼容性问题</p><h3 id="四、数据生产"><a href="#四、数据生产" class="headerlink" title="四、数据生产"></a>四、数据生产</h3><p>此情此景，对于该模块的业务，即数据生产过程，一般并不会让你来进行操作，数据生产是一套完整且严密的体系，这样可以保证数据的鲁棒性。但是如果涉及到项目的一体化方案的设计（数据的产生、存储、分析、展示），则必须清楚每一个环节是如何处理的，包括其中每个环境可能隐藏的问题；数据结构，数据内容可能出现的问题</p><h4 id="1、数据结构"><a href="#1、数据结构" class="headerlink" title="1、数据结构"></a>1、数据结构</h4><p>我们将在HBase中存储两个电话号码，以及通话建立的时间和通话持续时间，最后再加上一个flag作为判断第一个电话号码是否为主叫。姓名字段的存储我们可以放置于另外一张表做关联查询，当然也可以插入到当前表中</p><table><thead><tr><th align="center">列名</th><th align="center">解释</th><th align="center">举例</th></tr></thead><tbody><tr><td align="center">caller</td><td align="center">第一个手机号码</td><td align="center">15369468720</td></tr><tr><td align="center">callerName</td><td align="center">第一个手机号码人姓名(非必须)</td><td align="center">李雁</td></tr><tr><td align="center">callee</td><td align="center">第二个手机号码</td><td align="center">19920860202</td></tr><tr><td align="center">calleename</td><td align="center">第二个手机号码人姓名(非必须)</td><td align="center">卫艺</td></tr><tr><td align="center">dateTime</td><td align="center">建立通话的时间</td><td align="center">20181126091236</td></tr><tr><td align="center">date_time_ts</td><td align="center">建立通话的时间（时间戳形式）</td><td align="center"></td></tr><tr><td align="center">duration</td><td align="center">通话持续时间（秒）</td><td align="center">0820</td></tr><tr><td align="center">flag</td><td align="center">用于标记本次通话第一个字段(caller)是主叫还是被叫</td><td align="center">1为主叫，0为被叫</td></tr></tbody></table><h4 id="2、编写代码"><a href="#2、编写代码" class="headerlink" title="2、编写代码"></a>2、编写代码</h4><p><strong>思路</strong><br>a）创建Java集合类存放模拟的电话号码和联系人；<br>b） 随机选取两个手机号码当做“主叫”与“被叫”（注意判断两个手机号不能重复），产出<strong>caller</strong>与<strong>call2</strong>字段数据；<br>c） 创建随机生成通话建立时间的方法，可指定随机范围，最后生成通话建立时间，产出<strong>date_time</strong>字段数据；<br>d）随机一个通话时长，单位：秒，产出<strong>duration</strong>字段数据；<br>e）将产出的一条数据拼接封装到一个字符串中；<br>f）使用IO操作将产出的一条通话数据写入到本地文件中;</p><p>新建module项目：<strong>ct_producer</strong></p><p><strong>父pom.xml文件配置</strong></p><pre><code>&lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!-- https://mvnrepository.com/artifact/junit/junit --&gt;        &lt;dependency&gt;            &lt;groupId&gt;junit&lt;/groupId&gt;            &lt;artifactId&gt;junit&lt;/artifactId&gt;            &lt;version&gt;4.12&lt;/version&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;                &lt;version&gt;2.12.4&lt;/version&gt;                &lt;configuration&gt;                    &lt;skipTests&gt;true&lt;/skipTests&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;                &lt;version&gt;3.8.1&lt;/version&gt;                &lt;configuration&gt;                    &lt;source&gt;1.8&lt;/source&gt;                    &lt;target&gt;1.8&lt;/target&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;</code></pre><p>（1）随机输入一些手机号码以及联系人，保存于Java的集合中<br>新建类：ProductLog</p><pre><code>package producer;import java.io.FileOutputStream;import java.io.OutputStreamWriter;import java.text.DecimalFormat;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.*;public class ProductLog {    String startTime = &quot;2018-01-01&quot;;    String endTime = &quot;2018-12-31&quot;;    //存放tel的List集合    private List&lt;String&gt;  phoneList = new ArrayList&lt;String&gt;();    //存放tel和Name的Map集合    private Map&lt;String, String&gt; phoneNameMap = new HashMap&lt;&gt;();    /**     * 初始化数据     */    public void initPhone(){        phoneList.add(&quot;17078388295&quot;);        phoneList.add(&quot;13980337439&quot;);        phoneList.add(&quot;14575535933&quot;);        phoneList.add(&quot;18902496992&quot;);        phoneList.add(&quot;18549641558&quot;);        phoneList.add(&quot;17005930322&quot;);        phoneList.add(&quot;18468618874&quot;);        phoneList.add(&quot;18576581848&quot;);        phoneList.add(&quot;15978226424&quot;);        phoneList.add(&quot;15542823911&quot;);        phoneList.add(&quot;17526304161&quot;);        phoneList.add(&quot;15422018558&quot;);        phoneList.add(&quot;17269452013&quot;);        phoneList.add(&quot;17764278604&quot;);        phoneList.add(&quot;15711910344&quot;);        phoneList.add(&quot;15714728273&quot;);        phoneList.add(&quot;16061028454&quot;);        phoneList.add(&quot;16264433631&quot;);        phoneList.add(&quot;17601615878&quot;);        phoneList.add(&quot;15897468949&quot;);        phoneNameMap.put(&quot;17078388295&quot;, &quot;李为&quot;);        phoneNameMap.put(&quot;13980337439&quot;, &quot;王军&quot;);        phoneNameMap.put(&quot;14575535933&quot;, &quot;时俊&quot;);        phoneNameMap.put(&quot;18902496992&quot;, &quot;天机&quot;);        phoneNameMap.put(&quot;18549641558&quot;, &quot;蔡铭&quot;);        phoneNameMap.put(&quot;17005930322&quot;, &quot;陶尚&quot;);        phoneNameMap.put(&quot;18468618874&quot;, &quot;魏山帅&quot;);        phoneNameMap.put(&quot;18576581848&quot;, &quot;华倩&quot;);        phoneNameMap.put(&quot;15978226424&quot;, &quot;焦君山&quot;);        phoneNameMap.put(&quot;15542823911&quot;, &quot;钟尾田&quot;);        phoneNameMap.put(&quot;17526304161&quot;, &quot;司可可&quot;);        phoneNameMap.put(&quot;15422018558&quot;, &quot;官渡&quot;);        phoneNameMap.put(&quot;17269452013&quot;, &quot;上贵坡&quot;);        phoneNameMap.put(&quot;17764278604&quot;, &quot;时光机&quot;);        phoneNameMap.put(&quot;15711910344&quot;, &quot;李发&quot;);        phoneNameMap.put(&quot;15714728273&quot;, &quot;蒂冈&quot;);        phoneNameMap.put(&quot;16061028454&quot;, &quot;范德&quot;);        phoneNameMap.put(&quot;16264433631&quot;, &quot;周朝王&quot;);        phoneNameMap.put(&quot;17601615878&quot;, &quot;谢都都&quot;);        phoneNameMap.put(&quot;15897468949&quot;, &quot;刘何思&quot;);    }</code></pre><p>（2）创建随机生成通话时间的方法：randomDate<br>该时间生成后的格式为yyyy-MM-dd HH:mm:ss，并使之可以根据传入的起始时间和结束时间来随机生成</p><pre><code>   /**     * 注：传入时间要在时间[startTime, endTime]     * 公式：起始时间 + （结束时间 - 起始时间）* Math.random()     * @param startTime     * @param endTime     */    private String randomBuildTime(String startTime, String endTime) {        try {            SimpleDateFormat sdf1 = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);            Date startDate = sdf1.parse(startTime);            Date endDate = sdf1.parse(endTime);            if(endDate.getTime() &lt;= startDate.getTime()){                return null;            }            //公式：起始时间 + （结束时间 - 起始时间）* Math.random()            long randomTs = startDate.getTime() + (long) ((endDate.getTime() - startDate.getTime()) * Math.random());            Date resultDate = new Date(randomTs);            SimpleDateFormat sdf2 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);            String resultTimeString = sdf2.format(resultDate);            return resultTimeString;        } catch (ParseException e) {            e.printStackTrace();        }        return null;    }</code></pre><p>（3）创建生产日志一条日志的方法：productLog<br>随机抽取两个电话号码，随机产生通话建立时间，随机通话时长，将这几个字段拼接成一个字符串，然后return，便可以产生一条通话的记录。需要注意的是，如果随机出的两个电话号码一样，需要重新随机（随机过程可优化，但并非此次重点）。通话时长的随机为20分钟以内，即：60秒 * 30，并格式化为4位数字，例如：0600(10分钟)</p><pre><code>   /**     * 产生数据     * 格式： caller,callee,buildTime,duration     * @return     */    public String product(){        //ctrl + d 复制此行 ， ctrl + x 剪切此行 ，ctrl + y 删除此行        //主叫        String caller = null;        String callerName = null;        //被叫        String callee = null;        String calleeName = null;        //ctrl + alt + v 推导出前面的对象类型  Home前  End后        int callerIndex = (int) (Math.random() * phoneList.size());        caller = phoneList.get(callerIndex);        callerName = phoneNameMap.get(caller);        while(true) {            //ctrl + shift + 下  ：下移这行            int calleeIndex = (int) (Math.random() * phoneList.size());            callee = phoneList.get(calleeIndex);            calleeName = phoneNameMap.get(callee);            if(!caller.equals(callee)) break;        }        //第三个字段        String buildTime = randomBuildTime(startTime, endTime);        //第四个字段，最多时长        DecimalFormat df = new DecimalFormat(&quot;0000&quot;);        String duration = df.format((int) 30 * 60 * Math.random());        StringBuilder sb = new StringBuilder();        sb.append(caller + &quot;,&quot;).append(callee + &quot;,&quot;).append(buildTime + &quot;,&quot;).append( duration);        return sb.toString();    }</code></pre><p>（4）创建写入日志方法：writeLog<br>productLog每产生一条日志，便将日志写入到本地文件中，所以建立一个专门用于日志写入的方法，需要涉及到IO操作，需要注意的是，输出流每次写一条日之后需要flush，不然可能导致积攒多条数据才输出一次。最后需要将productLog方法放置于while死循环中执行</p><pre><code>/** * 把数据写到文件当中 * @param filePath */public void writeLog(String filePath){    try {        OutputStreamWriter osw = new OutputStreamWriter(new FileOutputStream(filePath, true), &quot;UTF-8&quot;);        while(true){            Thread.sleep(200);            String log = product();            System.out.println(log);             //一定要手动flush才可以确保每条数据都写入到文件一次            osw.write(log + &quot;\n&quot;);            osw.flush();        }    } catch (Exception e) {        e.printStackTrace();    }}</code></pre><p>（5）在主函数中初始化以上逻辑，并测试：</p><pre><code> public static void main(String[] args) {        //args = new String[]{&quot;E:\\CT Project file\\calllog.csv&quot;};        if(args == null || args.length &lt;= 0){            System.out.println(&quot;没写路径&quot;);            return ;        }        ProductLog productLog = new ProductLog();        productLog.initPhone();        productLog.writeLog(args[0]);    }</code></pre><h4 id="3、打包测试"><a href="#3、打包测试" class="headerlink" title="3、打包测试"></a>3、打包测试</h4><p>1）Maven打包方式<br>分别在Windows上和Linux中进行测试：<br>java -cp jar包的绝对路径 全类名 输出路径</p><p>2）将此包放在在/opt/jar下面，并写如下脚本</p><p><strong>product.sh</strong> </p><pre><code>#!bin.bashjava -cp /opt/jars/CT_producer-1.0-SNAPSHOT.jar producer.ProductLog /opt/jars/calllog.csv</code></pre><p>3）运行<br>sh product.sh<br>产生calllog.csv文件</p><h3 id="五、数据采集-消费-存储"><a href="#五、数据采集-消费-存储" class="headerlink" title="五、数据采集/消费(存储)"></a>五、数据采集/消费(存储)</h3><p>欢迎来到数据采集模块（消费），在企业中你要清楚流式数据采集框架Flume和Kafka的定位是什么。我们在此需要将实时数据通过Flume采集到Kafka然后供给给HBase消费</p><p>Flume：Cloudera公司研发<br>适合下游数据消费者不多的情况；<br>适合数据安全性要求不高的操作；<br>适合与Hadoop生态圈对接的操作</p><p>Kafka：linkedin公司研发<br>适合数据下游消费众多的情况；<br>适合数据安全性要求较高的操作（支持replication）；</p><p><strong>因此我们常用的一种模型是</strong>：<br>线上数据 –&gt; Flume –&gt; Kafka –&gt; Flume(根据情景增删该流程) –&gt; HDFS</p><p><strong>消费存储模块流程图</strong>：</p><p><img src="/medias/%E6%B6%88%E8%B4%B9%E5%AD%98%E5%82%A8%E6%A8%A1%E5%9D%97%E6%B5%81%E7%A8%8B%E5%9B%BE.PNG" alt="消费存储模块流程图"> </p><h4 id="1、数据采集：采集实时产生的数据到kafka集群"><a href="#1、数据采集：采集实时产生的数据到kafka集群" class="headerlink" title="1、数据采集：采集实时产生的数据到kafka集群"></a>1、数据采集：采集实时产生的数据到kafka集群</h4><p>0）基础配置</p><ul><li>配置Kafka 略</li><li>配置Flume(flume2kafka.conf)</li></ul><pre><code># definea1.sources = r1a1.sinks = k1a1.channels = c1# sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F -c +0 /opt/jars/calllog.csva1.sources.r1.shell = /bin/bash -c# sinka1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.brokerList = hsiehchou121:9092,hsiehchou122:9092,hsiehchou123:9092a1.sinks.k1.topic = callloga1.sinks.k1.batchSize = 20a1.sinks.k1.requiredAcks = 1# channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# binda1.sources.r1.channels = c1a1.sinks.k1.channel = c1</code></pre><p>1）进入Flume根目录下，启动flume<br>/opt/module/flume-1.8.0/bin/flume-ng agent –conf /opt/module/flume-1.8.0/conf/ –name a1 –conf-file /opt/jars/flume2kafka.conf</p><p>2）运行生产日志的任务脚本，观察kafka控制台消费者是否成功显示产生的数据<br>$ sh productlog.sh</p><h4 id="2、编写代码：数据消费（HBase）"><a href="#2、编写代码：数据消费（HBase）" class="headerlink" title="2、编写代码：数据消费（HBase）"></a>2、编写代码：数据消费（HBase）</h4><p>如果以上操作均成功，则开始编写操作HBase的代码，用于消费数据，将产生的数据实时存储在HBase中</p><p><strong>思路</strong>：<br>a） 编写Kafka消费者，读取kafka集群中缓存的消息，并打印到控制台以观察是否成功；</p><p>b）既然能够读取到kafka中的数据了，就可以将读取出来的数据写入到HBase中，所以编写调用HBaseAPI相关方法，将从Kafka中读取出来的数据写入到HBase；</p><p>c） 以上两步已经足够完成消费数据，存储数据的任务，但是涉及到解耦，所以过程中需要将一些属性文件外部化，HBase通用性方法封装到某一个类中</p><p>创建新的module项目：<strong>ct_consumer</strong></p><p><strong>pom.xml文件配置</strong></p><pre><code>&lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;junit&lt;/groupId&gt;            &lt;artifactId&gt;junit&lt;/artifactId&gt;            &lt;version&gt;4.12&lt;/version&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;            &lt;version&gt;0.11.0.2&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-client --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;            &lt;version&gt;1.3.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-server --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;            &lt;artifactId&gt;hbase-server&lt;/artifactId&gt;            &lt;version&gt;1.3.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --&gt;        &lt;dependency&gt;            &lt;groupId&gt;mysql&lt;/groupId&gt;            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;            &lt;version&gt;8.0.13&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;                &lt;version&gt;2.12.4&lt;/version&gt;                &lt;configuration&gt;                    &lt;skipTests&gt;true&lt;/skipTests&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;                &lt;version&gt;3.8.1&lt;/version&gt;                &lt;configuration&gt;                    &lt;source&gt;1.8&lt;/source&gt;                    &lt;target&gt;1.8&lt;/target&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;</code></pre><p>1）新建类：<strong>HBaseConsumer</strong>（kafka的package）<br>该类主要用于读取kafka中缓存的数据，然后调用HBaseAPI，持久化数据</p><pre><code>package kafka;import hbase.HBaseDao;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import utils.PropertiesUtil;import java.util.Arrays;public class HBaseConsumer {    public static void main(String[] args) {        //消费者API        KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;&gt;(PropertiesUtil.properties);        //kafka Topic        kafkaConsumer.subscribe(Arrays.asList(PropertiesUtil.getProperty(&quot;kafka.topics&quot;)));        //创建写入HBase的对象        HBaseDao hd = new HBaseDao();        while(true) {            //消费拉取数据            ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(100);            //遍历打印数据            for(ConsumerRecord&lt;String, String&gt; cr : records){                String value = cr.value();                //13980337439,16264433631,2018-02-08 10:27:32,1740                System.out.println(value);                //把数据写入到HBase中                hd.put(value);            }        }    }}</code></pre><p>2) 新建类：<strong>PropertiesUtil</strong>（utils的package）<br>该类主要用于将常用的项目所需的参数外部化，解耦，方便配置</p><pre><code>package utils;import java.io.IOException;import java.io.InputStream;import java.util.Properties;public class PropertiesUtil {    public static Properties properties = null;    static {        //ctrl + alt + v        InputStream is = ClassLoader.getSystemResourceAsStream(&quot;hbase_consumer.properties&quot;);        properties = new Properties();        try {            properties.load(is);        } catch (IOException e) {            e.printStackTrace();        }    }    public static String getProperty(String key){        return properties.getProperty(key);    }}</code></pre><p>3） 创建kafka.properties文件，并放置于resources目录下</p><pre><code># 设置kafka的brokerlistbootstrap.servers=hsiehchou121:9092,hsiehchou122:9092,hsiehchou123:9092# 设置消费者所属的消费组group.id=hbase_consumer_group# 设置是否自动确认offsetenable.auto.commit=true# 自动确认offset的时间间隔auto.commit.interval.ms=30000# 设置key，value的反序列化类的全名key.deserializer=org.apache.kafka.common.serialization.StringDeserializervalue.deserializer=org.apache.kafka.common.serialization.StringDeserializer# 以下为自定义属性设置# 设置本次消费的主题kafka.topics=calllog# 设置HBase的一些变量hbase.calllog.regions=6hbase.calllog.namespace=ns_cthbase.calllog.tablename=ns_ct:calllog</code></pre><p>4）将hdfs-site.xml、core-site.xml、hbase-site.xml、log4j.properties放置于resources目录</p><p>5）新建类：HBaseUtil（utils的package）<br>该类主要用于封装一些HBase的常用操作，比如创建命名空间，创建表等等</p><pre><code>package utils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HColumnDescriptor;import org.apache.hadoop.hbase.HTableDescriptor;import org.apache.hadoop.hbase.NamespaceDescriptor;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.Admin;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.text.DecimalFormat;import java.util.Iterator;import java.util.TreeSet;/** * 1、NameSpace ====&gt;  命名空间 * 2、createTable ===&gt; 表 * 3、isTable   ====&gt;  判断表是否存在 * 4、Region、RowKey、分区键 */public class HBaseUtil {    /**     * 初始化命名空间     *     * @param conf  配置对象     * @param namespace 命名空间的名字     */    public static void initNameSpace(Configuration conf, String namespace) throws IOException {        //获取链接connection        Connection connection = ConnectionFactory.createConnection(conf);        //获取admin对象        Admin admin = connection.getAdmin();        //创建命名空间，命名空间描述器        NamespaceDescriptor nd = NamespaceDescriptor                .create(namespace)                //add配置信息不强制加                .addConfiguration(&quot;create_time&quot;, String.valueOf(System.currentTimeMillis()))                .build();        //通过admin对象创建namespace        admin.createNamespace(nd);        close(admin,connection);    }    /**     * 初始化表     *     * @param conf     * @param tableName     * @param regions     * @param columnFamily     */    public static void createTable(Configuration conf, String tableName, int regions, String... columnFamily) throws IOException {        //获取链接connection        Connection connection = ConnectionFactory.createConnection(conf);        //获取admin对象        Admin admin = connection.getAdmin();        //如果表已存在，就返回        if (isExistTable(conf, tableName)){            return ;        }        //创建表对象        HTableDescriptor htd = new HTableDescriptor(TableName.valueOf(tableName));        for (String cf : columnFamily){            htd.addFamily(new HColumnDescriptor(cf));        }        //添加协处理器的全类名        htd.addCoprocessor(&quot;hbase.CalleeWriteObserver&quot;);        //通过admin创建表（htd（列族），分裂的regions）        admin.createTable(htd, getSplitKeys(regions));        //关闭        close(admin, connection);    }    /**     * 分区     *     * @param regions     * @return     */    private static byte[][] getSplitKeys(int regions) {        //第一步：定义分区键数组        String[] keys = new String[regions];        //分区位数格式化        DecimalFormat df = new DecimalFormat(&quot;00&quot;);        //  00|01|02|03|04|05        for (int i = 0; i &lt; regions; i++){            keys[i] = df.format(i) + &quot;|&quot;;        }        //第二步        byte[][] splitsKeys = new byte[regions][];        //分区间有序        TreeSet&lt;byte[]&gt; treeSet = new TreeSet&lt;&gt;(Bytes.BYTES_COMPARATOR);        for (int i = 0; i &lt; regions; i++){            treeSet.add(Bytes.toBytes(keys[i]));        }        //第三步        Iterator&lt;byte[]&gt; splitKeysIterator = treeSet.iterator();        int index = 0;        while (splitKeysIterator.hasNext()){            byte[] next = splitKeysIterator.next();            splitsKeys[index++] = next;        }        return splitsKeys;    }    /**     * 判断表是否存在     *     * @param conf     * @param tableName     */    public static boolean isExistTable(Configuration conf, String tableName) throws IOException {        //获取链接connection        Connection connection = ConnectionFactory.createConnection(conf);        //获取admin对象        Admin admin = connection.getAdmin();        //判断表API        boolean b = admin.tableExists(TableName.valueOf(tableName));        //关闭        close(admin, connection);        return b;    }    /**     * 关闭     *     * @param admin     * @param connection     */    public static void close(Admin admin, Connection connection) throws IOException {        if (admin != null) {            admin.close();        }        if (connection != null) {            connection.close();        }    }    /**     * regionCode, caller, buildTime, callee, flag, duration     * regionCode（rowkey前的离散串）     * duration（通话建立时间）     * 主叫（flag:1）：13980337439,16264433631,2018-02-08 10:27:32,1740   ==&gt;f1列族     * 被叫（flag:0）：16264433631,13980337439,2018-02-08 10:27:32,1740   ==&gt;f2列族     *     * 面试常问rowkey相关的问题：你们公司如何设计的RowKey？怎么设计RowKey才能避免热点问题（频繁访问某个区）?     *     * @param regionCode 散列的键     * @param caller     叫     * @param buildTime  建立时间     * @param callee     被叫     * @param flag       标明是主叫还是被叫     * @param duration   通话持续时间     * @return     */    public static String getRowKey(String regionCode, String caller, String buildTime, String callee, String flag, String duration){        StringBuilder sb = new StringBuilder();        sb.append(regionCode + &quot;_&quot;)                .append(caller + &quot;_&quot;)                .append(buildTime + &quot;_&quot;)                .append(callee + &quot;_&quot;)                .append(flag + &quot;_&quot;)                .append(duration);        return sb.toString();    }    /**     * 当数据进入HBase的Region的时候是足够的离散     *     * @param caller 主叫     * @param buildTime 通话建立时间     * @param regions region个数     * @return 返回分区号     */    public static String getRegionCode(String caller, String buildTime, int regions){        //取出主叫的后四位,lastPhone caller最后的后四位        String lastPhone = caller.substring(caller.length() - 4);        //取出年月   2018-02-08 10:27:32,1740 中取出年月        String yearMonth = buildTime                .replaceAll(&quot;-&quot;, &quot;&quot;)                .replaceAll(&quot;:&quot;, &quot;&quot;)                .replaceAll(&quot; &quot;, &quot;&quot;)                .substring(0, 6);        //离散操作1：做异或处理 ^        Integer x = Integer.valueOf(lastPhone) ^ Integer.valueOf(yearMonth);        //离散操作2：把离散1的值再做hashcode        int y = x.hashCode();        //最终想要的分区号        int regionCode = y % regions;        DecimalFormat df = new DecimalFormat(&quot;00&quot;);        return df.format(regionCode);    }}</code></pre><p>6）新建类：ConnectionInstance（utils的package）</p><pre><code>package utils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import java.io.IOException;public class ConnectionInstance {    private static Connection conn;    public static synchronized Connection getConnection(Configuration configuration) {        try {            if (conn == null || conn.isClosed()) {                conn = ConnectionFactory.createConnection(configuration);            }        } catch (IOException e) {            e.printStackTrace();        }        return conn;    }}</code></pre><p>7）新建类：HBaseDAO（完成以下内容后，考虑数据put的效率如何优化）（hbase的package）<br>该类主要用于执行具体的保存数据的操作，rowkey的生成规则等等</p><pre><code>package hbase;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.HTable;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.util.Bytes;import utils.ConnectionInstance;import utils.HBaseUtil;import utils.PropertiesUtil;import java.io.IOException;import java.text.SimpleDateFormat;import java.util.ArrayList;import java.util.List;public class HBaseDao {    public static final Configuration CONF;    private String namespace;    private int regions;    private String tableName;    private HTable table;    private Connection connection;    private SimpleDateFormat sdf1 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);    private SimpleDateFormat sdf2 = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);    //用来存放一小堆数据（30行），用于优化    private List&lt;Put&gt; cacheList = new ArrayList&lt;&gt;();    static {        CONF = HBaseConfiguration.create();    }    //Alt + Insert  Constructor    /**     * 用于构造命名空间和表     */    public HBaseDao() {        try {            namespace = PropertiesUtil.getProperty(&quot;hbase.calllog.namespace&quot;);            tableName = PropertiesUtil.getProperty(&quot;hbase.calllog.tablename&quot;);            regions = Integer.valueOf(PropertiesUtil.getProperty(&quot;hbase.calllog.regions&quot;));            if (!HBaseUtil.isExistTable(CONF, tableName)){                HBaseUtil.initNameSpace(CONF, namespace);                HBaseUtil.createTable(CONF, tableName, regions, &quot;f1&quot;, &quot;f2&quot;);            }        } catch (IOException e) {            e.printStackTrace();        }    }    /**     *     * @param value 13980337439,16264433631,2018-02-08 10:27:32,1740     */    public void put(String value) {        try {            if(cacheList.size() == 0){                connection = ConnectionInstance.getConnection(CONF);                table = (HTable) connection.getTable(TableName.valueOf(tableName));                table.setAutoFlushTo(false);                table.setWriteBufferSize(2 * 1024 * 1024);            }            //如果出现下标越界异常            String[] splitValue = value.split(&quot;,&quot;);            String caller = splitValue[0];            String callee = splitValue[1];            String buildTime = splitValue[2];            String duration = splitValue[3];            //散列得分区号            String regionCode = HBaseUtil.getRegionCode(caller, buildTime, regions);            //这个变量用于插入到HBase的列中            String buildTimeReplace = sdf2.format(sdf1.parse(buildTime));            //作为rowkey所需的参数            String buildTimeTs = String.valueOf(sdf1.parse(buildTime).getTime());            String rowkey = HBaseUtil.getRowKey(regionCode, caller, buildTimeReplace, callee, &quot;1&quot;, duration);            Put put = new Put(Bytes.toBytes(rowkey));            //通过put对象添加rowkey和列值，参数说明：(列族：f1)，(列名：caller)，（列值：caller）            //快捷键：ctrl + d            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;caller&quot;), Bytes.toBytes(caller));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;callee&quot;), Bytes.toBytes(callee));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;build_time&quot;), Bytes.toBytes(buildTimeReplace));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;build_time_ts&quot;), Bytes.toBytes(buildTimeTs));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;flag&quot;), Bytes.toBytes(&quot;1&quot;));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;duration&quot;), Bytes.toBytes(duration));            //把rowkey，列族，列名，列值放到cacheList的对象中            cacheList.add(put);            if(cacheList.size() &gt;= 30) {                table.put(cacheList);                table.flushCommits();                table.close();                cacheList.clear();            }        } catch (Exception e) {            e.printStackTrace();        }    }}</code></pre><p>8）新建类：HBaseDao（hbase的package）</p><pre><code>package hbase;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.HTable;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.util.Bytes;import utils.ConnectionInstance;import utils.HBaseUtil;import utils.PropertiesUtil;import java.io.IOException;import java.text.SimpleDateFormat;import java.util.ArrayList;import java.util.List;public class HBaseDao {    public static final Configuration CONF;    private String namespace;    private int regions;    private String tableName;    private HTable table;    private Connection connection;    private SimpleDateFormat sdf1 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);    private SimpleDateFormat sdf2 = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);    //用来存放一小堆数据（30行），用于优化    private List&lt;Put&gt; cacheList = new ArrayList&lt;&gt;();    static {        CONF = HBaseConfiguration.create();    }    //Alt + Insert  Constructor    /**     * 用于构造命名空间和表     */    public HBaseDao() {        try {            namespace = PropertiesUtil.getProperty(&quot;hbase.calllog.namespace&quot;);            tableName = PropertiesUtil.getProperty(&quot;hbase.calllog.tablename&quot;);            regions = Integer.valueOf(PropertiesUtil.getProperty(&quot;hbase.calllog.regions&quot;));            if (!HBaseUtil.isExistTable(CONF, tableName)){                HBaseUtil.initNameSpace(CONF, namespace);                HBaseUtil.createTable(CONF, tableName, regions, &quot;f1&quot;, &quot;f2&quot;);            }        } catch (IOException e) {            e.printStackTrace();        }    }    /**     *     * @param value 13980337439,16264433631,2018-02-08 10:27:32,1740     */    public void put(String value) {        try {            if(cacheList.size() == 0){                connection = ConnectionInstance.getConnection(CONF);                table = (HTable) connection.getTable(TableName.valueOf(tableName));                table.setAutoFlushTo(false);                table.setWriteBufferSize(2 * 1024 * 1024);            }            //如果出现下标越界异常            String[] splitValue = value.split(&quot;,&quot;);            String caller = splitValue[0];            String callee = splitValue[1];            String buildTime = splitValue[2];            String duration = splitValue[3];            //散列得分区号            String regionCode = HBaseUtil.getRegionCode(caller, buildTime, regions);            //这个变量用于插入到HBase的列中            String buildTimeReplace = sdf2.format(sdf1.parse(buildTime));            //作为rowkey所需的参数            String buildTimeTs = String.valueOf(sdf1.parse(buildTime).getTime());            String rowkey = HBaseUtil.getRowKey(regionCode, caller, buildTimeReplace, callee, &quot;1&quot;, duration);            Put put = new Put(Bytes.toBytes(rowkey));            //通过put对象添加rowkey和列值，参数说明：(列族：f1)，(列名：caller)，（列值：caller）            //快捷键：ctrl + d            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;caller&quot;), Bytes.toBytes(caller));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;callee&quot;), Bytes.toBytes(callee));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;build_time&quot;), Bytes.toBytes(buildTimeReplace));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;build_time_ts&quot;), Bytes.toBytes(buildTimeTs));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;flag&quot;), Bytes.toBytes(&quot;1&quot;));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;duration&quot;), Bytes.toBytes(duration));            //把rowkey，列族，列名，列值放到cacheList的对象中            cacheList.add(put);            if(cacheList.size() &gt;= 30) {                table.put(cacheList);                table.flushCommits();                table.close();                cacheList.clear();            }        } catch (Exception e) {            e.printStackTrace();        }    }}</code></pre><h4 id="3、运行测试：HBase消费数据"><a href="#3、运行测试：HBase消费数据" class="headerlink" title="3、运行测试：HBase消费数据"></a>3、运行测试：HBase消费数据</h4><p>尖叫提示：请将Linux允许打开的文件个数和进程数进行优化，优化RegionServer与Zookeeper会话的超时时间。（参考HBase文档中优化章节）<br>项目成功后，则将项目打包后在linux中运行测试</p><p><strong>打包HBase消费者代码</strong><br>a）  在windows中，进入工程的pom.xml所在目录下（建议将该工程的pom.xml文件拷贝到其他临时目录中，例如我把pom.xml文件拷贝到了F:\maven-lib\目录下），然后使用mvn命令下载工程所有依赖的jar包<br>mvn -DoutputDirectory=./lib -DgroupId=com.hsiehchou -DartifactId=ct_consumer -Dversion=r-1.0-SNAPSHOT dependency:copy-dependencies<br>b）  使用maven打包工程<br>c） 测试执行该jar包</p><p>方案一：推荐，使用通配符，将所有依赖加入到classpath中，不可使用<em>.jar的方式<br>注意：如果是在Linux中实行，注意文件夹之间的分隔符。自己的工程要单独在cp中指定，不要直接放在maven-lib/lib目录下<br>java -cp F:\maven-lib\CT_consumerr-1.0-SNAPSHOT.jar;F:\maven-lib\lib\</em> com.hsiehchou.CT_kafka.HBaseConsumer</p><p>方案二：最最推荐，使用java.ext.dirs参数将所有依赖的目录添加进classpath中<br>注意：-Djava.ext.dirs=属性后边的路径不能为”~”<br>java -Djava.ext.dirs=F:\maven-lib\lib\ -cp F:\maven-lib\CT_consumerr-1.0-SNAPSHOT.jar com.hsiehchou.CT_consumer.kafka.HBaseConsumer</p><h4 id="4、编写代码：优化数据存储方案"><a href="#4、编写代码：优化数据存储方案" class="headerlink" title="4、编写代码：优化数据存储方案"></a>4、编写代码：优化数据存储方案</h4><p>现在我们要使用HBase查找数据时，尽可能的使用rowKey去精准的定位数据位置，而非使用ColumnValueFilter或者SingleColumnValueFilter，按照单元格Cell中的Value过滤数据，这样做在数据量巨大的情况下，效率是极低的——如果要涉及到全表扫描。所以尽量不要做这样可怕的事情。注意，这并非ColumnValueFilter就无用武之地。现在，我们将使用协处理器，将数据一分为二</p><p><strong>思路</strong></p><p>a）编写协处理器类，用于协助处理HBase的相关操作（增删改查）<br>b）在协处理器中，一条主叫日志成功插入后，将该日志切换为被叫视角再次插入一次，放入到与主叫日志不同的列族中<br>c）重新创建hbase表，并设置为该表设置协处理器<br>d）编译项目，发布协处理器的jar包到hbase的lib目录下，并群发该jar包<br>e）修改hbase-site.xml文件，设置协处理器，并群发该hbase-site.xml文件</p><p><strong>编码</strong></p><p>1） 新建协处理器类：CalleeWriteObserver，并覆写postPut方法，该方法会在数据成功插入之后被回调（hbase的package）</p><pre><code>package hbase;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.Durability;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Table;import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver;import org.apache.hadoop.hbase.coprocessor.ObserverContext;import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;import org.apache.hadoop.hbase.regionserver.wal.WALEdit;import org.apache.hadoop.hbase.util.Bytes;import utils.HBaseUtil;import utils.PropertiesUtil;import java.io.IOException;import java.text.ParseException;import java.text.SimpleDateFormat;public class CalleeWriteObserver extends BaseRegionObserver {    SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);    //ctrl + 0    /**     * 插入主叫数据后，随即插入被叫数据     * @param e     * @param put     * @param edit     * @param durability     * @throws IOException     */    @Override    public void postPut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e,                        Put put,                        WALEdit edit,                        Durability durability) throws IOException {        //注意：一定到删除super.postPut(e, put, edit, durability);        //操作的目标表        String targetTableName = PropertiesUtil.getProperty(&quot;hbase.calllog.tablename&quot;);        //当前操作put后的表        String currentTableName = e.getEnvironment().getRegionInfo().getTable().getNameAsString();        //不是同一个表返回        if (!targetTableName.equals(currentTableName)){            return;        }        //05_18902496992_20180720182543_14575535933_1_0076        String oriRowKey = Bytes.toString(put.getRow());        System.out.println(oriRowKey);        String[] splitOriRowKey = oriRowKey.split(&quot;_&quot;);        String caller = splitOriRowKey[1];        String callee = splitOriRowKey[3];        String buildTime = splitOriRowKey[2];        String duration = splitOriRowKey[5];        //如果当前插入的是被叫数据，则直接返回(因为默认提供的数据全部为主叫数据)        String flag = splitOriRowKey[4];        String calleeflag = &quot;0&quot;;        if (flag.equals(calleeflag) ){            return;        }        flag = calleeflag;        Integer regions = Integer.valueOf(PropertiesUtil.getProperty(&quot;hbase.calllog.regions&quot;));        String regionCode = HBaseUtil.getRegionCode(callee, buildTime, regions);        String calleeRowKey = HBaseUtil.getRowKey(regionCode, callee, buildTime, caller, flag, duration);        String buildTimeTs = &quot;&quot;;        try {            buildTimeTs = String.valueOf(sdf.parse(buildTime).getTime());        } catch (ParseException e1) {            e1.printStackTrace();        }        Put calleePut = new Put(Bytes.toBytes(calleeRowKey));        calleePut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;callee&quot;), Bytes.toBytes(caller));        calleePut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;caller&quot;), Bytes.toBytes(callee));        calleePut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;build_time&quot;), Bytes.toBytes(buildTime));        calleePut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;build_time_ts&quot;), Bytes.toBytes(buildTimeTs));        calleePut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;flag&quot;), Bytes.toBytes(flag));        calleePut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;duration&quot;), Bytes.toBytes(duration));        Bytes.toBytes(100L);        Table table = e.getEnvironment().getTable(TableName.valueOf(targetTableName));        table.put(calleePut);        table.close();    }}</code></pre><p>2）重新创建HBase表，并设置为该表设置协处理器。在“表描述器”中调用addCoprocessor方法进行协处理器的设置，大概是这样的：（你需要找到你的建表的那部分代码，添加如下逻辑）<br>tableDescriptor.addCoprocessor(“hbase.CalleeWriteObserver”);</p><h4 id="5、运行测试：协处理器"><a href="#5、运行测试：协处理器" class="headerlink" title="5、运行测试：协处理器"></a>5、运行测试：协处理器</h4><p>重新编译项目，发布jar包到hbase的lib目录下（注意需群发）：<br>$ scp -r CT_consumer-1.0-SNAPSHOT.jar root@hsiehchou121:<code>pwd</code></p><p>重新修改<strong>hbase-site.xml</strong>：</p><pre><code>    &lt;property&gt;        &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;        &lt;value&gt;hbase.CalleeWriteObserver&lt;/value&gt;    &lt;/property&gt;</code></pre><p>完成以上步骤后，重新消费数据进行测试</p><h4 id="6、编写测试单元：范围查找数据"><a href="#6、编写测试单元：范围查找数据" class="headerlink" title="6、编写测试单元：范围查找数据"></a>6、编写测试单元：范围查找数据</h4><p><strong>思路</strong><br>a）已知要查询的手机号码以及起始时间节点和结束时间节点，查询该节点范围内的该手机号码的通话记录</p><p>b）拼装startRowKey和stopRowKey，即扫描范围，要想拼接出扫描范围，首先需要了解rowkey组成结构，我们再来复习一下，举个大栗子<br>rowkey：<br>分区号_手机号码1_通话建立时间_手机号码2_主(被)叫标记_通话持续时间<br>01_15837312345_20180725071833_1_0180</p><p>c）比如按月查询通话记录，则startRowKey举例：<br>regionHash_158373123456_20180805010000<br>stopRowKey举例：<br>regionHash_158373123456_20180805010000</p><p><strong>注意</strong>：startRowKey和stopRowKey设计时，后面的部分已经被去掉</p><p><strong>尖叫提示</strong>：rowKey的扫描范围为前闭后开</p><p><strong>尖叫提示</strong>：rowKey默认是有序的，排序规则为字符的按位比较<br>d）如果查找所有的，需要多次scan表，每次scan设置为下一个时间窗口即可，该操作可放置于for循环中</p><p><strong>编码</strong>：<br>e）<strong>运行测试</strong><br>观察是否已经按照时间范围查询出对应的数据</p><h4 id="7、将数据从本地读取到HBase"><a href="#7、将数据从本地读取到HBase" class="headerlink" title="7、将数据从本地读取到HBase"></a>7、将数据从本地读取到HBase</h4><p>1）<strong>启动ZooKeeper</strong>（配置了全局环境变量）</p><pre><code>zkServer.sh start</code></pre><p>2）<strong>启动Kafka</strong>（配置了全局环境变量）</p><pre><code>kafka-server-start.sh /root/hd/kafka/config/server.properties &amp;  </code></pre><p><strong>创建主题</strong></p><pre><code>bin/kafka-topics.sh --zookeeper hsiehchou121:2181 --topic calllog --create --replication-factor 1 --partitions 3</code></pre><p><strong>列出所有主题</strong></p><pre><code>bin/kafka-topics.sh --zookeeper hsiehchou121:2181 --list</code></pre><p><strong>启动 Kafka消费者</strong></p><pre><code>bin/kafka-console-consumer.sh --bootstrap-server hsiehchou121:9092 --topic calllog --from-beginning </code></pre><p>3）<strong>启动Hadoop</strong>（配置了全局环境变量）</p><pre><code>start-all.sh</code></pre><p>4）<strong>启动HBase</strong>（配置了全局环境变量）<br><strong>start-hbase.sh</strong></p><p>5）<strong>启动Flume</strong>（没有配置全局环境变量，去flume目录下）</p><pre><code>bin/flume-ng agent --conf conf/ --name a1 --conf-file myagent/flume2kafka.conf</code></pre><p>6）<strong>IDEA打包CT_consumer.jar</strong><br>此jar包要放入hbase的lib下面，不然HBase写不进数据</p><p>7）在IDEA里面<strong>运行HBaseConsumer.java</strong></p><h4 id="8、总结"><a href="#8、总结" class="headerlink" title="8、总结"></a>8、总结</h4><p>数据（本地）-&gt;Flume采集数据-&gt;Kafka消费数据-&gt;HBase</p><h3 id="六、数据分析"><a href="#六、数据分析" class="headerlink" title="六、数据分析"></a>六、数据分析</h3><p>我们的数据已经完整的采集到了HBase集群中，这次我们需要对采集到的数据进行分析，统计出我们想要的结果。注意，在分析的过程中，我们不一定会采取一个业务指标对应一个MapReduce-Job的方式，如果情景允许，我们会采取一个MapReduce分析多个业务指标的方式来进行任务</p><p><strong>数据分析模块流程图</strong></p><p><img src="/medias/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%A8%A1%E5%9D%97%E6%B5%81%E7%A8%8B%E5%9B%BE.PNG" alt="数据分析模块流程图"></p><p><strong>业务指标</strong><br>a）用户每天主叫通话个数统计，通话时间统计<br>b）用户每月通话记录统计，通话时间统计<br>c）用户之间亲密关系统计。（通话次数与通话时间体现用户亲密关系）</p><h4 id="1、MySQL表结构设计"><a href="#1、MySQL表结构设计" class="headerlink" title="1、MySQL表结构设计"></a>1、MySQL表结构设计</h4><p>我们将分析的结果数据保存到Mysql中，以方便Web端进行查询展示<br>1）表：<strong>db_telecom.tb_contacts</strong></p><p>用于存放用户手机号码与联系人姓名</p><table><thead><tr><th align="center">列</th><th align="center">备注</th><th align="center">类型</th></tr></thead><tbody><tr><td align="center">id</td><td align="center">自增主键</td><td align="center">int(11) NOT NULL</td></tr><tr><td align="center">telephone</td><td align="center">手机号码</td><td align="center">varchar(255) NOT NULL</td></tr><tr><td align="center">name</td><td align="center">联系人姓名</td><td align="center">varchar(255) NOT NULL</td></tr></tbody></table><p>2）表：<strong>db_telecom.tb_call</strong></p><p>用于存放某个时间维度下通话次数与通话时长的总和</p><table><thead><tr><th align="center">列</th><th align="center">备注</th><th align="center">类型</th></tr></thead><tbody><tr><td align="center">id_date_contact</td><td align="center">复合主键（联系人维度id，时间维度id）</td><td align="center">varchar(255) NOT NULL</td></tr><tr><td align="center">id_date_dimension</td><td align="center">时间维度id</td><td align="center">int(11) NOT NULL</td></tr><tr><td align="center">id_contact</td><td align="center">查询人的电话号码</td><td align="center">int(11) NOT NULL</td></tr><tr><td align="center">call_sum</td><td align="center">通话次数总和</td><td align="center">int(11) NOT NULL DEFAULT 0</td></tr><tr><td align="center">call_duration_sum</td><td align="center">通话时长总和</td><td align="center">int(11) NOT NULL DEFAULT 0</td></tr></tbody></table><p>3）表：<strong>db_telecom.tb_dimension_date</strong></p><p>用于存放时间维度的相关数据</p><table><thead><tr><th align="center">列</th><th align="center">备注</th><th align="center">类型</th></tr></thead><tbody><tr><td align="center">id</td><td align="center">自增主键</td><td align="center">int(11) NOT NULL</td></tr><tr><td align="center">year</td><td align="center">年，当前通话信息所在年</td><td align="center">int(11) NOT NULL</td></tr><tr><td align="center">month</td><td align="center">月，当前通话信息所在月，如果按照年来统计信息，则month为-1</td><td align="center">int(11) NOT NULL</td></tr><tr><td align="center">day</td><td align="center">日，当前通话信息所在日，如果是按照月来统计信息，则day为-1</td><td align="center">int(11) NOT NULL</td></tr></tbody></table><p>MySQL的建表语句：</p><p>CREATE TABLE <code>tb_call</code> (<br>  <code>id_date_contact</code> varchar(255) NOT NULL,<br>  <code>id_date_dimension</code> int(11) NOT NULL,<br>  <code>id_contact</code> int(11) NOT NULL,<br>  <code>call_sum</code> int(11) NOT NULL,<br>  <code>call_duration_sum</code> int(11) NOT NULL,<br>  PRIMARY KEY (<code>id_date_contact</code>)<br>) ENGINE=InnoDB DEFAULT CHARSET=utf8;</p><p>CREATE TABLE <code>tb_contacts</code> (<br>  <code>id</code> int(11) NOT NULL AUTO_INCREMENT,<br>  <code>telephone</code> varchar(255) NOT NULL,<br>  <code>name</code> varchar(255) NOT NULL,<br>  PRIMARY KEY (<code>id</code>)<br>) ENGINE=InnoDB AUTO_INCREMENT=21 DEFAULT CHARSET=utf8;</p><p>CREATE TABLE <code>tb_dimension_date</code> (<br>  <code>id</code> int(11) NOT NULL AUTO_INCREMENT,<br>  <code>year</code> int(11) NOT NULL,<br>  <code>month</code> int(11) NOT NULL,<br>  <code>day</code> int(11) NOT NULL,<br>  PRIMARY KEY (<code>id</code>)<br>) ENGINE=InnoDB AUTO_INCREMENT=263 DEFAULT CHARSET=utf8;</p><h4 id="2、需求：按照不同的维度统计通话"><a href="#2、需求：按照不同的维度统计通话" class="headerlink" title="2、需求：按照不同的维度统计通话"></a>2、需求：按照不同的维度统计通话</h4><p>根据需求目标，设计出如上表结构。我们需要按照时间范围（年月日），结合MapReduce统计出所属时间范围内所有手机号码的通话次数总和以及通话时长总和。<br>思路：<br>a）维度，即某个角度，某个视角，按照时间维度来统计通话，比如我想统计2018年所有月份所有日子的通话记录，那这个维度我们大概可以表述为2018年<code>*</code>月<code>*</code>日<br>b）通过Mapper将数据按照不同维度聚合给Reducer<br>c）通过Reducer拿到按照各个维度聚合过来的数据，进行汇总，输出。<br>d）根据业务需求，将Reducer的输出通过Outputformat把数据<br>数据输入：HBase<br>数据输出：MySQL<br>HBase中数据源结构：</p><table><thead><tr><th align="center">标签</th><th align="center">举例&amp;说明</th></tr></thead><tbody><tr><td align="center">rowkey</td><td align="center">hashregion_caller_datetime_callee_flag_duration;  01_15837312345_20180527081033_13766889900_1_0180</td></tr><tr><td align="center">family</td><td align="center">f1列族：存放主叫信息； f2列族：存放被叫信息</td></tr><tr><td align="center">caller</td><td align="center">第一个手机号码</td></tr><tr><td align="center">callee</td><td align="center">第二个手机号码</td></tr><tr><td align="center">date_time</td><td align="center">通话建立的时间，例如：20181017081520</td></tr><tr><td align="center">date_time_ts</td><td align="center">date_time对应的时间戳形式</td></tr><tr><td align="center">duration</td><td align="center">通话时长(单位：秒)</td></tr><tr><td align="center">flag</td><td align="center">标记caller是主叫还是被叫（caller的身份与call2的身份互斥）</td></tr></tbody></table><p>a）已知目标，那么需要结合目标思考已有数据是否能够支撑目标实现；<br>b） 根据目标数据结构，构建MySQL表结构，建表；<br>c）思考代码需要涉及到哪些功能模块，建立不同功能模块对应的包结构。<br>d）描述数据，一定是基于某个维度（视角）的，所以构建维度类。比如按照“年”与“手机号码”的组合作为key聚合所有的数据，便可以统计这个手机号码，这一年的相关结果<br>e）自定义OutputFormat用于对接MySQL，使数据输出<br>f）创建相关工具类</p><h4 id="3、环境准备"><a href="#3、环境准备" class="headerlink" title="3、环境准备"></a>3、环境准备</h4><p>1） <strong>新建module：ct_analysis</strong><br><strong>pom文件配置</strong></p><pre><code>  &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;junit&lt;/groupId&gt;            &lt;artifactId&gt;junit&lt;/artifactId&gt;            &lt;version&gt;4.12&lt;/version&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;mysql&lt;/groupId&gt;            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;            &lt;version&gt;8.0.13&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;            &lt;version&gt;1.3.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;            &lt;artifactId&gt;hbase-server&lt;/artifactId&gt;            &lt;version&gt;1.3.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;            &lt;artifactId&gt;hbase-common&lt;/artifactId&gt;            &lt;version&gt;1.3.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.aspectj&lt;/groupId&gt;            &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt;            &lt;version&gt;1.8.10&lt;/version&gt;        &lt;/dependency&gt;        &lt;!--简化javabean--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;version&gt;1.16.18&lt;/version&gt;            &lt;scope&gt;provided&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;                &lt;version&gt;2.12.4&lt;/version&gt;                &lt;configuration&gt;                    &lt;skipTests&gt;true&lt;/skipTests&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;                &lt;version&gt;3.8.1&lt;/version&gt;                &lt;configuration&gt;                    &lt;source&gt;1.8&lt;/source&gt;                    &lt;target&gt;1.8&lt;/target&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;</code></pre><p>2）<strong>创建包结构</strong>，根包：<strong>com.hsiehchou</strong>(不同颜色代表不同层级的递进)</p><p><img src="/medias/CT_XZ.PNG" alt="CT_XZ"></p><p>直接看CT_analysis</p><p>3） <strong>类表</strong><br>|  类名    |    备注    |<br>| :——–: | :——–:|<br>| DimensionConverter | 负责实际的维度转id功能接口|<br>| DimensionConverterImpl  |  DimensionConverter  实现类，负责实际的维度转id功能 |<br>| BaseDimension  |  维度（key）基类   |<br>| BaseValue    | 值（value）基类    |<br>| ComDimension  | 时间维度+联系人维度的组合维度  |<br>| ContactDimension | 联系人维度  |<br>| DateDimension    |  时间维度   |<br>| CountDurationValue  |  通话次数与通话时长的封装 |<br>| CountDurationMapper |  数据分析的Mapper类，继承自TableMapper   |<br>| MysqlOutputFormat  | 自定义Outputformat，对接Mysql  |<br>| CountDurationReducer  | 数据分析的Reducer类，继承自Reduccer  |<br>| CountDurationRunner | 数据分析的驱动类，组装Job |<br>| JDBCInstance    |  获取连接实例   |<br>| JDBCUtils  |  连接Mysql的工具类   |<br>| LRUCache   |  用于缓存已知的维度id，减少对mysql的操作次数，提高效率   |  </p><h4 id="4、编写代码：数据分析"><a href="#4、编写代码：数据分析" class="headerlink" title="4、编写代码：数据分析"></a>4、编写代码：数据分析</h4><p>1）创建类：<strong>CountDurationMapper</strong></p><pre><code>package mapper;import kv.key.ComDimension;import kv.key.ContactDimension;import kv.key.DateDimension;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableMapper;import org.apache.hadoop.hbase.util.Bytes;import org.apache.hadoop.io.Text;import java.io.IOException;import java.util.HashMap;import java.util.Map;public class CountDurationMapper extends TableMapper&lt;Comparable, Text&gt; {    private ComDimension comDimension = new ComDimension();    private Text durationText = new Text();    private Map&lt;String, String&gt; phoneMap;    @Override    protected void setup(Context context) throws IOException, InterruptedException {        phoneMap = new HashMap&lt;&gt;(20);        //批量修改名字Ctrl + Alt + Shift + J        phoneMap.put(&quot;17078388295&quot;, &quot;李为&quot;);        phoneMap.put(&quot;13980337439&quot;, &quot;王军&quot;);        phoneMap.put(&quot;14575535933&quot;, &quot;时俊&quot;);        phoneMap.put(&quot;18902496992&quot;, &quot;天机&quot;);        phoneMap.put(&quot;18549641558&quot;, &quot;蔡铭&quot;);        phoneMap.put(&quot;17005930322&quot;, &quot;陶尚&quot;);        phoneMap.put(&quot;18468618874&quot;, &quot;魏山帅&quot;);        phoneMap.put(&quot;18576581848&quot;, &quot;华倩&quot;);        phoneMap.put(&quot;15978226424&quot;, &quot;焦君山&quot;);        phoneMap.put(&quot;15542823911&quot;, &quot;钟尾田&quot;);        phoneMap.put(&quot;17526304161&quot;, &quot;司可可&quot;);        phoneMap.put(&quot;15422018558&quot;, &quot;官渡&quot;);        phoneMap.put(&quot;17269452013&quot;, &quot;上贵坡&quot;);        phoneMap.put(&quot;17764278604&quot;, &quot;时光机&quot;);        phoneMap.put(&quot;15711910344&quot;, &quot;李发&quot;);        phoneMap.put(&quot;15714728273&quot;, &quot;蒂冈&quot;);        phoneMap.put(&quot;16061028454&quot;, &quot;范德&quot;);        phoneMap.put(&quot;16264433631&quot;, &quot;周朝王&quot;);        phoneMap.put(&quot;17601615878&quot;, &quot;谢都都&quot;);        phoneMap.put(&quot;15897468949&quot;, &quot;刘何思&quot;);    }    @Override    protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException {        //05_18902496992_1525454104000_15711910344_1_1705        String rowkey = Bytes.toString(key.get());        String[] splits = rowkey.split(&quot;_&quot;);        if (&quot;0&quot;.equals(splits[4])){            return;        }        //聚合的是主叫数据        String caller = splits[1];        String callee = splits[3];        String buildTime = splits[2];        String duration = splits[5];        durationText.set(duration);        String year = buildTime.substring(0,4);        String month = buildTime.substring(4,6);        String day = buildTime.substring(6,8);        //年、月、日整数        DateDimension yearDimension = new DateDimension(year, &quot;-1&quot;, &quot;-1&quot;);        DateDimension monthDimension = new DateDimension(year, month, &quot;-1&quot;);        DateDimension dayDimension = new DateDimension(year, month, day);        //主叫callerContactDimension        ContactDimension callerContactDimension = new ContactDimension(caller, phoneMap.get(caller));        comDimension.setContactDimension(callerContactDimension);        //年        comDimension.setDateDimension(yearDimension);        context.write(comDimension, durationText);        //月        comDimension.setDateDimension(monthDimension);        context.write(comDimension, durationText);        //日        comDimension.setDateDimension(dayDimension);        context.write(comDimension, durationText);        //被叫callerContactDimension        ContactDimension calleeContactDimension = new ContactDimension(callee, phoneMap.get(callee));        comDimension.setContactDimension(calleeContactDimension);        //年        comDimension.setDateDimension(yearDimension);        context.write(comDimension, durationText);        //月        comDimension.setDateDimension(monthDimension);        context.write(comDimension, durationText);        //日        comDimension.setDateDimension(dayDimension);        context.write(comDimension, durationText);    }}</code></pre><p>2）创建类：<strong>CountDurationReducer</strong></p><pre><code>package reducer;import kv.key.ComDimension;import kv.value.CountDurationValue;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;import java.io.IOException;public class CountDurationReducer extends Reducer&lt;ComDimension, Text, ComDimension, CountDurationValue&gt; {    private CountDurationValue countDurationValue = new CountDurationValue();    @Override    protected void reduce(ComDimension key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException {        int callSum = 0;        int callDuration = 0;        for (Text t : values){            callSum++;            callDuration += Integer.valueOf(t.toString());        }        countDurationValue.setCallSum(String.valueOf(callSum));        countDurationValue.setCallDurationSum(String.valueOf(callDuration));        context.write(key, countDurationValue);    }}</code></pre><p>3）创建类：<strong>CountDurationRunner</strong></p><pre><code>package runner;import kv.key.ComDimension;import kv.value.CountDurationValue;import mapper.CountDurationMapper;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.Admin;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import org.apache.hadoop.hbase.client.Scan;import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;import outputformat.MysqlOutputFormat;import reducer.CountDurationReducer;import java.io.IOException;public class CountDurationRunner implements Tool {    private Configuration conf = null;    @Override    public void setConf(Configuration conf) {        this.conf = HBaseConfiguration.create(conf);    }    @Override    public Configuration getConf() {        return this.conf;    }    @Override    public int run(String[] strings) throws Exception {        //得到conf        Configuration conf = this.getConf();        //实例化Job        Job job = Job.getInstance(conf);        job.setJarByClass(CountDurationRunner.class);        //组装Mapper InputFormat        initHBaseInputConfig(job);        //组装Reducer OutputFormay        initReducerOutputConfig(job);        return job.waitForCompletion(true) ? 0:1;    }    private void initHBaseInputConfig(Job job) {        Connection connection = null;        Admin admin = null;        try {            String tableName = &quot;ns_ct:calllog&quot;;            connection = ConnectionFactory.createConnection(job.getConfiguration());            admin = connection.getAdmin();            if (!admin.tableExists(TableName.valueOf(tableName))){                throw new RuntimeException(&quot;无法找到目标表&quot;);            }            Scan scan = new Scan();            //可以优化            TableMapReduceUtil.initTableMapperJob(                    tableName,                    scan,                    CountDurationMapper.class,                    ComDimension.class,                    Text.class,                    job,                    true);        } catch (IOException e) {            e.printStackTrace();        }finally {            try {                if (admin != null){                    admin.close();                }                if (connection != null &amp;&amp; !connection.isClosed()){                    connection.close();                }            } catch (IOException e) {                e.printStackTrace();            }        }    }    private void initReducerOutputConfig(Job job){        job.setReducerClass(CountDurationReducer.class);        job.setOutputKeyClass(ComDimension.class);        job.setOutputKeyClass(CountDurationValue.class);        job.setOutputFormatClass(MysqlOutputFormat.class);    }    public static void main(String[] args) {        try {            int status = ToolRunner.run(new CountDurationRunner(), args);            System.exit(status);        } catch (Exception e) {            e.printStackTrace();        }    }}</code></pre><p>4）创建类：<strong>MysqlOutputFormat</strong></p><pre><code>package outputformat;import converter.DimensionConverterImpl;import kv.key.ComDimension;import kv.value.CountDurationValue;import org.apache.hadoop.fs.Path;import org.apache.hadoop.mapreduce.*;import org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import utils.JDBCInstance;import utils.JDBCUtils;import java.io.IOException;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.SQLException;public class MysqlOutputFormat extends OutputFormat&lt;ComDimension, CountDurationValue&gt; {    private OutputCommitter committer = null;    @Override    public RecordWriter&lt;ComDimension, CountDurationValue&gt; getRecordWriter(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {        //初始化JDBC连接对象        Connection conn = null;        conn = JDBCInstance.getInstance();        try {            //出问题的点之一，报空指针            conn.setAutoCommit(false);        } catch (SQLException e) {            throw new RuntimeException(e.getMessage());        }        return new MysqlRecordWriter(conn);    }    //输出校验    @Override    public void checkOutputSpecs(JobContext jobContext) throws InterruptedException {    }    @Override    public OutputCommitter getOutputCommitter(TaskAttemptContext context) throws IOException {        //此方法点击OutputFormat（按Ctrl + H的源码，复制getOutputCommitter，在FileOutputFormat）        if (committer == null){            String name = context.getConfiguration().get(FileOutputFormat.OUTDIR);            Path outputPath = name==null ? null:new Path(name);            committer = new FileOutputCommitter(outputPath, context);        }        return committer;    }    private static class MysqlRecordWriter extends RecordWriter&lt;ComDimension, CountDurationValue&gt;{        private DimensionConverterImpl dci = new DimensionConverterImpl();        private Connection conn = null;        private PreparedStatement preparedStatement = null;        private String insertSQL =null;        private int count = 0;        private final int BATCH_SIZE = 500;//批次大小        public MysqlRecordWriter(Connection conn){            this.conn = conn;        }        /**         * 输出到mysql         * @param key         * @param value         * @throws IOException         * @throws InterruptedException         */        @Override        public void write(ComDimension key, CountDurationValue value) throws IOException, InterruptedException {            try{                //tb_call                //id_date_contact, id_date_dimension, id_cantact, call_sum, call_duration_sum                //year month day                int idDateDimension = dci.getDimensionID(key.getDateDimension());                //telephone name                int idContactDimension = dci.getDimensionID(key.getContactDimension());                String idDateContact = idDateDimension + &quot;_&quot; + idContactDimension;                int callSum = Integer.valueOf(value.getCallSum());                int callDurationSum = Integer.valueOf(value.getCallDurationSum());                if (insertSQL == null){                    insertSQL = &quot;INSERT INTO `tb_call` (`id_date_contact`, `id_date_dimension`, `id_contact`,  `call_sum`, `call_duration_sum`) values (?,?,?,?,?) ON DUPLICATE KEY UPDATE `id_date_contact` = ?;&quot;;                }                if (preparedStatement == null){                    preparedStatement = conn.prepareStatement(insertSQL);                }                //本次SQL                int i = 0;                preparedStatement.setString(++i, idDateContact);                preparedStatement.setInt(++i, idDateDimension);                preparedStatement.setInt(++i, idContactDimension);                preparedStatement.setInt(++i, callSum);                preparedStatement.setInt(++i, callDurationSum);                //无则插入，有则更新的判断依据，增加批次                preparedStatement.setString(++i, idDateContact);                preparedStatement.addBatch();                //当前缓存了多少个sql语句等待批量执行，计数器                count++;                if (count &gt;= BATCH_SIZE){                    preparedStatement.executeBatch();//执行批处理命令                    conn.commit();                    count = 0;                    preparedStatement.clearBatch();//清除批处理命令                }            }catch (Exception e){                e.printStackTrace();            }        }        @Override        public void close(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {            try {                if (preparedStatement != null){                    preparedStatement.executeBatch();                        this.conn.commit();                }            } catch (SQLException e) {                e.printStackTrace();            }finally {                JDBCUtils.close(conn, preparedStatement, null);            }        }    }}</code></pre><p>5）创建类：<strong>BaseDimension</strong></p><pre><code>package kv.base;import org.apache.hadoop.io.WritableComparable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;public abstract class BaseDimension implements WritableComparable&lt;BaseDimension&gt; {    public abstract int compareTo(BaseDimension o);    //将字节写入二进制流    public abstract void write(DataOutput out) throws IOException;    //从二进制流读取字节    public abstract void readFields(DataInput in) throws IOException;}</code></pre><p>6）创建类：<strong>BaseValue</strong></p><pre><code>package kv.base;import org.apache.hadoop.io.Writable;public abstract class BaseValue implements Writable {}</code></pre><p>7）创建类：<strong>ComDimension</strong></p><pre><code>package kv.key;import kv.base.BaseDimension;import lombok.AllArgsConstructor;import lombok.Getter;import lombok.NoArgsConstructor;import lombok.Setter;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;@Getter@Setter@NoArgsConstructor@AllArgsConstructorpublic class ComDimension extends BaseDimension {    //联系人维度    private ContactDimension contactDimension = new ContactDimension();    //时间维度    private DateDimension dateDimension = new DateDimension();    @Override    public int compareTo(BaseDimension o) {        ComDimension o1 = (ComDimension) o;        int result = this.dateDimension.compareTo(o1.dateDimension);        if (result != 0) {            return result;        }        result = this.contactDimension.compareTo(o1.contactDimension);        return result;    }    @Override    public void write(DataOutput out) throws IOException {        contactDimension.write(out);        dateDimension.write(out);    }    @Override    public void readFields(DataInput in) throws IOException {        this.contactDimension.readFields(in);        this.dateDimension.readFields(in);    }}</code></pre><p>8）创建类：<strong>ContactDimension</strong></p><pre><code>package kv.key;import kv.base.BaseDimension;import lombok.AllArgsConstructor;import lombok.Getter;import lombok.NoArgsConstructor;import lombok.Setter;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;/** * 联系人维度类 */@Getter@Setter@NoArgsConstructor@AllArgsConstructorpublic class ContactDimension extends BaseDimension {    //手机号码    private String telephone;    //姓名    private String name;    @Override    public int compareTo(BaseDimension o) {        ContactDimension o1 = (ContactDimension) o;        int result = this.name.compareTo(o1.name);        if (result != 0){            return result;        }        result = this.telephone.compareTo(o1.telephone);        return result;    }    //将字节写入二进制流    @Override    public void write(DataOutput out) throws IOException {        out.writeUTF(this.telephone);        out.writeUTF(this.name);    }    //从二进制流读取字节    // Alt + Enter    @Override    public void readFields(DataInput in) throws IOException {        this.telephone = in.readUTF();        this.name = in.readUTF();    }}</code></pre><p>9）创建类：<strong>DateDimension</strong></p><pre><code>package kv.key;import kv.base.BaseDimension;import lombok.AllArgsConstructor;import lombok.Getter;import lombok.NoArgsConstructor;import lombok.Setter;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;/** * 时间维度类 */@Getter@Setter@NoArgsConstructor@AllArgsConstructorpublic class DateDimension extends BaseDimension {    //时间维度：当前通话信息所在年    private String year;    //时间维度：当前通话信息所在月,如果按照年来统计信息，则month为-1    private String month;    //时间维度：当前通话信息所在日,如果按照年来统计信息，则day为-1。    private String day;    @Override    public int compareTo(BaseDimension o) {        DateDimension o1 = (DateDimension) o;        int result = this.year.compareTo(o1.year);        if (result != 0){            return result;        }        result = this.month.compareTo(o1.month);        if (result != 0){           return result;        }        result = this.day.compareTo(o1.day);        return result;    }    @Override    public void write(DataOutput out) throws IOException {        out.writeUTF(this.year);        out.writeUTF(this.month);        out.writeUTF(this.day);    }    @Override    public void readFields(DataInput in) throws IOException {        this.year = in.readUTF();        this.month = in.readUTF();        this.day = in.readUTF();    }}</code></pre><p>10）创建类：<strong>CountDurationValue</strong></p><pre><code>package kv.value;import kv.base.BaseValue;import lombok.AllArgsConstructor;import lombok.Getter;import lombok.NoArgsConstructor;import lombok.Setter;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;@Setter@Getter@AllArgsConstructor@NoArgsConstructorpublic class CountDurationValue extends BaseValue {    //某个维度通话次数总和    private String callSum;    //某个维度通话时间总和    private String callDurationSum;    @Override    public void write(DataOutput dataOutput) throws IOException {        dataOutput.writeUTF(this.callSum);        dataOutput.writeUTF(this.callDurationSum);    }    @Override    public void readFields(DataInput dataInput) throws IOException {        this.callSum = dataInput.readUTF();        this.callDurationSum = dataInput.readUTF();    }}</code></pre><p>11） 创建类：<strong>JDBCUtil</strong></p><pre><code>package utils;import java.sql.*;public class JDBCUtils {    private static final String MYSQL_DRIVER_CLASS = &quot;com.mysql.cj.jdbc.Driver&quot;;    private static final String MYSQL_URL = &quot;jdbc:mysql://hsiehchou121:3306/db_telecom?useUnicode=true&amp;characterEncoding=UTF-8&quot;;    private static final String MYSQL_USERNAME = &quot;root&quot;;    private static final String MYSQL_PASSWORD = &quot;root&quot;;    /**     * 实例化JDBC连接器     * @return     */    public static Connection getConnection(){        try {            Class.forName(MYSQL_DRIVER_CLASS);            return DriverManager.getConnection(MYSQL_URL, MYSQL_USERNAME, MYSQL_PASSWORD);        } catch (ClassNotFoundException e) {            e.printStackTrace();        } catch (SQLException e) {            e.printStackTrace();        }        return null;    }    public static void close(Connection connection, Statement statement, ResultSet resultSet){        try {            if (resultSet != null &amp;&amp; !resultSet.isClosed()){                resultSet.close();            }            if (statement != null &amp;&amp; !statement.isClosed()){                statement.close();            }            if (connection != null &amp;&amp; !connection.isClosed()){                connection.close();            }        } catch (SQLException e) {            e.printStackTrace();        }    }}</code></pre><p>12）创建类：<strong>JDBCInstance</strong></p><pre><code>package utils;import java.sql.Connection;import java.sql.SQLException;/** * 获取链接实例 */public class JDBCInstance {    private static Connection connection = null;    public JDBCInstance() {    }    public static Connection getInstance(){        try {            if (connection == null || connection.isClosed() || !connection.isValid(3)){                connection = JDBCUtils.getConnection();            }        } catch (SQLException e) {            e.printStackTrace();        }        return connection;    }}</code></pre><p>13） 创建接口：<strong>DimensionConverter</strong></p><pre><code>package converter;import kv.base.BaseDimension;public interface DimensionConverter {    int getDimensionID(BaseDimension dimension);}</code></pre><p>14）创建类：<strong>DimensionConverterImpl</strong></p><pre><code>package converter;import kv.base.BaseDimension;import kv.key.ContactDimension;import kv.key.DateDimension;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import utils.JDBCInstance;import utils.JDBCUtils;import utils.LRUCache;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.ResultSet;import java.sql.SQLException;/** * 1、根据传入的维度数据，得到该数据对应的在表中的主键id * ** 做内存缓存，LRUCache * 分支 * -- 缓存中有数据 -&gt; 直接返回id * -- 缓存中无数据 -&gt; * ** 查询Mysql * 分支： * -- Mysql中有该条数据 -&gt; 直接返回id -&gt; 将本次读取到的id缓存到内存中 * -- Mysql中没有该数据  -&gt; 插入该条数据 -&gt; 再次反查该数据，得到id并返回 -&gt; 缓存到内存中 */public class DimensionConverterImpl implements DimensionConverter {    // Logger 打印该类的日志，取代resources里的log4j.properties    private static final Logger logger = LoggerFactory.getLogger(DimensionConverterImpl.class);    //对象线程化，用于每个线程管理自己的JDBC连接器    private ThreadLocal&lt;Connection&gt; threadLocalConnection = new ThreadLocal&lt;&gt;();    //构建内存缓存对象    private LRUCache lruCache = new LRUCache(3000);    public DimensionConverterImpl() {        //jvm关闭时，释放资源        Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; JDBCUtils.close(threadLocalConnection.get(), null, null)));    }    @Override    public int getDimensionID(BaseDimension dimension) {        //1、根据传入的维度对象获取对应的主键id，先从LRUCache中获取        //时间维度：date_dimension_year_month_day, 10        //联系人维度：contact_dimension_telephone_name(到了电话号码就不会重复了), 12        String cacheKey = getCacheKey(dimension);        //尝试获取缓存的id        if (lruCache.containsKey(cacheKey)) {            return lruCache.get(cacheKey);        }        //没有得到缓存id，需要执行select操作        //sqls包含了1组sql语句：查询和插入        String[] sqls = null;        if (dimension instanceof DateDimension) {            sqls = getDateDimensionSQL();        } else if (dimension instanceof ContactDimension) {            sqls = getContactDimensionSQL();        } else {            throw new RuntimeException(&quot;没有匹配到对应维度信息.&quot;);        }        //准备对Mysql表进行操作，先查询，有可能再插入        Connection conn = this.getConnection();        int id = -1;        synchronized (this) {            id = execSQL(conn, sqls, dimension);        }        //将刚查询的id加入到缓存中        lruCache.put(cacheKey, id);        return id;    }    /**     * 得到当前线程维护的Connection对象     *     * @return     */    public Connection getConnection() {        Connection conn = null;        try {            conn = threadLocalConnection.get();            if (conn == null || conn.isClosed()) {                conn = JDBCInstance.getInstance();                threadLocalConnection.set(conn);            }        } catch (SQLException e) {            e.printStackTrace();        }        return conn;    }    /**     * @param conn      JDBC连接器     * @param sqls      长度为2，第一个位置为查询语句，第二个位置为插入语句     * @param dimension 对应维度所保存的数据     * @return     */    private int execSQL(Connection conn, String[] sqls, BaseDimension dimension) {        PreparedStatement preparedStatement = null;        ResultSet resultSet = null;        try {            //1            //查询的preparedStatement            preparedStatement = conn.prepareStatement(sqls[0]);            //根据不同的维度，封装不同的SQL语句            setArguments(preparedStatement, dimension);            //执行查询            resultSet = preparedStatement.executeQuery();            if (resultSet.next()) {                int result = resultSet.getInt(1);                //释放资源                JDBCUtils.close(null, preparedStatement, resultSet);                return result;            }            //释放资源            JDBCUtils.close(null, preparedStatement, resultSet);            //2            //执行插入，封装插入的sql语句            preparedStatement = conn.prepareStatement(sqls[1]);            setArguments(preparedStatement, dimension);            //执行插入            preparedStatement.executeUpdate();            //释放资源            JDBCUtils.close(null, preparedStatement, null);            //3            //查询的preparedStatement            preparedStatement = conn.prepareStatement(sqls[0]);            //根据不同的维度，封装不同的SQL语句            setArguments(preparedStatement, dimension);            //执行查询            resultSet = preparedStatement.executeQuery();            if (resultSet.next()) {                return resultSet.getInt(1);            }        } catch (SQLException e) {            e.printStackTrace();        } finally {            //释放资源            JDBCUtils.close(null, preparedStatement, resultSet);        }        return -1;    }    /**     * 设置SQL语句的具体参数     *     * @param preparedStatement     * @param dimension     */    private void setArguments(PreparedStatement preparedStatement, BaseDimension dimension) {        int i = 0;        try {            if (dimension instanceof DateDimension) {                //可以优化                DateDimension dateDimension = (DateDimension) dimension;                preparedStatement.setString(++i, dateDimension.getYear());                preparedStatement.setString(++i, dateDimension.getMonth());                preparedStatement.setString(++i, dateDimension.getDay());            } else if (dimension instanceof ContactDimension) {                ContactDimension contactDimension = (ContactDimension) dimension;                preparedStatement.setString(++i, contactDimension.getTelephone());                preparedStatement.setString(++i, contactDimension.getName());            }        } catch (SQLException e) {            e.printStackTrace();        }    }    /**     * 返回联系人表的查询和插入语句     *     * @return     */    private String[] getContactDimensionSQL() {        String query = &quot;SELECT `id` FROM `tb_contacts` WHERE `telephone` = ? AND `name` = ? ORDER BY `id`;&quot;;        String insert = &quot;INSERT INTO `tb_contacts`(`telephone`, `name`) VALUES(?, ?);&quot;;        return new String[]{query, insert};    }    /**     * 返回时间表的查询和插入语句     *     * @return     */    private String[] getDateDimensionSQL() {        String query = &quot;SELECT `id` FROM `tb_dimension_date` WHERE `year` = ? AND `month` = ? AND `day` = ? ORDER BY `id`;&quot;;        String insert = &quot;INSERT INTO `tb_dimension_date`(`year`,`month`,`day`) VALUES(?, ?, ?);&quot;;        return new String[]{query, insert};    }    /**     * 根据维度信息得到维度对应的缓存键     *     * @param dimension     * @return     */    private String getCacheKey(BaseDimension dimension) {        StringBuilder sb = new StringBuilder();        if (dimension instanceof DateDimension) {            DateDimension dateDimension = (DateDimension) dimension;            sb.append(&quot;date_dimension&quot;)                    .append(dateDimension.getYear())                    .append(dateDimension.getMonth())                    .append(dateDimension.getDay());        } else if (dimension instanceof ContactDimension) {            ContactDimension contactDimension = (ContactDimension) dimension;            sb.append(&quot;contact_dimension&quot;)                    .append(contactDimension.getTelephone());        }        return sb.toString();    }}</code></pre><p>15） 创建类：<strong>LRUCache</strong></p><pre><code>package utils;import java.util.LinkedHashMap;import java.util.Map;public class LRUCache extends LinkedHashMap&lt;String, Integer&gt; {    private static  final long serialVersionUID = 1L;    protected int maxElements;    public LRUCache(int maxSize){        super(maxSize, 0.75F, true);        this.maxElements = maxSize;    }    @Override    protected boolean removeEldestEntry(Map.Entry&lt;String, Integer&gt; eldest) {        return (size() &gt; this.maxElements);    }}</code></pre><h4 id="5、运行测试"><a href="#5、运行测试" class="headerlink" title="5、运行测试"></a>5、运行测试</h4><p>1）将mysql驱动包放入到<strong>/opt/jars的lib目录下</strong><br><strong>mysql-connector-java-8.0.13.jar</strong> </p><p>2）<strong>提交任务</strong></p><pre><code>$ /root/hd/hadoop-2.8.4/bin/yarn jar /opt/jars/CT_analysis-1.0-SNAPSHOT.jar runner.CountDurationRunner -libjars /opt/jars/lib/mysql-connector-java-8.0.13.jar</code></pre><p>观察Mysql中的结果</p><h3 id="七、数据展示"><a href="#七、数据展示" class="headerlink" title="七、数据展示"></a>七、数据展示</h3><p>令人兴奋的时刻马上到了，接下来我们需要将某人按照不同维度查询出来的结果，展示到web页面上<br><strong>数据展示模块流程图</strong></p><p><img src="/medias/%E6%95%B0%E6%8D%AE%E5%B1%95%E7%A4%BA%E6%A8%A1%E5%9D%97%E6%B5%81%E7%A8%8B%E5%9B%BE.PNG" alt="数据展示模块流程图"></p><h4 id="1、环境准备"><a href="#1、环境准备" class="headerlink" title="1、环境准备"></a>1、环境准备</h4><p>1）新建module或项目：<strong>CT_web</strong><br>pom.xml配置文件：</p><pre><code>&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;  &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;  &lt;artifactId&gt;CT_web&lt;/artifactId&gt;  &lt;packaging&gt;war&lt;/packaging&gt;  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;  &lt;name&gt;ct_web Maven Webapp&lt;/name&gt;  &lt;url&gt;http://maven.apache.org&lt;/url&gt;  &lt;dependencies&gt;    &lt;!-- https://mvnrepository.com/artifact/junit/junit --&gt;    &lt;dependency&gt;      &lt;groupId&gt;junit&lt;/groupId&gt;      &lt;artifactId&gt;junit&lt;/artifactId&gt;      &lt;version&gt;4.12&lt;/version&gt;      &lt;scope&gt;test&lt;/scope&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;mysql&lt;/groupId&gt;      &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;      &lt;version&gt;8.0.13&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;c3p0&lt;/groupId&gt;      &lt;artifactId&gt;c3p0&lt;/artifactId&gt;      &lt;version&gt;0.9.1.2&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;org.mybatis&lt;/groupId&gt;      &lt;artifactId&gt;mybatis&lt;/artifactId&gt;      &lt;version&gt;3.2.1&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;org.springframework&lt;/groupId&gt;      &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt;      &lt;version&gt;4.3.3.RELEASE&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;org.springframework&lt;/groupId&gt;      &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt;      &lt;version&gt;4.3.3.RELEASE&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;org.springframework&lt;/groupId&gt;      &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt;      &lt;version&gt;4.3.3.RELEASE&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;org.mybatis&lt;/groupId&gt;      &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt;      &lt;version&gt;1.3.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;org.aspectj&lt;/groupId&gt;      &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt;      &lt;version&gt;1.8.10&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;javax.servlet&lt;/groupId&gt;      &lt;artifactId&gt;servlet-api&lt;/artifactId&gt;      &lt;version&gt;2.5&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;javax.servlet&lt;/groupId&gt;      &lt;artifactId&gt;jstl&lt;/artifactId&gt;      &lt;version&gt;1.2&lt;/version&gt;    &lt;/dependency&gt;      &lt;dependency&gt;          &lt;groupId&gt;org.springframework&lt;/groupId&gt;          &lt;artifactId&gt;spring-beans&lt;/artifactId&gt;          &lt;version&gt;4.3.3.RELEASE&lt;/version&gt;      &lt;/dependency&gt;  &lt;/dependencies&gt;  &lt;build&gt;    &lt;finalName&gt;ct_web&lt;/finalName&gt;    &lt;plugins&gt;      &lt;plugin&gt;        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;        &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;        &lt;version&gt;2.12.4&lt;/version&gt;        &lt;configuration&gt;          &lt;skipTests&gt;true&lt;/skipTests&gt;        &lt;/configuration&gt;      &lt;/plugin&gt;      &lt;plugin&gt;        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;        &lt;version&gt;3.8.1&lt;/version&gt;        &lt;configuration&gt;          &lt;source&gt;1.8&lt;/source&gt;          &lt;target&gt;1.8&lt;/target&gt;        &lt;/configuration&gt;      &lt;/plugin&gt;    &lt;/plugins&gt;  &lt;/build&gt;&lt;/project&gt;</code></pre><p>2）创建包结构，根包：com.hsiehchou<br>bean<br>controller<br>dao<br>3）类表</p><table><thead><tr><th align="center">类名</th><th align="center">备注</th></tr></thead><tbody><tr><td align="center">CallLog</td><td align="center">用于封装数据分析结果的JavaBean</td></tr><tr><td align="center">QueryInfo</td><td align="center">用于封装向服务器发来的请求参数</td></tr><tr><td align="center">CallLogHandler</td><td align="center">用于处理请求的Controller</td></tr><tr><td align="center">CallLogDAO</td><td align="center">查询某人某个维度通话记录的DAO</td></tr></tbody></table><p>4）web目录结构，web部分的根目录：webapp</p><table><thead><tr><th align="center">文件夹名</th><th align="center">备注</th></tr></thead><tbody><tr><td align="center">css</td><td align="center">存放css静态资源的文件夹</td></tr><tr><td align="center">html</td><td align="center">存放html静态资源的文件夹</td></tr><tr><td align="center">images</td><td align="center">存放图片静态资源文件夹</td></tr><tr><td align="center">js</td><td align="center">存放js静态资源的文件夹</td></tr><tr><td align="center">jsp</td><td align="center">存放jsp页面的文件夹</td></tr><tr><td align="center">WEB-INF</td><td align="center">存放web相关配置的文件夹</td></tr></tbody></table><p>5） resources目录下创建spring相关配置文件</p><p><strong>dbconfig.properties</strong>：用于存放数据库连接配置</p><pre><code>jdbc.user=rootjdbc.password=rootjdbc.jdbcUrl=jdbc:mysql://hsiehchou121:3306/db_telecom?useUnicode=true&amp;characterEncoding=UTF-8jdbc.driverClass=com.mysql.cj.jdbc.Driver</code></pre><p><strong>applicationContext.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;       xmlns:p=&quot;http://www.springframework.org/schema/p&quot;       xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot;       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd        http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd        http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd        http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd        http://www.springframework.org/schema/jee http://www.springframework.org/schema/jee/spring-jee.xsd        http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd&quot;&gt;    &lt;context:property-placeholder location=&quot;classpath:dbconfig.properties&quot;/&gt;    &lt;bean id=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt;        &lt;property name=&quot;user&quot; value=&quot;${jdbc.user}&quot;/&gt;        &lt;property name=&quot;password&quot; value=&quot;${jdbc.password}&quot;/&gt;        &lt;property name=&quot;driverClass&quot; value=&quot;${jdbc.driverClass}&quot;/&gt;        &lt;property name=&quot;jdbcUrl&quot; value=&quot;${jdbc.jdbcUrl}&quot;/&gt;    &lt;/bean&gt;    &lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt;        &lt;constructor-arg name=&quot;dataSource&quot; value=&quot;#{dataSource}&quot;&gt;&lt;/constructor-arg&gt;    &lt;/bean&gt;    &lt;bean id=&quot;namedParameterJdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate&quot;&gt;        &lt;constructor-arg name=&quot;dataSource&quot; value=&quot;#{dataSource}&quot;&gt;&lt;/constructor-arg&gt;    &lt;/bean&gt;    &lt;!-- 包扫描 --&gt;    &lt;context:component-scan base-package=&quot;controller&quot;&gt;&lt;/context:component-scan&gt;    &lt;context:component-scan base-package=&quot;dao&quot;&gt;&lt;/context:component-scan&gt;    &lt;!-- 配置视图解析器--&gt;    &lt;bean id=&quot;viewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt;        &lt;property name=&quot;prefix&quot; value=&quot;/&quot;/&gt;        &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt;    &lt;/bean&gt;    &lt;!--&lt;mvc:annotation-driven /&gt;--&gt;    &lt;!--&lt;mvc:default-servlet-handler /&gt;--&gt;    &lt;!--&lt;mvc:resources location=&quot;/images/&quot; mapping=&quot;/images/**&quot;/&gt;--&gt;    &lt;!--&lt;mvc:resources location=&quot;/js/&quot; mapping=&quot;/js/**&quot;/&gt;--&gt;    &lt;!--&lt;mvc:resources location=&quot;/css/&quot; mapping=&quot;/css/**&quot;/&gt;--&gt;&lt;/beans&gt;</code></pre><p>6） webapp的WEB-INF目录下创建web相关配置<br><strong>web.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd&quot;         version=&quot;3.1&quot;&gt;    &lt;display-name&gt;SpringMVC_CRUD&lt;/display-name&gt;    &lt;!-- spring拦截器 --&gt;    &lt;servlet&gt;        &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt;        &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt;        &lt;init-param&gt;            &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;            &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt;        &lt;/init-param&gt;        &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;    &lt;/servlet&gt;    &lt;servlet-mapping&gt;        &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt;        &lt;url-pattern&gt;/&lt;/url-pattern&gt;    &lt;/servlet-mapping&gt;    &lt;welcome-file-list&gt;        &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt;    &lt;/welcome-file-list&gt;    &lt;filter&gt;        &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt;        &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt;        &lt;init-param&gt;            &lt;param-name&gt;encoding&lt;/param-name&gt;            &lt;param-value&gt;utf-8&lt;/param-value&gt;        &lt;/init-param&gt;        &lt;init-param&gt;            &lt;param-name&gt;forceEncoding&lt;/param-name&gt;            &lt;param-value&gt;true&lt;/param-value&gt;        &lt;/init-param&gt;    &lt;/filter&gt;    &lt;filter-mapping&gt;        &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt;        &lt;url-pattern&gt;/*&lt;/url-pattern&gt;    &lt;/filter-mapping&gt;    &lt;servlet-mapping&gt;        &lt;servlet-name&gt;default&lt;/servlet-name&gt;        &lt;url-pattern&gt;*.jpg&lt;/url-pattern&gt;    &lt;/servlet-mapping&gt;    &lt;servlet-mapping&gt;        &lt;servlet-name&gt;default&lt;/servlet-name&gt;        &lt;url-pattern&gt;*.js&lt;/url-pattern&gt;    &lt;/servlet-mapping&gt;    &lt;servlet-mapping&gt;        &lt;servlet-name&gt;default&lt;/servlet-name&gt;        &lt;url-pattern&gt;*.css&lt;/url-pattern&gt;    &lt;/servlet-mapping&gt;&lt;/web-app&gt;</code></pre><p>7）拷贝js框架到webapp的js目录下<br>框架名称：<br>echarts.min.js</p><h4 id="2、编写代码-1"><a href="#2、编写代码-1" class="headerlink" title="2、编写代码"></a>2、编写代码</h4><p>思路：<br>a）首先测试数据通顺以及完整性，写一个联系人的测试用例。<br>b）测试通过后，通过输入手机号码以及时间参数，查询指定维度的数据，并以图表展示。<br>代码：<br>1）新建类： <strong>CallLog</strong></p><pre><code>package bean;/** * 用于存放返回给用户的数据 */public class CallLog {    private String id_date_contact;    private String id_date_dimension;    private String id_contact;    private String call_sum;    private String call_duration_sum;    private String telephone;    private String name;    private String year;    private String month;    private String day;    public String getId_date_contact() {        return id_date_contact;    }    public void setId_date_contact(String id_date_contact) {        this.id_date_contact = id_date_contact;    }    public String getId_date_dimension() {        return id_date_dimension;    }    public void setId_date_dimension(String id_date_dimension) {        this.id_date_dimension = id_date_dimension;    }    public String getId_contact() {        return id_contact;    }    public void setId_contact(String id_contact) {        this.id_contact = id_contact;    }    public String getCall_sum() {        return call_sum;    }    public void setCall_sum(String call_sum) {        this.call_sum = call_sum;    }    public String getCall_duration_sum() {        return call_duration_sum;    }    public void setCall_duration_sum(String call_duration_sum) {        this.call_duration_sum = call_duration_sum;    }    public String getTelephone() {        return telephone;    }    public void setTelephone(String telephone) {        this.telephone = telephone;    }    public String getName() {        return name;    }    public void setName(String name) {        this.name = name;    }    public String getYear() {        return year;    }    public void setYear(String year) {        this.year = year;    }    public String getMonth() {        return month;    }    public void setMonth(String month) {        this.month = month;    }    public String getDay() {        return day;    }    public void setDay(String day) {        this.day = day;    }    @Override    public String toString() {        return &quot;CallLog{&quot; +                &quot;call_sum=&#39;&quot; + call_sum + &#39;\&#39;&#39; +                &quot;, call_duration_sum=&#39;&quot; + call_duration_sum + &#39;\&#39;&#39; +                &quot;, telephone=&#39;&quot; + telephone + &#39;\&#39;&#39; +                &quot;, name=&#39;&quot; + name + &#39;\&#39;&#39; +                &quot;, year=&#39;&quot; + year + &#39;\&#39;&#39; +                &quot;, month=&#39;&quot; + month + &#39;\&#39;&#39; +                &quot;, day=&#39;&quot; + day + &#39;\&#39;&#39; +                &#39;}&#39;;    }}</code></pre><p>2）新建类：<strong>QueryInfo</strong></p><pre><code>package bean;/** * 该类用于存放用户请求的数据 */public class QueryInfo {    private String telephone;    private String year;    private String month;    private String day;    public QueryInfo() {        super();    }    public QueryInfo(String telephone, String year, String month, String day) {        super();        this.telephone = telephone;        this.year = year;        this.month = month;        this.day = day;    }    public String getTelephone() {        return telephone;    }    public void setTelephone(String telephone) {        this.telephone = telephone;    }    public String getYear() {        return year;    }    public void setYear(String year) {        this.year = year;    }    public String getMonth() {        return month;    }    public void setMonth(String month) {        this.month = month;    }    public String getDay() {        return day;    }    public void setDay(String day) {        this.day = day;    }}</code></pre><p>3）新建类： <strong>CallLogDAO</strong></p><pre><code>package dao;import bean.CallLog;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.jdbc.core.BeanPropertyRowMapper;import org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate;import org.springframework.stereotype.Repository;import java.util.HashMap;import java.util.List;@Repositorypublic class CallLogDAO {    @Autowired    private NamedParameterJdbcTemplate namedParameterJdbcTemplate;    public List&lt;CallLog&gt; getCallLogList(HashMap&lt;String, String&gt; paramsMap) {        //按照年统计：统计某个用户，1年12个月的所有的数据（不精确到day）        String sql = &quot;SELECT `call_sum`, `call_duration_sum`, `telephone`, `name`, `year` , `month`, `day` FROM tb_dimension_date t4 INNER JOIN ( SELECT `id_date_dimension`, `call_sum`, `call_duration_sum`, `telephone`, `name` FROM tb_call t2 INNER JOIN ( SELECT `id`, `telephone`, `name` FROM tb_contacts WHERE telephone = :telephone ) t1 ON t2.id_contact = t1.id ) t3 ON t4.id = t3.id_date_dimension WHERE `year` = :year AND `month` != :month AND `day` = :day ORDER BY `year`, `month`;&quot;;        BeanPropertyRowMapper&lt;CallLog&gt; beanPropertyRowMapper = new BeanPropertyRowMapper&lt;&gt;(CallLog.class);        List&lt;CallLog&gt; list = namedParameterJdbcTemplate.query(sql, paramsMap, beanPropertyRowMapper);        return list;    }}</code></pre><p>4）新建类：<strong>CallLogHandler</strong></p><pre><code>package controller;import bean.CallLog;import bean.QueryInfo;import dao.CallLogDAO;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.RequestMapping;import java.util.HashMap;import java.util.List;@Controllerpublic class CallLogHandler {    @RequestMapping(&quot;/queryCallLogList&quot;)    public String queryCallLog(Model model, QueryInfo queryInfo){        ApplicationContext applicationContext = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;);        CallLogDAO callLogDAO = applicationContext.getBean(CallLogDAO.class);        HashMap&lt;String, String&gt; hashMap = new HashMap&lt;&gt;();        hashMap.put(&quot;telephone&quot;, queryInfo.getTelephone());        hashMap.put(&quot;year&quot;, queryInfo.getYear());        hashMap.put(&quot;month&quot;, queryInfo.getMonth());        hashMap.put(&quot;day&quot;, queryInfo.getDay());        List&lt;CallLog&gt; list = callLogDAO.getCallLogList(hashMap);        StringBuilder dateSB = new StringBuilder();        StringBuilder callSumSB = new StringBuilder();        StringBuilder callDurationSumSB = new StringBuilder();        for(int i = 0; i &lt; list.size(); i++){            CallLog callLog = list.get(i);            //1月, 2月, ....12月,            dateSB.append(callLog.getMonth() + &quot;月,&quot;);            callSumSB.append(callLog.getCall_sum() + &quot;,&quot;);            callDurationSumSB.append(callLog.getCall_duration_sum() + &quot;,&quot;);        }        dateSB.deleteCharAt(dateSB.length() - 1);        callSumSB.deleteCharAt(callSumSB.length() - 1);        callDurationSumSB.deleteCharAt(callDurationSumSB.length() - 1);        //通过model返回数据        model.addAttribute(&quot;telephone&quot;, list.get(0).getTelephone());        model.addAttribute(&quot;name&quot;, list.get(0).getName());        model.addAttribute(&quot;date&quot;, dateSB.toString());        model.addAttribute(&quot;count&quot;, callSumSB.toString());        model.addAttribute(&quot;duration&quot;, callDurationSumSB.toString());        return &quot;jsp/CallLogListEchart&quot;;    }}</code></pre><p>5）新建：<strong>index.jsp</strong></p><pre><code>&lt;%@ taglib prefix=&quot;c&quot; uri=&quot;http://java.sun.com/jsp/jstl/core&quot; %&gt;&lt;%@ page language=&quot;java&quot; contentType=&quot;text/html; charset=utf-8&quot;         pageEncoding=&quot;utf-8&quot; %&gt;&lt;%    String path = request.getContextPath();%&gt;&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.w3.org/TR/html4/loose.dtd&quot;&gt;&lt;html&gt;    &lt;head&gt;        &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;        &lt;title&gt;电信查询系统&lt;/title&gt;        &lt;link href=&quot;//cdn.bootcss.com/bootstrap/3.3.6/css/bootstrap.min.css&quot; rel=&quot;stylesheet&quot;&gt;        &lt;script src=&quot;//cdn.bootcss.com/jquery/2.1.1/jquery.min.js&quot;&gt;&lt;/script&gt;        &lt;script src=&quot;//cdn.bootcss.com/bootstrap/3.3.6/js/bootstrap.min.js&quot;&gt;&lt;/script&gt;    &lt;/head&gt;    &lt;body&gt;        &lt;div style=&quot;width:410px;margin: 0 auto;&quot;&gt;            &lt;h3&gt;电信查询用户通话次数和通话时长系统&lt;/h3&gt;            &lt;br /&gt;            &lt;form role=&quot;form&quot; action=&quot;/queryCallLogList&quot; method=&quot;post&quot;&gt;                &lt;div class=&quot;form-group&quot; style=&quot;margin-bottom: 0;&quot;&gt;                    &lt;label for=&quot;name&quot;&gt;手机号码&lt;/label&gt;                    &lt;input type=&quot;text&quot; name=&quot;telephone&quot; class=&quot;form-control&quot; id=&quot;name&quot; placeholder=&quot;请输入手机号码&quot;&gt;                &lt;/div&gt;                &lt;div class=&quot;form-group&quot; style=&quot;margin-bottom: 0;&quot;&gt;                    &lt;label for=&quot;name&quot;&gt;年&lt;/label&gt;                    &lt;input type=&quot;text&quot; name=&quot;year&quot; class=&quot;form-control&quot; id=&quot;name&quot; placeholder=&quot;请输入年&quot;&gt;                &lt;/div&gt;                &lt;div class=&quot;form-group&quot; style=&quot;margin-bottom: 0;&quot;&gt;                    &lt;label for=&quot;name&quot;&gt;月&lt;/label&gt;                    &lt;input type=&quot;text&quot; name=&quot;month&quot; class=&quot;form-control&quot; id=&quot;name&quot; placeholder=&quot;请输入月&quot;&gt;                &lt;/div&gt;                &lt;div class=&quot;form-group&quot; style=&quot;margin-bottom: 0;&quot;&gt;                    &lt;label for=&quot;name&quot;&gt;日&lt;/label&gt;                    &lt;input type=&quot;text&quot; name=&quot;day&quot; class=&quot;form-control&quot; id=&quot;name&quot; placeholder=&quot;请输入日&quot;&gt;                &lt;/div&gt;                &lt;button type=&quot;submit&quot; class=&quot;btn btn-default&quot;&gt;查询&lt;/button&gt;            &lt;/form&gt;        &lt;/div&gt;        &lt;br /&gt;        &lt;div style=&quot;width: 1000px;margin: 0 auto;&quot;&gt;            &lt;table class=&quot;table&quot;&gt;                &lt;h4&gt;数据库电话号码表&lt;/h4&gt;                &lt;thead&gt;                    &lt;tr&gt;                        &lt;th&gt;姓名&lt;/th&gt;                        &lt;th&gt;手机号&lt;/th&gt;                        &lt;th&gt;&lt;/th&gt;                        &lt;th&gt;姓名&lt;/th&gt;                        &lt;th&gt;手机号&lt;/th&gt;                        &lt;th&gt;&lt;/th&gt;                        &lt;th&gt;姓名&lt;/th&gt;                        &lt;th&gt;手机号&lt;/th&gt;                        &lt;th&gt;&lt;/th&gt;                        &lt;th&gt;姓名&lt;/th&gt;                        &lt;th&gt;手机号&lt;/th&gt;                        &lt;th&gt;&lt;/th&gt;                        &lt;th&gt;姓名&lt;/th&gt;                        &lt;th&gt;手机号&lt;/th&gt;                    &lt;/tr&gt;                &lt;/thead&gt;                &lt;tbody&gt;                    &lt;tr class=&quot;active&quot;&gt;                        &lt;td&gt;李为&lt;/td&gt;                        &lt;td&gt;17078388295&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;王军&lt;/td&gt;                        &lt;td&gt;13980337439&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;时俊&lt;/td&gt;                        &lt;td&gt;14575535933&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;天机&lt;/td&gt;                        &lt;td&gt;18902496992&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;蔡铭&lt;/td&gt;                        &lt;td&gt;18549641558&lt;/td&gt;                    &lt;/tr&gt;                    &lt;tr class=&quot;success&quot;&gt;                        &lt;td&gt;陶尚&lt;/td&gt;                        &lt;td&gt;17005930322&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;魏山帅&lt;/td&gt;                        &lt;td&gt;18468618874&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;华倩&lt;/td&gt;                        &lt;td&gt;18576581848&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;焦君山&lt;/td&gt;                        &lt;td&gt;15978226424&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;钟尾田&lt;/td&gt;                        &lt;td&gt;15542823911&lt;/td&gt;                    &lt;/tr&gt;                    &lt;tr  class=&quot;warning&quot;&gt;                        &lt;td&gt;司可可&lt;/td&gt;                        &lt;td&gt;17526304161&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;官渡&lt;/td&gt;                        &lt;td&gt;15422018558&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;上贵坡&lt;/td&gt;                        &lt;td&gt;17269452013&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;时光机&lt;/td&gt;                        &lt;td&gt;17764278604&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;李发&lt;/td&gt;                        &lt;td&gt;15711910344&lt;/td&gt;                    &lt;/tr&gt;                    &lt;tr  class=&quot;danger&quot;&gt;                        &lt;td&gt;蒂冈&lt;/td&gt;                        &lt;td&gt;15714728273&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;范德&lt;/td&gt;                        &lt;td&gt;16061028454&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;周朝王&lt;/td&gt;                        &lt;td&gt;16264433631&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;谢都都&lt;/td&gt;                        &lt;td&gt;17601615878&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;刘何思&lt;/td&gt;                        &lt;td&gt;15897468949&lt;/td&gt;                    &lt;/tr&gt;                &lt;/tbody&gt;            &lt;/table&gt;        &lt;/div&gt;    &lt;/body&gt;&lt;/html&gt;</code></pre><p>6）新建：<strong>CallLogListEchart.jsp</strong></p><pre><code>&lt;%--  Created by IntelliJ IDEA.  User: Z  Date: 2017/10/28  Time: 14:36  To change this template use File | Settings | File Templates.--%&gt;&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; isELIgnored=&quot;false&quot; %&gt;&lt;html&gt;&lt;head&gt;    &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;    &lt;title&gt;显示通话记录&lt;/title&gt;    &lt;script type=&quot;text/javascript&quot; src=&quot;../js/echarts.min.js&quot;&gt;&lt;/script&gt;    &lt;%--&lt;script type=&quot;text/javascript&quot; src=&quot;${pageContext.request.contextPath}/js/echarts.min.js&quot;&gt;&lt;/script&gt;--%&gt;    &lt;%--&lt;script type=&quot;text/javascript&quot; src=&quot;${pageContext.request.contextPath}/jquery-3.2.0.min.js&quot;&gt;&lt;/script&gt;--%&gt;    &lt;%--&lt;script type=&quot;text/javascript&quot; src=&quot;http://echarts.baidu.com/gallery/vendors/echarts/echarts-all-3.js&quot;&gt;&lt;/script&gt;--%&gt;&lt;/head&gt;&lt;body style=&quot;height: 100%; margin: 0; background-color: #3C3F41&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;    h3 {        font-size: 12px;        color: #ffffff;        display: inline    }&lt;/style&gt;&lt;h4 style=&quot;color: #ffffff;text-align:center&quot;&gt;通话月单查询：${requestScope.name}&lt;/h4&gt;&lt;%--&lt;h3 style=&quot;margin-left: 70%&quot;&gt;通话次数&lt;/h3&gt;--%&gt;&lt;%--&lt;h3 style=&quot;margin-left: 20%&quot;&gt;通话时长&lt;/h3&gt;--%&gt;&lt;div id=&quot;container1&quot; style=&quot;height: 80%; width: 50%; float:left&quot;&gt;&lt;/div&gt;&lt;div id=&quot;container2&quot; style=&quot;height: 80%; width: 50%; float:right&quot;&gt;&lt;/div&gt;&lt;script type=&quot;text/javascript&quot;&gt;    var telephone = &quot;${requestScope.telephone}&quot;    var name = &quot;${requestScope.name}&quot;    var date = &quot;${requestScope.date}&quot;//1月,2月,3月,xxxxx    var count = &quot;${requestScope.count}&quot;    var duration = &quot;${requestScope.duration}&quot;    var pieData = converterFun(duration.split(&quot;,&quot;), date.split(&quot;,&quot;))    callog1();    callog2();    function converterFun(duration, date) {        var array = [];        for (var i = 0; i &lt; duration.length; i++) {            var map = {};            map[&#39;value&#39;] = parseFloat(duration[i]);            map[&#39;name&#39;] = date[i];            array.push(map);        }        return array;    }    function callog1() {        var dom = document.getElementById(&quot;container1&quot;);        var myChart = echarts.init(dom);        myChart.showLoading();        var option = {            title: {                text: &#39;通话次数&#39;,                textStyle: {                    //文字颜色                    color: &#39;#ffffff&#39;,                    //字体风格,&#39;normal&#39;,&#39;italic&#39;,&#39;oblique&#39;                    fontStyle: &#39;normal&#39;,                    //字体粗细 &#39;normal&#39;,&#39;bold&#39;,&#39;bolder&#39;,&#39;lighter&#39;,100 | 200 | 300 | 400...                    fontWeight: &#39;bold&#39;,                    //字体系列                    fontFamily: &#39;sans-serif&#39;,                    //字体大小                    fontSize: 13                },                itemGap: 12,            },            grid: {                x: 80,                y: 60,                x2: 80,                y2: 60,                backgroundColor: &#39;rgba(0,0,0,0)&#39;,                borderWidth: 1,                borderColor: &#39;#ffffff&#39;            },            tooltip: {                trigger: &#39;axis&#39;            },            legend: {                borderColor: &#39;#ffffff&#39;,                itemGap: 10,                data: [&#39;通话次数&#39;],                textStyle: {                    color: &#39;#ffffff&#39;// 图例文字颜色                }            },            toolbox: {                show: false,                feature: {                    dataZoom: {                        yAxisIndex: &#39;none&#39;                    },                    dataView: {readOnly: false},                    magicType: {type: [&#39;line&#39;, &#39;bar&#39;]},                    restore: {},                    saveAsImage: {}                }            },            xAxis: {                data: date.split(&quot;,&quot;),                axisLine: {                    lineStyle: {                        color: &#39;#ffffff&#39;,                        width: 2                    }                }            },            yAxis: {                axisLine: {                    lineStyle: {                        color: &#39;#ffffff&#39;,                        width: 2                    }                }            },            series: [                {                    type: &#39;line&#39;,                    data: count.split(&quot;,&quot;),                    itemStyle: {                        normal: {                            color: &#39;#ffca29&#39;,                            lineStyle: {                                color: &#39;#ffd80d&#39;,                                width: 2                            }                        }                    },                    markPoint: {                        data: [                            {type: &#39;max&#39;, name: &#39;最大值&#39;},                            {type: &#39;min&#39;, name: &#39;最小值&#39;}                        ]                    },                    markLine: {                        data: [                            {type: &#39;average&#39;, name: &#39;平均值&#39;}                        ]                    }                }            ]        };        if (option &amp;&amp; typeof option === &quot;object&quot;) {            myChart.setOption(option, true);        }        myChart.hideLoading()    }    function callog2() {        var dom = document.getElementById(&quot;container2&quot;);        var myChart = echarts.init(dom);        myChart.showLoading();        var option = {            title: {                text: &#39;通话时长&#39;,                textStyle: {                    //文字颜色                    color: &#39;#ffffff&#39;,                    //字体风格,&#39;normal&#39;,&#39;italic&#39;,&#39;oblique&#39;                    fontStyle: &#39;normal&#39;,                    //字体粗细 &#39;normal&#39;,&#39;bold&#39;,&#39;bolder&#39;,&#39;lighter&#39;,100 | 200 | 300 | 400...                    fontWeight: &#39;bold&#39;,                    //字体系列                    fontFamily: &#39;sans-serif&#39;,                    //字体大小                    fontSize: 13                },                itemGap: 12,            },            tooltip: {                trigger: &#39;item&#39;,                formatter: &quot;{a} &lt;br/&gt;{b} : {c} ({d}%)&quot;            },            visualMap: {                show: false,                min: Math.min.apply(null, duration.split(&quot;,&quot;)),                max: Math.max.apply(null, duration.split(&quot;,&quot;)),                inRange: {                    colorLightness: [0, 0.5]                }            },            series: [                {                    name: &#39;通话时长&#39;,                    type: &#39;pie&#39;,                    radius: &#39;55%&#39;,                    center: [&#39;50%&#39;, &#39;50%&#39;],                    data: pieData.sort(function (a, b) {                        return a.value - b.value;                    }),                    roseType: &#39;radius&#39;,                    label: {                        normal: {                            textStyle: {                                color: &#39;rgba(255, 255, 255, 0.3)&#39;                            }                        }                    },                    labelLine: {                        normal: {                            lineStyle: {                                color: &#39;rgba(255, 255, 255, 0.3)&#39;                            },                            smooth: 0.2,                            length: 10,                            length2: 20                        }                    },                    itemStyle: {                        normal: {                            color: &#39;#01c1c2&#39;,                            shadowBlur: 200,                            shadowColor: &#39;rgba(0, 0, 0, 0.5)&#39;                        }                    },                    animationType: &#39;scale&#39;,                    animationEasing: &#39;elasticOut&#39;,                    animationDelay: function (idx) {                        return Math.random() * 200;                    }                }            ]        };        if (option &amp;&amp; typeof option === &quot;object&quot;) {            myChart.setOption(option, true);        }        myChart.hideLoading()    }&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;</code></pre><h4 id="3、最终预览"><a href="#3、最终预览" class="headerlink" title="3、最终预览"></a>3、最终预览</h4><p>查询人通话时长与通话次数统计大概如下所示：</p><p><strong>首页</strong></p><p><img src="/medias/index.PNG" alt="index"></p><p><strong>统一展示</strong></p><p><img src="/medias/%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%B1%95%E7%A4%BA.PNG" alt="图形化展示"></p><h3 id="八、定时任务"><a href="#八、定时任务" class="headerlink" title="八、定时任务"></a>八、定时任务</h3><p>新的数据每天都会产生，所以我们每天都需要更新离线的分析结果，所以此时我们可以用各种各样的定时任务调度工具来完成此操作。此例我们使用crontab来执行该操作。<br>1）编写任务脚本：<strong>analysis.sh</strong></p><pre><code>#!/bin/bash/root/hd/hadoop-2.8.4/bin/yarn jar /opt/jars/CT_analysis-1.0-SNAPSHOT.jar runner.CountDurationRunner -libjars /root/hd/hadoop-2.8.4/lib/*</code></pre><p>2） 制定crontab任务</p><pre><code># .------------------------------------------minute(0~59)# | .----------------------------------------hours(0~23)# | | .--------------------------------------day of month(1~31)# | | | .------------------------------------month(1~12)# | | | | .----------------------------------day of week(0~6)# | | | | | .--------------------------------command# | | | | | |# | | | | | |0 0 * * * /opt/jars/analysis.sh</code></pre><p>3）考虑数据处理手段是否安全<br>a、定时任务统计结果是否会重复<br>b、定时任务处理的数据是否全面</p><h3 id="九、项目总结"><a href="#九、项目总结" class="headerlink" title="九、项目总结"></a>九、项目总结</h3><p>重新总结梳理整个项目流程和方法论<br>1、实现月查询（某个月每一天的数据展示）<br>2、用户亲密度展示<br>3、考虑Hive实现<br>4、用户按照时间区间，查找所有的通话数据<br>5、给读者建议—–按代码来—–》成功运行——》掌握此项目</p><h3 id="十、附录"><a href="#十、附录" class="headerlink" title="十、附录"></a>十、附录</h3><h4 id="1、-flume-myagent-flume2kafka-conf"><a href="#1、-flume-myagent-flume2kafka-conf" class="headerlink" title="1、/flume/myagent/flume2kafka.conf"></a>1、/flume/myagent/flume2kafka.conf</h4><pre><code># definea1.sources = r1a1.sinks = k1a1.channels = c1# sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F -c +0 /opt/jars/calllog.csva1.sources.r1.shell = /bin/bash -c# sinka1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.brokerList = hsiehchou121:9092,hsiehchou122:9092,hsiehchou123:9092a1.sinks.k1.topic = callloga1.sinks.k1.batchSize = 20a1.sinks.k1.requiredAcks = 1# channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# binda1.sources.r1.channels = c1a1.sinks.k1.channel = c1</code></pre><h4 id="2、core-site-xml"><a href="#2、core-site-xml" class="headerlink" title="2、core-site.xml"></a>2、core-site.xml</h4><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;    &lt;!-- 指定hdfs的nameservice为mycluster --&gt;    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://mycluster&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定hadoop临时目录 --&gt;    &lt;property&gt;        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;        &lt;value&gt;/root/hd/hadoop-2.8.4/tmp&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定zookeeper地址 --&gt;    &lt;property&gt;        &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;        &lt;value&gt;hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181&lt;/value&gt;    &lt;/property&gt;    &lt;!--&lt;property&gt;        &lt;name&gt;ipc.client.connect.max.retries&lt;/name&gt;        &lt;value&gt;30&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;ipc.client.connect.retry.interval&lt;/name&gt;        &lt;value&gt;1000&lt;/value&gt;    &lt;/property&gt; --&gt;&lt;/configuration&gt;</code></pre><h4 id="3、hdfs-site-xml"><a href="#3、hdfs-site-xml" class="headerlink" title="3、hdfs-site.xml"></a>3、hdfs-site.xml</h4><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;     &lt;!--指定hdfs的nameservice为mycluster，需要和core-site.xml中的保持一致 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.nameservices&lt;/name&gt;        &lt;value&gt;mycluster&lt;/value&gt;    &lt;/property&gt;    &lt;!-- mycluster下面有两个NameNode，分别是nn1，nn2 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;        &lt;value&gt;nn1,nn2&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn1的RPC通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;        &lt;value&gt;hsiehchou121:9000&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn1的http通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;        &lt;value&gt;hsiehchou121:50070&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn2的RPC通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;        &lt;value&gt;hsiehchou122:9000&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn2的http通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;        &lt;value&gt;hsiehchou122:50070&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定NameNode的日志在JournalNode上的存放位置 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;        &lt;value&gt;qjournal://hsiehchou121:8485;hsiehchou122:8485;/mycluster&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;        &lt;value&gt;/root/hd/hadoop-2.8.4/journal&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 开启NameNode失败自动切换 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 配置失败自动切换实现方式 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;        &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;        &lt;value&gt;            sshfence            shell(/bin/true)        &lt;/value&gt;    &lt;/property&gt;    &lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;        &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 配置sshfence隔离机制超时时间 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;        &lt;value&gt;30000&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="4、hbase-site-xml"><a href="#4、hbase-site-xml" class="headerlink" title="4、hbase-site.xml"></a>4、hbase-site.xml</h4><pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--/** * * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements.  See the NOTICE file * distributed with this work for additional information * regarding copyright ownership.  The ASF licenses this file * to you under the Apache License, Version 2.0 (the * &quot;License&quot;); you may not use this file except in compliance * with the License.  You may obtain a copy of the License at * *     http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an &quot;AS IS&quot; BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */--&gt;&lt;configuration&gt;     &lt;property&gt;         &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;         &lt;value&gt;true&lt;/value&gt;     &lt;/property&gt;     &lt;property&gt;         &lt;name&gt;hbase.rootdir&lt;/name&gt;         &lt;value&gt;hdfs://mycluster/hbase&lt;/value&gt;     &lt;/property&gt;     &lt;property&gt;         &lt;name&gt;hbase.master.port&lt;/name&gt;         &lt;value&gt;16000&lt;/value&gt;     &lt;/property&gt;     &lt;property&gt;         &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;         &lt;value&gt;hsiehchou121,hsiehchou122,hsiehchou123&lt;/value&gt;     &lt;/property&gt;     &lt;!-- hbase的元数据信息存储在zookeeper的位置 --&gt;     &lt;property&gt;         &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;         &lt;value&gt;/root/hd/zookeeper-3.4.10/zkData&lt;/value&gt;     &lt;/property&gt;     &lt;property&gt;         &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;         &lt;value&gt;2181&lt;/value&gt;     &lt;/property&gt;     &lt;property&gt;         &lt;name&gt;zookeeper.session.timeout&lt;/name&gt;         &lt;value&gt;120000&lt;/value&gt;     &lt;/property&gt;     &lt;property&gt;         &lt;name&gt;hbase.zookeeper.property.tickTime&lt;/name&gt;         &lt;value&gt;6000&lt;/value&gt;     &lt;/property&gt;     &lt;!-- 保证HBase之间时间同步 --&gt;    &lt;property&gt;        &lt;name&gt;hbase.master.maxclockskew&lt;/name&gt;        &lt;value&gt;180000&lt;/value&gt;        &lt;description&gt;Time difference of regionserver from master&lt;/description&gt;    &lt;/property&gt;    &lt;!-- 使用HBase Coprocessor协处理器 --&gt;    &lt;property&gt;        &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;        &lt;value&gt;hbase.CalleeWriteObserver&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="5、log4j-properties"><a href="#5、log4j-properties" class="headerlink" title="5、log4j.properties"></a>5、log4j.properties</h4><pre><code># Licensed to the Apache Software Foundation (ASF) under one# or more contributor license agreements.  See the NOTICE file# distributed with this work for additional information# regarding copyright ownership.  The ASF licenses this file# to you under the Apache License, Version 2.0 (the# &quot;License&quot;); you may not use this file except in compliance# with the License.  You may obtain a copy of the License at##     http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# Define some default values that can be overridden by system propertieshbase.root.logger=INFO,consolehbase.security.logger=INFO,consolehbase.log.dir=.hbase.log.file=hbase.log# Define the root logger to the system property &quot;hbase.root.logger&quot;.log4j.rootLogger=${hbase.root.logger}# Logging Thresholdlog4j.threshold=ALL## Daily Rolling File Appender#log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.DRFA.File=${hbase.log.dir}/${hbase.log.file}# Rollver at midnightlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd# 30-day backup#log4j.appender.DRFA.MaxBackupIndex=30log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout# Pattern format: Date LogLevel LoggerName LogMessagelog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n# Rolling File Appender propertieshbase.log.maxfilesize=256MBhbase.log.maxbackupindex=20# Rolling File Appenderlog4j.appender.RFA=org.apache.log4j.RollingFileAppenderlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}log4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}log4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}log4j.appender.RFA.layout=org.apache.log4j.PatternLayoutlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n## Security audit appender#hbase.security.log.file=SecurityAuth.audithbase.security.log.maxfilesize=256MBhbase.security.log.maxbackupindex=20log4j.appender.RFAS=org.apache.log4j.RollingFileAppenderlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}log4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}log4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}log4j.appender.RFAS.layout=org.apache.log4j.PatternLayoutlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%nlog4j.category.SecurityLogger=${hbase.security.logger}log4j.additivity.SecurityLogger=false#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.visibility.VisibilityController=TRACE## Null Appender#log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender## console# Add &quot;console&quot; to rootlogger above if you want to use this#log4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.target=System.errlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%nlog4j.appender.asyncconsole=org.apache.hadoop.hbase.AsyncConsoleAppenderlog4j.appender.asyncconsole.target=System.err# Custom Logging levelslog4j.logger.org.apache.zookeeper=INFO#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUGlog4j.logger.org.apache.hadoop.hbase=INFO# Make these two classes INFO-level. Make them DEBUG to see more zk debug.log4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFOlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO#log4j.logger.org.apache.hadoop.dfs=DEBUG# Set this class to log INFO only otherwise its OTT# Enable this to get detailed connection error/retry logging.# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG# Uncomment the below if you want to remove logging of client region caching&#39;# and scan of hbase:meta messages# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO# Prevent metrics subsystem start/stop messages (HBASE-17722)log4j.logger.org.apache.hadoop.metrics2.impl.MetricsConfig=WARNlog4j.logger.org.apache.hadoop.metrics2.impl.MetricsSinkAdapter=WARNlog4j.logger.org.apache.hadoop.metrics2.impl.MetricsSystemImpl=WARN</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据项目 </tag>
            
            <tag> 电信 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink练习</title>
      <link href="/2019/05/18/flink-lian-xi/"/>
      <url>/2019/05/18/flink-lian-xi/</url>
      
        <content type="html"><![CDATA[<h3 id="一、Flink开发IDEA环境搭建与测试"><a href="#一、Flink开发IDEA环境搭建与测试" class="headerlink" title="一、Flink开发IDEA环境搭建与测试"></a>一、Flink开发IDEA环境搭建与测试</h3><h4 id="1、IDEA开发环境"><a href="#1、IDEA开发环境" class="headerlink" title="1、IDEA开发环境"></a>1、IDEA开发环境</h4><p>先虚拟机联网，然后执行yum -y install nc<br>nc是用来打开端口的工具<br>然后nc -l 9000  </p><p><strong>1.pom文件设置</strong></p><pre><code>&lt;properties&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;        &lt;encoding&gt;UTF-8&lt;/encoding&gt;        &lt;scala.version&gt;2.11.12&lt;/scala.version&gt;        &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt;        &lt;hadoop.version&gt;2.8.4&lt;/hadoop.version&gt;        &lt;flink.version&gt;1.6.1&lt;/flink.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;            &lt;artifactId&gt;scala-library&lt;/artifactId&gt;            &lt;version&gt;${scala.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;            &lt;artifactId&gt;flink-java&lt;/artifactId&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;            &lt;artifactId&gt;flink-streaming-java_${scala.binary.version}&lt;/artifactId&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;            &lt;artifactId&gt;flink-scala_${scala.binary.version}&lt;/artifactId&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;            &lt;artifactId&gt;flink-streaming-scala_${scala.binary.version}&lt;/artifactId&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;            &lt;artifactId&gt;flink-table_${scala.binary.version}&lt;/artifactId&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;            &lt;artifactId&gt;flink-clients_${scala.binary.version}&lt;/artifactId&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;            &lt;artifactId&gt;flink-connector-kafka-0.10_${scala.binary.version}&lt;/artifactId&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;            &lt;version&gt;${hadoop.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;mysql&lt;/groupId&gt;            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;            &lt;version&gt;5.1.38&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;            &lt;version&gt;1.2.22&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt;        &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;                &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;                &lt;version&gt;3.2.0&lt;/version&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;goals&gt;                            &lt;goal&gt;compile&lt;/goal&gt;                            &lt;goal&gt;testCompile&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;args&gt;                                &lt;!-- &lt;arg&gt;-make:transitive&lt;/arg&gt; --&gt;                                &lt;arg&gt;-dependencyfile&lt;/arg&gt;                                &lt;arg&gt;${project.build.directory}/.scala_dependencies&lt;/arg&gt;                            &lt;/args&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;                &lt;version&gt;2.18.1&lt;/version&gt;                &lt;configuration&gt;                    &lt;useFile&gt;false&lt;/useFile&gt;                    &lt;disableXmlReport&gt;true&lt;/disableXmlReport&gt;                    &lt;includes&gt;                        &lt;include&gt;**/*Test.*&lt;/include&gt;                        &lt;include&gt;**/*Suite.*&lt;/include&gt;                    &lt;/includes&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;                &lt;version&gt;3.0.0&lt;/version&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;shade&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;filters&gt;                                &lt;filter&gt;                                    &lt;artifact&gt;*:*&lt;/artifact&gt;                                    &lt;excludes&gt;                                        &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;                                        &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;                                        &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;                                    &lt;/excludes&gt;                                &lt;/filter&gt;                            &lt;/filters&gt;                            &lt;transformers&gt;                                &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;                                    &lt;mainClass&gt;org.apache.spark.WordCount&lt;/mainClass&gt;                                &lt;/transformer&gt;                            &lt;/transformers&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;</code></pre><h4 id="2、Flink开发流程"><a href="#2、Flink开发流程" class="headerlink" title="2、Flink开发流程"></a>2、Flink开发流程</h4><p>Flink具有特殊类DataSet并DataStream在程序中表示数据。您可以将它们视为可以包含重复项的不可变数据集合。在DataSet数据有限的情况下，对于一个DataStream元素的数量可以是无界的</p><p>这些集合在某些关键方面与常规Java集合不同。首先，它们是不可变的，这意味着一旦创建它们就无法添加或删除元素。你也不能简单地检查里面的元素</p><p>集合最初通过在弗林克程序添加源创建和新的集合从这些通过将它们使用API方法如衍生map，filter等等</p><p>Flink程序看起来像是转换数据集合的常规程序。每个程序包含相同的基本部分：<br>1）获取execution environment,<br>final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</p><p>2）加载/创建初始化数据<br>DataStream<code>&lt;String&gt;</code> text = env.readTextFile(“file:///path/to/file”);</p><p>3）指定此数据的转换<br>val mapped = input.map { x =&gt; x.toInt }</p><p>4）指定放置计算结果的位置<br>writeAsText(String path)<br>print()</p><p>5）触发程序执行<br>在local模式下执行程序<br>execute()<br>将程序达成jar运行在线上<br>./bin/flink run <br>-m hsiehchou121:8081 <br>./examples/batch/WordCount.jar <br>–input  hdfs:///user/root/input/wc.txt <br>–output  hdfs:///user/root/output2  \</p><p>####3、WordCount案例</p><p>1）<strong>Scala代码</strong></p><pre><code>import org.apache.flink.api.java.utils.ParameterToolimport org.apache.flink.streaming.api.scala.StreamExecutionEnvironmentimport org.apache.flink.streaming.api.windowing.time.Timeobject SocketWindowWordCountScala {  def main(args: Array[String]) : Unit = {    // 定义一个数据类型保存单词出现的次数    case class WordWithCount(word: String, count: Long)    // port 表示需要连接的端口    val port: Int = try {      ParameterTool.fromArgs(args).getInt(&quot;port&quot;)    } catch {      case e: Exception =&gt; {        System.err.println(&quot;No port specified. Please run &#39;SocketWindowWordCount --port &lt;port&gt;&#39;&quot;)        return      }    }    // 获取运行环境    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment    // 连接此socket获取输入数据    val text = env.socketTextStream(&quot;hsiehchou121&quot;, port, &#39;\n&#39;)    //需要加上这一行隐式转换 否则在调用flatmap方法的时候会报错    import org.apache.flink.api.scala._    // 解析数据, 分组, 窗口化, 并且聚合求SUM    val windowCounts = text      .flatMap { w =&gt; w.split(&quot;\\s&quot;) }      .map { w =&gt; WordWithCount(w, 1) }      .keyBy(&quot;word&quot;)      .timeWindow(Time.seconds(5), Time.seconds(1))      .sum(&quot;count&quot;)    // 打印输出并设置使用一个并行度    windowCounts.print().setParallelism(1)    env.execute(&quot;Socket Window WordCount&quot;)  }}</code></pre><p>2）<strong>Java代码</strong></p><pre><code>import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.util.Collector;public class WordCount {    //先在虚拟机上打开你的端口号 nc -l 9000    public static void main(String[] args) throws Exception {        //定义socket的端口号        int port;        try{            ParameterTool parameterTool = ParameterTool.fromArgs(args);            port = parameterTool.getInt(&quot;port&quot;);        }catch (Exception e){            System.err.println(&quot;没有指定port参数，使用默认值9000&quot;);            port = 9000;        }        //获取运行环境        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        //连接socket获取输入的数据        DataStreamSource&lt;String&gt; text = env.socketTextStream(&quot;192.168.1.52&quot;, port, &quot;\n&quot;);        //计算数据        DataStream&lt;WordWithCount&gt; windowCount = text.flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() {            public void flatMap(String value, Collector&lt;WordWithCount&gt; out) throws Exception {                String[] splits = value.split(&quot;\\s&quot;);                for (String word:splits) {                    out.collect(new WordWithCount(word,1L));                }            }        })//打平操作，把每行的单词转为&lt;word,count&gt;类型的数据                .keyBy(&quot;word&quot;)//针对相同的word数据进行分组                .timeWindow(Time.seconds(2),Time.seconds(1))//指定计算数据的窗口大小和滑动窗口大小                .sum(&quot;count&quot;);        //把数据打印到控制台        windowCount.print()                .setParallelism(1);//使用一个并行度        //注意：因为flink是懒加载的，所以必须调用execute方法，上面的代码才会执行        env.execute(&quot;streaming word count&quot;);    }    /**     * 主要为了存储单词以及单词出现的次数     */    public static class WordWithCount{        public String word;        public long count;        public WordWithCount(){}        public WordWithCount(String word, long count) {            this.word = word;            this.count = count;        }        @Override        public String toString() {            return &quot;WordWithCount{&quot; +                    &quot;word=&#39;&quot; + word + &#39;\&#39;&#39; +                    &quot;, count=&quot; + count +                    &#39;}&#39;;        }    }}</code></pre><p>3）<strong>运行测试</strong></p><p>首先，使用nc命令启动一个本地监听，命令是：<br>[root@hsiehchou121 ~]$ nc -l 9000</p><p>通过netstat命令观察9000端口。 netstat -anlp | grep 9000，启动监听如果报错：-bash: nc: command not found，请先安装nc，在线安装命令：yum -y install nc。<br>然后，IDEA上运行flink官方案例程序<br>hsiehchou121上输入<br>[root@hsiehchou121 ~] nc -l 9000<br>learn flink<br>hadoop storm flink<br>flink flink hsiehchou</p><p>4）<strong>集群测试</strong></p><p>这里单机测试官方案例<br>[root@hsiehchou121 flink-1.6.1]$ pwd<br>/opt/flink-1.6.1</p><p>[root@hsiehchou121 flink-1.6.1]$ ./bin/start-cluster.sh<br>Starting cluster.<br>Starting standalonesession daemon on host hsiehchou121.<br>Starting taskexecutor daemon on host hsiehchou121.</p><p>[root@hsiehchou121 flink-1.6.1]$ jps<br>StandaloneSessionClusterEntrypoint<br>TaskManagerRunner<br>Jps</p><p>[root@hsiehchou121 flink-1.6.1]$ ./bin/flink run examples/streaming/SocketWindowWordCount.jar –port 9000<br>单词在5秒的时间窗口（处理时间，翻滚窗口）中计算并打印到stdout。监视TaskManager的输出文件并写入一些文本nc（输入在点击后逐行发送到Flink）：</p><h4 id="4、使用IDEA开发离线程序"><a href="#4、使用IDEA开发离线程序" class="headerlink" title="4、使用IDEA开发离线程序"></a>4、使用IDEA开发离线程序</h4><p>Dataset是flink的常用程序，数据集通过source进行初始化，例如读取文件或者序列化集合，然后通过transformation（filtering、mapping、joining、grouping）将数据集转成，然后通过sink进行存储，既可以写入hdfs这种分布式文件系统，也可以打印控制台，flink可以有很多种运行方式，如local、flink集群、yarn等.<br>1）<strong>scala程序</strong></p><pre><code>import org.apache.flink.api.scala.ExecutionEnvironmentimport org.apache.flink.api.scala._object WordCountScala{  def main(args: Array[String]) {    //初始化环境    val env = ExecutionEnvironment.getExecutionEnvironment    //从字符串中加载数据    val text = env.fromElements(      &quot;Who&#39;s there?&quot;,      &quot;I think I hear them. Stand, ho! Who&#39;s there?&quot;)    //分割字符串、汇总tuple、按照key进行分组、统计分组后word个数    val counts = text.flatMap { _.toLowerCase.split(&quot;\\W+&quot;) filter { _.nonEmpty } }      .map { (_, 1) }      .groupBy(0)      .sum(1)    //打印    counts.print()  }}</code></pre><p>2） <strong>Java程序</strong></p><pre><code>import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.util.Collector;public class WordCountJava {    public static void main(String[] args) throws Exception {        //构建环境        final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();        //通过字符串构建数据集        DataSet&lt;String&gt; text = env.fromElements(                &quot;Who&#39;s there?&quot;,                &quot;I think I hear them. Stand, ho! Who&#39;s there?&quot;);        //分割字符串、按照key进行分组、统计相同的key个数        DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; wordCounts = text                .flatMap(new LineSplitter())                .groupBy(0)                .sum(1);        //打印        wordCounts.print();    }    //分割字符串的方法    public static class LineSplitter implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; {        @Override        public void flatMap(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) {            for (String word : line.split(&quot; &quot;)) {                out.collect(new Tuple2&lt;String, Integer&gt;(word, 1));            }        }    }}</code></pre><p>3）<strong>运行</strong></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink基础</title>
      <link href="/2019/05/16/flink-ji-chu/"/>
      <url>/2019/05/16/flink-ji-chu/</url>
      
        <content type="html"><![CDATA[<h3 id="一、Flink概述"><a href="#一、Flink概述" class="headerlink" title="一、Flink概述"></a>一、Flink概述</h3><p>官网：<a href="http://flink.apache.org/" target="_blank" rel="noopener">http://flink.apache.org/</a><br>MapReduce-&gt;MaxCompute<br>HBase-&gt;部门<br>QuickBI<br>DataV<br>Hive-&gt;高德地图<br>Storm-&gt;JStorm</p><p>2019年1月 阿里正式开源Flink-&gt;Blink<br>Apache Flink是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。Flink设计为在所有常见的集群环境中运行，以内存速度和任何规模执行计算。</p><p>大数据计算框架</p><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>Flink核心是一个流式的数据流执行引擎，其针对数据流的分布式计算提供了数据分布、数据通信以及容错机制等功能。基于流执行引擎，Flink提供了诸多更高抽象层的API以便用户编写分布式任务：</p><p>DataSet API，对静态数据进行批处理操作，将静态数据抽象成分布式的数据集，用户可以方便地使用Flink提供的各种操作符对分布式数据集进行处理，支持Java、Scala和Python</p><p>DataStream API，对数据流进行流处理操作，将流式的数据抽象成分布式的数据流，用户可以方便地对分布式数据流进行各种操作，支持Java和Scala</p><p>Table API，对结构化数据进行查询操作，将结构化数据抽象成关系表，并通过类SQL的DSL对关系表进行各种查询操作，支持Java和Scala</p><p>此外，Flink还针对特定的应用领域提供了领域库，例如：<br>Flink ML，Flink的机器学习库，提供了机器学习Pipelines API并实现了多种机器学习算法<br>Gelly，Flink的图计算库，提供了图计算的相关API及多种图计算算法实现</p><h4 id="2、统一的批处理与流处理系统"><a href="#2、统一的批处理与流处理系统" class="headerlink" title="2、统一的批处理与流处理系统"></a>2、统一的批处理与流处理系统</h4><p>在大数据处理领域，批处理任务与流处理任务一般被认为是两种不同的任务，一个大数据项目一般会被设计为只能处理其中一种任务，例如Apache Storm、Apache Smaza只支持流处理任务，而Aapche MapReduce、Apache Tez、Apache Spark只支持批处理任务。Spark Streaming是Apache Spark之上支持流处理任务的子系统，看似一个特例，实则不然——Spark Streaming采用了一种micro-batch的架构，即把输入的数据流切分成细粒度的batch，并为每一个batch数据提交一个批处理的Spark任务，所以Spark Streaming本质上还是基于Spark批处理系统对流式数据进行处理，和Apache Storm、Apache Smaza等完全流式的数据处理方式完全不同。通过其灵活的执行引擎，Flink能够同时支持批处理任务与流处理任务</p><p>在执行引擎这一层，流处理系统与批处理系统最大不同在于节点间的数据传输方式</p><p>对于一个流处理系统，其节点间数据传输的标准模型是：当一条数据被处理完成后，序列化到缓存中，然后立刻通过网络传输到下一个节点，由下一个节点继续处理</p><p>而对于一个批处理系统，其节点间数据传输的标准模型是：当一条数据被处理完成后，序列化到缓存中，并不会立刻通过网络传输到下一个节点，当缓存写满，就持久化到本地硬盘上，当所有数据都被处理完成后，才开始将处理后的数据通过网络传输到下一个节点</p><p>这两种数据传输模式是两个极端，对应的是流处理系统对低延迟的要求和批处理系统对高吞吐量的要求。Flink的执行引擎采用了一种十分灵活的方式，同时支持了这两种数据传输模型。Flink以固定的缓存块为单位进行网络数据传输，用户可以通过缓存块超时值指定缓存块的传输时机。如果缓存块的超时值为0，则Flink的数据传输方式类似上文所提到流处理系统的标准模型，此时系统可以获得最低的处理延迟。如果缓存块的超时值为无限大，则Flink的数据传输方式类似上文所提到批处理系统的标准模型，此时系统可以获得最高的吞吐量</p><p>同时缓存块的超时值也可以设置为0到无限大之间的任意值</p><p>缓存块的超时阈值越小，则Flink流处理执行引擎的数据处理延迟越低，但吞吐量也会降低，<br>反之亦然。通过调整缓存块的超时阈值，用户可根据需求灵活地权衡系统延迟和吞吐量</p><h4 id="3、架构"><a href="#3、架构" class="headerlink" title="3、架构"></a>3、架构</h4><p>要了解一个系统，一般都是从架构开始。我们关心的问题是：系统部署成功后各个节点都启动了哪些服务，各个服务之间又是怎么交互和协调的。下方是 Flink 集群启动后架构图</p><p><img src="/medias/Flink%20%E9%9B%86%E7%BE%A4%E5%90%AF%E5%8A%A8%E5%90%8E%E6%9E%B6%E6%9E%84%E5%9B%BE.PNG" alt="Flink 集群启动后架构图"></p><p>当 Flink 集群启动后，首先会启动一个 JobManger 和一个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程</p><p>Client 为提交 Job 的客户端，可以是运行在任何机器上（与 JobManager 环境连通即可）。提交 Job 后，Client 可以结束进程（Streaming的任务），也可以不结束并等待结果返回</p><p>JobManager 主要负责调度 Job 并协调 Task 做 checkpoint，职责上很像 Storm 的 Nimbus。从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行</p><p>TaskManager 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自己的上游建立 Netty 连接，接收数据并处理</p><p>可以看到 Flink 的任务调度是多线程模型，并且不同Job/Task混合在一个 TaskManager 进程中。虽然这种方式可以有效提高 CPU 利用率，但是个人不太喜欢这种设计，因为不仅缺乏资源隔离机制，同时也不方便调试。类似 Storm 的进程模型，一个JVM 中只跑该 Job 的 Tasks 实际应用中更为合理</p><p>Flink编程模型</p><p><img src="/medias/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B.PNG" alt="Flink编程模型"></p><h3 id="二、Flink特点"><a href="#二、Flink特点" class="headerlink" title="二、Flink特点"></a>二、Flink特点</h3><p>1）mapreduce<br>2）storm<br>3）spark</p><p>适用于所有企业，不同企业有不同的业务场景。处理数据量，模型都不一样</p><p><strong>Flink</strong></p><h4 id="1、随处部署应用"><a href="#1、随处部署应用" class="headerlink" title="1、随处部署应用"></a>1、随处部署应用</h4><p>与其它组件集成！<br>flink是分布式系统，需要计算资源才可执行程序。flink可以与常见的集群资源管理器进行集成(H<br>adoop Yarn,Apache Mesos..)</p><p>可以单独作为独立集群运行</p><p>通过不同部署模式实现</p><p>这些模式允许flink以其惯有的方式进行交互</p><p>当我们部署flink应用程序时，Flink会根据应用程序配置的并行性自动识别所需资源。从资源管理<br>器中请求它们</p><p>如果发生故障，flink会请求新的资源来替换发生故障的容器</p><p>提交或控制程序都通过REST调用进行，简化Flink在许多环境的集成。孵化…</p><h4 id="2、以任何比例应用程序（小集群、无限集群）"><a href="#2、以任何比例应用程序（小集群、无限集群）" class="headerlink" title="2、以任何比例应用程序（小集群、无限集群）"></a>2、以任何比例应用程序（小集群、无限集群）</h4><p>Flink旨在以任何规模运行有状态流应用程序。应用程序可以并行化在集群中分布和同时执行程<br>序</p><p>因此，我们的应用集群可以利用无限的cpu和磁盘与网络IO</p><p>Flink可以轻松的维护非常大的应用程序状态<br>用户可拓展性报告：</p><ul><li>应用程序每天可以处理万亿个事件</li><li>应用程序每天可以维护多个TB的状态</li><li>应用程序可以在数千个内核运行</li></ul><h4 id="3、利用内存中的性能"><a href="#3、利用内存中的性能" class="headerlink" title="3、利用内存中的性能"></a>3、利用内存中的性能</h4><p>有状态Flink应用程序针对于对本地状态访问进行了优化。任务状态始终的保留在内存中，或者如果<br>大小超过了可用内存，则保存在访问高效的磁盘数据结构中(SSD 机械/固态)</p><p>任务可以通过访问本地来执行所有计算。从来产生极小的延迟</p><p>Flink定期和异步检查本地状态持久存储来保持出现故障时一次状态的一致性</p><h3 id="三、有界无界"><a href="#三、有界无界" class="headerlink" title="三、有界无界"></a>三、有界无界</h3><h4 id="1、无界"><a href="#1、无界" class="headerlink" title="1、无界"></a>1、无界</h4><p>有开始，没有结束…<br>处理实时数据</p><h4 id="2、有界"><a href="#2、有界" class="headerlink" title="2、有界"></a>2、有界</h4><p>有开始，有结束…<br>处理批量数据</p><h3 id="四、无界数据集应用场景（实时计算）"><a href="#四、无界数据集应用场景（实时计算）" class="headerlink" title="四、无界数据集应用场景（实时计算）"></a>四、无界数据集应用场景（实时计算）</h3><p>1）源源不断的日志数据<br>2）web应用，指标分析<br>3）移动设备终端(分析app状况)<br>4）应用在任何数据源不断产生的项目中</p><h3 id="五、Flink运行模型"><a href="#五、Flink运行模型" class="headerlink" title="五、Flink运行模型"></a>五、Flink运行模型</h3><p>1）<strong>流计算</strong><br>数据源源不断产生，我们的需求是源源不断的处理</p><p>程序需要一直保持在计算的状态</p><p>2）<strong>批处理</strong><br>计算一段完整的数据集，计算成功后释放资源，那么此时工作结束</p><h3 id="六、Flink的使用"><a href="#六、Flink的使用" class="headerlink" title="六、Flink的使用"></a>六、Flink的使用</h3><h4 id="1、处理结果准确"><a href="#1、处理结果准确" class="headerlink" title="1、处理结果准确"></a>1、处理结果准确</h4><p>无论是有序数据还是延迟到达的数据</p><h4 id="2、容错机制"><a href="#2、容错机制" class="headerlink" title="2、容错机制"></a>2、容错机制</h4><p>有状态：保持每次的结果往下传递，实现累加。DAG（有向无环图）</p><h4 id="3、有很强大的吞吐量和低延迟"><a href="#3、有很强大的吞吐量和低延迟" class="headerlink" title="3、有很强大的吞吐量和低延迟"></a>3、有很强大的吞吐量和低延迟</h4><p>计算速度快，吞吐量处理的量级大</p><h4 id="4、精准的维护一次的应用状态"><a href="#4、精准的维护一次的应用状态" class="headerlink" title="4、精准的维护一次的应用状态"></a>4、精准的维护一次的应用状态</h4><p>storm:会发生要么多计算一次，要么漏计算</p><h4 id="5、支持大规模的计算"><a href="#5、支持大规模的计算" class="headerlink" title="5、支持大规模的计算"></a>5、支持大规模的计算</h4><p>可以运行在数千台节点上</p><h4 id="6、支持流处理和窗口化操作"><a href="#6、支持流处理和窗口化操作" class="headerlink" title="6、支持流处理和窗口化操作"></a>6、支持流处理和窗口化操作</h4><h4 id="7、版本化处理"><a href="#7、版本化处理" class="headerlink" title="7、版本化处理"></a>7、版本化处理</h4><h4 id="8、检查点机制实现精准的一次性计算保证"><a href="#8、检查点机制实现精准的一次性计算保证" class="headerlink" title="8、检查点机制实现精准的一次性计算保证"></a>8、检查点机制实现精准的一次性计算保证</h4><p>checkpoint</p><h4 id="9、支持yarn与mesos资源管理器"><a href="#9、支持yarn与mesos资源管理器" class="headerlink" title="9、支持yarn与mesos资源管理器"></a>9、支持yarn与mesos资源管理器</h4><h3 id="七、flink单节点安装部署"><a href="#七、flink单节点安装部署" class="headerlink" title="七、flink单节点安装部署"></a>七、flink单节点安装部署</h3><p>1）下载安装包<br>2）上传<br>3）解压<br>tar -zxvf .tar<br>4）启动<br>bin/start-cluster.sh<br>5）访问ui界面<br><a href="http://192.168.116.201:8081" target="_blank" rel="noopener">http://192.168.116.201:8081</a></p><h3 id="八、搭建Flink1-6-1分布式集群"><a href="#八、搭建Flink1-6-1分布式集群" class="headerlink" title="八、搭建Flink1.6.1分布式集群"></a>八、搭建Flink1.6.1分布式集群</h3><h4 id="1、Flink的下载"><a href="#1、Flink的下载" class="headerlink" title="1、Flink的下载"></a>1、Flink的下载</h4><p>安装包下载地址：<a href="http://flink.apache.org/downloads.html" target="_blank" rel="noopener">http://flink.apache.org/downloads.html</a>  ，选择对应Hadoop的Flink版本下载<br>[root@hsiehchou121 software]$ wget <a href="http://mirrors.tuna.tsinghua.edu.cn/apache/flink/flink-1.6.1/flink-1.6.1-bin-hadoop28-scala_2.11.tgz" target="_blank" rel="noopener">http://mirrors.tuna.tsinghua.edu.cn/apache/flink/flink-1.6.1/flink-1.6.1-bin-hadoop28-scala_2.11.tgz</a><br>[root@hsiehchou121 software]$ ll<br>-rw-rw-r– 1 root root 301867081 Sep 15 15:47 flink-1.6.1-bin-hadoop28-scala_2.11.tgz<br>Flink 有三种部署模式，分别是 Local、Standalone Cluster 和 Yarn Cluster</p><h4 id="2、Local模式"><a href="#2、Local模式" class="headerlink" title="2、Local模式"></a>2、Local模式</h4><p>对于 Local 模式来说，JobManager 和 TaskManager 会公用一个 JVM 来完成 Workload</p><p>如果要验证一个简单的应用，Local 模式是最方便的。实际应用中大多使用 Standalone 或者 Yarn Cluster，而local模式只是将安装包解压启动（./bin/start-cluster.sh）即可，在这里不在演示</p><h4 id="3、Standalone-模式"><a href="#3、Standalone-模式" class="headerlink" title="3、Standalone 模式"></a>3、Standalone 模式</h4><p>快速入门教程地址：<br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/quickstart/setup_quickstart.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.6/quickstart/setup_quickstart.html</a></p><p>1）  <strong>软件要求</strong><br>•    Java 1.8.x或更高版本<br>•    ssh（必须运行sshd才能使用管理远程组件的Flink脚本）</p><p><strong>集群部署规划</strong></p><table><thead><tr><th align="center">节点名称</th><th align="center">master</th><th align="center">worker</th><th align="center">zookeeper</th></tr></thead><tbody><tr><td align="center">hsiehchou121</td><td align="center">master</td><td align="center"></td><td align="center">zookeeper</td></tr><tr><td align="center">hsiehchou122</td><td align="center">master</td><td align="center">worker</td><td align="center">zookeeper</td></tr><tr><td align="center">hsiehchou123</td><td align="center"></td><td align="center">worker</td><td align="center">zookeeper</td></tr></tbody></table><p>2）<strong>解压</strong></p><p>[root@hsiehchou121 software]$ <code>tar -zxvf flink-1.6.1-bin-hadoop27-scala_2.11.tgz -C /opt/module/</code><br>[root@hsiehchou121 software]$ cd /opt/module/</p><p>[root@hsiehchou121 module]$ ll<br>drwxr-xr-x 8 root root 125 Sep 15 04:47 flink-1.6.1</p><p>3）<strong>修改配置文件</strong></p><p>[root@hsiehchou121 conf]$ ls<br>flink-conf.yaml       log4j-console.properties  log4j-yarn-session.properties  logback.xml       masters  sql-client-defaults.yaml<br>log4j-cli.properties  log4j.properties          logback-console.xml            logback-yarn.xml  slaves   zoo.cfg</p><p>修改flink/conf/masters，slaves，flink-conf.yaml</p><p>[root@hsiehchou121 conf]$ sudo vi masters<br>hsiehchou121:8081</p><p>[root@hsiehchou121 conf]$ sudo vi slaves<br>hsiehchou122<br>hsiehchou123</p><p>[root@hsiehchou121 conf]$ sudo vi flink-conf.yaml<br>taskmanager.numberOfTaskSlots：2   //52行 和storm slot类似<br>jobmanager.rpc.address: hsiehchou121  //33行</p><p>可选配置：<br>•    每个JobManager（jobmanager.heap.mb）的可用内存量<br>•    每个TaskManager（taskmanager.heap.mb）的可用内存量<br>•    每台机器的可用CPU数量（taskmanager.numberOfTaskSlots）<br>•    集群中的CPU总数（parallelism.default）和<br>•    临时目录（taskmanager.tmp.dirs）</p><p>4）<strong>拷贝安装包到各节点</strong></p><p>[root@hsiehchou121 module]$ scp -r flink-1.6.1/ root@hsiehchou122:<code>pwd</code><br>[root@hsiehchou121 module]$ scp -r flink-1.6.1/ root@hsiehchou123:<code>pwd</code></p><p>5） <strong>配置环境变量</strong></p><p>配置所有节点Flink的环境变量<br>[root@hsiehchou121 flink-1.6.1]$ sudo vi /etc/profile<br>export FLINK_HOME=/opt/module/flink-1.6.1<br>export PATH=<code>$PATH:$</code>FLINK_HOME/bin</p><p>[root@hsiehchou121 flink-1.6.1]$ source /etc/profile</p><p>6）<strong>启动Flink</strong></p><p>[root@hsiehchou121 flink-1.6.1]$ ./bin/start-cluster.sh<br>Starting cluster.<br>Starting standalonesession daemon on host hsiehchou121.<br>Starting taskexecutor daemon on host hsiehchou122.<br>Starting taskexecutor daemon on host hsiehchou123.</p><p>jps查看进程<br>hsiehchou121<br>2122 StandaloneSessionClusterEntrypoint<br>2172 Jps</p><p>hsiehchou122<br>1616 TaskManagerRunner<br>1658 Jps</p><p>hsiehchou123<br>1587 TaskManagerRunner<br>1627 Jps</p><p>7） <strong>WebUI查看</strong></p><p><a href="http://hsiehchou121:8081" target="_blank" rel="noopener">http://hsiehchou121:8081</a></p><p>8）<strong>Flink的HA</strong></p><p>首先，我们需要知道 Flink 有两种部署的模式，分别是 Standalone 以及 Yarn Cluster 模式。对于 Standalone 来说，Flink 必须依赖于 Zookeeper 来实现 JobManager 的 HA（Zookeeper 已经成为了大部分开源框架 HA 必不可少的模块）。在 Zookeeper 的帮助下，一个 Standalone 的 Flink 集群会同时有多个活着的 JobManager，其中只有一个处于工作状态，其他处于 Standby 状态。当工作中的 JobManager 失去连接后（如宕机或 Crash），Zookeeper 会从 Standby 中选举新的 JobManager 来接管 Flink 集群</p><p>对于 Yarn Cluaster 模式来说，Flink 就要依靠 Yarn 本身来对 JobManager 做 HA 了。其实这里完全是 Yarn 的机制。对于 Yarn Cluster 模式来说，JobManager 和 TaskManager 都是被 Yarn 启动在 Yarn 的 Container 中。此时的 JobManager，其实应该称之为 Flink Application Master。也就说它的故障恢复，就完全依靠着 Yarn 中的 ResourceManager（和 MapReduce 的 AppMaster 一样）。由于完全依赖了 Yarn，因此不同版本的 Yarn 可能会有细微的差异。这里不再做深究</p><p>（1） <strong>修改配置文件</strong></p><p>修改flink-conf.yaml，HA模式下，jobmanager不需要指定，在master file中配置，由zookeeper选出leader与standby。<br><strong>jobmanager.rpc.address: hsiehchou121</strong><br>high-availability:zookeeper   //73行</p><p><strong>指定高可用模式（必须） //88行</strong><br>high-availability.zookeeper.quorum:hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181</p><p><strong>ZooKeeper仲裁是ZooKeeper服务器的复制组，它提供分布式协调服务（必须） //82行</strong><br>high-availability.storageDir:hdfs:///flink/ha/       </p><p><strong>JobManager元数据保存在文件系统storageDir中，只有指向此状态的指针存储在ZooKeeper中（必须） //没有</strong><br>high-availability.zookeeper.path.root:/flink         </p><p><strong>根ZooKeeper节点，在该节点下放置所有集群节点（推荐） //没有</strong><br>high-availability.cluster-id:/flinkCluster           </p><p><strong>&amp;&amp;&amp;&amp;&amp;自定义集群（推荐）</strong><br>state.backend: filesystem<br>state.checkpoints.dir: hdfs:///flink/checkpoints<br>state.savepoints.dir: hdfs:///flink/checkpoints</p><p><strong>修改conf/zoo.cfg</strong><br>server.1=hsiehchou121:2888:3888<br>server.2=hsiehchou122:2888:3888<br>server.3=hsiehchou123:2888:3888</p><p><strong>修改conf/masters</strong><br>hsiehchou121:8081<br>hsiehchou122:8081</p><p><strong>修改slaves</strong><br>hsiehchou122<br>hsiehchou123<br>同步配置文件conf到各节点</p><p>（2） <strong>启动HA</strong></p><p>先启动zookeeper集群各节点（测试环境中也可以用Flink自带的start-zookeeper-quorum.sh），启动dfs ,再启动flink<br>[root@hsiehchou121 flink-1.6.1]$ start-cluster.sh </p><p>WebUI查看，这是会自动产生一个主Master，如下</p><p>（3） <strong>验证HA</strong></p><p>手动杀死hsiehchou122上的master，此时，hsiehchou121上的备用master转为主mater</p><p>（4）<strong>手动将JobManager / TaskManager实例添加到群集</strong></p><p>您可以使用bin/jobmanager.sh和bin/taskmanager.sh脚本将JobManager和TaskManager实例添加到正在运行的集群中</p><p>添加JobManager<br>bin/jobmanager.sh ((start|start-foreground) <code>[host] [webui-port]</code>)|stop|stop-all</p><p>添加TaskManager<br>bin/taskmanager.sh start|start-foreground|stop|stop-all</p><p>[root@hsiehchou122 flink-1.6.1]$ jobmanager.sh start hsiehchou122<br>新添加的为从master</p><p>9）<strong>运行测试任务</strong></p><p>[root@hsiehchou121 flink-1.6.1]$ flink run -m hsiehchou121:8081 ./examples/batch/WordCount.jar –input /opt/wcinput/wc.txt –output /opt/wcoutput/</p><p>[root@hsiehchou121 flink-1.6.1]$ bin/flink run -m hsiehchou121:8081 ./examples/batch/WordCount.jar –input hdfs:///emp.csv –output hdfs:///user/root/output2</p><h4 id="4、Yarn-Cluster模式"><a href="#4、Yarn-Cluster模式" class="headerlink" title="4、Yarn Cluster模式"></a>4、Yarn Cluster模式</h4><p>1）<strong>引入</strong><br>在一个企业中，为了最大化的利用集群资源，一般都会在一个集群中同时运行多种类型的 Workload。因此 Flink 也支持在 Yarn 上面运行。首先，让我们了解下 Yarn 和 Flink 的关系</p><p><img src="/medias/Yarn%20%E5%92%8C%20Flink%20%E7%9A%84%E5%85%B3%E7%B3%BB.PNG" alt="Yarn 和 Flink 的关系"></p><p>在图中可以看出，Flink 与 Yarn 的关系与 MapReduce 和 Yarn 的关系是一样的。Flink 通过 Yarn 的接口实现了自己的 App Master。当在 Yarn 中部署了 Flink，Yarn 就会用自己的 Container 来启动 Flink 的 JobManager（也就是 App Master）和 TaskManager</p><p>启动新的Flink YARN会话时，客户端首先检查所请求的资源（容器和内存）是否可用。之后，它将包含Flink和配置的jar上传到HDFS（步骤1）</p><p>客户端的下一步是请求（步骤2）YARN容器以启动ApplicationMaster（步骤3）。由于客户端将配置和jar文件注册为容器的资源，因此在该特定机器上运行的YARN的NodeManager将负责准备容器（例如，下载文件）。完成后，将启动ApplicationMaster（AM）</p><p>该JobManager和AM在同一容器中运行。一旦它们成功启动，AM就知道JobManager（它自己的主机）的地址。它正在为TaskManagers生成一个新的Flink配置文件（以便它们可以连接到JobManager）。该文件也上传到HDFS。此外，AM容器还提供Flink的Web界面。YARN代码分配的所有端口都是临时端口。这允许用户并行执行多个Flink YARN会话</p><p>之后，AM开始为Flink的TaskManagers分配容器，这将从HDFS下载jar文件和修改后的配置。完成这些步骤后，即可建立Flink并准备接受作业</p><p>2）<strong>修改环境变量</strong></p><p>export  HADOOP_CONF_DIR= /opt/module/hadoop-2.8.4/etc/hadoop</p><p>3）<strong>部署启动</strong> </p><p>[root@hsiehchou121 flink-1.6.1]$ yarn-session.sh -d -s 1 -tm 800 -n 2<br>-n : TaskManager的数量，相当于executor的数量</p><p>-s : 每个JobManager的core的数量，executor-cores。建议将slot的数量设置每台机器的处理器数量</p><p>-tm : 每个TaskManager的内存大小，executor-memory</p><p>-jm : JobManager的内存大小，driver-memory</p><p>上面的命令的意思是，同时向Yarn申请3个container，其中 2 个 Container 启动 TaskManager（-n 2），每个 TaskManager 拥有两个 Task Slot（-s 2），并且向每个 TaskManager 的 Container 申请 800M 的内存，以及一个ApplicationMaster（Job Manager）</p><p>Flink部署到Yarn Cluster后，会显示Job Manager的连接细节信息<br>Flink on Yarn会覆盖下面几个参数，如果不希望改变配置文件中的参数，可以动态的通过-D选项指定，如<br> -Dfs.overwrite-files=true -Dtaskmanager.network.numberOfBuffers=16368</p><p>jobmanager.rpc.address：因为JobManager会经常分配到不同的机器上</p><p>taskmanager.tmp.dirs：使用Yarn提供的tmp目录</p><p>parallelism.default：如果有指定slot个数的情况下</p><p>yarn-session.sh会挂起进程，所以可以通过在终端使用CTRL+C或输入stop停止yarn-session</p><p>如果不希望Flink Yarn client长期运行，Flink提供了一种detached YARN session，启动时候加上参数-d或—detached</p><p>在上面的命令成功后，我们就可以在 Yarn Application 页面看到 Flink 的纪录</p><p>如果在虚拟机中测试，可能会遇到错误。这里需要注意内存的大小，Flink 向 Yarn 会申请多个 Container，但是 Yarn 的配置可能限制了 Container 所能申请的内存大小，甚至 Yarn 本身所管理的内存就很小。这样很可能无法正常启动 TaskManager，尤其当指定多个 TaskManager 的时候。因此，在启动 Flink 之后，需要去 Flink 的页面中检查下 Flink 的状态。这里可以从 RM 的页面中，直接跳转（点击 Tracking UI）</p><p>yarn-session.sh启动命令参数如下：</p><p>[root@hsiehchou121 flink-1.6.1]$ yarn-session.sh –help<br>Usage:<br>   Required<br>     -n,–container <code>&lt;arg&gt;</code>   Number of YARN container to allocate (=Number of Task Managers)<br>   Optional<br>     -D &lt;property=value&gt;             use value for given property<br>     -d,–detached                   If present, runs the job in detached mode<br>     -h,–help                       Help for the Yarn session CLI.<br>     -id,–applicationId <code>&lt;arg&gt;</code>       Attach to running YARN session<br>     -j,–jar <code>&lt;arg&gt;</code>                  Path to Flink jar file<br>     -jm,–jobManagerMemory <code>&lt;arg&gt;</code>    Memory for JobManager Container with optional unit (default: MB)<br>     -m,–jobmanager <code>&lt;arg&gt;</code>           Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified i<br>n the configuration.     -n,–container <code>&lt;arg&gt;</code>            Number of YARN container to allocate (=Number of Task Managers)<br>     -nl,–nodeLabel <code>&lt;arg&gt;</code>           Specify YARN node label for the YARN application<br>     -nm,–name <code>&lt;arg&gt;</code>                Set a custom name for the application on YARN<br>     -q,–query                      Display available YARN resources (memory, cores)<br>     -qu,–queue <code>&lt;arg&gt;</code>               Specify YARN queue.<br>     -s,–slots <code>&lt;arg&gt;</code>                Number of slots per TaskManager<br>     -st,–streaming                 Start Flink in streaming mode<br>     -t,–ship <code>&lt;arg&gt;</code>                 Ship files in the specified directory (t for transfer)<br>     -tm,–taskManagerMemory <code>&lt;arg&gt;</code>   Memory per TaskManager Container with optional unit (default: MB)<br>     -yd,–yarndetached              If present, runs the job in detached mode (deprecated; use non-YARN specific option instead)<br>     -z,–zookeeperNamespace <code></code>   Namespace to create the Zookeeper sub-paths for high availability mode</p><p>4）<strong>提交任务</strong></p><p>之后，我们可以通过这种方式提交我们的任务<br>[root@hsiehchou121 flink-1.6.1]$ ./bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar –input /opt/wcinput/wc.txt –output /opt/wcoutput/<br>bin/flink run -m yarn-cluster -yn2 examples/batch/WordCount.jar –input /input/ –output /XZ</p><p>以上命令在参数前加上y前缀，-yn表示TaskManager个数</p><p>在这个模式下，同样可以使用-m yarn-cluster提交一个”运行后即焚”的detached yarn（-yd）作业到yarn cluster</p><p>5）<strong>停止yarn cluster</strong></p><p>yarn application -kill application_1539058959130_0001</p><p>6） <strong>Yarn模式的HA</strong></p><p>应用最大尝试次数（<strong>yarn-site.xml</strong>），您必须配置为尝试应用的最大数量的设置yarn-site.xml，当前YARN版本的默认值为2（表示允许单个JobManager失败）<br><code>&lt;property&gt;</code><br><code>&lt;name&gt;</code>yarn.resourcemanager.am.max-attempts<code>&lt;/name&gt;</code><br>  <code>&lt;value&gt;</code>4<code>&lt;/value&gt;</code><br> <code>&lt;description&gt;</code>The maximum number of application master execution attempts<code>&lt;/description&gt;</code><br><code>&lt;/property&gt;</code></p><p>申请尝试（<strong>flink-conf.yaml</strong>），您还必须配置最大尝试次数conf/flink-conf.yaml： yarn.application-attempts：10</p><p>示例：<strong>高度可用的YARN会话</strong><br>配置HA模式和zookeeper法定人数在conf/flink-conf.yaml：<br>high-availability: zookeeper<br>high-availability.zookeeper.quorum: hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181<br>high-availability.storageDir: hdfs:///flink/recovery<br>high-availability.zookeeper.path.root: /flink<br>yarn.application-attempts: 10</p><p>配置ZooKeeper的服务器中conf/zoo.cfg（目前它只是可以运行每台机器的单一的ZooKeeper服务器）：<br>server.1=hsiehchou121:2888:3888<br>server.2=hsiehchou122:2888:3888<br>server.3=hsiehchou123:2888:3888</p><p><strong>启动ZooKeeper仲裁</strong>：<br>$ bin / start-zookeeper-quorum.sh</p><p><strong>启动HA群集</strong>：<br>$ bin / yarn-session.sh -n 2</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Storm练习</title>
      <link href="/2019/05/14/storm-lian-xi/"/>
      <url>/2019/05/14/storm-lian-xi/</url>
      
        <content type="html"><![CDATA[<p><strong>Storm练习</strong></p><h3 id="一、需求"><a href="#一、需求" class="headerlink" title="一、需求"></a>一、需求</h3><p>需求：统计网站访问量(实时统计)</p><p>技术选型：特点（数据量大、做计算、实时）</p><p>实时计算框架：storm<br>1）spout<br>    数据源，接入数据<br>    本地文件</p><p>2）bolt<br>    业务逻辑处理<br>    切分数据<br>    查到网址</p><p>3）bolt<br>    累加次数求和</p><h3 id="二、代码编写"><a href="#二、代码编写" class="headerlink" title="二、代码编写"></a>二、代码编写</h3><ol><li>PvCountSpout.java</li></ol><pre><code>package com.hsiehchou.pvcount;import java.io.BufferedReader;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;import java.io.InputStreamReader;import java.util.Map;import org.apache.storm.spout.SpoutOutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.IRichSpout;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Values;public class PvCountSpout implements IRichSpout {    private SpoutOutputCollector collector;    private BufferedReader br;    private String line;    @Override    public void nextTuple() {        //发送读取数据的每一行        try {            while((line = br.readLine()) != null) {                //发送数据到splitbolt                collector.emit(new Values(line));                //设置延迟                Thread.sleep(500);            }        } catch (IOException | InterruptedException e) {            e.printStackTrace();        }    }    @Override    public void open(Map arg0, TopologyContext arg1, SpoutOutputCollector collector) {        this.collector = collector;        //读取文件        try {            br = new BufferedReader(new InputStreamReader(new FileInputStream(&quot;e:/weblog.log&quot;)));        } catch (FileNotFoundException e) {            e.printStackTrace();        }    }    @Override    public void declareOutputFields(OutputFieldsDeclarer declarer) {        //声明        declarer.declare(new Fields(&quot;logs&quot;));    }    //处理Tuple成功 回调的方法    @Override    public void ack(Object arg0) {    }    //如果spout在失效的模式中，调用此方法来激活    @Override    public void activate() {    }    //在spout程序关闭前执行，不能保证一定执行，kill -9是不执行  storm kill是不执行    @Override    public void close() {    }    //在spout失效期间，nextTuple不会被调用    @Override    public void deactivate() {    }    //处理Tuple失败回调的方法    @Override    public void fail(Object arg0) {    }    //配置    @Override    public Map&lt;String, Object&gt; getComponentConfiguration() {        return null;    }}</code></pre><ol start="2"><li>PvCountSplitBolt.java</li></ol><pre><code>package com.hsiehchou.pvcount;import java.util.Map;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.IRichBolt;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Tuple;import org.apache.storm.tuple.Values;public class PvCountSplitBolt implements IRichBolt {    private OutputCollector collector;    private int pvnum = 0;    //一个bolt即将关闭时调用，不能保证一定会被调用    @Override    public void cleanup() {    }    //业务逻辑-分布式-集群-并发度-线程（接收Tuple然后进行处理）资源清理    @Override    public void execute(Tuple input) {        //1.获取数据        String line = input.getStringByField(&quot;logs&quot;);        //2.切分数据        String[] fields = line.split(&quot;\t&quot;);        String session_id = fields[1];        //3.局部累加        if(session_id != null) {            //列累加            pvnum++;            //输出            collector.emit(new Values(Thread.currentThread().getId(),pvnum));        }    }    //初始化时调用    @Override    public void prepare(Map arg0, TopologyContext arg1, OutputCollector collector) {        this.collector = collector;    }    //声明    @Override    public void declareOutputFields(OutputFieldsDeclarer declarer) {        //声明输出字段        declarer.declare(new Fields(&quot;threadid&quot;,&quot;pvnum&quot;));    }    //配置    @Override    public Map&lt;String, Object&gt; getComponentConfiguration() {        return null;    }}</code></pre><ol start="3"><li>PvCountBolt.java</li></ol><pre><code>package com.hsiehchou.pvcount;import java.util.HashMap;import java.util.Iterator;import java.util.Map;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.IRichBolt;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.tuple.Tuple;public class PvCountBolt implements IRichBolt {    private HashMap&lt;Long, Integer&gt; hashmap = new HashMap&lt;&gt;();    @Override    public void cleanup() {    }    //全局累加求和 业务逻辑    @Override    public void execute(Tuple input) {        //1.获取数据        Long threadid = input.getLongByField(&quot;threadid&quot;);        Integer pvnum = input.getIntegerByField(&quot;pvnum&quot;);        //2.创建集合 存储(threadid,pvnum)        hashmap.put(threadid,pvnum);        //3.累加求和        Iterator&lt;Integer&gt; iterator = hashmap.values().iterator();        //4.清空之前的数据        int sumnum = 0;        while(iterator.hasNext()) {            sumnum += iterator.next();        }        System.out.println(Thread.currentThread().getName() + &quot;总访问量为：&quot; + sumnum);    }    @Override    public void prepare(Map arg0, TopologyContext arg1, OutputCollector arg2) {    }    @Override    public void declareOutputFields(OutputFieldsDeclarer arg0) {    }    @Override    public Map&lt;String, Object&gt; getComponentConfiguration() {        return null;    }}</code></pre><ol start="4"><li>PvCountDriver.java</li></ol><pre><code>package com.hsiehchou.pvcount;import org.apache.storm.Config;import org.apache.storm.LocalCluster;import org.apache.storm.topology.TopologyBuilder;import org.apache.storm.tuple.Fields;public class PvCountDriver {    public static void main(String[] args) {        //1.hadoop --&gt; Job Storm --&gt; Topology 创建拓扑        TopologyBuilder builder = new TopologyBuilder();        builder.setSpout(&quot;PvCountSpout&quot;, new PvCountSpout(), 1);        //builder.setBolt(&quot;PvCountSplitBolt&quot;, new PvCountSplitBolt(), 6).setNumTasks(4).shuffleGrouping(&quot;PvCountSpout&quot;);        builder.setBolt(&quot;PvCountSplitBolt&quot;, new PvCountSplitBolt(), 6).setNumTasks(4)            .fieldsGrouping(&quot;PvCountSpout&quot;, new Fields(&quot;logs&quot;));        //builder.setBolt(&quot;PvCountSumBolt&quot;, new PvCountBolt(), 1).shuffleGrouping(&quot;PvCountSplitBolt&quot;);        builder.setBolt(&quot;PvCountSumBolt&quot;, new PvCountBolt(), 1).fieldsGrouping(&quot;PvCountSplitBolt&quot;, new Fields(&quot;pvnum&quot;));        Config conf = new Config();        conf.setNumWorkers(1);        LocalCluster localCluster = new LocalCluster();        localCluster.submitTopology(&quot;pvcountsum&quot;, conf, builder.createTopology());    }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Storm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Storm集群和集成</title>
      <link href="/2019/05/12/storm-ji-qun-he-ji-cheng/"/>
      <url>/2019/05/12/storm-ji-qun-he-ji-cheng/</url>
      
        <content type="html"><![CDATA[<h3 id="一、Storm集群任务提交流程"><a href="#一、Storm集群任务提交流程" class="headerlink" title="一、Storm集群任务提交流程"></a>一、Storm集群任务提交流程</h3><p><img src="/medias/Storm%E9%9B%86%E7%BE%A4%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B.PNG" alt="Storm集群任务提交流程"></p><h3 id="二、Storm内部通信机制"><a href="#二、Storm内部通信机制" class="headerlink" title="二、Storm内部通信机制"></a>二、Storm内部通信机制</h3><p><img src="/medias/Worker%E8%BF%9B%E7%A8%8B1.PNG" alt="Worker进程"></p><p><img src="/medias/Worker.PNG" alt="Worker"></p><h3 id="三、集成Storm"><a href="#三、集成Storm" class="headerlink" title="三、集成Storm"></a>三、集成Storm</h3><h4 id="1、与JDBC集成"><a href="#1、与JDBC集成" class="headerlink" title="1、与JDBC集成"></a>1、与JDBC集成</h4><ul><li><p>将Storm Bolt处理的结果插入MySQL数据库中</p></li><li><p>需要依赖的jar包<br>    <code>$STORM_HOME</code>\external\sql\storm-sql-core*.jar<br>    <code>$STORM_HOME</code>\external\storm-jdbc\storm-jdbc-1.0.3.jar<br>    mysql的驱动<br>    commons-lang3-3.1.jar</p></li><li><p>与JDBC集成的代码实现<br>    修改主程序WordCountTopology，增加如下代码：</p><pre><code>//创建一个JDBCBolt将结果插入数据库中builder.setBolt(&quot;wordcount_jdbcBolt&quot;, createJDBCBolt()).shuffleGrouping(&quot;wordcount_count&quot;);</code></pre></li></ul><p>增加一个新方法创建JDBCBolt组件</p><pre><code>//创建JDBC Insert Bolt组件//需要事先在MySQL数据库中创建对应的表，resultprivate static IRichBolt createJDBCBolt(){    ConnectionProvider connectionProvider = new MyConnectionProvider();    JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(&quot;aaa&quot;, connectionProvider);    return new JdbcInsertBolt(connectionProvider, simpleJdbcMapper).withTableName(&quot;result&quot;).withQueryTimeoutSecs(30);}</code></pre><p>    实现ConnectionProvider接口</p><pre><code>class MyConnectionProvider implements ConnectionProvider{    private static String driver = &quot;com.mysql.cj.jdbc.Driver&quot;;    private static String url = &quot;jdbc:mysql://192.168.116.121:3306/demo&quot;;    private static String user = &quot;root&quot;;    private static String password = &quot;password&quot;;    //静态块    static{//注册驱动        try{            Class.forName(driver);        }catch(ClassNotFoundException e){            throw new ExceptionInInitializerError(e);        }    }    @Override    public Connection getConnection(){        try{            return DriverManager.getConnection(url, user, password);        }catch{            e.printStackTrace();        }            return null;        }    public void cleanup(){}    public void prepare(){}}</code></pre><p>    修改WordCountSplitBolt组件，将统计后的结果发送给下一个组件写入MySQL</p><pre><code>public class WordCountSplitBolt extends BaseRichBolt {    private Map&lt;String, Integer&gt; result = new HashMap&lt;String, Integer&gt;();    private OutputCollector collector;    @Override    public void execute(Tuple tuple){        String word = tuple.getStringByField(&quot;word&quot;);        int count = tuple.getIntegerByField(&quot;count&quot;);        if(result.containsKey(word)) {            int total = result.get(word);            result.put(word, total+count);        }else{            result.put(word, 1);        }        //直接输出到屏幕        //System.out.println(&quot;输出的结果是：&quot; + result);        //将统计结果发送下一个Bolt，插入数据        this.collector.emit(new Values*(word, result.get(word));    }    @Override    public void prepare(Map arg0, TopologyContext arg1, OutputCollector collector) {        this.collector = collector;    }    @Override    public void declareOutputFields(OutputFieldsDecler declare){        declare.declarer(new Fields(&quot;word&quot;, &quot;sum&quot;));    }}</code></pre><h4 id="2、与Redis集成"><a href="#2、与Redis集成" class="headerlink" title="2、与Redis集成"></a>2、与Redis集成</h4><p>Redis是一个key-value存储系统。和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set –有序集合)和hash（哈希类型）。与Memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步</p><p>Redis 是一个高性能的key-value数据库。Redis的出现，很大程度补偿了memcached这类key/value存储的不足，在部分场合可以对关系数据库起到很好的补充作用。它提供了Java，C/C++，C#，PHP，JavaScript，Perl，Object-C，Python，Ruby，Erlang等客户端，使用很方便。</p><p>Redis支持主从同步。数据可以从主服务器向任意数量的从服务器上同步，从服务器可以是关联其他从服务器的主服务器</p><p>    修改代码：WordCountTopology.java</p><pre><code>builder.setBolt(&quot;wordcount_redisBolt&quot;, createRedisBolt()).shuffleGrouping(&quot;wordcount_count&quot;);//创建Redis Bolt 组件private staticIRichBolt createRedisBolt(){    JedisPoolConfig.Builder builder = new JedisPoolConfig.Builder();    builder.setHost(&quot;192.168.116.121&quot;);    builder.setPort(6379);    JedisPoolConfig poolConfig = builder.build();    //RedisStoreMapper用于指定存入Redis中的数据格式    return new RedisStoreBolt(poolConfig, new RedisStoreMapper(){        @Override        public RedisDateTypeDescription getDataTypeDescription(){            return new RedisDateTypeDescription(RedisDateTypeDescription.RedisDateType.HASH, &quot;wordCount&quot;);        }        @Override        public String getValueFromTuple(){            return String.valueOf(tuple.getIntegerByField(&quot;total&quot;));        }        @Override        public String getKeyFromTuple(){            return tuple.getStringByField(&quot;word&quot;);            }    })}</code></pre><h4 id="3、与HDFS集成"><a href="#3、与HDFS集成" class="headerlink" title="3、与HDFS集成"></a>3、与HDFS集成</h4><ul><li><p>需要的jar包：<br>    <code>$STORM_HOME</code>\external\storm-hdfs\storm-hdfs-1.0.3.jar<br>    HDFS相关的jar包</p></li><li><p>开发新的bolt组件</p></li></ul><pre><code>//创建一个新的HDFS Bolt组件，把前一个bolt组件处理的结果存入HDFSprivate static IRichBolt createHDFSBolt(){    HdfsBolt bolt = new HdfsBolt();    //HDFS的位置    bolt.withFsUrl(&quot;hdfs://192.168.116.121:9000&quot;);    //数据保存在HDFS上的目录    bolt.withFileNameFormat(new DefaultFileNameFormat().withPath(&quot;/stormdata&quot;));    //写入HDFS的数据的分隔符 | 结果：Beijing|10    bolt.withRecordFormat(new DelimitedRecordFormat().withFieldDelimiter(&quot;|&quot;));    //每5M的数据生成一个文件    bolt.withRotationPolicy(new FileSizeRotationPolicy(5.0f, Units.MB));    //Bolt输出tuple,当tuple达到一定的大小（没1K），与HDFS进行同步    bolt.withSyncPolicy(new CountSyncPolicy(1000));    return bolt;}</code></pre><h4 id="4、与HBase集成"><a href="#4、与HBase集成" class="headerlink" title="4、与HBase集成"></a>4、与HBase集成</h4><ul><li><p>需要的jar包：HBase的相关包</p></li><li><p>开发新的bolt组件（WordCountBoltHBase.java）</p><pre><code>/*** 在HBase中创建表，保存数据* create &#39;result&#39;,&#39;info&#39;*/public class WordCountBoltHBase extends BaseRichBolt {public void execute(Tuple tuple){   //如何处理？将上一个bolt组件发送过来的结果，存入HBase   //取出上一个组件发送过来的数据   String word = tuple.getStringByField(&quot;word&quot;);   int total = tuple.getIntegerByField(&quot;total&quot;);   //构造一个Put对象   Put put = new Put(Bytes.toBytes(word));   put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;word&quot;), Bytes.toBytes(word));   put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;total&quot;), Bytes.toBytes(String.valueOf(total)));   //把数据插入HBase   try{       table.put(put);   }catch(Exception e){       e.printStackTrace();   }}public void prepare(Map arg0, TopologyContext arg1, OutputCollector arg2){   //初始化   //指定ZK的地址   Configuration conf = new Configuration();   conf.set(&quot;hbase.zookeeper.quorum&quot;,&quot;192.168.116.121&quot;);   //创建table的客户端   try{       table = new HTable(conf, &quot;result&quot;);   }catch(Exception ex){       ex.printStackTrace();   }}}</code></pre></li></ul><h4 id="5、与Apache-Kafka集成"><a href="#5、与Apache-Kafka集成" class="headerlink" title="5、与Apache Kafka集成"></a>5、与Apache Kafka集成</h4><ul><li>注意：需要把slf4j-log4j12-1.6.4.jar包去掉，有冲突（有两个）</li></ul><pre><code>private static IRichSpout creatKafkaSpout(){    //定义ZK地址    BrokerHosts hosts = new ZkHosts(&quot;hadoop121:2181,hadoop122:2181,hadoop123:2181&quot;);    //指定Topic的信息    SpoutConfig spoutConf = new SpoutConfig(hosts, &quot;mydemo2&quot;, &quot;/mydemo2&quot;, UUID.randomUUID().toString());    //定义收到消息的Schema格式    spoutConf.scheme = new SchemeAsMultiScheme(new Scheme(){        @Override        public Fields getOutputFields(){            return new Fields(&quot;sentence&quot;);        }        @Override        public List&lt;Object&gt; deserialize(ByteBuffer buffer){            try{                String msg = (Charset.forName(&quot;UTF-8&quot;).newDecoder()).decode(buffer).asReadOnlyBuffer().toString();                System.out.println(&quot;**********收到的数据是msg&quot; + msg);                return new Values(msg);            }catch(Exception e){                e.printStackTrace();            }            return null;        }    });    return new KafkaSpout(spoutConf);}</code></pre><h4 id="6、与Hive集成"><a href="#6、与Hive集成" class="headerlink" title="6、与Hive集成"></a>6、与Hive集成</h4><ul><li>由于集成Storm和Hive依赖的jar较多，并且冲突的jar包很多，强烈建议使用Maven来搭建新的工程</li></ul><pre><code>&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;        &lt;artifactId&gt;storm-core&lt;/artifactId&gt;        &lt;version&gt;1.0.3&lt;/version&gt;        &lt;scope&gt;provided&lt;/scope&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;        &lt;artifactId&gt;storm-hive&lt;/artifactId&gt;        &lt;version&gt;1.0.3&lt;/version&gt;        &lt;type&gt;jar&lt;/type&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;</code></pre><ul><li>需要对Hive做一定的配置（在hive-site.xml文件中）：</li></ul><pre><code>&lt;property&gt;  &lt;name&gt;hive.in.test&lt;/name&gt;  &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;</code></pre><ul><li>需要使用下面的语句在hive中创建表：</li></ul><pre><code>create table wordcount(word string,total int)clustered by (word) into 10 bucketsstored as orc TBLPROPERTIES(&#39;transactional&#39;=&#39;true&#39;);</code></pre><ul><li>启动metastore服务：hive –service metastore</li><li>开发新的bolt组件，用于将前一个bolt处理的结果写入Hive</li></ul><pre><code>private static IRichBolt createHiveBolt(){    //设置环境变量，能找到winutils.exe    System.setProperty(&quot;hadoop.home.dir&quot;, &quot;D:\\tools\\hadoop-2.8.4&quot;);    //作用：将bolt组件处理后的结果tuple，如何存入hive表    DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper().withColumnFields(new Fields(&quot;word&quot;, &quot;total&quot;));    //配置Hive的参数信息    HiveOptions options = new HiveOptions(&quot;thrift://hadoop121:9083&quot;,//hive的metastore                                          &quot;default&quot;,//hive数据库的名字                                          &quot;wordcount&quot;,//保存数据的表                            mapper)                            .withTxnsPerBatch(10)                            .withBatchSize(1000)                            .withIdleTimeout(10);    //创建一个Hive的bolt组件，将单词计数后的结果存入hive    HiveBolt bolt = new HiveBolt(options);    return bolt;}</code></pre><ul><li>为了测试的方便，我们依然采用之前随机产生字符串的Spout组件产生数据</li></ul><h4 id="7、与JMS集成"><a href="#7、与JMS集成" class="headerlink" title="7、与JMS集成"></a>7、与JMS集成</h4><p>JMS即Java消息服务（Java Message Service）应用程序接口，是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。Java消息服务是一个与具体平台无关的API，绝大多数MOM提供商都对JMS提供支持</p><p>JMS的两种消息类型：Queue和Topic<br>基于Weblogic的JMS架构 ：</p><p><img src="/medias/%E5%9F%BA%E4%BA%8EWeblogic%E7%9A%84JMS%E6%9E%B6%E6%9E%84.PNG" alt="基于Weblogic的JMS架构"></p><pre><code>private static IRichBolt createJMSBolt(){    //创建一个JMS Bolt，将前一个bolt发送过来的数据 写入JMS    JmsBolt bolt = new JmsBolt();    //指定JMSBolt的provider    bolt.setJmsProvider(new MyJMSProvider());    //指定bolt如何解析信息    bolt.setJmsMessageProducer(new JmsMessageProducer(){        @Override        public Message toMessage(Session session, ITuple tuple) throws JMSException {            //取出上一个组件发送过来的数据            String word = tuple.getStringByField(&quot;word&quot;);            int total = tuple.getIntegerByField(&quot;total&quot;);            return session.createTextMessage(word + &quot;   &quot; + total);        }    });    return bolt;}</code></pre><ul><li>需要的weblogic的jar包</li></ul><p>wljmsclient.jar<br>wlclient.jar</p><ul><li>permission javax.management.MBeanTrustPermission “register”;</li></ul>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Storm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>私有网络软件仓库搭建和挂载网络系统镜像</title>
      <link href="/2019/05/10/si-you-wang-luo-ruan-jian-cang-ku-da-jian-he-gua-zai-wang-luo-xi-tong-jing-xiang/"/>
      <url>/2019/05/10/si-you-wang-luo-ruan-jian-cang-ku-da-jian-he-gua-zai-wang-luo-xi-tong-jing-xiang/</url>
      
        <content type="html"><![CDATA[<h3 id="一、私有网络软件仓库"><a href="#一、私有网络软件仓库" class="headerlink" title="一、私有网络软件仓库"></a>一、私有网络软件仓库</h3><p>在集群安装的过程中，要求每个节点都必须挂载光驱， 而对于每台节点都手动的去挂载光驱太麻烦，也不方便。这里使用每个节点都指向同一个私有网络镜像来解决这个问题</p><p>我们的集群采用的是全离线安装，也不可能逐个节点的安装，同样是也使用指向同一个私有的网络软件包来解决</p><p>因此选择在hadoop-4上搭建一个私有的网络软件仓库，以下是搭建的全过程</p><h4 id="1、上传镜像"><a href="#1、上传镜像" class="headerlink" title="1、上传镜像"></a>1、上传镜像</h4><p>搭建私有网络镜像使用的镜像文件最好就使用安装系统的镜像，这里是选择了CentOS6.7x64的镜像，将其上传到hadoop1的/disk下（需新建/disk目录）<br>mkdir disk<br>上传CentOS-6.7-x86_64-bin-DVD1.iso</p><h4 id="2、挂载镜像"><a href="#2、挂载镜像" class="headerlink" title="2、挂载镜像"></a>2、挂载镜像</h4><p>首先创建文件夹 /media/CentOS ：<br>mkdir -p /media/CentOS</p><p>挂载镜像 ：<br>mount -o loop /disk/CentOS-6.7-x86_64-bin-DVD1.iso /media/CentOS/</p><p>进入目录/etc/yum.repos.d ：<br>cd /etc/yum.repos.d</p><p>修改CentOS-Base.repo的名称 ：<br>mv CentOS-Base.repo CentOS-Base.repo.bak</p><p>修改CentOS-Media.repo文件 ：<br>vim CentOS-Media.repo</p><p>修改如下 ：<br>将enable=0改成enable=1</p><p>清除yum的缓存 ：<br>yum clean metadata<br>yum clean dbcache</p><p>查看是否挂载成功 ：<br>yum list | wc -l </p><p>这是统计镜像中有多少个软件包的命令，CentOS6.7x64位的系统的软件包个数一般在3000以上</p><h4 id="3、安装http-如果已经安装可以省略，但是需要启动，一b般最小化安装不会安装此服务"><a href="#3、安装http-如果已经安装可以省略，但是需要启动，一b般最小化安装不会安装此服务" class="headerlink" title="3、安装http(如果已经安装可以省略，但是需要启动，一b般最小化安装不会安装此服务)"></a>3、安装http(如果已经安装可以省略，但是需要启动，一b般最小化安装不会安装此服务)</h4><p>检查是否安装<br>service httpd status</p><p>网络镜像需要通过http请求访问，因此需要安装http:<br>yum –y install http</p><p>启动http服务，并让其开机自启 ：<br>service httpd start<br>chkconfig httpd on</p><p>由于http的默认端口为80，通过浏览器访问 ：<br>192.168.116.201:80</p><p>创建网络软件仓库目录 ：<br>mkdir –p /var/www/html</p><p>http默认将上面的目录作为软件仓库的目录</p><h4 id="4、安装createrepo-如果已经安装省略"><a href="#4、安装createrepo-如果已经安装省略" class="headerlink" title="4、安装createrepo(如果已经安装省略)"></a>4、安装createrepo(如果已经安装省略)</h4><p>该软件使用来生成http镜像的网络识别路径的：<br>yum -y install createrepo</p><p>到此 私有的网络软件仓库搭建完成</p><h3 id="二、挂载网络系统镜像"><a href="#二、挂载网络系统镜像" class="headerlink" title="二、挂载网络系统镜像"></a>二、挂载网络系统镜像</h3><h4 id="1、创建网络系统镜像"><a href="#1、创建网络系统镜像" class="headerlink" title="1、创建网络系统镜像"></a>1、创建网络系统镜像</h4><p>将从镜像中挂载的文件拷贝到软件仓库的目录中<br>cp -r /media/CentOS /var/www/html/</p><p>删除目录repodata<br>cd /var/www/html/CentOS<br>rm -rf ./repodata</p><p>生成新的软件路径目录repodata<br>createrepo .</p><p>也可以通过网络访问查看（192.168.116.201/CentOS/）：</p><p>到此网络镜像创建成功</p><h4 id="2、使用网络系统镜像"><a href="#2、使用网络系统镜像" class="headerlink" title="2、使用网络系统镜像"></a>2、使用网络系统镜像</h4><p>解除对镜像文件的挂载 ：<br>umount /media/CentOS</p><p>如下， 目录下无文件则说明解除挂载成功<br>  [root@hadoop1 yum.repos.d]# ll  /media/CentOS</p><p>如果出现如下说明有进程在占用挂载点<br> [root@hadoop1 yum.repos.d]# fuser -m /media/CentOS/<br> /media/CentOS 3157</p><p>出现这种情况，表示还有进程在使用/medis/CentOS挂载点，那么此时可以借助fuser命令找出占用目录/medis/CentOS的所有进程，然后kill掉，此时就可以umount 了<br>fuser -m /media/CentOS/</p><p>修改文件CentOS-Media.repo让其指向刚才创建的网络镜像<br>vim /etc/yum.repos.d/CentOS-Media.repo</p><p>修改如下：（修改前的配置参考3.1.2）<br> baseurl=<a href="https://192.168.116.201/CentOS/" target="_blank" rel="noopener">https://192.168.116.201/CentOS/</a></p><p>清楚yum的缓存， 并查看软件包个数<br>yum clean metadata<br>yum clean dbcache<br>yum list | wc -l </p>]]></content>
      
      
      <categories>
          
          <category> CentOS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CentOS </tag>
            
            <tag> 私有网络软件仓库 </tag>
            
            <tag> 挂载网络系统镜像 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据图片汇总</title>
      <link href="/2019/05/06/da-shu-ju-tu-pian-hui-zong/"/>
      <url>/2019/05/06/da-shu-ju-tu-pian-hui-zong/</url>
      
        <content type="html"><![CDATA[<h3 id="1、大数据课程概述与大数据背景知识"><a href="#1、大数据课程概述与大数据背景知识" class="headerlink" title="1、大数据课程概述与大数据背景知识"></a>1、大数据课程概述与大数据背景知识</h3><h4 id="（1）数据仓库与大数据"><a href="#（1）数据仓库与大数据" class="headerlink" title="（1）数据仓库与大数据"></a>（1）数据仓库与大数据</h4><p><img src="/medias/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E5%A4%A7%E6%95%B0%E6%8D%AE.PNG" alt="数据仓库与大数据"></p><h4 id="（2）PageRank"><a href="#（2）PageRank" class="headerlink" title="（2）PageRank"></a>（2）PageRank</h4><p><img src="/medias/PageRank.PNG" alt="PageRank"></p><h4 id="（3）MR基本原理"><a href="#（3）MR基本原理" class="headerlink" title="（3）MR基本原理"></a>（3）MR基本原理</h4><p><img src="/medias/MR%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86.PNG" alt="MR基本原理"></p><h4 id="（4）HDFS原理"><a href="#（4）HDFS原理" class="headerlink" title="（4）HDFS原理"></a>（4）HDFS原理</h4><p><img src="/medias/HDFS%E5%8E%9F%E7%90%86.PNG" alt="HDFS原理"></p><h4 id="（5）bigtable与Habase"><a href="#（5）bigtable与Habase" class="headerlink" title="（5）bigtable与Habase"></a>（5）bigtable与Habase</h4><p><img src="/medias/bigtable%E4%B8%8EHabase.PNG" alt="bigtable与Habase"></p><h3 id="2、搭建Hadoop的环境"><a href="#2、搭建Hadoop的环境" class="headerlink" title="2、搭建Hadoop的环境"></a>2、搭建Hadoop的环境</h3><h4 id="（1）1-PNG"><a href="#（1）1-PNG" class="headerlink" title="（1）1.PNG"></a>（1）1.PNG</h4><p><img src="/medias/1.PNG" alt="1"></p><h4 id="（2）2-PNG"><a href="#（2）2-PNG" class="headerlink" title="（2）2.PNG"></a>（2）2.PNG</h4><p><img src="/medias/2.PNG" alt="2"></p><h4 id="（3）3-PNG"><a href="#（3）3-PNG" class="headerlink" title="（3）3.PNG"></a>（3）3.PNG</h4><p><img src="/medias/3.PNG" alt="3"></p><h4 id="（4）4-PNG"><a href="#（4）4-PNG" class="headerlink" title="（4）4.PNG"></a>（4）4.PNG</h4><p><img src="/medias/4.PNG" alt="4"></p><h3 id="3、HDFS基础与操作"><a href="#3、HDFS基础与操作" class="headerlink" title="3、HDFS基础与操作"></a>3、HDFS基础与操作</h3><h4 id="（1）startup"><a href="#（1）startup" class="headerlink" title="（1）startup"></a>（1）startup</h4><p><img src="/medias/startup.PNG" alt="startup"></p><h3 id="4、HDFS上传与下载的原理"><a href="#4、HDFS上传与下载的原理" class="headerlink" title="4、HDFS上传与下载的原理"></a>4、HDFS上传与下载的原理</h3><h4 id="（1）HDFS-Upload"><a href="#（1）HDFS-Upload" class="headerlink" title="（1）HDFS_Upload"></a>（1）HDFS_Upload</h4><p><img src="/medias/HDFS_Upload.PNG" alt="HDFS_Upload"></p><h4 id="（2）HDFS-DownLoad"><a href="#（2）HDFS-DownLoad" class="headerlink" title="（2）HDFS_DownLoad"></a>（2）HDFS_DownLoad</h4><p><img src="/medias/HDFS_DownLoad.PNG" alt="HDFS_DownLoad"></p><h3 id="5、HDFS-工作机制"><a href="#5、HDFS-工作机制" class="headerlink" title="5、HDFS-工作机制"></a>5、HDFS-工作机制</h3><h4 id="（1）namenode工作机制"><a href="#（1）namenode工作机制" class="headerlink" title="（1）namenode工作机制"></a>（1）namenode工作机制</h4><p><img src="/medias/namenode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.PNG" alt="namenode工作机制"></p><h4 id="（2）datanode工作机制"><a href="#（2）datanode工作机制" class="headerlink" title="（2）datanode工作机制"></a>（2）datanode工作机制</h4><p><img src="/medias/datanode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.PNG" alt="datanode工作机制"></p><h3 id="6、MapReduce基础"><a href="#6、MapReduce基础" class="headerlink" title="6、MapReduce基础"></a>6、MapReduce基础</h3><h4 id="（1）mapreduce思想"><a href="#（1）mapreduce思想" class="headerlink" title="（1）mapreduce思想"></a>（1）mapreduce思想</h4><p><img src="/medias/mapreduce%E6%80%9D%E6%83%B3.PNG" alt="mapreduce思想"></p><h3 id="7、MapReduce分布式编程模型"><a href="#7、MapReduce分布式编程模型" class="headerlink" title="7、MapReduce分布式编程模型"></a>7、MapReduce分布式编程模型</h3><h4 id="（1）maptask决定机制"><a href="#（1）maptask决定机制" class="headerlink" title="（1）maptask决定机制"></a>（1）maptask决定机制</h4><p><img src="/medias/maptask%E5%86%B3%E5%AE%9A%E6%9C%BA%E5%88%B6.PNG" alt="maptask决定机制"></p><h3 id="8、MapReduce案例分析"><a href="#8、MapReduce案例分析" class="headerlink" title="8、MapReduce案例分析"></a>8、MapReduce案例分析</h3><h4 id="（1）yarn工作流程"><a href="#（1）yarn工作流程" class="headerlink" title="（1）yarn工作流程"></a>（1）yarn工作流程</h4><p><img src="/medias/yarn%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.PNG" alt="yarn工作流程"></p><h4 id="（2）日志格式"><a href="#（2）日志格式" class="headerlink" title="（2）日志格式"></a>（2）日志格式</h4><p><img src="/medias/%E6%97%A5%E5%BF%97%E6%A0%BC%E5%BC%8F.PNG" alt="日志格式"></p><h3 id="9、分区排序"><a href="#9、分区排序" class="headerlink" title="9、分区排序"></a>9、分区排序</h3><h4 id="（1）mapreduce流程"><a href="#（1）mapreduce流程" class="headerlink" title="（1）mapreduce流程"></a>（1）mapreduce流程</h4><p><img src="/medias/mapreduce%E6%B5%81%E7%A8%8B.PNG" alt="mapreduce流程"></p><h3 id="10、Shuffle机制"><a href="#10、Shuffle机制" class="headerlink" title="10、Shuffle机制"></a>10、Shuffle机制</h3><h4 id="（1）shuffle机制"><a href="#（1）shuffle机制" class="headerlink" title="（1）shuffle机制"></a>（1）shuffle机制</h4><p><img src="/medias/shuffle%E6%9C%BA%E5%88%B6.PNG" alt="shuffle机制"></p><h3 id="11、mapjoin与reducejoin"><a href="#11、mapjoin与reducejoin" class="headerlink" title="11、mapjoin与reducejoin"></a>11、mapjoin与reducejoin</h3><h4 id="（1）yarn架构介绍"><a href="#（1）yarn架构介绍" class="headerlink" title="（1）yarn架构介绍"></a>（1）yarn架构介绍</h4><p><img src="/medias/yarn%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D.PNG" alt="yarn架构介绍"></p><h3 id="12、Zookeeper介绍"><a href="#12、Zookeeper介绍" class="headerlink" title="12、Zookeeper介绍"></a>12、Zookeeper介绍</h3><h4 id="（1）zookeeper功能"><a href="#（1）zookeeper功能" class="headerlink" title="（1）zookeeper功能"></a>（1）zookeeper功能</h4><p><img src="/medias/zookeeper%E5%8A%9F%E8%83%BD.PNG" alt="zookeeper功能"></p><h4 id="（2）选举机制"><a href="#（2）选举机制" class="headerlink" title="（2）选举机制"></a>（2）选举机制</h4><p><img src="/medias/%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6.PNG" alt="选举机制"></p><h3 id="13、Hive"><a href="#13、Hive" class="headerlink" title="13、Hive"></a>13、Hive</h3><h4 id="（1）Hive架构原理"><a href="#（1）Hive架构原理" class="headerlink" title="（1）Hive架构原理"></a>（1）Hive架构原理</h4><p><img src="/medias/Hive%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86.PNG" alt="Hive架构原理"></p><h3 id="14、Flume"><a href="#14、Flume" class="headerlink" title="14、Flume"></a>14、Flume</h3><h4 id="（1）多channel多sink流程"><a href="#（1）多channel多sink流程" class="headerlink" title="（1）多channel多sink流程"></a>（1）多channel多sink流程</h4><p><img src="/medias/%E5%A4%9Achannel%E5%A4%9Asink%E6%B5%81%E7%A8%8B.PNG" alt="多channel多sink流程"></p><h3 id="15、HBase"><a href="#15、HBase" class="headerlink" title="15、HBase"></a>15、HBase</h3><h4 id="（1）HBase架构图"><a href="#（1）HBase架构图" class="headerlink" title="（1）HBase架构图"></a>（1）HBase架构图</h4><p><img src="/medias/HBase%E6%9E%B6%E6%9E%84%E5%9B%BE.PNG" alt="HBase架构图"></p><h4 id="（2）HBase数据读取流程"><a href="#（2）HBase数据读取流程" class="headerlink" title="（2）HBase数据读取流程"></a>（2）HBase数据读取流程</h4><p><img src="/medias/HBase%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E6%B5%81%E7%A8%8B.PNG" alt="HBase数据读取流程"></p><h4 id="（3）HBase读取数据的详细流程"><a href="#（3）HBase读取数据的详细流程" class="headerlink" title="（3）HBase读取数据的详细流程"></a>（3）HBase读取数据的详细流程</h4><p><img src="/medias/HBase%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E7%9A%84%E8%AF%A6%E7%BB%86%E6%B5%81%E7%A8%8B.PNG" alt="HBase读取数据的详细流程"></p><h4 id="（4）HBase写入数据的详细流程"><a href="#（4）HBase写入数据的详细流程" class="headerlink" title="（4）HBase写入数据的详细流程"></a>（4）HBase写入数据的详细流程</h4><p><img src="/medias/HBase%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE%E7%9A%84%E8%AF%A6%E7%BB%86%E6%B5%81%E7%A8%8B.PNG" alt="HBase写入数据的详细流程"></p><h3 id="16、Scala"><a href="#16、Scala" class="headerlink" title="16、Scala"></a>16、Scala</h3><h4 id="（1）Scala高阶函数"><a href="#（1）Scala高阶函数" class="headerlink" title="（1）Scala高阶函数"></a>（1）Scala高阶函数</h4><p><img src="/medias/Scala%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0.PNG" alt="Scala高阶函数"></p><h4 id="（2）Actor01"><a href="#（2）Actor01" class="headerlink" title="（2）Actor01"></a>（2）Actor01</h4><p><img src="/medias/Actor01.PNG" alt="Actor01"></p><h4 id="（3）NewAkkaSystem"><a href="#（3）NewAkkaSystem" class="headerlink" title="（3）NewAkkaSystem"></a>（3）NewAkkaSystem</h4><p><img src="/medias/NewAkkaSystem.PNG" alt="NewAkkaSystem"></p><h4 id="（4）PingPongExample"><a href="#（4）PingPongExample" class="headerlink" title="（4）PingPongExample"></a>（4）PingPongExample</h4><p><img src="/medias/PingPongExample.PNG" alt="PingPongExample"></p><h3 id="17、Spark"><a href="#17、Spark" class="headerlink" title="17、Spark"></a>17、Spark</h3><h4 id="（1）Spark体系架构"><a href="#（1）Spark体系架构" class="headerlink" title="（1）Spark体系架构"></a>（1）Spark体系架构</h4><p><img src="/medias/Spark%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84.PNG" alt="Spark体系架构"></p><h4 id="（2）大数据HA"><a href="#（2）大数据HA" class="headerlink" title="（2）大数据HA"></a>（2）大数据HA</h4><p><img src="/medias/%E5%A4%A7%E6%95%B0%E6%8D%AEHA.PNG" alt="大数据HA"></p><h4 id="（3）蒙特卡洛求PI"><a href="#（3）蒙特卡洛求PI" class="headerlink" title="（3）蒙特卡洛求PI"></a>（3）蒙特卡洛求PI</h4><p><img src="/medias/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%B1%82PI.PNG" alt="蒙特卡洛求PI"></p><h4 id="（4）spark的调用任务过程"><a href="#（4）spark的调用任务过程" class="headerlink" title="（4）spark的调用任务过程"></a>（4）spark的调用任务过程</h4><p><img src="/medias/spark%E7%9A%84%E8%B0%83%E7%94%A8%E4%BB%BB%E5%8A%A1%E8%BF%87%E7%A8%8B.PNG" alt="spark的调用任务过程"></p><h4 id="（5）RDD"><a href="#（5）RDD" class="headerlink" title="（5）RDD"></a>（5）RDD</h4><p><img src="/medias/RDD.PNG" alt="RDD"></p><h4 id="（6）WordCount程序分析"><a href="#（6）WordCount程序分析" class="headerlink" title="（6）WordCount程序分析"></a>（6）WordCount程序分析</h4><p><img src="/medias/WordCount%E7%A8%8B%E5%BA%8F%E5%88%86%E6%9E%90.PNG" alt="WordCount程序分析"></p><h4 id="（7）aggregate聚合操作"><a href="#（7）aggregate聚合操作" class="headerlink" title="（7）aggregate聚合操作"></a>（7）aggregate聚合操作</h4><p><img src="/medias/aggregate%E8%81%9A%E5%90%88%E6%93%8D%E4%BD%9C.PNG" alt="aggregate聚合操作"></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> 图片 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Storm基础</title>
      <link href="/2019/05/04/storm-ji-chu/"/>
      <url>/2019/05/04/storm-ji-chu/</url>
      
        <content type="html"><![CDATA[<p><strong>流式计算专题</strong><br>批量计算、实时计算、离线计算、流式计算</p><p>共同点：<br>数据源  –&gt;   采集数据   –&gt;   task worker –&gt;   task worker  –&gt;  sink 输出</p><p><strong>批量计算和流式计算</strong><br>区别：<br>处理数据粒度不一样</p><p>批量计算每次处理一定大小的数据块。流式计算，每次处理一条记录</p><p>流式计算可以提供类似批量计算的功能，为什么我们还要批量计算系统？</p><p>1、流式系统的吞吐量不如批量系统</p><p>2、流式系统无法提供精准的计算</p><ol><li>任务类型不一样</li><li>流式计算会一直运行</li><li>数据源的区别<br>对于批量计算而言，数据是有限数据<br>而对于流式计算，是无限数据</li></ol><h3 id="一、Storm—-是最早流式计算框架"><a href="#一、Storm—-是最早流式计算框架" class="headerlink" title="一、Storm—-是最早流式计算框架"></a>一、Storm—-是最早流式计算框架</h3><h4 id="1、Storm概述"><a href="#1、Storm概述" class="headerlink" title="1、Storm概述"></a>1、Storm概述</h4><p><strong>1）什么是Storm</strong><br>网址：<a href="http://storm.apache.org/" target="_blank" rel="noopener">http://storm.apache.org/</a><br>Apache Storm是一个<strong>免费的开源分布式实时计算系统</strong>。Storm可以<strong>轻松可靠地处理无限数据流</strong>，<strong>实现Hadoop对批处理所做的实时处理</strong>。Storm非常<strong>简单</strong>，可以<strong>与任何编程语言一起使用</strong>，并且使用起来很有趣！</p><p>Storm为<strong>分布式实时计算</strong>提供了一组通用原语，可被用于“<strong>流处理</strong>”之中，<strong>实时处理消息并更新数据库</strong>。这是管理队列及工作者集群的另一种方式。 Storm也可被用于“<strong>连续计算</strong>”（continuous computation），对数据流做连续查询，在计算时就将结果以流的形式输出给用户。它还可被用于“<strong>分布式RPC</strong>”，以并行的方式运行昂贵的运算</p><p>Storm可以<strong>方便地在</strong>一个计算机<strong>集群中编写与扩展复杂</strong>的<strong>实时计算</strong>，Storm用于<strong>实时处理</strong>，就好比 Hadoop 用于批处理。Storm<strong>保证每个消息都会得到处理</strong>，而且它<strong>很快</strong>——<strong>在一个小集群中</strong>，<strong>每秒</strong>可以<strong>处理数以百万计的消息</strong>。更棒的是你<strong>可以使用任意编程语言来</strong>做<strong>开发</strong></p><p>Storm有许多用例：<strong>实时分析</strong>，<strong>在线机器学习</strong>，<strong>连续计算</strong>，<strong>分布式RPC</strong>，<strong>ETL</strong>等。风暴很快：一个基准测试表示每个节点<strong>每秒处理超过一百万个元组。</strong>它具有<strong>可扩展性</strong>，<strong>容错性</strong>，可<strong>确保</strong>您的<strong>数据得到处理</strong>，并且<strong>易于设置和操作</strong></p><p>Storm<strong>集成</strong>了您<strong>已经使用的排队和数据库技术</strong>。<strong>Storm拓扑消耗数据流</strong>并<strong>以任意复杂的方式处理这些流</strong>，然后<strong>在计算的每个阶段之间重新划分流</strong></p><h4 id="2、离线计算和流式计算"><a href="#2、离线计算和流式计算" class="headerlink" title="2、离线计算和流式计算"></a>2、离线计算和流式计算</h4><p>①　<strong>离线计算</strong></p><ul><li>离线计算：批量获取数据、批量传输数据、周期性批量计算数据、数据展示</li><li>代表技术：Sqoop批量导入数据、HDFS批量存储数据、MapReduce批量计算、Hive、Flume批量获取数据、Sqoop批量传输、HDFS/Hive/HBase批量存储、MR/Hive计算数据、BI</li></ul><p>②　<strong>流式计算</strong></p><ul><li>流式计算：数据实时产生、数据实时传输、数据实时计算、实时展示</li><li>代表技术：Flume实时获取数据、Kafka/metaq实时数据存储、Storm/JStorm实时数据计算、Redis实时结果缓存、持久化存储(mysql)、阿里实时展示(DataV/QuickBI)</li></ul><p>一句话总结：将源源不断产生的数据实时收集并实时计算，尽可能快的得到计算结果</p><p>③　<strong>Storm与Hadoop的区别</strong></p><table><thead><tr><th align="center">Storm用于实时计算</th><th align="center">Hadoop用于离线计算</th></tr></thead><tbody><tr><td align="center">Storm处理的数据保存在内存中，源源不断中，一批一批</td><td align="center">Hadoop处理的数据保存在文件系统</td></tr><tr><td align="center">Storm的数据通过网络传输进来</td><td align="center">Hadoop的数据保存在磁盘中</td></tr><tr><td align="center">Storm与Hadoop的编程模型相似</td><td align="center"></td></tr></tbody></table><p><strong>Storm与Hadoop</strong><br><strong>角色</strong></p><table><thead><tr><th align="center">hadoop</th><th align="center">storm</th></tr></thead><tbody><tr><td align="center">JobTracker</td><td align="center">Nimbus</td></tr><tr><td align="center">TaskTracker</td><td align="center">Supervisor</td></tr><tr><td align="center">Child</td><td align="center">Worker</td></tr></tbody></table><p><strong>应用名称</strong></p><table><thead><tr><th align="center">hadoop</th><th align="center">storm</th></tr></thead><tbody><tr><td align="center">Job</td><td align="center">Topology</td></tr></tbody></table><p><strong>编程接口</strong></p><table><thead><tr><th align="center">hadoop</th><th align="center">storm</th></tr></thead><tbody><tr><td align="center">Mapper/Reducer</td><td align="center">Spout/Bolt</td></tr></tbody></table><h4 id="3、Storm的体系结构"><a href="#3、Storm的体系结构" class="headerlink" title="3、Storm的体系结构"></a>3、Storm的体系结构</h4><p><img src="/medias/nimbus.PNG" alt="nimbus"></p><p><img src="/medias/topology.PNG" alt="topology"></p><ul><li><p><strong>Nimbus</strong>：负责资源分配和任务调度</p></li><li><p><strong>Supervisor</strong>：负责接受Nimbus分配的任务，启动和停止属于自己管理的worker进程。通过配置文件设置当前Supervisor上启动多少个Worker</p></li><li><p><strong>Worker</strong>：运行具体处理组件逻辑的进程。Worker运行的任务类型只有两种，一种是Spout任务，一种是Bolt任务</p></li><li><p><strong>Executor</strong>：Storm 0.8之后，Executor为Worker进程中的具体的物理线程，同一个Spout/Bolt的Task可能会共享一个物理线程，一个Executor中只能运行隶属于同一个Spout/Bolt的Task</p></li><li><p><strong>Task</strong>：Worker中每一个Spout/Bolt的线程称为一个Task. 在Storm0.8之后，Task不再与物理线程对应，不同Spout/Bolt的Task可能会共享一个物理线程，该线程称为Executor</p></li></ul><p><img src="/medias/Worker%E8%BF%9B%E7%A8%8B.PNG" alt="Worker进程"></p><h4 id="4、Storm编程模型"><a href="#4、Storm编程模型" class="headerlink" title="4、Storm编程模型"></a>4、Storm编程模型</h4><p><img src="/medias/Storm%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B.PNG" alt="Storm编程模型"></p><p><strong>tuple</strong>：元组<br>是消息传输的基本单元</p><p><strong>Spout</strong>：水龙头<br>Storm的核心抽象。拓扑的流的来源。Spout通常从外部数据源读取数据。转换为t敺内部的源数据。<br>主要方法：<br>nextTuple() -》 发出一个新的元组到拓扑<br>ack()<br>fail()</p><p><strong>Bolt</strong>：转接头<br>Bolt是对流的处理节点。Bolt作用：过滤、业务、连接运算</p><p><strong>Topology</strong>：拓扑<br>是一个实时的应用程序<br>永远运行除非被杀死<br>Spout到Bolt是一个连接流</p><h4 id="5、Storm的运行机制"><a href="#5、Storm的运行机制" class="headerlink" title="5、Storm的运行机制"></a>5、Storm的运行机制</h4><p><img src="/medias/Nimbus-Supervisor.PNG" alt="Nimbus-Supervisor"></p><ul><li>整个处理流程的组织协调不用用户去关心，用户只需要去定义每一个步骤中的具体业务处理逻辑</li><li>具体执行任务的角色是Worker，Worker执行任务时具体的行为则有我们定义的业务逻辑决定</li></ul><p><img src="/medias/Storm%E7%89%A9%E7%90%86%E9%9B%86%E7%BE%A4%E7%BB%93%E6%9E%84.PNG" alt="Storm物理集群结构"></p><h4 id="6、Storm的集群安装配置"><a href="#6、Storm的集群安装配置" class="headerlink" title="6、Storm的集群安装配置"></a>6、Storm的集群安装配置</h4><p><strong>（1）Storm集群安装部署</strong><br>1）准备工作</p><table><thead><tr><th align="center">hsiehchou121</th><th align="center">hsiehchou122</th><th align="center">hsiehchou123</th></tr></thead><tbody><tr><td align="center">storm01</td><td align="center">storm02</td><td align="center">storm03</td></tr></tbody></table><p>2）下载安装包<br><a href="http://storm.apache.org/downloads.html" target="_blank" rel="noopener">http://storm.apache.org/downloads.html</a></p><p>3）上传</p><p>4）解压<br>tar -zxvf apache-storm-1.1.0.tar.gz<br>mv apache-storm-1.1.0 storm</p><p>5）<strong>设置环境变量</strong><br><strong>#STORM_HOME</strong><br>export STORM_HOME=/root/hd/storm<br>export PATH=<code>$STORM_HOME/bin:$PATH</code><br>source /etc/profile</p><p>6）<strong>修改配置文件</strong><br>$ <strong>vi storm.yaml</strong></p><pre><code>#设置Zookeeper的主机名称storm.zookeeper.servers:- &quot;hsiehchou121&quot;- &quot;hsiehchou122&quot;- &quot;hsiehchou123&quot;#设置主节点的主机名称nimbus.seeds: [&quot;hsiehchou121&quot;]#设置Storm的数据存储路径storm.local.dir: &quot;/root/hd/storm/data&quot;#设置Worker的端口号supervisor.slots.ports:- 6700- 6701- 6702- 6703</code></pre><p>++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br><strong>注意</strong>：如果要搭建<strong>Storm的HA</strong>，只需要在<strong>nimbus.seeds</strong>中<strong>设置多个nimbus</strong>即可<br>++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br>mkdir data</p><p>7）<strong>分发到其他机器</strong><br>[root@hsiehchou121 hd]# scp -r storm/ hsiehchou122:<code>$PWD</code><br>[root@hsiehchou121 hd]# scp -r storm/ hsiehchou123:<code>$PWD</code></p><h4 id="7、启动和查看Storm"><a href="#7、启动和查看Storm" class="headerlink" title="7、启动和查看Storm"></a>7、启动和查看Storm</h4><p>0）<strong>启动ZooKeeper</strong><br>zkServer.sh start</p><p>1）在nimbus.host所属的机器上启动 nimbus服务和logviewer服务</p><p><strong>启动nimbus</strong></p><ul><li>storm nimbus &amp;</li></ul><p>*<em>启动logviewer *</em></p><ul><li>storm logviewer &amp;</li></ul><p>2）在nimbus.host所属的机器上启动ui服务<br><strong>启动ui界面</strong></p><ul><li>storm ui &amp;</li></ul><p>3）在其它个节点上启动supervisor服务和logviewer服务<br><strong>启动supervisor</strong></p><ul><li>storm supervisor &amp;</li></ul><p><strong>启动logviewer</strong></p><ul><li>storm logviewer &amp;</li></ul><p>4）查看Storm集群：访问nimbus.host:/8080，即可看到storm的ui界面<br><a href="http://hsiehchou121:8080/index.html" target="_blank" rel="noopener">http://hsiehchou121:8080/index.html</a></p><h4 id="8、Storm的常用命令"><a href="#8、Storm的常用命令" class="headerlink" title="8、Storm的常用命令"></a>8、Storm的常用命令</h4><p>有许多简单且有用的命令可以用来管理拓扑，它们可以提交、杀死、禁用、再平衡拓扑</p><p><strong>1）查看命令帮助</strong><br>storm help</p><p><strong>2）查看版本</strong><br>storm version</p><p><strong>3）查看当前正在运行拓扑及其状态</strong><br>storm list</p><p><strong>4）提交任务命令格式</strong><br>storm jar 【jar路径】 【拓扑包名.拓扑类名】 【拓扑名称】<br>storm jar <code>[/路径/.jar][全类名]</code>[拓扑名称]</p><p><strong>5）杀死任务命令格式</strong><br>storm kill 【拓扑名称】 -w 10<br>（执行kill命令时可以通过-w [等待秒数]指定拓扑停用以后的等待时间）<br>storm kill topology-name -w 10</p><p><strong>6）停用任务命令格式</strong><br>storm deactivte  【拓扑名称】<br>storm deactivate topology-name</p><p><strong>7）启用任务命令格式</strong><br>storm activate【拓扑名称】<br>storm activate topology-name</p><p><strong>8）重新部署任务命令格式</strong><br>storm rebalance  【拓扑名称】<br>storm rebalance topology-name<br>再平衡使你重分配集群任务。这是个很强大的命令。比如，你向一个运行中的集群增加了节点。再平衡命令将会停用拓扑，然后在相应超时时间之后重分配工人，并重启拓扑</p><h3 id="二、Storm编程案例"><a href="#二、Storm编程案例" class="headerlink" title="二、Storm编程案例"></a>二、Storm编程案例</h3><h4 id="1、WordCount及流程分析"><a href="#1、WordCount及流程分析" class="headerlink" title="1、WordCount及流程分析"></a>1、WordCount及流程分析</h4><p>通过查看Storm UI上每个组件的events链接，可以查看Storm的每个组件（spout、blot）发送的消息。但Storm的event logger的功能默认是禁用的，需要在配置文件中设置：topology.eventlogger.executors: 1，具体说明如下：</p><ul><li>“topology.eventlogger.executors”: 0     默认，禁用</li><li>“topology.eventlogger.executors”: 1     一个topology分配一个Event Logger</li><li>“topology.eventlogger.executors”: nil     每个worker.分配一个Event Logger</li></ul><p><strong>WordCount的数据流程分析</strong></p><p><img src="/medias/WordCount%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90.PNG" alt="WordCount的数据流程分析"></p><h4 id="2、Storm编程案例：WordCount"><a href="#2、Storm编程案例：WordCount" class="headerlink" title="2、Storm编程案例：WordCount"></a>2、Storm编程案例：WordCount</h4><p>流式计算一般架构图：</p><p><img src="/medias/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97%E4%B8%80%E8%88%AC%E6%9E%B6%E6%9E%84%E5%9B%BE.PNG" alt="流式计算一般架构图"></p><ul><li>Flume用来获取数据</li><li>Kafka用来临时保存数据</li><li>Strom用来计算数据</li><li>Redis是个内存数据库，用来保存数据</li></ul><p>代码编写：</p><ol><li>创建Spout（WordCountSpout）组件采集数据，作为整个Topology的数据源</li></ol><p><strong>WordCountSpout类</strong></p><pre><code>package com.hsiehchou.wc;import java.util.Map;import org.apache.storm.spout.SpoutOutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichSpout;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Values;/** * 需求：单词计数  Hello ni hao! Hello China! *  * 实现接口：IRichSpout  IRichBolt * 继承抽象类：BaseRichSpout  BaseRichBolt * @author hsiehchou */public class WordCountSpout extends BaseRichSpout {    //定义收集器    private SpoutOutputCollector collector;    //发送数据    @Override    public void nextTuple() {        //1.发送数据        collector.emit(new Values(&quot;I am a boy!&quot;));        //2.设置延迟        try {            Thread.sleep(1000);        } catch (InterruptedException e) {            e.printStackTrace();        }    }    //创建收集器    @Override    public void open(Map arg0, TopologyContext arg1, SpoutOutputCollector collector) {        this.collector = collector;    }    //声明    @Override    public void declareOutputFields(OutputFieldsDeclarer declare) {        //起别名        declare.declare(new Fields(&quot;hsiehchou&quot;));    }}</code></pre><ol start="2"><li>创建Bolt（WordCountSplitBolt）组件进行分词操作</li></ol><p><strong>WordCountSplitBolt类</strong></p><pre><code>package com.hsiehchou.wc;import java.util.Map;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Tuple;import org.apache.storm.tuple.Values;public class WordCountSplitBolt extends BaseRichBolt {    //数据继续发送到下一个bolt    private OutputCollector collector;    //业务逻辑    @Override    public void execute(Tuple in) {        //1.获取数据        String line = in.getStringByField(&quot;hsiehchou&quot;);        //2.切分数据        String[] fields = line.split(&quot; &quot;);        //3.&lt;单词,1&gt;发送出去，下一个bolt（累加求和）        for(String w:fields) {            collector.emit(new Values(w,1));        }    }    //初始化    @Override    public void prepare(Map arg0, TopologyContext arg1, OutputCollector collector) {        this.collector = collector;    }    //声明描述    @Override    public void declareOutputFields(OutputFieldsDeclarer declare) {        declare.declare(new Fields(&quot;word&quot;,&quot;sum&quot;));    }}</code></pre><ol start="3"><li>创建Bolt（WordCountBoltCount）组件进行单词计数作</li></ol><p><strong>WordCountBoltCount类</strong></p><pre><code>package com.hsiehchou.wc;import java.util.HashMap;import java.util.Map;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.tuple.Tuple;public class WordCountBoltCount extends BaseRichBolt {    private Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;();    //累加求和    @Override    public void execute(Tuple in) {        //1.获取数据        String word = in.getStringByField(&quot;word&quot;);        Integer sum = in.getIntegerByField(&quot;sum&quot;);        //2.业务处理        if(map.containsKey(word)) {            //之前出现的次数            Integer count = map.get(word);            //已有的            map.put(word, count + sum);        }else {            map.put(word, sum);        }        //3.打印控制台        System.err.println(Thread.currentThread().getId() + &quot;单位为：&quot; + word + &quot;\t 当前出现次数为：&quot; + map.get(word));        }    @Override    public void prepare(Map arg0, TopologyContext arg1, OutputCollector arg2) {    }    @Override    public void declareOutputFields(OutputFieldsDeclarer arg0) {    }}</code></pre><ol start="4"><li>也可以将主程序Topology（WordCountTopology）提交到Storm集群运行</li></ol><p><strong>WordCountTopology类</strong></p><pre><code>package com.hsiehchou.wc;import org.apache.storm.Config;import org.apache.storm.LocalCluster;import org.apache.storm.topology.TopologyBuilder;import org.apache.storm.tuple.Fields;public class WordCountTopology {    public static void main(String[] args) {        //1.hadoop --&gt; Job Storm --&gt; Topology 创建拓扑        TopologyBuilder builder = new TopologyBuilder();        //2.指定设置        builder.setSpout(&quot;WordCountSpout&quot;, new WordCountSpout(), 1);        builder.setBolt(&quot;WordCountSplitBolt&quot;, new WordCountSplitBolt(), 4).fieldsGrouping(&quot;WordCountSpout&quot;, new Fields(&quot;hsiehchou&quot;));        builder.setBolt(&quot;WordCountBoltCount&quot;, new WordCountBolt(), 2).fieldsGrouping(&quot;WordCountSplitBolt&quot;, new Fields(&quot;word&quot;));        //3.创建配置信息        Config conf = new Config();        //4.提交任务        LocalCluster localCluster = new LocalCluster();        localCluster.submitTopology(&quot;wordcounttopology&quot;, conf, builder.createTopology());    }}</code></pre><h4 id="3、集群部署"><a href="#3、集群部署" class="headerlink" title="3、集群部署"></a>3、集群部署</h4><p><strong>对WordCountDriver类进行修改，把本地模式修改为集群模式</strong></p><pre><code>public class WordCountDriver {        //集群模式运行        try {            StormSubmitter.submitTopology(args[0], conf, builder.createTopology());        } catch (AlreadyAliveException | InvalidTopologyException | AuthorizationException e) {            e.printStackTrace();        }        //4.提交任务        //LocalCluster localCluster = new LocalCluster();        //localCluster.submitTopology(&quot;wordcounttopology&quot;, conf, builder.createTopology());}</code></pre><p><strong>提交到集群</strong></p><p>storm jar StormWordCount.jar com.hsiehchou.wc.WordCountDriver wordcount01</p><h3 id="三、分组策略"><a href="#三、分组策略" class="headerlink" title="三、分组策略"></a>三、分组策略</h3><p>1）fields Grouping</p><p><strong>按照字段分组</strong><br>相同字段发送到一个task中<br>fieldsGrouping<br>builder.setBolt(“WordCountSplitBolt”, new WordCountSplitBolt(), 4).fieldsGrouping(“WordCountSpout”, new Fields(“hsiehchou”));</p><p>builder.setBolt(“WordCountBolt”, new WordCountBolt(), 2).fieldsGrouping(“WordCountSplitBolt”, new Fields(“word”));</p><p>2）shuffle Grouping</p><p><strong>随机分组</strong><br>轮询。平均分配。随机分发tuple，保证每个bolt中的tuple数量相同<br>shuffleGrouping<br>builder.setBolt(“WordCountSplitBolt”, new WordCountSplitBolt(), 4).shuffleGrouping(“WordCountSpout”);</p><p>builder.setBolt(“WordCountBolt”, new WordCountBolt(), 2).shuffleGrouping(“WordCountSplitBolt”);</p><p>3）None Grouping</p><p><strong>不分组</strong><br>采用这种策略每个bolt中接收的单词不同<br>noneGrouping<br>builder.setBolt(“WordCountSplitBolt”, new WordCountSplitBolt(), 4).noneGrouping(“WordCountSpout”);</p><p>builder.setBolt(“WordCountBolt”, new WordCountBolt(), 2).noneGrouping(“WordCountSplitBolt”);</p><p>4）All Grouping</p><p><strong>广播发送</strong><br>tuple分发给每一个bolt<br>allGrouping<br>builder.setBolt(“WordCountSplitBolt”, new WordCountSplitBolt(), 4).allGrouping(“WordCountSpout”);</p><p>builder.setBolt(“WordCountBolt”, new WordCountBolt(), 2).allGrouping(“WordCountSplitBolt”);</p><p>5）Global Grouping</p><p><strong>全局分组</strong><br>分配给task id值最小的<br>根据线程id判断，只分配给线程id最小的<br>builder.setBolt(“WordCountSplitBolt”, new WordCountSplitBolt(), 4).globalGrouping(“WordCountSpout”);</p><p>builder.setBolt(“WordCountBolt”, new WordCountBolt(), 2).globalGrouping(“WordCountSplitBolt”);</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Storm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop的HA高可用</title>
      <link href="/2019/04/29/hadoop-de-ha-gao-ke-yong-ke-xing/"/>
      <url>/2019/04/29/hadoop-de-ha-gao-ke-yong-ke-xing/</url>
      
        <content type="html"><![CDATA[<p><strong>Hadoop的HA高可用（可行）</strong></p><h3 id="一、集群的规划"><a href="#一、集群的规划" class="headerlink" title="一、集群的规划"></a>一、集群的规划</h3><p><strong>ZooKeeper集群</strong></p><table><thead><tr><th align="center">192.168.116.121</th><th align="center">192.168.116.122</th><th align="center">192.168.116.123</th></tr></thead><tbody><tr><td align="center">hsiehchou121</td><td align="center">hsiehchou122</td><td align="center">hsiehchou123</td></tr></tbody></table><p><strong>Hadoop集群</strong></p><table><thead><tr><th align="center">192.168.116.121</th><th align="center">192.168.116.122</th><th align="center">192.168.116.123</th><th align="center">192.168.116.124</th></tr></thead><tbody><tr><td align="center">hsiehchou121</td><td align="center">hsiehchou122</td><td align="center">hsiehchou123</td><td align="center">hsiehchou124</td></tr><tr><td align="center">NameNode1</td><td align="center">NameNode2</td><td align="center">DataNode1</td><td align="center">DataNode2</td></tr><tr><td align="center">ResourceManager1</td><td align="center">ResourceManager2</td><td align="center">NodeManager1</td><td align="center">NodeManager2</td></tr><tr><td align="center">Journalnode</td><td align="center">Journalnode</td><td align="center"></td><td align="center"></td></tr></tbody></table><h3 id="二、准备工作"><a href="#二、准备工作" class="headerlink" title="二、准备工作"></a>二、准备工作</h3><p>1、安装JDK<br>2、配置环境变量<br>3、配置免密码登录<br>4、配置主机名</p><h3 id="三、配置Zookeeper（在192-168-116-121安装）"><a href="#三、配置Zookeeper（在192-168-116-121安装）" class="headerlink" title="三、配置Zookeeper（在192.168.116.121安装）"></a>三、配置Zookeeper（在192.168.116.121安装）</h3><p>在主节点（hsiehchou121）上配置ZooKeeper</p><h4 id="1、配置-root-hd-zookeeper-3-4-10-conf-zoo-cfg文件"><a href="#1、配置-root-hd-zookeeper-3-4-10-conf-zoo-cfg文件" class="headerlink" title="1、配置/root/hd/zookeeper-3.4.10/conf/zoo.cfg文件"></a>1、配置/root/hd/zookeeper-3.4.10/conf/zoo.cfg文件</h4><pre><code>dataDir=/root/hd/zookeeper-3.4.10/zkData+++++++++++++++zkconfig+++++++++++++++++server.1=hsiehchou121:2888:3888server.2=hsiehchou122:2888:3888server.3=hsiehchou123:2888:3888</code></pre><h4 id="2、在-root-training-zookeeper-3-4-6-tmp目录下创建一个myid的空文件"><a href="#2、在-root-training-zookeeper-3-4-6-tmp目录下创建一个myid的空文件" class="headerlink" title="2、在/root/training/zookeeper-3.4.6/tmp目录下创建一个myid的空文件"></a>2、在/root/training/zookeeper-3.4.6/tmp目录下创建一个myid的空文件</h4><p>echo 1 &gt; /root/hd/zookeeper-3.4.10/tmp/myid</p><h4 id="3、将配置好的ZooKeeper拷贝到其他节点，同时修改各自的myid文件"><a href="#3、将配置好的ZooKeeper拷贝到其他节点，同时修改各自的myid文件" class="headerlink" title="3、将配置好的ZooKeeper拷贝到其他节点，同时修改各自的myid文件"></a>3、将配置好的ZooKeeper拷贝到其他节点，同时修改各自的myid文件</h4><p>scp -r /root/hd/zookeeper-3.4.10/ hsiehchou122:/root/hd<br>scp -r /root/hd/zookeeper-3.4.10/ hsiehchou123:/root/hd</p><h3 id="四、安装Hadoop集群（在hsiehchou121上安装）"><a href="#四、安装Hadoop集群（在hsiehchou121上安装）" class="headerlink" title="四、安装Hadoop集群（在hsiehchou121上安装）"></a>四、安装Hadoop集群（在hsiehchou121上安装）</h3><h4 id="1、修改hadoop-env-sh"><a href="#1、修改hadoop-env-sh" class="headerlink" title="1、修改hadoop-env.sh"></a>1、修改hadoop-env.sh</h4><pre><code>export JAVA_HOME=/root/hd/jdk1.8.0_192</code></pre><h4 id="2、修改core-site-xml"><a href="#2、修改core-site-xml" class="headerlink" title="2、修改core-site.xml"></a>2、修改core-site.xml</h4><pre><code>&lt;configuration&gt;    &lt;!-- 指定hdfs的nameservice为mycluster --&gt;    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://mycluster&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定hadoop临时目录 --&gt;    &lt;property&gt;        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;        &lt;value&gt;/root/hd/hadoop-2.8.4/tmp&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定zookeeper地址 --&gt;    &lt;property&gt;        &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;        &lt;value&gt;hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="3、修改hdfs-site-xml（配置这个nameservice中有几个NameNode）"><a href="#3、修改hdfs-site-xml（配置这个nameservice中有几个NameNode）" class="headerlink" title="3、修改hdfs-site.xml（配置这个nameservice中有几个NameNode）"></a>3、修改hdfs-site.xml（配置这个nameservice中有几个NameNode）</h4><pre><code>&lt;configuration&gt;     &lt;!--指定hdfs的nameservice为mycluster，需要和core-site.xml中的保持一致 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.nameservices&lt;/name&gt;        &lt;value&gt;mycluster&lt;/value&gt;    &lt;/property&gt;    &lt;!-- mycluster下面有两个NameNode，分别是nn1，nn2 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;        &lt;value&gt;nn1,nn2&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn1的RPC通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;        &lt;value&gt;hsiehchou121:9000&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn1的http通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;        &lt;value&gt;hsiehchou121:50070&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn2的RPC通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;        &lt;value&gt;hsiehchou122:9000&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn2的http通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;        &lt;value&gt;hsiehchou122:50070&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定NameNode的日志在JournalNode上的存放位置 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;        &lt;value&gt;qjournal://hsiehchou121:8485;hsiehchou122:8485;/mycluster&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;        &lt;value&gt;/root/hd/hadoop-2.8.4/journal&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 开启NameNode失败自动切换 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 配置失败自动切换实现方式 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;        &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;        &lt;value&gt;            sshfence            shell(/bin/true)        &lt;/value&gt;    &lt;/property&gt;    &lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;        &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 配置sshfence隔离机制超时时间 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;        &lt;value&gt;30000&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="4、修改mapred-site-xml"><a href="#4、修改mapred-site-xml" class="headerlink" title="4、修改mapred-site.xml"></a>4、修改mapred-site.xml</h4><pre><code>&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;        &lt;value&gt;yarn&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="5、修改yarn-site-xml"><a href="#5、修改yarn-site-xml" class="headerlink" title="5、修改yarn-site.xml"></a>5、修改yarn-site.xml</h4><pre><code>&lt;configuration&gt;    &lt;!-- 开启RM高可靠 --&gt;    &lt;property&gt;       &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;       &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定RM的cluster id --&gt;    &lt;property&gt;       &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;       &lt;value&gt;yarncluster&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定RM的名字 --&gt;    &lt;property&gt;       &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;       &lt;value&gt;rm1,rm2&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 分别指定RM的地址 --&gt;    &lt;property&gt;       &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;       &lt;value&gt;hsiehchou121&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;       &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;       &lt;value&gt;hsiehchou122&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定zk集群地址 --&gt;    &lt;property&gt;       &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;       &lt;value&gt;hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;       &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;       &lt;value&gt;mapreduce_shuffle&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;         &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;         &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;          &lt;value&gt;32768&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;        &lt;value&gt;32768&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;        &lt;value&gt;4096&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;        &lt;value&gt;24&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;        &lt;value&gt;/tmp/yarn-logs&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="6、修改slaves"><a href="#6、修改slaves" class="headerlink" title="6、修改slaves"></a>6、修改slaves</h4><p>hsiehchou123<br>hsiehchou124</p><h4 id="7、将配置好的hadoop拷贝到其他节点"><a href="#7、将配置好的hadoop拷贝到其他节点" class="headerlink" title="7、将配置好的hadoop拷贝到其他节点"></a>7、将配置好的hadoop拷贝到其他节点</h4><p>scp -r /root/hd/hadoop-2.8.4/ root@hsiehchou122:/root/hd/<br>scp -r /root/hd/hadoop-2.8.4/ root@hsiehchou123:/root/hd/<br>scp -r /root/hd/hadoop-2.8.4/ root@hsiehchou124:/root/hd/</p><h3 id="五、启动Zookeeper集群"><a href="#五、启动Zookeeper集群" class="headerlink" title="五、启动Zookeeper集群"></a>五、启动Zookeeper集群</h3><h4 id="1、启动Zookeeper集群"><a href="#1、启动Zookeeper集群" class="headerlink" title="1、启动Zookeeper集群"></a>1、启动Zookeeper集群</h4><p>[root@hsiehchou121 hadoop-2.8.4]# <strong>zkServer.sh start</strong><br>[root@hsiehchou122 hadoop-2.8.4]# <strong>zkServer.sh start</strong><br>[root@hsiehchou123 hadoop-2.8.4]# <strong>zkServer.sh start</strong></p><h3 id="六、在hsiehchou121和hsiehchou122上启动journalnode"><a href="#六、在hsiehchou121和hsiehchou122上启动journalnode" class="headerlink" title="六、在hsiehchou121和hsiehchou122上启动journalnode"></a>六、在hsiehchou121和hsiehchou122上启动journalnode</h3><p><strong>hadoop-daemon.sh start journalnode</strong></p><h3 id="七、格式化HDFS（在hsiehchou121上执行）"><a href="#七、格式化HDFS（在hsiehchou121上执行）" class="headerlink" title="七、格式化HDFS（在hsiehchou121上执行）"></a>七、格式化HDFS（在hsiehchou121上执行）</h3><h4 id="1-格式化ZooKeeper"><a href="#1-格式化ZooKeeper" class="headerlink" title="1. 格式化ZooKeeper"></a>1. 格式化ZooKeeper</h4><p>[root@hsiehchou121 hadoop-2.8.4]# <strong>bin/hdfs zkfc -formatZK</strong></p><h4 id="2、启动HDFS"><a href="#2、启动HDFS" class="headerlink" title="2、启动HDFS"></a>2、启动HDFS</h4><p>1）在各个JournalNode节点上，输入以下命令启动journalnode服务<br>[root@hsiehchou121 hadoop-2.8.4]# <strong>sbin/hadoop-daemon.sh start journalnode</strong></p><p>2）在[nn1]上，对其进行格式化，并启动<br>[root@hsiehchou121 hadoop-2.8.4]# <strong>bin/hdfs namenode -format</strong><br>[root@hsiehchou121 hadoop-2.8.4]# <strong>sbin/hadoop-daemon.sh start namenode</strong></p><p>3）在[nn2]上，同步nn1的元数据信息<br>[root@hsiehchou122 hadoop-2.8.4]# <strong>bin/hdfs namenode -bootstrapStandby</strong></p><h3 id="八、在hsiehchou121上启动Hadoop集群"><a href="#八、在hsiehchou121上启动Hadoop集群" class="headerlink" title="八、在hsiehchou121上启动Hadoop集群"></a>八、在hsiehchou121上启动Hadoop集群</h3><p>[root@hsiehchou121 hadoop-2.8.4]#  <strong>start-all.sh</strong></p><p><strong>日志</strong><br>This script is Deprecated. Instead use start-dfs.sh and start-yar<br>Starting namenodes on [hsiehchou121 hsiehchou122]<br>hsiehchou121: starting namenode, logging to /root/hd/hadoop-2.8.4-hsiehchou121.out<br>hsiehchou122: starting namenode, logging to /root/hd/hadoop-2.8.4-hsiehchou122.out<br>hsiehchou124: starting datanode, logging to /root/hd/hadoop-2.8.4-hsiehchou124.out<br>hsiehchou123: starting datanode, logging to /root/hd/hadoop-2.8.4-hsiehchou123.out<br>Starting journal nodes [hsiehchou121 hsiehchou122 ]<br>hsiehchou121: starting journalnode, logging to /root/hd/hadoop-2.alnode-hsiehchou121.out<br>hsiehchou122: starting journalnode, logging to /root/hd/hadoop-2.alnode-hsiehchou122.out<br>Starting ZK Failover Controllers on NN hosts [hsiehchou121 hsiehc<br>hsiehchou121: starting zkfc, logging to /root/hd/hadoop-2.8.4/logou121.out<br>hsiehchou122: starting zkfc, logging to /root/hd/hadoop-2.8.4/logou122.out<br>starting yarn daemons<br>starting resourcemanager, logging to /root/hd/hadoop-2.8.4/logs/ysiehchou121.out<br>hsiehchou123: starting nodemanager, logging to /root/hd/hadoop-2.ager-hsiehchou123.out<br>hsiehchou124: starting nodemanager, logging to /root/hd/hadoop-2.ager-hsiehchou124.out</p><p>hsiehchou122上的ResourceManager需要单独启动<br><strong>命令</strong><br>[root@hsiehchou121 hadoop-2.8.4]# <strong>./sbin/yarn-daemon.sh start resourcemanager</strong></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hadoop HA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka</title>
      <link href="/2019/04/26/kafka/"/>
      <url>/2019/04/26/kafka/</url>
      
        <content type="html"><![CDATA[<p><strong>离线部分</strong><br>Hadoop-&gt;离线计算(hdfs / mapreduce) yarn<br>zookeeper-&gt;分布式协调（动物管理员）<br>hive-&gt;数据仓库（离线计算 / sql）easy coding<br>flume-&gt;数据采集<br>sqoop-&gt;数据迁移mysql-&gt;hdfs/hive hdfs/hive-&gt;mysql<br>Azkaban-&gt;任务调度工具<br>hbase-&gt;数据库（nosql）列式存储 读写速度</p><p><strong>实时</strong><br>kafka<br>storm</p><h3 id="一、Kafka是什么"><a href="#一、Kafka是什么" class="headerlink" title="一、Kafka是什么"></a>一、Kafka是什么</h3><p>kafka一般用来缓存数据</p><h4 id="1、开源消息系统"><a href="#1、开源消息系统" class="headerlink" title="1、开源消息系统"></a>1、开源消息系统</h4><h4 id="2、最初是LinkedIn公司开发，2011年开源"><a href="#2、最初是LinkedIn公司开发，2011年开源" class="headerlink" title="2、最初是LinkedIn公司开发，2011年开源"></a>2、最初是LinkedIn公司开发，2011年开源</h4><p>2012年10月从Apache Incubator毕业</p><p>项目目标是为处理实时数据，提供一个统一、高通量、低等待的平台</p><h4 id="3、Kafka是一个分布式消息队列"><a href="#3、Kafka是一个分布式消息队列" class="headerlink" title="3、Kafka是一个分布式消息队列"></a>3、Kafka是一个分布式消息队列</h4><p>消息根据Topic来归类，发送消息 Producer，接收 Consumer</p><p>kafka集群有多个kafka实例组成，每个实例成为broker</p><h4 id="4、依赖于-Zookeeper-集群"><a href="#4、依赖于-Zookeeper-集群" class="headerlink" title="4、依赖于 Zookeeper 集群"></a>4、依赖于 Zookeeper 集群</h4><p>无论是kafka集群，还是 Producer、Consumer 都依赖于 Zookeeper 集群保存元信息，来保证系统可用性</p><p><img src="/medias/kafka%E4%BB%8B%E7%BB%8D.PNG" alt="kafka介绍"></p><p><strong>官网</strong><br><a href="http://kafka.apache.org/" target="_blank" rel="noopener">http://kafka.apache.org/</a><br>ApacheKafka?是一个分布式流媒体平台<br>流媒体平台有三个关键功能：</p><ol><li>发布和订阅记录流，类似于消息队列或企业消息传递系统</li><li>以容错的持久方式存储记录流</li><li>记录发生时处理流</li></ol><p>Kafka通常用于两大类应用：<br>构建可在系统或应用程序之间可靠获取数据的实时流数据管道<br>构建转换或响应数据流的实时流应用程序</p><p>kafka在流计算中，kafka主要功能是用来缓存数据，storm可以通过消费kafka中的数据进行流计算，是一套开源的消息系统，由scala写成，支持javaAPI。</p><p>kafka最初由LinkedIn公司开发，2011年开源，2012年从Apache毕业，是一个分布式消息队列，kafka读消息保存采用Topic进行归类</p><h3 id="二、消息队列"><a href="#二、消息队列" class="headerlink" title="二、消息队列"></a>二、消息队列</h3><p>点对点<br>发布、订阅模式</p><p><strong>角色</strong><br>发送消息：Producer(生产者)<br>接收消息：Consumer(消费者)</p><h3 id="三、为什么需要消息队列"><a href="#三、为什么需要消息队列" class="headerlink" title="三、为什么需要消息队列"></a>三、为什么需要消息队列</h3><h4 id="1、解耦"><a href="#1、解耦" class="headerlink" title="1、解耦"></a>1、解耦</h4><p>为了避免出现问题</p><h4 id="2、冗余"><a href="#2、冗余" class="headerlink" title="2、冗余"></a>2、冗余</h4><p>消息队列把数据进行持久化，直到他们已经被完全处理</p><h4 id="3、扩展性（拓展性）"><a href="#3、扩展性（拓展性）" class="headerlink" title="3、扩展性（拓展性）"></a>3、扩展性（拓展性）</h4><p>可增加处理过程</p><h4 id="4、灵活性"><a href="#4、灵活性" class="headerlink" title="4、灵活性"></a>4、灵活性</h4><p>面对访问量剧增，不会因为超负荷请求而完全瘫痪</p><h4 id="5、可恢复性"><a href="#5、可恢复性" class="headerlink" title="5、可恢复性"></a>5、可恢复性</h4><p>一部分组件失效，不会影响整个系统。可以进行恢复</p><h4 id="6、顺序保证（相对）"><a href="#6、顺序保证（相对）" class="headerlink" title="6、顺序保证（相对）"></a>6、顺序保证（相对）</h4><p>kafka保证一个Partition内部的消息有序，对消息进行有序处理</p><h4 id="7、缓冲"><a href="#7、缓冲" class="headerlink" title="7、缓冲"></a>7、缓冲</h4><p>控制数据流经过系统的速度</p><h4 id="8、异步通信"><a href="#8、异步通信" class="headerlink" title="8、异步通信"></a>8、异步通信</h4><p>akka，消息队列提供了异步处理的机制</p><p>很多时候，用户不想也不需要立即处理消息，消息队列提供异步处理机制，允许用户把消息放入队列，但不立即处理</p><h3 id="四、Kafka架构"><a href="#四、Kafka架构" class="headerlink" title="四、Kafka架构"></a>四、Kafka架构</h3><p><img src="/medias/kafka%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84.PNG" alt="kafka系统架构"></p><h4 id="1、Producer：消息生产者"><a href="#1、Producer：消息生产者" class="headerlink" title="1、Producer：消息生产者"></a>1、Producer：消息生产者</h4><p>就是往kafka中发消息的客户端</p><h4 id="2、Consumer：消息消费者"><a href="#2、Consumer：消息消费者" class="headerlink" title="2、Consumer：消息消费者"></a>2、Consumer：消息消费者</h4><p>向kafka broker中取消息的客户端</p><h4 id="3、Topic-理解为队列"><a href="#3、Topic-理解为队列" class="headerlink" title="3、Topic 理解为队列"></a>3、Topic 理解为队列</h4><h4 id="4、Consumer-Group-消费者组"><a href="#4、Consumer-Group-消费者组" class="headerlink" title="4、Consumer Group 消费者组"></a>4、Consumer Group 消费者组</h4><p>组内有多个消费者实例，共享一个公共的ID，即groupID<br>组内所有消费者协调在一起，消费Topic<br>每个分区，只能有同一个消费组内的一个Consumer消费</p><h4 id="5、broker"><a href="#5、broker" class="headerlink" title="5、broker"></a>5、broker</h4><p>一台kafka服务器就是一个broker</p><h4 id="6、partition：一个topic分为多个partition"><a href="#6、partition：一个topic分为多个partition" class="headerlink" title="6、partition：一个topic分为多个partition"></a>6、partition：一个topic分为多个partition</h4><p>每个partition是一个有序队列<br>kafka保证按一个partition中的顺序将消息发送个consumer<br>不能保证topic整体有序</p><h4 id="7、offset：Kafka存储文件按照offset-kafka命名"><a href="#7、offset：Kafka存储文件按照offset-kafka命名" class="headerlink" title="7、offset：Kafka存储文件按照offset.kafka命名"></a>7、offset：Kafka存储文件按照offset.kafka命名</h4><p><img src="/medias/kafka%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B%E5%9B%BE.PNG" alt="kafka读写流程图"></p><h3 id="五、Kafka部署"><a href="#五、Kafka部署" class="headerlink" title="五、Kafka部署"></a>五、Kafka部署</h3><p>前提：Zookeeper</p><p>官网下载安装包<br><a href="http://kafka.apache.org/downloads" target="_blank" rel="noopener">http://kafka.apache.org/downloads</a></p><p>上传tar</p><p>解压<br>[root@hsiehchou121 hd]# tar -zxvf kafka_2.11-2.1.1.tgz </p><p>[root@hsiehchou121 hd]# mv kafka_2.11-2.1.1 kafka</p><p>在kafka目录中，创建一个logs文件夹<br>[root@hsiehchou121 kafka]# mkdir logs</p><p>如果不创建，默认放在 /tmp 目录下</p><p><strong>修改文件</strong><br><strong>config/server.properties</strong><br>21 broker.id=0<br>broker 的 全局唯一编号，不能重复</p><p>新增<br>22 delete.topic.enable=true<br>允许删除topic</p><p><code>#</code>The number of threads that the server uses for receiving requests from the network and sending responses to the network<br>42 num.network.threads=3<br>处理网络请求的线程数量</p><p><code>#</code>The number of threads that the server uses for processing requests, which may include disk I/O<br>46 num.io.threads=8<br>用来处理磁盘IO的线程数量</p><p><code>#</code>A comma separated list of directories under which to store log files<br>62 log.dirs=/root/hd/kafka/logs<br>kafka运行日志存放的路径</p><p><code>#</code> The default number of log partitions per topic. More partitions allow greater<br><code>#</code>parallelism for consumption, but this will also result in more files across<br><code>#</code> the brokers.<br>67 num.partitions=1<br>当前主题在broker上的分区个数</p><p><code>#</code>· The number of threads per data directory to be used for log recovery at start        up and flushing at shutdown.<br><code>#</code> This value is recommended to be increased for installations with data dirs lo        cated in RAID array.<br> 71 num.recovery.threads.per.data.dir=1<br>恢复和清理data下的线程数量</p><p>125 zookeeper.connect=hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181<br>zookeeper相关信息</p><p><code>#</code> Timeout in ms for connecting to zookeeper<br>128 zookeeper.connection.timeout.ms=6000<br>zookeeper连接超时的时间</p><p>添加全局环境变量，以便于在任何地方都可以启动<br>[root@hsiehchou121 config]# vi /etc/profile<br><code>#</code>kafka_home<br>export KAFKA_HOME=/root/hd/kafka<br>export PATH=<code>$KAFKA_HOME/bin:$PATH</code></p><p>[root@hsiehchou121 config]# source /etc/profile</p><p>分发安装包<br>注意：要修改配置文件中 borker.id的值<br>broker id 不得重复<br>21 broker.id=0 —- hsiehchou121<br>21 broker.id=1 —- hsiehchou122<br>21 broker.id=2 —- hsiehchou123</p><p><strong>启动kafka集群</strong><br>首先需要先启动zookeeper<br>zkServer.sh start<br>[root@hsiehchou121 kafka]# zkServer.sh start<br>[root@hsiehchou122 kafka]# zkServer.sh start<br>[root@hsiehchou123 kafka]# zkServer.sh start</p><p>（<strong>必须先启动！！！！！！！！！</strong>）<br>./bin/kafka-server-start.sh config/server.properties &amp;<br><strong>&amp;====后台运行</strong><br>[root@hsiehchou121 kafka]# ./bin/kafka-server-start.sh config/server.properties &amp;<br>[root@hsiehchou122 kafka]# ./bin/kafka-server-start.sh config/server.properties &amp;<br>[root@hsiehchou123 kafka]# ./bin/kafka-server-start.sh config/server.properties &amp;</p><p>jps命令可以看Kafka进程<br>[root@hsiehchou121 kafka]# jps<br>7104 Jps<br>6742 Kafka<br>6715 QuorumPeerMain</p><p>关闭命令：<br>./bin/kafka-server-stop.sh stop<br>或者./bin/kafka-server-stop.sh </p><h3 id="六、Kafka命令行操作"><a href="#六、Kafka命令行操作" class="headerlink" title="六、Kafka命令行操作"></a>六、Kafka命令行操作</h3><h4 id="1、查看当前服务器中所有的topic"><a href="#1、查看当前服务器中所有的topic" class="headerlink" title="1、查看当前服务器中所有的topic"></a>1、查看当前服务器中所有的topic</h4><p>[root@hsiehchou121 kafka]# ./bin/kafka-topics.sh <code>--zookeeper</code> hsiehchou121:2181 <code>--list</code></p><h4 id="2、创建topic"><a href="#2、创建topic" class="headerlink" title="2、创建topic"></a>2、创建topic</h4><p>[root@hsiehchou121 kafka]# ./bin/kafka-topics.sh <code>--zookeeper</code> hsiehchou121:2181 <code>--create</code> <code>--replication-factor</code> 3 <code>--partitions</code> 1 <code>--topic</code> second</p><p>[root@hsiehchou121 kafka]# ./bin/kafka-topics.sh <code>--zookeeper</code> hsiehchou121:2181 <code>--create</code> <code>--replication-factor</code> 3 <code>--partitions</code> 1 <code>--topic</code> xz</p><p><code>--zookeeper</code> ：连接zk集群<br><code>--create</code> ：创建<br><code>--replication-factor</code>： 副本<br><code>--partitions</code> ：分区<br><code>--topic</code> ：主题名</p><h4 id="3、删除topic"><a href="#3、删除topic" class="headerlink" title="3、删除topic"></a>3、删除topic</h4><p>[root@hsiehchou121 kafka]# ./bin/kafka-topics.sh <code>--zookeeper</code> hsiehchou121:2181 <code>--delete</code> <code>--topic</code> xz</p><h4 id="4、发送消息"><a href="#4、发送消息" class="headerlink" title="4、发送消息"></a>4、发送消息</h4><p>[root@hsiehchou121 kafka]# ./bin/kafka-console-producer.sh <code>--broker-list</code> hsiehchou121:9092 <code>--topic</code> second<br><code>&gt;</code> 输入消息</p><h4 id="5、消费消息"><a href="#5、消费消息" class="headerlink" title="5、消费消息"></a>5、消费消息</h4><p>[root@hsiehchou122 kafka]# ./bin/kafka-console-consumer.sh <code>--bootstrap-server</code> hsiehchou121:9092 <code>--from-beginning</code> <code>--topic</code> second<br>接收消息</p><p><strong>注意：</strong>这里的<code>--from-beginning</code>  是从头开始消费，不加则是消费当前正在发送到该topic的消息</p><h4 id="6、查看主题描述信息"><a href="#6、查看主题描述信息" class="headerlink" title="6、查看主题描述信息"></a>6、查看主题描述信息</h4><p>[root@hsiehchou121 kafka]# ./bin/kafka-topics.sh –zookeeper hsiehchou121:2181 –describe –topic second<br>Topic:second    PartitionCount:1    ReplicationFactor:3    Configs:<br>    Topic: second    Partition: 0    Leader: 2    Replicas: 2,1,0    Isr: 2,1,0</p><h4 id="7、消费者组消费"><a href="#7、消费者组消费" class="headerlink" title="7、消费者组消费"></a>7、消费者组消费</h4><p>修改 consumer.properties 配置文件<br>consumer group id<br>group.id=xz</p><h4 id="8、启动生产者"><a href="#8、启动生产者" class="headerlink" title="8、启动生产者"></a>8、启动生产者</h4><p>[root@hsiehchou121 kafka]# ./bin/kafka-console-producer.sh <code>--broker-list</code> hsiehchou121:9092 <code>--topic</code> second</p><h4 id="9、启动消费者"><a href="#9、启动消费者" class="headerlink" title="9、启动消费者"></a>9、启动消费者</h4><p>[root@hsiehchou122 kafka]# ./bin/kafka-console-consumer.sh <code>--bootstrap-server</code> hsiehchou121:9092 <code>--topic</code> second <code>--consumer.config</code> config/consumer.properties</p><h3 id="七、Kafka工作流程分析"><a href="#七、Kafka工作流程分析" class="headerlink" title="七、Kafka工作流程分析"></a>七、Kafka工作流程分析</h3><h4 id="1、Kafka生产过程分析"><a href="#1、Kafka生产过程分析" class="headerlink" title="1、Kafka生产过程分析"></a>1、Kafka生产过程分析</h4><p>1）<strong>写入方式</strong><br>producer采用推（push）模式将消息发布到broker，每条消息都被追加（append）到分区（patition）中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障kafka吞吐率）</p><p>2） <strong>分区</strong>（Partition）<br>    Kafka集群有多个消息代理服务器（broker-server）组成，发布到Kafka集群的每条消息都有一个类别，用主题（topic）来表示。通常，不同应用产生不同类型的数据，可以设置不同的主题。一个主题一般会有多个消息的订阅者，当生产者发布消息到某个主题时，订阅了这个主题的消费者都可以接收到生成者写入的新消息</p><p>Kafka集群为每个主题维护了分布式的分区（partition）日志文件，物理意义上可以把主题（topic）看作进行了分区的日志文件（partition log）。主题的每个分区都是一个有序的、不可变的记录序列，新的消息会不断追加到日志中。分区中的每条消息都会按照时间顺序分配到一个单调递增的顺序编号，叫做偏移量（offset），这个偏移量能够唯一地定位当前分区中的每一条消息</p><p>消息发送时都被发送到一个topic，其本质就是一个目录，而topic是由一些Partition Logs(分区日志)组成，其组织结构如下图所示：<br>下图中的topic有3个分区，每个分区的偏移量都从0开始，不同分区之间的偏移量都是独立的，不会相互影响</p><p><img src="/medias/partitions.PNG" alt="partitions"></p><p>我们可以看到，每个Partition中的消息都是有序的，生产的消息被不断追加到Partition log上，其中的每一个消息都被赋予了一个唯一的offset值</p><p>发布到Kafka主题的每条消息包括键值和时间戳。消息到达服务器端的指定分区后，都会分配到一个自增的偏移量。原始的消息内容和分配的偏移量以及其他一些元数据信息最后都会存储到分区日志文件中。消息的键也可以不用设置，这种情况下消息会均衡地分布到不同的分区</p><ol><li>分区的原因<br>（1）方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了。<br>（2）可以提高并发，因为可以以Partition为单位读写了。<br>传统消息系统在服务端保持消息的顺序，如果有多个消费者消费同一个消息队列，服务端会以消费存储的顺序依次发送给消费者。但由于消息是异步发送给消费者的，消息到达消费者的顺序可能是无序的，这就意味着在并行消费时，传统消息系统无法很好地保证消息被顺序处理。虽然我们可以设置一个专用的消费者只消费一个队列，以此来解决消息顺序的问题，但是这就使得消费处理无法真正执行。<br>Kafka比传统消息系统有更强的顺序性保证，它使用主题的分区作为消息处理的并行单元。Kafka以分区作为最小的粒度，将每个分区分配给消费者组中不同的而且是唯一的消费者，并确保一个分区只属于一个消费者，即这个消费者就是这个分区的唯一读取线程。那么，只要分区的消息是有序的，消费者处理的消息顺序就有保证。每个主题有多个分区，不同的消费者处理不同的分区，所以Kafka不仅保证了消息的有序性，也做到了消费者的负载均衡。</li><li>分区的原则<br>（1）指定了patition，则直接使用；<br>（2）未指定patition但指定key，通过对key的value进行hash出一个patition<br>（3）patition和key都未指定，使用轮询选出一个patition。</li></ol><p><strong>DefaultPartitioner类</strong></p><pre><code>public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {        List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);        int numPartitions = partitions.size();        if (keyBytes == null) {            int nextValue = nextValue(topic);            List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);            if (availablePartitions.size() &gt; 0) {                int part = Utils.toPositive(nextValue) % availablePartitions.size();                return availablePartitions.get(part).partition();            } else {                // no partitions are available, give a non-available partition                return Utils.toPositive(nextValue) % numPartitions;            }        } else {            // hash the keyBytes to choose a partition            return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;        }    }</code></pre><p>3）副本（Replication）<br>同一个partition可能会有多个replication（对应 server.properties 配置中的 default.replication.factor=N）。没有replication的情况下，一旦broker 宕机，其上所有 patition 的数据都不可被消费，同时producer也不能再将数据存于其上的patition。引入replication之后，同一个partition可能会有多个replication，而这时需要在这些replication之间选出一个leader，producer和consumer只与这个leader交互，其它replication作为follower从leader 中复制数据</p><p>4） 写入流程<br> producer写入消息流程如下：</p><ol><li>producer先从zookeeper的 “/brokers/…/state”节点找到该partition的leader</li><li>producer将消息发送给该leader</li><li>leader将消息写入本地log</li><li>followers从leader pull消息，写入本地log后向leader发送ACK</li><li>leader收到所有ISR中的replication的ACK后，增加HW（high watermark，最后commit 的offset）并向producer发送ACK</li></ol><h4 id="2、Broker-保存消息"><a href="#2、Broker-保存消息" class="headerlink" title="2、Broker 保存消息"></a>2、Broker 保存消息</h4><p>1）存储方式<br>物理上把topic分成一个或多个patition（对应 server.properties 中的num.partitions=3配置），每个patition物理上对应一个文件夹（该文件夹存储该patition的所有消息和索引文件），如下：</p><pre><code>[root @hsiehchou121 logs]$ lldrwxrwxr-x. 2 root root 4096 4月   6 14:37 first-0drwxrwxr-x. 2 root root 4096 4月   6 14:35 first-1drwxrwxr-x. 2 root root 4096 4月   6 14:37 first-2[root @hsiehchou121 logs]$ cd first-0[root @hsiehchou121 first-0]$ ll-rw-rw-r--. 1 root root 10485760 4月   6 14:33 00000000000000000000.index-rw-rw-r--. 1 root root     219 4月   6 15:07 00000000000000000000.log-rw-rw-r--. 1 root root 10485756 4月   6 14:33 00000000000000000000.timeindex-rw-rw-r--. 1 root root       8 4月   6 14:37 leader-epoch-checkpoint</code></pre><p>2）存储策略<br>无论消息是否被消费，kafka都会保留所有消息。有两种策略可以删除旧数据：</p><ol><li>基于时间：log.retention.hours=168</li><li>基于大小：log.retention.bytes=1073741824<br>需要注意的是，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。</li></ol><p>3）Zookeeper存储结构<br><img src="/medias/kafka%20zookeeper%E7%9A%84%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84.PNG" alt="kafka zookeeper的存储结构"><br>注意：producer不在zk中注册，消费者在zk中注册</p><h4 id="3、Kafka消费过程分析"><a href="#3、Kafka消费过程分析" class="headerlink" title="3、Kafka消费过程分析"></a>3、Kafka消费过程分析</h4><p>kafka提供了两套consumer API：高级Consumer API和低级API<br>1）消费模型<br>    消息由生产者发布到Kafka集群后，会被消费者消费。消息的消费模型有两种：推送模型（push）和拉取模型（pull）</p><p>基于推送模型（push）的消息系统，由消息代理记录消费者的消费状态。消息代理在将消息推送到消费者后，标记这条消息为已消费，但这种方式无法很好地保证消息被处理。比如，消息代理把消息发送出去后，当消费进程挂掉或者由于网络原因没有收到这条消息时，就有可能造成消息丢失（因为消息代理已经把这条消息标记为已消费了，但实际上这条消息并没有被实际处理）。如果要保证消息被处理，消息代理发送完消息后，要设置状态为“已发送”，只有收到消费者的确认请求后才更新为“已消费”，这就需要消息代理中记录所有的消费状态，这种做法显然是不可取的</p><p>Kafka采用拉取模型，由消费者自己记录消费状态，每个消费者互相独立地顺序读取每个分区的消息。如下图所示，有两个消费者（不同消费者组）拉取同一个主题的消息，消费者A的消费进度是3，消费者B的消费进度是6。消费者拉取的最大上限通过最高水位（watermark）控制，生产者最新写入的消息如果还没有达到备份数量，对消费者是不可见的。这种由消费者控制偏移量的优点是：消费者可以按照任意的顺序消费消息。比如，消费者可以重置到旧的偏移量，重新处理之前已经消费过的消息；或者直接跳到最近的位置，从当前的时刻开始消费</p><p>在一些消息系统中，消息代理会在消息被消费之后立即删除消息。如果有不同类型的消费者订阅同一个主题，消息代理可能需要冗余地存储同一消息；或者等所有消费者都消费完才删除，这就需要消息代理跟踪每个消费者的消费状态，这种设计很大程度上限制了消息系统的整体吞吐量和处理延迟。Kafka的做法是生产者发布的所有消息会一致保存在Kafka集群中，不管消息有没有被消费。用户可以通过设置保留时间来清理过期的数据，比如，设置保留策略为两天。那么，在消息发布之后，它可以被不同的消费者消费，在两天之后，过期的消息就会自动清理掉</p><p>2）高级API</p><ol><li>高级API优点<br>高级API 写起来简单<br>不需要自行去管理offset，系统通过zookeeper自行管理。<br>不需要管理分区，副本等情况，.系统自动管理。<br>消费者断线会自动根据上一次记录在zookeeper中的offset去接着获取数据（默认设置1分钟更新一下zookeeper中存的offset）<br>可以使用group来区分对同一个topic 的不同程序访问分离开来（不同的group记录不同的offset，这样不同程序读取同一个topic才不会因为offset互相影响）</li><li>高级API缺点<br>不能自行控制offset（对于某些特殊需求来说）<br>不能细化控制如分区、副本、zk等</li></ol><p>3）低级API</p><ol><li>低级 API 优点<br>能够让开发者自己控制offset，想从哪里读取就从哪里读取。<br>自行控制连接分区，对分区自定义进行负载均衡<br>对zookeeper的依赖性降低（如：offset不一定非要靠zk存储，自行存储offset即可，比如存在文件或者内存中）</li><li>低级API缺点<br>太过复杂，需要自行控制offset，连接哪个分区，找到分区leader 等。</li></ol><p>4）消费者组<br>消费者是以consumer group消费者组的方式工作，由一个或者多个消费者组成一个组，共同消费一个topic。每个分区在同一时间只能由group中的一个消费者读取，但是多个group可以同时消费这个partition。在图中，有一个由三个消费者组成的group，有一个消费者读取主题中的两个分区，另外两个分别读取一个分区。某个消费者读取某个分区，也可以叫做某个消费者是某个分区的拥有者</p><p>在这种情况下，消费者可以通过水平扩展的方式同时读取大量的消息。另外，如果一个消费者失败了，那么其他的group成员会自动负载均衡读取之前失败的消费者读取的分区</p><p>5）消费方式<br>consumer采用pull（拉）模式从broker中读取数据</p><p>push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息</p><p>对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率，同时consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义</p><p>pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直等待数据到达。为了避免这种情况，我们在我们的拉请求中有参数，允许消费者请求在等待数据到达的“长轮询”中进行阻塞（并且可选地等待到给定的字节数，以确保大的传输大小）</p><p>6） 消费者组案例</p><ol><li>需求：测试同一个消费者组中的消费者，同一时刻只能有一个消费者消费</li><li>案例实操<br> （1）在hsiehchou121、hsiehchou122上修改/root/hd/kafka/config/consumer.properties配置文件中的group.id属性为任意组名</li></ol><p>[root@hsiehchou122 config]$ vi consumer.properties<br>    group.id=root</p><p>规划：hsiehchou121生产者，hsiehchou122消费者，hsiehchou123消费者<br>（2）在hsiehchou122、hsiehchou123上分别启动消费者</p><pre><code>[root@hsiehchou122 kafka]$ ./bin/kafka-console-consumer.sh --bootstrap-server hsiehchou121:9092 --topic second --consumer.config config/consumer.properties[root@hsiehchou123 kafka]$ ./bin/kafka-console-consumer.sh --bootstrap-server hsiehchou121:9092 --topic second --consumer.config config/consumer.properties</code></pre><p>（3）在hsiehchou121上启动生产者</p><pre><code>[root@hsiehchou121 kafka]$ bin/kafka-console-producer.sh --broker-list hsiehchou121:9092 --topic first&gt;hello world</code></pre><p>（4）查看hsiehchou122和hsiehchou123的接收者<br>        同一时刻只有一个消费者接收到消息</p><h3 id="八、-Kafka-API实战"><a href="#八、-Kafka-API实战" class="headerlink" title="八、 Kafka API实战"></a>八、 Kafka API实战</h3><h4 id="1、-环境准备"><a href="#1、-环境准备" class="headerlink" title="1、 环境准备"></a>1、 环境准备</h4><p>1）在eclipse中创建一个java工程<br>2）在工程的根目录创建一个lib文件夹<br>3）解压kafka安装包，将安装包libs目录下的jar包拷贝到工程的lib目录下，并build path<br>4）启动zk和kafka集群，在kafka集群中打开一个消费者</p><pre><code>[root@hsiehchou121 kafka]$ bin/kafka-console-consumer.sh --zookeeper hsiehchou121:2181 --topic first</code></pre><h4 id="2、Kafka生产者Java-API"><a href="#2、Kafka生产者Java-API" class="headerlink" title="2、Kafka生产者Java API"></a>2、Kafka生产者Java API</h4><p>1）<strong>创建生产者</strong><br><strong>Producer1 类</strong></p><pre><code>package com.hsiehchou.kafka.kafka_producer;import java.util.Properties;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;/** * kafka * @author hsiehchou */public class Producer1 {    public static void main(String[] args) {        //1.配置生产者的属性        Properties prop = new Properties();        //2.参数配置        //kafka节点的地址，Kafka服务端的主机名和端口号        prop.put(&quot;bootstrap.servers&quot;, &quot;192.168.116.121:9092&quot;);        //发送消息是否等待应答，等待所有副本节点的应答        prop.put(&quot;acks&quot;, &quot;all&quot;);        //配置发送消息失败重试，消息发送最大尝试次数        prop.put(&quot;retries&quot;, &quot;0&quot;);        //配置批量处理消息的大小，一批消息处理大小        prop.put(&quot;batch.size&quot;, &quot;16384&quot;);        //配置批量处理数据的延迟，请求延时        prop.put(&quot;linger.ms&quot;, &quot;1&quot;);        //配置内存缓冲区的大小，发送缓存区内存大小        prop.put(&quot;buffer.memory&quot;, 33445552);        //消息在发送前必须要序列化，key序列化        prop.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);        prop.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);        //3.实例化producer        KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(prop);        //4.发送消息        for (int i = 0; i &lt; 50; i++) {            producer.send(new ProducerRecord&lt;String, String&gt;(&quot;first&quot;, Integer.toString(i), &quot;hello world-&quot; + i));        }        //5.关闭资源        producer.close();        }}</code></pre><p>[root@hsiehchou122 kafka]# ./bin/kafka-console-consumer.sh <code>--bootstrap-server</code> hsiehchou121:9092 <code>--topic</code> first <code>--consumer.config</code> config/consumer.properties<br>hello world-0<br>hello world-1<br>hello world-2<br>hello world-3<br><code>......</code><br>hello world-49</p><p>2）创建生产者带回调函数<br><strong>Producer2类</strong></p><pre><code>package com.hsiehchou.kafka.kafka_producer;import java.util.Properties;import org.apache.kafka.clients.producer.Callback;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;public class Producer2 {    public static void main(String[] args) {        //1.配置生产者的属性        Properties prop = new Properties();        //2.参数配置        //kafka节点的地址，Kafka服务端的主机名和端口号        prop.put(&quot;bootstrap.servers&quot;, &quot;192.168.116.121:9092&quot;);        //发送消息是否等待应答，等待所有副本节点的应答        prop.put(&quot;acks&quot;, &quot;all&quot;);        //配置发送消息失败重试，消息发送最大尝试次数        prop.put(&quot;retries&quot;, &quot;0&quot;);        //配置批量处理消息的大小，一批消息处理大小        prop.put(&quot;batch.size&quot;, &quot;16384&quot;);        //配置批量处理数据的延迟，请求延时        prop.put(&quot;linger.ms&quot;, &quot;1&quot;);        //配置内存缓冲区的大小，发送缓存区内存大小        prop.put(&quot;buffer.memory&quot;, 33445552);        //消息在发送前必须要序列化，key序列化        prop.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);        prop.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);        //3.实例化producer        KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(prop);        //4.发送消息        for (int i = 0; i &lt; 50; i++) {            producer.send(new ProducerRecord&lt;String, String&gt;(&quot;first&quot;, Integer.toString(i), &quot;hello world-&quot; + i), new Callback() {                public void onCompletion(RecordMetadata metadata, Exception exception) {                    //如果metadata不为null，拿到当前的数据偏移量与分区                    if(metadata != null) {                        System.out.println(metadata.topic() + &quot;----&quot; + metadata.offset() + &quot;----&quot; + metadata.partition());                    }                }            });        }        //5.关闭资源            producer.close();        }}</code></pre><p>[root@hsiehchou122 kafka]# ./bin/kafka-console-consumer.sh <code>--bootstrap-server</code> hsiehchou121:9092 <code>--topic</code> three <code>--consumer.config</code> config/consumer.properties</p><p>控制台输出：<br>first<code>----</code>00<code>----</code>0<br>first<code>----</code>01<code>----</code>0<br>first<code>----</code>02<code>----</code>0<br>first<code>----</code>03<code>----</code>0<br><code>......</code><br>first<code>----</code>48<code>----</code>0<br>first<code>----</code>49<code>----</code>0</p><p>3）自定义分区生产者<br><strong>Partition1 类</strong></p><pre><code>package com.hsiehchou.kafka.kafka_producer;import java.util.Map;import org.apache.kafka.clients.producer.Partitioner;import org.apache.kafka.common.Cluster;/** * 自定义分区 * @author hsiehchou */public class Partition1 implements Partitioner{    //设置    public void configure(Map&lt;String, ?&gt; configs) {    }    //分区逻辑，控制分区    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {        return 2;    }    //释放资源    public void close() {    }}</code></pre><p><strong>Producer2类中新增</strong></p><pre><code>prop.put(&quot;partitioner.class&quot;, &quot;com.hsiehchou.kafka.kafka_producer.Partition1&quot;);</code></pre><p>[root@hsiehchou122 kafka]# ./bin/kafka-console-consumer.sh <code>--bootstrap-server</code> hsiehchou121:9092 <code>--topic</code> three <code>--consumer.config</code> config/consumer.properties<br>first—–1—-2<br>first—–1—-2<br>first—–1—-2<br><code>......</code><br>first—–1—-2<br>first—–1—-2</p><h4 id="3、Kafka消费者Java-API"><a href="#3、Kafka消费者Java-API" class="headerlink" title="3、Kafka消费者Java API"></a>3、Kafka消费者Java API</h4><p><strong>Consumer1 类</strong></p><pre><code>package com.hsiehchou.kafka.kafka_consumer;import java.util.Arrays;import java.util.Properties;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;/** * 创建消费者 * @author hsiehchou */public class Consumer1 {    public static void main(String[] args) {        //配置消费者属性        Properties prop = new Properties();        //2.参数配置        //kafka节点的地址，Kafka服务端的主机名和端口号        prop.put(&quot;bootstrap.servers&quot;, &quot;192.168.116.122:9092&quot;);        //配置消费者组        prop.put(&quot;group.id&quot;, &quot;g1&quot;);        //配置是否自动确认offset        prop.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);        //序列化        prop.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);        prop.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);        //3.实例消费者，定义consumer        final KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(prop);        //释放资源        //5.释放资源，线程安全        Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() {            public void run() {                if(consumer != null) {                    consumer.close();                }            }        }));        //订阅消息        consumer.subscribe(Arrays.asList(&quot;first&quot;));        //4.拉消息 推push 拉poll        while(true) {            ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);            //遍历消息            for(ConsumerRecord&lt;String, String&gt; record:records) {                System.out.println(record.topic() + &quot;--------&quot; + record.value());            }        }    }}</code></pre><p><strong>Producer1 类</strong><br>跟之前的一样，此处省略</p><p><strong>strong text</strong>启动kafka集群<br>[root@hsiehchou121 kafka]# ./bin/kafka-server-start.sh config/server.properties &amp;<br>[root@hsiehchou122 kafka]# ./bin/kafka-server-start.sh config/server.properties &amp;<br>[root@hsiehchou123 kafka]# ./bin/kafka-server-start.sh config/server.properties &amp;<br>分别对Consumer1类和Producer1类依次进行Run  as  Java Application</p><p><strong>控制台输出</strong><br>first——–1556248624834-hello world-0<br>first——–1556248625017-hello world-1<br>first——–1556248625017-hello world-2<br>first——–1556248625017-hello world-3<br><code>......</code><br>first——–1556248625021-hello world-48<br>first——–1556248625021-hello world-49</p><h3 id="九、Kafka-producer拦截器-interceptor"><a href="#九、Kafka-producer拦截器-interceptor" class="headerlink" title="九、Kafka producer拦截器(interceptor)"></a>九、Kafka producer拦截器(interceptor)</h3><h4 id="1、拦截器原理"><a href="#1、拦截器原理" class="headerlink" title="1、拦截器原理"></a>1、拦截器原理</h4><p>Producer拦截器(interceptor)是在Kafka 0.10版本被引入的，主要用于实现clients端的定制化控制逻辑</p><p>对于producer而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如<strong>修改消息</strong>等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。Intercetpor的实现接口是<strong>org.apache.kafka.clients.producer.ProducerInterceptor</strong>，其定义的方法包括：<br>1）<strong>configure(configs)</strong><br>获取配置信息和初始化数据时调用</p><p>2）<strong>onSend(ProducerRecord)</strong><br>该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。<strong>用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区</strong>，否则会影响目标分区的计算</p><p>3）<strong>onAcknowledgement(RecordMetadata, Exception)</strong><br><strong>该方法会在消息被应答或消息发送失败时调用</strong>，并且通常都是在producer回调逻辑触发之前。onAcknowledgement运行在producer的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率</p><p>4）<strong>close</strong><br><strong>关闭interceptor，主要用于执行一些资源清理工作</strong><br>如前所述，interceptor可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外<strong>倘若指定了多个interceptor，则producer将按照指定顺序调用它们</strong>，并仅仅是捕获每个interceptor可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意</p><h4 id="2、拦截器案例"><a href="#2、拦截器案例" class="headerlink" title="2、拦截器案例"></a>2、拦截器案例</h4><p>1）需求：<br>实现一个简单的双interceptor组成的拦截链。第一个interceptor会在消息发送前将时间戳信息加到消息value的最前部；第二个interceptor会在消息发送后更新成功发送消息数或失败发送消息数。<br>2）案例实操<br>（1）增加时间戳拦截器<br><strong>TimeInterceptor 类</strong></p><pre><code>package com.hsiehchou.kafka.interceptor;import java.util.Map;import org.apache.kafka.clients.producer.ProducerInterceptor;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;public class TimeInterceptor implements ProducerInterceptor&lt;String, String&gt; {    //配置信息    public void configure(Map&lt;String, ?&gt; configs) {    }    //业务逻辑    public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) {        return new ProducerRecord&lt;String, String&gt;(                record.topic(),                 record.partition(),                record.timestamp(),                record.key(),                System.currentTimeMillis() + &quot;-&quot; + record.value()        );    }    //发送失败调用    public void onAcknowledgement(RecordMetadata metadata, Exception exception) {    }    //关闭资源    public void close() {    }}</code></pre><p>（2）统计发送消息成功和发送失败消息数，并在producer关闭时打印这两个计数器<br><strong>CounterInterceptor 类</strong></p><pre><code>package com.hsiehchou.kafka.interceptor;import java.util.Map;import org.apache.kafka.clients.producer.ProducerInterceptor;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;public class CounterInterceptor implements ProducerInterceptor&lt;String, String&gt; {    private int errorCounter = 0;    private int successCounter = 0;    public void configure(Map&lt;String, ?&gt; configs) {    }    public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) {        return record;    }    public void onAcknowledgement(RecordMetadata metadata, Exception exception) {        //统计成功和失败的次数        if(exception == null) {            successCounter++;        }else {            errorCounter++;        }    }    public void close() {        //保存结果        System.out.println(&quot;Successful sent:&quot; + successCounter);        System.out.println(&quot;Failed sent:&quot; + errorCounter);    }}</code></pre><p><strong>在Producer1类中增加</strong></p><pre><code>//拦截器        ArrayList&lt;String&gt; inList = new ArrayList&lt;String&gt;();         inList.add(&quot;com.hsiehchou.kafka.interceptor.TimeInterceptor&quot;);        inList.add(&quot;com.hsiehchou.kafka.interceptor.CounterInterceptor&quot;);        prop.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, inList);</code></pre><p>测试一：<br>[root@hsiehchou122 kafka]# ./bin/kafka-server-start.sh config/server.properties &amp;</p><p><code>--from beginning</code> 是从头开始消费，不加则是消费当前正在发送到该topic的消息<br>[root@hsiehchou123 kafka]#./bin/kafka-console-consumer.sh <code>--bootstrap-server</code> hsiehchou121:9092 <code>--topic</code> first <code>--from-beginning</code></p><p>测试二：<br>[root@hsiehchou122 kafka]# ./bin/kafka-server-start.sh config/server.properties &amp;</p><p>[root@hsiehchou122 kafka]#  ./bin/kafka-console-consumer.sh <code>--bootstrap-server</code> hsiehchou121:9092 <code>--topic</code> first <code>--consumer.config</code> config/consumer.properties<br>结果：<br>1556253447912-hello world-0<br>1556253448100-hello world-1<br>1556253448100-hello world-2<br>1556253448100-hello world-3<br>1556253448100-hello world-4<br><code>......</code><br>1556253448100-hello world-12<br>1556253448100-hello world-13<br>1556253448104-hello world-14<br>1556253448104-hello world-15<br>1556253448104-hello world-16<br><code>......</code><br>1556253448104-hello world-48<br>1556253448104-hello world-49</p><p>Successful sent:50<br>Failed sent:0</p><h3 id="十、Kafka-Streams"><a href="#十、Kafka-Streams" class="headerlink" title="十、Kafka Streams"></a>十、Kafka Streams</h3><h4 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h4><p>1）Kafka Streams<br>Kafka Streams。Apache Kafka开源项目的一个组成部分。是一个功能强大，易于使用的库。用于在Kafka上构建高可分布式、拓展性，容错的应用程序</p><p>2）Kafka Streams特点</p><ol><li>功能强大<br>高扩展性，弹性，容错 </li><li>轻量级<br>无需专门的集群<br>一个库，而不是框架</li><li>完全集成<br>100%的Kafka 0.10.0版本兼容<br>易于集成到现有的应用程序 </li><li>实时性<br>毫秒级延迟<br>并非微批处理<br>窗口允许乱序数据<br>允许迟到数据</li></ol><p>3）为什么要有Kafka Stream<br>当前已经有非常多的流式处理系统，最知名且应用最多的开源流式处理系统有Spark Streaming和Apache Storm。Apache Storm发展多年，应用广泛，提供记录级别的处理能力，当前也支持SQL on Stream。而Spark Streaming基于Apache Spark，可以非常方便与图计算，SQL处理等集成，功能强大，对于熟悉其它Spark应用开发的用户而言使用门槛低。另外，目前主流的Hadoop发行版，如Cloudera和Hortonworks，都集成了Apache Storm和Apache Spark，使得部署更容易</p><p>既然Apache Spark与Apache Storm拥用如此多的优势，那为何还需要Kafka Stream呢？主要有如下原因<br>第一，Spark和Storm都是流式处理框架，而Kafka Stream提供的是一个基于Kafka的流式处理类库。框架要求开发者按照特定的方式去开发逻辑部分，供框架调用。开发者很难了解框架的具体运行方式，从而使得调试成本高，并且使用受限。而Kafka Stream作为流式处理类库，直接提供具体的类给开发者调用，整个应用的运行方式主要由开发者控制，方便使用和调试</p><p>第二，虽然Cloudera与Hortonworks方便了Storm和Spark的部署，但是这些框架的部署仍然相对复杂。而Kafka Stream作为类库，可以非常方便的嵌入应用程序中，它对应用的打包和部署基本没有任何要求</p><p>第三，就流式处理系统而言，基本都支持Kafka作为数据源。例如Storm具有专门的kafka-spout，而Spark也提供专门的spark-streaming-kafka模块。事实上，Kafka基本上是主流的流式处理系统的标准数据源。换言之，大部分流式系统中都已部署了Kafka，此时使用Kafka Stream的成本非常低</p><p>第四，使用Storm或Spark Streaming时，需要为框架本身的进程预留资源，如Storm的supervisor和Spark on YARN的node manager。即使对于应用实例而言，框架本身也会占用部分资源，如Spark Streaming需要为shuffle和storage预留内存。但是Kafka作为类库不占用系统资源</p><p>第五，由于Kafka本身提供数据持久化，因此Kafka Stream提供滚动部署和滚动升级以及重新计算的能力</p><p>第六，由于Kafka Consumer Rebalance机制，Kafka Stream可以在线动态调整并行度</p><h4 id="2、Kafka-Stream数据清洗案例"><a href="#2、Kafka-Stream数据清洗案例" class="headerlink" title="2、Kafka Stream数据清洗案例"></a>2、Kafka Stream数据清洗案例</h4><p>1）需求：<br>    实时处理单词带有”&gt;&gt;&gt;”前缀的内容。例如输入”itstar&gt;&gt;&gt;ximenqing”，最终处理成“ximenqing”<br>2）需求分析：<br>3）案例实操</p><ol><li>创建一个工程，并添加jar包</li><li>创建主类</li></ol><p><strong>Application类</strong></p><pre><code>package com.hsiehchou.kafka.kafka_stream;import java.util.Properties;import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.Topology;import org.apache.kafka.streams.processor.Processor;import org.apache.kafka.streams.processor.ProcessorSupplier;/** *  * 需求：对数据进行清洗操作 * 思路：xie-hs    把-清洗掉 * @author hsiehchou */public class Application {    public static void main(String[] args) {        //1.定义主题 发送到 另外一个主题中 数据清洗        String oneTopic = &quot;t1&quot;;        String twoTopic = &quot;t1&quot;;        //2.设置参数        Properties prop = new Properties();        prop.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;logProcessor&quot;);        prop.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;192.168.116.121:9092,192.168.116.122:9092,192.168.116.123:9092&quot;);        //3.实例化对象        StreamsConfig config = new StreamsConfig(prop);        //4. 流计算 拓扑        Topology builder = new Topology();         //5.定义kafka组件数据源        builder.addSource(&quot;Source&quot;, oneTopic).addProcessor(&quot;Processor&quot;, new ProcessorSupplier&lt;byte[], byte[]&gt;() {            public Processor&lt;byte[], byte[]&gt; get() {                return new LogProcesser();            }            //从哪里来        }, &quot;Source&quot;)        //到哪里去        .addSink(&quot;Sink&quot;, twoTopic, &quot;Processor&quot;);        //6.实例化KafkaStream        KafkaStreams kafkaStreams = new KafkaStreams(builder, prop);        kafkaStreams.start();    }}</code></pre><p><strong>LogPRocessor类</strong></p><pre><code>package com.hsiehchou.kafka.kafka_stream;import org.apache.kafka.streams.processor.Processor;import org.apache.kafka.streams.processor.ProcessorContext;public class LogProcesser implements Processor&lt;byte[], byte[]&gt;{    private ProcessorContext context;    //初始化    public void init(ProcessorContext context) {        //传输        this.context = context;    }    //业务逻辑    public void process(byte[] key, byte[] value) {        //1.拿到消息数据        String message = new String(value);        //2.如果包含 -  去除        if(message.contains(&quot;-&quot;)) {            //3.把-去掉之后去掉左侧数据            message = message.split(&quot;-&quot;)[1];            //4.发送数据            context.forward(key, message.getBytes());        }    }    //释放资源    public void close() {    }}</code></pre><p>[root@hsiehchou121 kafka]#./bin/kafka-topics.sh <code>--zookeeper</code> hsiehchou121:2181 <code>--create</code> <code>--replication-factor</code> 3 <code>--partitions</code> 1 <code>--topic</code> t1</p><p>[root@hsiehchou122 kafka]# ./bin/kafka-console-consumer.sh <code>--bootstrap-server</code> hsiehchou122:9092 <code>--topic</code> t2 <code>--from-beginning</code> -<code>-consumer.config</code> config/consumer.properties</p><p>[root@hsiehchou121 kafka]# ./bin/kafka-console-producer.sh <code>--broker-list</code> hsiehchou121:9092 <code>--topic</code> t1</p><h3 id="十一、CDH搭建："><a href="#十一、CDH搭建：" class="headerlink" title="十一、CDH搭建："></a>十一、CDH搭建：</h3><p><a href="https://juejin.im/post/5a55814e518825734859d69a" target="_blank" rel="noopener">https://juejin.im/post/5a55814e518825734859d69a</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker入门操作</title>
      <link href="/2019/04/24/docker-ru-men-cao-zuo/"/>
      <url>/2019/04/24/docker-ru-men-cao-zuo/</url>
      
        <content type="html"><![CDATA[<p><strong>docker</strong><br>2013年发布</p><h3 id="一、环境配置难题"><a href="#一、环境配置难题" class="headerlink" title="一、环境配置难题"></a>一、环境配置难题</h3><p>开发环境运行没有问题，生产不能用，因为生产缺乏某些组件</p><p>换一台机器，需要重新配置一遍</p><p>能不能从根本上解决问题：安装的时候，把原始环境，一模一样地安装一遍</p><h3 id="二、虚拟机"><a href="#二、虚拟机" class="headerlink" title="二、虚拟机"></a>二、虚拟机</h3><p>带环境安装的一种解决方案</p><p>缺点：<br>占用资源多：虚拟机本身需要消耗资源，程序1MB，环境几百MB</p><p>冗余步骤多：虚拟机是完整的操作系统，一些系统级别的操作步骤，无法跳过，比如用户登录</p><p>启动慢：启动操作系统要多久，启动虚拟机就要多久</p><h3 id="三、Linux容器"><a href="#三、Linux容器" class="headerlink" title="三、Linux容器"></a>三、Linux容器</h3><p>针对虚拟机的缺点，Linux发展出另外的一种虚拟化技术：Linux容器</p><p>Linux容器不是模拟完整的操作系统，而是对进程进行隔离，即在正常进程的外面，套一个保护层，对于容器里面的进程来说，它接触到的资源都是虚拟的，实现与底层系统的隔离</p><p>优点：<br>启动快：容器里面的应用，直接就是底层系统中的一个进程，启动容器相当于启动本机的进程。而不是启动操作系统</p><p>占用资源少：容器只占用需要的资源，不占用没有用到的资源</p><p>体积小：只包含用到的组件，而虚拟机包含了整个操作系统。所以容器文件比虚拟机文件小的多</p><h3 id="四、Docker是什么"><a href="#四、Docker是什么" class="headerlink" title="四、Docker是什么"></a>四、Docker是什么</h3><p>Docker属于Linux容器的一种封装，提供了简单易用的容器使用接口</p><p>Docker将应用程序与该程序的依赖，打包到一个文件里面，运行这个文件，就会产生一个虚拟容器</p><p>程序在虚拟容器中运行，就好像运行在真正的物理机上一样</p><p>Docker提供版本管理、复制、分享、修改等功能，就像管理普通代码一样管理Docker容器</p><h3 id="五、Docker的用途"><a href="#五、Docker的用途" class="headerlink" title="五、Docker的用途"></a>五、Docker的用途</h3><p>Docker的主要用途，目前有三大类</p><h4 id="1、提供一次性的环境"><a href="#1、提供一次性的环境" class="headerlink" title="1、提供一次性的环境"></a>1、提供一次性的环境</h4><p>本地测试他人的软件程序</p><h4 id="2、提供弹性的云服务"><a href="#2、提供弹性的云服务" class="headerlink" title="2、提供弹性的云服务"></a>2、提供弹性的云服务</h4><p>Docker容器可以随开随关，很适合动态的扩容和缩容</p><h4 id="3、组建微服务架构"><a href="#3、组建微服务架构" class="headerlink" title="3、组建微服务架构"></a>3、组建微服务架构</h4><p>通过多个容器，一台机器可以跑多个服务，在本机就可以模拟出微服务架构</p><h3 id="六、Docker安装"><a href="#六、Docker安装" class="headerlink" title="六、Docker安装"></a>六、Docker安装</h3><h4 id="1、Linux安装"><a href="#1、Linux安装" class="headerlink" title="1、Linux安装"></a>1、Linux安装</h4><p>Docker要求CentOS内核版本高于3.10<br>uname -r 查看内核版本</p><p>安装必要的系统工具：<br>yum install -y yum-utils device-mapper-persistent-data lvm2</p><p>添加软件源信息：<br>yum-config-manager –add-repo <a href="http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo" target="_blank" rel="noopener">http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</a></p><p>更新 yum 缓存：<br>yum makecache fast</p><p>安装 Docker-ce：<br>yum -y install docker-ce</p><p>启动 Docker 后台服务<br>systemctl start docker</p><p>测试运行 hello-world<br>docker run hello-world</p><p>看到hello from docker证明安装成功</p><h4 id="2、windows安装"><a href="#2、windows安装" class="headerlink" title="2、windows安装"></a>2、windows安装</h4><p>win10专业版，直接安装 docker for windows 即可</p><p>win10普通版、win7 win8 ,需要安装 docker tool box</p><p>toolbox 配置：<br>右键  Docker Quickstart Terminal<br>“C:\Program Files\Git\bin\bash.exe” <code>--login</code> -i “C:\Program Files\Docker Toolbox\start.sh”<br>把这个位置配成你本机的git位置                        修改后面这个脚本</p><p>DOCKER_MACHINE=”D:\Docker Toolbox\docker-machine.exe”</p><p>STEP=”Looking for vboxmanage.exe”<br>VBOXMANAGE=”C:\Program Files\Oracle\VirtualBox\VBoxManage.exe”</p><pre><code>#if [ ! -z &quot;$VBOX_MSI_INSTALL_PATH&quot; ]; then#VBOXMANAGE=&quot;${VBOX_MSI_INSTALL_PATH}VBoxManage.exe&quot;#else#VBOXMANAGE=&quot;${VBOX_INSTALL_PATH}VBoxManage.exe&quot;#fi</code></pre><h3 id="七、image文件"><a href="#七、image文件" class="headerlink" title="七、image文件"></a>七、image文件</h3><p>Docker把应用程序及其依赖，打包在image文件里面，只有通过image文件，才能生成docker容器</p><p>Docker可以根据image文件生成容器实例</p><p>image文件可以继承。在实际开发中，一个image文件往往通过继承另一个image文件，加上一些个性化的设置而生成</p><p>启动容器<br>docker run hello-world</p><p>列出所有image文件<br>docker image ls</p><p>删除image文件<br>docker image rm image文件名</p><p>使用docker-machine stop default停掉Docker的虚拟机<br>使用docker-machine start default开启Docker的虚拟机</p><h3 id="八、配置阿里云docker镜像加速器"><a href="#八、配置阿里云docker镜像加速器" class="headerlink" title="八、配置阿里云docker镜像加速器"></a>八、配置阿里云docker镜像加速器</h3><ol><li><p>注册阿里云，获得专属加速器地址</p></li><li><p>hsiehchou@DESKTOP-KJDN870 MINGW64 /d/Docker Toolbox<br>$ docker-machine ssh default<br>docker@default:<del>$ sudo sed -i “s|EXTRA_ARGS=’|EXTRA_ARGS=’–regis<br>try-mirror=https://<code>********</code>.mirror.aliyuncs.com |g” /var/lib/boot<br>2docker/profile<br>docker@default:</del>$ exit</p></li><li><p>hsiehchou@DESKTOP-KJDN870 MINGW64 /d/Docker Toolbox<br>$ docker-machine restart default</p></li></ol><h3 id="九、安装redis"><a href="#九、安装redis" class="headerlink" title="九、安装redis"></a>九、安装redis</h3><h4 id="1、搜索镜像"><a href="#1、搜索镜像" class="headerlink" title="1、搜索镜像"></a>1、搜索镜像</h4><p>docker search redis</p><h4 id="2、拉取镜像"><a href="#2、拉取镜像" class="headerlink" title="2、拉取镜像"></a>2、拉取镜像</h4><p>docker pull redis</p><h4 id="3、启动redis"><a href="#3、启动redis" class="headerlink" title="3、启动redis"></a>3、启动redis</h4><p>docker run <code>--name</code> myredis -p 6379:6379 -d redis redis-server</p><p>-d表示后台运行</p><p>-p表示端口号，左边的6379表示win10系统端口考，右边表示容器中redis端口号</p><p><code>--name</code>表示运行redis镜像的实例名称</p><h4 id="4、进入Image的小环境"><a href="#4、进入Image的小环境" class="headerlink" title="4、进入Image的小环境"></a>4、进入Image的小环境</h4><p>docker exec -it  <code>COXTAINER ID</code> bash<br>redis-cli</p><p>hsiehchou@DESKTOP-KJDN870 MINGW64 /d/Docker Toolbox<br>$ docker run <code>--name</code> myredis12 -p 6379:6379 -d redis redis-server<br>9e37ae7338d05b83f666a95fe3677aa85b1481c8fa519a79e9008a5a5e9e909d</p><p>hsiehchou@DESKTOP-KJDN870 MINGW64 /d/Docker Toolbox<br>$ docker ps<br>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES<br>9e37ae7338d0        redis               “docker-entrypoint.s”   5 seconds ago       Up 6 seconds        0.0.0.0:6379-&gt;6379/tcp   myredis12</p><p>hsiehchou@DESKTOP-KJDN870 MINGW64 /d/Docker Toolbox<br>$ docker exec -it 9e37ae7338d0 bash<br>root@9e37ae7338d0:/data# redis-cli<br>127.0.0.1:6379&gt;</p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker入门 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git简单操作</title>
      <link href="/2019/04/23/git-jian-dan-cao-zuo/"/>
      <url>/2019/04/23/git-jian-dan-cao-zuo/</url>
      
        <content type="html"><![CDATA[<p><strong>git 版本控制系统</strong><br>git是一个版本控制系统</p><h3 id="一、什么是版本控制系统"><a href="#一、什么是版本控制系统" class="headerlink" title="一、什么是版本控制系统"></a>一、什么是版本控制系统</h3><h4 id="1、概念"><a href="#1、概念" class="headerlink" title="1、概念"></a>1、概念</h4><p>版本控制是一种 记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统</p><p>（<em>）记录文件的所有历史变化<br>（</em>）随时可恢复到任何一个历史状态<br>（<em>）多人协作开发或修改<br>（</em>）错误恢复<br>（*）多功能并行开发</p><p>产品–&gt; 新加功能A —&gt; 单独拉一个新分支 –&gt; 开发完成后合并到master或者丢弃</p><h4 id="2、分类"><a href="#2、分类" class="headerlink" title="2、分类"></a>2、分类</h4><p>本地版本控制系统<br>集中化版本控制系统        SVN<br>分布式版本控制系统        Git</p><h4 id="3、基本概念"><a href="#3、基本概念" class="headerlink" title="3、基本概念"></a>3、基本概念</h4><p>repository ：存放所有文件及其历史信息<br>checkout    ：取出或切换到执行版本的文件<br>version    ：表示一个版本<br>tag    ：记录标识一个主要版本。2.0 3.0。用来标识一个特定的version</p><h4 id="4、不同版本控制系统优缺点"><a href="#4、不同版本控制系统优缺点" class="headerlink" title="4、不同版本控制系统优缺点"></a>4、不同版本控制系统优缺点</h4><p><strong>本地</strong><br>优点：<br>简单，很多系统中内置。适合保存文本文件（配置文件、文章、信件）</p><p>缺点：<br>    只支持管理少量的文件，不支持基于项目的管理<br>    支持的文件类型单一<br>    不支持网络，无法实现多人协作</p><p><strong>集中式版本控制系统</strong><br>优点：<br>    适合多人团队协作开发<br>    代码集中化管理</p><p>缺点：<br>    单点故障<br>    必须联网工作，无法单机工作</p><p>解决：<br><strong>分布式版本控制系统</strong><br>集合集中式版本控制系统优点<br>支持离线工作，先提交到本地仓库，再在某个时间上传到远程仓库<br>每个计算机都是一个完整仓库：强备份</p><h3 id="二、git分布式版本管理系统"><a href="#二、git分布式版本管理系统" class="headerlink" title="二、git分布式版本管理系统"></a>二、git分布式版本管理系统</h3><p>由Linux创始人开发，作为Linux内核代码管理系统使用</p><p>Git在设计时考虑了很多方面设计目标</p><p>特点：<br>速度<br>简单的设计<br>对非线性开发模式的强力支持（允许上千个并行开发的分支）<br>完全分布式<br>有能力管理超大规模项目（挑战：速度和数据量）</p><p>Git原理：保存快照而非保存区别<br>Git保存时，相当于保存了当下所有文件的一个整体快照<br>所以，每个版本都是独立的。随时想取某一个版本，可以很快取出来</p><h3 id="三、安装git"><a href="#三、安装git" class="headerlink" title="三、安装git"></a>三、安装git</h3><p>Git 的工作区域：</p><p>Git repository：：    最终确定的文件保存到仓库，作为一个新的版本<br>staging area：            暂存已经修改的文件<br>woking directory：    工作目录</p><p>安装git<br>从 <a href="https://git-scm.com/" target="_blank" rel="noopener">https://git-scm.com/</a> 下载windows版本git<br>全使用默认值，一直下一步</p><h3 id="四、创建仓库和基本操作"><a href="#四、创建仓库和基本操作" class="headerlink" title="四、创建仓库和基本操作"></a>四、创建仓库和基本操作</h3><p>git安装好后，需要一些基本设置<br>设置用户名：git config –global user.name “hsiehchou”<br>设置邮箱：git config –global user.email  “<a href="mailto:417952939@qq.com">417952939@qq.com</a>“</p><p>查看所有设置：git config –list</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git init</strong><br>Reinitialized existing Git repository in C:/Users/hsiehchou/Desktop/git demo/.git/</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ ll -a</strong><br>total 52<br>drwxr-xr-x 1 hsiehchou 197121 0 4月  24 12:45 ./<br>drwxr-xr-x 1 hsiehchou 197121 0 4月  24 10:44 ../<br>drwxr-xr-x 1 hsiehchou 197121 0 4月  24 12:46 .git/</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git status</strong><br>On branch master</p><p>No commits yet</p><p>nothing to commit (create/copy files and use “git add” to track)<br>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ touch README</strong></p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ vi README</strong></p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git status</strong><br>On branch master</p><p>No commits yet</p><p><strong>未追踪的文件</strong><br><strong>Untracked files</strong>:<br>  (use “git add <code>&lt;file&gt;</code>…” to include in what will be committed)<br> README</p><p>nothing added to commit but untracked files present (use “git add” to track)</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>新建文件，默认是未追踪的文件<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git add README</strong><br>warning: LF will be replaced by CRLF in README.<br>The file will have its original line endings in your working directory.</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git status</strong><br>On branch master</p><p>No commits yet</p><p>Changes to be committed:<br>  (use “git rm –cached <code>&lt;file&gt;</code>…” to unstage)<br> new file:   README</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>git add 提交到了暂存区域<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git commit -m “add README”</strong><br>[master (root-commit) f50e736] add README<br> 1 file changed, 1 insertion(+)<br> create mode 100644 README</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br><strong>查看历史</strong><br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git log</strong><br>commit f50e7362b5573e9b9862f7fb65a9e3f6fa98913a (HEAD -&gt; master)<br>Author: hsiehchou <a href="mailto:&#52;&#49;&#x37;&#57;&#x35;&#x32;&#x39;&#51;&#57;&#x40;&#x71;&#x71;&#46;&#x63;&#x6f;&#x6d;">&#52;&#49;&#x37;&#57;&#x35;&#x32;&#x39;&#51;&#57;&#x40;&#x71;&#x71;&#46;&#x63;&#x6f;&#x6d;</a><br>Date:   Wed Apr 24 12:52:51 2019 +0800<br>add README</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br><strong>修改文件</strong><br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ vim README</strong></p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br><strong>检测到被修改了</strong><br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git status</strong><br>On branch master<br>Changes not staged for commit:<br>  (use “git add <code>&lt;file&gt;</code>…” to update what will be committed)<br>  (use “git checkout – <code>&lt;file&gt;</code>…” to discard changes in working directory)</p><p> modified:   README</p><p>no changes added to commit (use “git add” and/or “git commit -a”)</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br><strong>提交到仓库</strong><br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git commit -a -m “modify README”</strong><br>warning: LF will be replaced by CRLF in README.<br>The file will have its original line endings in your working directory<br>[master 278ec6a] modify README<br> 1 file changed, 1 insertion(+)</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br><strong>删除文件</strong><br>rm README<br>git rm README<br>git commit -m “delete README”</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ rm README</strong></p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git rm README</strong><br>rm ‘README’</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git commit -m “delete README”</strong><br>[master b82ec4f] delete README<br> 1 file changed, 2 deletions(-)<br> delete mode 100644 README</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br><strong>checkout 某个版本</strong><br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git log</strong><br>commit b82ec4f7ccf718abac6dc630b7049618d179418f (HEAD -&gt; master)<br>Author: hsiehchou <a href="mailto:&#x34;&#49;&#x37;&#57;&#x35;&#x32;&#x39;&#51;&#x39;&#64;&#113;&#x71;&#x2e;&#99;&#x6f;&#x6d;">&#x34;&#49;&#x37;&#57;&#x35;&#x32;&#x39;&#51;&#x39;&#64;&#113;&#x71;&#x2e;&#99;&#x6f;&#x6d;</a><br>Date:   Wed Apr 24 12:56:45 2019 +0800</p><p> delete README</p><p>commit 278ec6a869f73af71539785f6893259726f9902e<br>Author: hsiehchou <a href="mailto:&#x34;&#x31;&#x37;&#x39;&#53;&#x32;&#57;&#x33;&#57;&#x40;&#113;&#113;&#x2e;&#99;&#111;&#109;">&#x34;&#x31;&#x37;&#x39;&#53;&#x32;&#57;&#x33;&#57;&#x40;&#113;&#113;&#x2e;&#99;&#111;&#109;</a><br>Date:   Wed Apr 24 12:56:00 2019 +0800</p><p>modify README</p><p>commit f50e7362b5573e9b9862f7fb65a9e3f6fa98913a<br>Author: hsiehchou <a href="mailto:&#52;&#49;&#55;&#x39;&#x35;&#50;&#57;&#x33;&#x39;&#x40;&#113;&#x71;&#x2e;&#x63;&#x6f;&#109;">&#52;&#49;&#55;&#x39;&#x35;&#50;&#57;&#x33;&#x39;&#x40;&#113;&#x71;&#x2e;&#x63;&#x6f;&#109;</a><br>Date:   Wed Apr 24 12:52:51 2019 +0800</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo ((b82ec4f…))<br><strong>$ git checkout 278ec6a869f73af71539785f6893259726f9902e</strong><br>Previous HEAD position was b82ec4f delete README<br>HEAD is now at 278ec6a modify README</p><p>You are in ‘detached HEAD’ state. You can look around, make experimental<br>changes and commit them, and you can discard any commits you make in this<br>state without impacting any branches by performing another checkout.</p><p>If you want to create a new branch to retain commits you create, you may<br>do so (now or later) by using -b with the checkout command again. Example:</p><p>  git checkout -b <code>&lt;new-branch-name&gt;</code></p><p>HEAD is now at 278ec6a modify README</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo ((278ec6a…))<br><strong>$ ll</strong><br>total 1<br>-rw-r–r– 1 hsiehchou 197121 22 4月  24 12:58 README</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo ((278ec6a…))<br><strong>$ cat README</strong><br>Hello World!<br>fsdggd</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo ((278ec6a…))<br><strong>$ git checkout master</strong><br>Previous HEAD position was 278ec6a modify README<br>Switched to branch ‘master’</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ ll</strong><br>total 0</p><h3 id="五、git远程仓库"><a href="#五、git远程仓库" class="headerlink" title="五、git远程仓库"></a>五、git远程仓库</h3><p>实现代码共享<br>远程仓库实际保存了本地.git文件夹下的东西，内容几乎一样<br>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ touch README2</strong></p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ vim README2</strong></p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git add README2</strong><br>warning: LF will be replaced by CRLF in README2.<br>The file will have its original line endings in your working directory</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git commit -a -m “README2”</strong><br>[master 0c4f333] README2<br> 1 file changed, 1 insertion(+)<br> create mode 100644 README2</p><p><strong>git 远程仓库访问协议</strong>：<br>ssh协议<br>git协议<br>http https协议：一般用于开源项目</p><p>常用远程仓库实现：<br>1、github<br>2、自己搭建git仓库服务器 gitlab、码云</p><p>举例：在自己的github中，关联本地仓库<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ cd ~/.ssh</strong><br>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/.ssh<br><strong>$ ll</strong><br>total 9<br>-rw-r–r– 1 hsiehchou 197121 1823 4月  15 23:35 id_rsa<br>-rw-r–r– 1 hsiehchou 197121  398 4月  15 23:35 id_rsa.pub<br>-rw-r–r– 1 hsiehchou 197121 1197 4月  16 14:09 known_hosts</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/.ssh<br><strong>$ cat id_rsa.pub</strong></p><p> <strong>创建公钥</strong><br>ssh-keygen -t rsa -C “<a href="mailto:417952939@qq.com">417952939@qq.com</a>“命令</p><p>测试是否能够正常连接github<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ ssh -T <a href="mailto:git@github.com">git@github.com</a></strong><br>Hi hsiehchou! You’ve successfully authenticated, but GitHub does not provide shell access.</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git remote add origin <a href="mailto:git@github.com">git@github.com</a>:hsiehchou/git-test.git</strong></p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ ll</strong><br>total 1<br>-rw-r–r– 1 root 197121 15 4月  21 22:13 README2</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git push -u origin master</strong><br>Enumerating objects: 11, done.<br>Counting objects: 100% (11/11), done.<br>Delta compression using up to 4 threads<br>Compressing objects: 100% (4/4), done.<br>Writing objects: 100% (11/11), 826 bytes | 103.00 KiB/s, done.<br>Total 11 (delta 0), reused 0 (delta 0)<br>To github.com:hsiehchou/git-test.git</p><ul><li>[new branch]      master -&gt; master<br>Branch ‘master’ set up to track remote branch ‘master’ from ‘origin’.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop HA和HBase HA</title>
      <link href="/2019/04/22/hadoop-ha-he-hbase-ha/"/>
      <url>/2019/04/22/hadoop-ha-he-hbase-ha/</url>
      
        <content type="html"><![CDATA[<p><strong>Hadoop  HBase HA</strong></p><p>保证所有的服务器时间都相同</p><h3 id="一、Hadoop-HA"><a href="#一、Hadoop-HA" class="headerlink" title="一、Hadoop HA"></a>一、Hadoop HA</h3><p><strong>HDFS HA</strong></p><p>/root/hd/hadoop-2.8.4/etc/hadoop 下是所有hadoop配置文件</p><h4 id="1、core-site-xml"><a href="#1、core-site-xml" class="headerlink" title="1、core-site.xml"></a>1、core-site.xml</h4><pre><code>&lt;configuration&gt;    &lt;property&gt;         &lt;name&gt;fs.defaultFS&lt;/name&gt;         &lt;value&gt;hdfs://mycluster&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;          &lt;value&gt;hsiehchou123:2181,hsiehchou124:2181&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;         &lt;value&gt;/root/hd/hadoop-2.8.4/tmp&lt;/value&gt;:    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="2、hdfs-site-xml"><a href="#2、hdfs-site-xml" class="headerlink" title="2、hdfs-site.xml"></a>2、hdfs-site.xml</h4><pre><code>&lt;configuration&gt;    &lt;property&gt;         &lt;name&gt;dfs.nameservices&lt;/name&gt;         &lt;value&gt;mycluster&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;         &lt;value&gt;nn1,nn2&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;         &lt;value&gt;hsiehchou121:8020&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;         &lt;value&gt;hsiehchou122:8020&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;         &lt;value&gt;hsiehchou121:50070&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;         &lt;value&gt;hsiehchou122:50070&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;         &lt;value&gt;qjournal://hsiehchou123:8485;hsiehchou124:8485/mycluster&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;         &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;         &lt;value&gt;sshfence&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;         &lt;value&gt;/root/.ssh/id_dsa&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;          &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>NameNode节点一般配置2台；<strong>qjournal</strong>—— journal节点一般配置3台<br>我这里开始只有四台，所以，journal节点我只分配了两台</p><h4 id="3、yarn-site-xml"><a href="#3、yarn-site-xml" class="headerlink" title="3、yarn-site.xml"></a>3、yarn-site.xml</h4><pre><code>&lt;configuration&gt;    &lt;!-- Site specific YARN configuration properties --&gt;    &lt;!-- Site specific YARN configuration properties --&gt;    &lt;property&gt;         &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;         &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;         &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;          &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;          &lt;value&gt;yarncluster&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;          &lt;value&gt;rm1,rm2&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;          &lt;value&gt;hsiehchou121&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;          &lt;value&gt;hsiehchou122&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;          &lt;value&gt;hsiehchou123,hsiehchou124&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;          &lt;value&gt;32768&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;        &lt;value&gt;32768&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;        &lt;value&gt;4096&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;        &lt;value&gt;24&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;        &lt;value&gt;/tmp/yarn-logs&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>scp -r  hadoop/ hsiehchou122:/root/hd/hadoop-2.8.4/etc<br>scp -r  hadoop/ hsiehchou123:/root/hd/hadoop-2.8.4/etc<br>scp -r  hadoop/ hsiehchou124:/root/hd/hadoop-2.8.4/etc</p><p>配置好后，分发到所有节点，启动ZooKeeper后<br>start-all.sh 即可启动所有</p><h3 id="二、HBase-HA"><a href="#二、HBase-HA" class="headerlink" title="二、HBase HA"></a>二、HBase HA</h3><p>修改配置文件，分发到所有几点，启动即可<br>注意：要启动两个Master，其中一个需要手动启动</p><p>注意：Hbase安装时，需要对应Hadoop版本</p><p>hbase hbase-2.1.4  对应 hadoop  2.8.4</p><p>通常情况下，把Hadoop  core-site hdfs-site 拷贝到hbase conf下</p><p>修改 hbase-env.sh<br>修改  hbase-site.xml</p><h4 id="1、hbase-env-sh"><a href="#1、hbase-env-sh" class="headerlink" title="1、hbase-env.sh"></a>1、hbase-env.sh</h4><p>export JAVA_HOME=/root/hd/jdk1.8.0_192</p><p>export HBASE_MANAGES_ZK=false<br>关闭hbase自带的zookeeper，使用集群zookeeper</p><h4 id="2、hbase-site-xml"><a href="#2、hbase-site-xml" class="headerlink" title="2、hbase-site.xml"></a>2、hbase-site.xml</h4><pre><code>&lt;configuration&gt;&lt;property&gt;&lt;name&gt;hbase.cluster.distributed&lt;/name&gt;&lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt;&lt;name&gt;hbase.rootdir&lt;/name&gt;&lt;value&gt;hdfs://mycluster/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt;&lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;&lt;value&gt;hsiehchou123,hsiehchou124&lt;/value&gt; &lt;/property&gt; &lt;property&gt;&lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;&lt;value&gt;2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt;&lt;name&gt;zookeeper.session.timeout&lt;/name&gt;&lt;value&gt;120000&lt;/value&gt; &lt;/property&gt; &lt;property&gt;&lt;name&gt;hbase.zookeeper.property.tickTime&lt;/name&gt;&lt;value&gt;6000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>启动HBase<br>需要从另一台服务器上单独启动Master<br>./hbase-daemon.sh start master</p><p>通过以下网站可以看到信息<br><a href="http://192.168.116.122:16010/master-status" target="_blank" rel="noopener">http://192.168.116.122:16010/master-status</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hadoop HA </tag>
            
            <tag> HBase HA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>内存数据库专题（MemCached和Redis）</title>
      <link href="/2019/04/18/nei-cun-shu-ju-ku-zhuan-ti-memcached-he-redis/"/>
      <url>/2019/04/18/nei-cun-shu-ju-ku-zhuan-ti-memcached-he-redis/</url>
      
        <content type="html"><![CDATA[<p><strong>内存数据库专题</strong><br>为什么要把数据存入内存？<br>快</p><p>常见的内存数据库：<br>MemCached：看成Redis前身，严格来说，MemCached不能叫数据库，只能叫缓存<br>不支持持久化。如果内存停电，数据丢失</p><p>Redis：内存数据库，支持持久化，支持HA</p><p>Oracle TimesTen</p><p>session一致性</p><p>MemCached + keepalive实现</p><h3 id="一、Memcached"><a href="#一、Memcached" class="headerlink" title="一、Memcached"></a>一、Memcached</h3><h4 id="1、基本原理和体系架构"><a href="#1、基本原理和体系架构" class="headerlink" title="1、基本原理和体系架构"></a>1、基本原理和体系架构</h4><p>（<em>）在内存中，维护了一张巨大的Hash表<br>（</em>）通过路由算法来决定数据存储的位置。—&gt; 客户端路由<br><img src="/medias/Memcached%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E5%92%8C%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84.PNG" alt="Memcached基本原理和体系架构"></p><p>注意：默认，官方版本MemCached实例彼此不会进行通信</p><p>第三方版本可以实现通信</p><h4 id="2、安装配置MemCached"><a href="#2、安装配置MemCached" class="headerlink" title="2、安装配置MemCached"></a>2、安装配置MemCached</h4><p>前提：<br>（1）gcc编译器<br>yum install gcc<br>gcc –version</p><p>（2）libevent库<br>tar -zxvf libevent-2.0.21-stable.tar.gz<br>cd libevent-2.0.21-stable<br>./configure <code>--prefix</code>=/root/hd/libevent<br>make<br>make install</p><p>tar -zxvf memcached-1.4.25.tar.gz<br>cd memcached-1.4.25</p><p>./configure <code>--prefix</code>=/root/hd/memcached <code>--with-libevent</code>=/root/hd/libevent<br>make<br>make install<br>cd bin/<br>./memcached -u root -d -m 128 -p 11211<br>./memcached -u root -d -m 128 -p 11212<br>./memcached -u root -d -m 128 -p 11213<br>ps -ef | grep memcached</p><p><strong>注意</strong>：<br>-u：root用户需要注明（其他用户可以不写）<br>-d：启动守护线程（在后天运行）<br>-m：占用多少内存<br>-p：运行在哪个端口    </p><h4 id="3、操作MemCached"><a href="#3、操作MemCached" class="headerlink" title="3、操作MemCached"></a>3、操作MemCached</h4><p>（*）命令行<br>telnet 192.168.116.121 11211</p><p>保存数据：<br>set 如果key存在，替换原来的值<br>add 如果key存在，返回错误</p><table><thead><tr><th>set key1</th><th align="center">0</th><th align="center">0</th><th align="center">4</th></tr></thead><tbody><tr><td>key名字</td><td align="center">标识位</td><td align="center">数据过期时间0表示不过期</td><td align="center">value的长度</td></tr><tr><td>abcd</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td>get key1</td><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table><p>统计命令<br>stats items<br>stats</p><pre><code>package day1;import net.spy.memcached.MemcachedClient;import net.spy.memcached.internal.OperationFuture;import java.io.IOException;import java.io.Serializable;import java.net.InetSocketAddress;import java.util.ArrayList;import java.util.List;import java.util.concurrent.ExecutionException;import java.util.concurrent.Future;/** * java 操作Memcached */public class Demo1 {    public static void main(String[] args) throws Exception {        //hello();        //testGet();        //setStudent();        testSets() ;    }</code></pre><p>[root@hsiehchou121 memcached]# cd bin/<br>[root@hsiehchou121 bin]# ./memcached -u root -d -m 128 -p 11211<br>[root@hsiehchou121 bin]# ./memcached -u root -d -m 128 -p 11212<br>[root@hsiehchou121 bin]# ./memcached -u root -d -m 128 -p 11213</p><p><strong>插入数据</strong></p><pre><code> public static void hello() throws Exception {        //连接到集群，set key        //建立MemcachedClient实例，指定Memcached服务器的IP地址和端口        MemcachedClient client = new MemcachedClient(new InetSocketAddress(&quot;192.168.116.121&quot;,11211));        Future&lt;Boolean&gt; f = client.set(&quot;key1&quot;, 0, &quot;hello World&quot;);        if (f.get().booleanValue()){            client.shutdown();        }    }</code></pre><p>[root@hsiehchou121 ~]# telnet 192.168.116.121 11211<br>get key1</p><p><strong>查询数据</strong></p><pre><code>public static void testGet() throws Exception{    //连接到集群，set key    //建立MemcachedClient实例，指定Memcached服务器的IP地址和端口    MemcachedClient client = new MemcachedClient(new InetSocketAddress(&quot;192.168.116.121&quot;,11211));    //按照key取值，不存在的话返回null    Object o = client.get(&quot;key1&quot;);    System.out.println(&quot;取到的值为： &quot; + o);    client.shutdown();}</code></pre><p>取到的值为：hello World</p><p><strong>插入类</strong></p><pre><code>   public static void setStudent() throws  Exception{        //连接到集群，set key        //建立MemcachedClient实例，指定Memcached服务器的IP地址和端口        MemcachedClient client = new MemcachedClient(new InetSocketAddress(&quot;192.168.116.121&quot;,11211));        Future&lt;Boolean&gt; f = client.set(&quot;stu1&quot;, 0, new Student());        if (f.get().booleanValue()){            client.shutdown();        }    }</code></pre><p>[root@hsiehchou121 ~]# telnet 192.168.116.121 11211<br>get stu1</p><p><strong>基于客户端的分布式插入数据</strong></p><pre><code> public static void testSets() throws Exception{        //测试客户端路由算法        //构造每台Memcached服务器加入List        List&lt;InetSocketAddress&gt; list = new ArrayList&lt;&gt;();        list.add(new InetSocketAddress(&quot;192.168.116.121&quot;,11211));        list.add(new InetSocketAddress(&quot;192.168.116.121&quot;,11212));        list.add(new InetSocketAddress(&quot;192.168.116.121&quot;,11213));        //建立Memcached Client实例        MemcachedClient client = new MemcachedClient(list);        for (int i = 0; i &lt; 20; i++){            System.out.println(&quot;插入数据：&quot; + i);            client.set(&quot;key&quot;+i,0, &quot;value&quot;+i);//(key1,value1)(key2,value2)            Thread.sleep(1000);        }        client.shutdown();    }</code></pre><p>[root@hsiehchou121 ~]# telnet 192.168.116.121 11211<br>get key1<br>VALUE key1 0 6<br>value1<br>[root@hsiehchou121 ~]# telnet 192.168.116.121 11212<br>get key2<br>VALUE key2 0 6<br>value2<br>[root@hsiehchou121 ~]# telnet 192.168.116.121 11213<br>get key3<br>VALUE key3 0 6<br>value3</p><pre><code>}class Student implements Serializable{}</code></pre><h4 id="4、MemCached路由算法"><a href="#4、MemCached路由算法" class="headerlink" title="4、MemCached路由算法"></a>4、MemCached路由算法</h4><p>1）<strong>求余数hash算法</strong><br>用key做hash运算得到一个整数，根据余数路由<br>例如：服务器端有三台MemCached服务器<br>根据key，做hash运算<br>7%3=1，那么就路由到第2台服务器<br>6%3=0，那么路由到第1台服务器<br>5%3=2，那么路由到第3台服务器</p><p>优点：数据分布均衡在多台服务器中，适合大多数据需求<br>缺点：如果需要扩容或者有宕机的情况，会造成数据的丢失</p><p>2）<strong>一致性hash算法</strong><br><strong>基本原理</strong>：<br>key1  1-333 放在node1上<br>key2 334-666放在node2上<br>key3 667-1000放在hsiehchou121上</p><p><strong>一致性hash算法下扩容</strong><br>key1  1-333 放在node1上<br>key2 334-666放在node2上<br>key3 667-831放在hsiehchou121上<br><strong>key4 832-1000放在node4上</strong><br>如果扩容，增加一个新的节点，只影响扩容的节点，对其他节点不影响</p><p><strong>一致性hash算法下DOWN机</strong><br>key1  1-333 放在node1上<br>key2 334-666放在node2上<br>key3 667-1000放在hsiehchou121上<br>如果宕机，对key1 和 key2不产生影响，只对key3产生影响</p><h4 id="5、MemCached的主主复制和HA"><a href="#5、MemCached的主主复制和HA" class="headerlink" title="5、MemCached的主主复制和HA"></a>5、MemCached的主主复制和HA</h4><p>1）Memcached主主复制<br><strong>架构图</strong>   </p><table><thead><tr><th align="center">主服务器</th><th align="center"></th><th align="center">主服务器</th></tr></thead><tbody><tr><td align="center">memcached</td><td align="center">&lt;——-&gt;</td><td align="center">memcached</td></tr></tbody></table><ol><li>安装具有复制功能的memcached版本<br>tar zxvf memcached-1.2.8-repcached-2.2.tar.gz<br>cd memcached-1.2.8-repcached-2.2<br>./configure <code>--prefix</code>=/root/hd/memcached_replication <pre><code>      `--with-libevent`=/root/hd/libevent/ `--enable-replication`</code></pre>make<br>make install</li></ol><p><strong>出现以下错误</strong></p><pre><code>memcached.c: 696: error: `IOV MAX` undeclared (first use in this function)memcached.c: 696: error: (Each undeclared identifier is reported only oncememcached.c: 696: error: for each function it appears in.)</code></pre><p><strong>解决办法</strong><br><strong>编辑memcached.c文件如下</strong><br>55 /* FreeBSD 4.x doesn’t have IOV_MAX exposed. */<br>56 #ifndef IOV_MAX<br>57 #if defined(<strong>FreeBSD</strong>) || defined(<strong>APPLE</strong>)<br>58 # define IOV_MAX 1024<br>59 #endif<br>60 #endif</p><p><strong>修改成如下形式</strong><br>55 /* FreeBSD 4.x doesn’t have IOV_MAX exposed. */<br>56 #ifndef IOV_MAX<br>57 //#if defined(<strong>FreeBSD</strong>) || defined(<strong>APPLE</strong>)<br>58 # define IOV_MAX 1024<br>59 //#endif<br>60 #endif</p><p>启动第一台MemCached，使用-x指定对端服务器的地址<br>./memcached -u root -d  -m 128 -x 192.168.116.121</p><p>启动第二台MemCached，使用-x指定对端服务器的地址<br>./memcached -u root -d  -m 128 -x 192.168.116.122</p><p><strong>出现以下错误</strong><br>./memcached: error while loading shared libraries: libevent-2.0.so.5: cannot open shared object file: No such file or directory</p><p><strong>解决办法</strong><br>查找 libevent-2.0.so.5<br>whereis libevent-2.0.so.5</p><p>使用ldd命令查看memcached命令，发现找不到<br>[root@hsiehchou121 bin]# ldd /root/hd/memcached-1.2.8-repcached-2.2/bin/memcached<br>linux-gate.so.1 =&gt; (0x00255000)<br>libevent-2.0.so.5 =&gt; not found<br>libc.so.6 =&gt; /lib/libc.so.6 (0x00110000)<br>/lib/ld-linux.so.2(0x003a4000)</p><p><strong>建立软连接</strong><br>ln -s /root/hd/libevent/lib/libevent-2.0.so.5 /usr/lib/libevent-2.0.so.5</p><p>2）Memcached的HA（High Availablity）<br>Keepalived是一个交换机制的软件。Keepalived的作用是检测服务器的状态，如果有一台web服务器死机，或工作出现故障，Keepalived将检测到，并将有故障的服务器从系统中剔除，同时使用其他服务器代替该服务器的工作，当服务器工作正常后Keepalived自动将服务器加入到服务器群中，这些工作全部自动完成，不需要人工干涉，需要人工做的只是修复故障的服务器</p><p>利用Keepalived实现MemCached的主主复制高可用架构<br>    Keepalived在memcached主服务器产生一个虚拟IP（VIP）<br>    Keepalived可以通过不断的检测memcached主服务器的11211端口是否正常工作，<br>    如果发现memcached Down机，虚拟IP就从主服务器移到从服务器</p><p>配置Keepalived（每台机器都要配置）<br>    rpm -ivh keepalived-1.2.13-4.el6.i686.rpm</p><p>    配置：主从节点都要配置，配置文件<br>    /etc/keepalived/keepalived.conf</p><p><strong>主节点配置信息</strong></p><pre><code>! Configuration File for keepalivedglobal_defs {    notification_email {      417952939@qq.com    }    notification_email_from collen_training@126.com    smtp_server 127.0.0.1    smtp_connect_timeout 30    router_id LVS_DEVEL}vrrp_instance VI_1 {    state MASTER    interface eth0    virtual_router_id 51    priority 101    advert_int 1    authentication {     auth_type PASS     auth_pass 1111    }    virtual_ipaddress {      192.168.116.88    }}</code></pre><p><strong>从节点配置信息</strong></p><pre><code>! Configuration File for keepalivedglobal_defs {    notification_email {       417952939@qq.com    }    notification_email_from collen_training@126.com    smtp_server 127.0.0.1    smtp_connect_timeout 30    router_id LVS_DEVEL}vrrp_instance VI_1 {    state MASTER    interface eth0    virtual_router_id 51    priority 100    advert_int 1    authentication {      auth_type PASS      auth_pass 1111    }    virtual_ipaddress {      192.168.116.88    }}</code></pre><p>验证Keepalived: 使用命令 <code>ip ad sh</code> 查看虚拟ip地址<br>inet 192.168.116.88/32 scope global eth0</p><h3 id="二、Redis"><a href="#二、Redis" class="headerlink" title="二、Redis"></a>二、Redis</h3><h4 id="1、Redis简介"><a href="#1、Redis简介" class="headerlink" title="1、Redis简介"></a>1、Redis简介</h4><p>（1）Redis的前身：Memcached<br>（2）和Memcached区别“<br>（<em>）支持持久化：RDB快照、AOF日志<br>（</em>）支持丰富的数据类型</p><h4 id="2、安装Redis"><a href="#2、安装Redis" class="headerlink" title="2、安装Redis"></a>2、安装Redis</h4><p>833  tar -zxvf redis-3.0.5.tar.gz<br>839  cd redis-3.0.5/<br>841  make<br>842  make PREFIX=/root/hd/redis install</p><p>redis-benchmark ：        Redis提供的压力测试工具。模拟产生客户端的压力<br>redis-check-aof：            检查aof日志文件<br>redis-check-dump：        检查rdb文件<br>redis-cli：                Redis客户端脚本<br>redis-sentinel：            哨兵<br>redis-server：            Redis服务器脚本</p><p>核心配置文件:redis.conf<br>[root@hsiehchou121 redis-3.0.5]# cp redis.conf /root/hd/redis/<br>[root@hsiehchou121 redis]# mkdir conf<br>[root@hsiehchou121 redis]# mv redis.conf conf/<br>[root@hsiehchou121 conf]# vi redis.conf </p><p>42 <strong>daemonize</strong> yes  //后台方式运行<br>50 port 6379</p><p>启动redis ./bin/redis-server conf/redis.conf<br>检测是否启动好<br>[root@hsiehchou121 redis]# ./bin/redis-server conf/redis.conf </p><h4 id="3、操作Redis"><a href="#3、操作Redis" class="headerlink" title="3、操作Redis"></a>3、操作Redis</h4><p>1）命令行<br>redis-cli<br>./bin/redis-cli</p><p>127.0.0.1:6379&gt; set key1 value1<br>OK<br>127.0.0.1:6379&gt; get key1<br>“value1”<br>127.0.0.1:6379&gt; keys *</p><p>1) “key1”</p><p>对数据的操作：<br>127.0.0.1:6379&gt; set money 100<br>OK<br>127.0.0.1:6379&gt; incr money<br>(integer) 101<br>127.0.0.1:6379&gt; get money<br>“101”<br>127.0.0.1:6379&gt; incrby money 10000<br>(integer) 10101</p><p>2）数据类型<br>①　字符串<br>127.0.0.1:6379&gt; set key1 “hello”<br>OK<br>127.0.0.1:6379&gt; get key1<br>“hello”<br>127.0.0.1:6379&gt;<br>127.0.0.1:6379&gt; append key1 “<code>*******</code>“<br>(integer) 12<br>127.0.0.1:6379&gt; get key1<br>“hello<code>*******</code>“</p><p>②　链表<br>127.0.0.1:6379&gt;<br>127.0.0.1:6379&gt; lpush list 11 22 33 44 55<br>(integer) 5<br>127.0.0.1:6379&gt; lrange list 0 2</p><p>1) “55”<br>2) “44”<br>3) “33”<br>127.0.0.1:6379&gt; lrange list 0 -1</p><p>1) “55”<br>2) “44”<br>3) “33”<br>4) “22”<br>5) “11”<br>127.0.0.1:6379&gt; lpop list<br>“55”</p><p>③　Hash<br>127.0.0.1:6379&gt; hset hashkey1 name ls<br>(integer) 1<br>127.0.0.1:6379&gt; hset hashkey2 age 23<br>(integer) 1<br>127.0.0.1:6379&gt; hmset user001 name ls age 23 gender mals<br>OK<br>127.0.0.1:6379&gt; hmset user002 name xz age 24 gender mals<br>OK<br>127.0.0.1:6379&gt; hmget user001 name age gender</p><p>1) “ls”<br>2) “23”<br>3) “mals”<br>127.0.0.1:6379&gt; hgetall user001</p><p>1) “name”<br>2) “ls”<br>3) “age”<br>4) “23”<br>5) “gender”<br>6) “mals”</p><p>④　无序集合<br>无序，不可重复的集合<br>127.0.0.1:6379&gt; sadd setkey1 11 22 33 44 55<br>(integer) 5<br>127.0.0.1:6379&gt; sadd setkey2 33 44 55 66 77 88<br>(integer) 6<br>127.0.0.1:6379&gt; smembers setkey1</p><p>1) “11”<br>2) “22”<br>3) “33”<br>4) “44”<br>5) “55”</p><p>sdiif： 差集<br>sinter： 交集<br>suntion：并集</p><p>⑤　有序集合<br>有序可以重复的集合，根据一个score来进行排序<br>127.0.0.1:6379&gt; zadd chinese 90 Tom 92 Nary 83 Nike<br>(integer) 3<br>127.0.0.1:6379&gt; zrange chinese 0 100</p><p>1) “Nike”<br>2) “Tom”<br>3) “Nary”<br>127.0.0.1:6379&gt; zrange chinese 0 100 withscores</p><p>1) “Nike”<br>2) “83”<br>3) “Tom”<br>4) “90”<br>5) “Nary”<br>6) “92”</p><p>⑥　Redis数据类型案例分析：网站统计用户登录的次数<br>a.    1亿个用户，有经常登录的，也有不经常登录的<br>b.    如何来记录用户的登录信息<br>c.    如何查询活跃用户：比如：一周内，登录3次的</p><p>    解决方案一：采用关系型数据库<br>建立表：记录一周内，每天登录的情况</p><p>采用关系型数据库保存登录信息存在的问题，每天产生一亿条数据，一周就是7亿条数据</p><p>    解决方案二：采用Redis存储登录信息<br>可以使用Redis的setbit，登录与否：有1和0就可以表示<br>一亿个用户，每天是否登录，用1或者0表示即可，每天产生约12M的数据<br>一亿个用户一周的登录信息：<br>12M*7=84M</p><pre><code>3）Java Api①　基本操作    @Test    public void testString(){        Jedis jedis = new Jedis(&quot;192.168.116.121&quot;,6379);        //添加数据        jedis.set(&quot;name&quot;,&quot;ls&quot;);//向key--&gt;name中放入了value--&gt;ls        System.out.println(jedis.get(&quot;name&quot;));//执行结果：ls        jedis.append(&quot;name&quot;,&quot; is my lover&quot;);//拼接        System.out.println(jedis.get(&quot;name&quot;));//执行结果：ls is my lover        jedis.del(&quot;name&quot;);//删除某个键        System.out.println(jedis.get(&quot;name&quot;));//执行结果：null        //设置多个键值对        jedis.mset(&quot;name&quot;,&quot;tom&quot;,&quot;age&quot;,&quot;23&quot;,&quot;qq&quot;,&quot;123456789&quot;);        jedis.incr(&quot;age&quot;);//进行加1操作        System.out.println(jedis.get(&quot;name&quot;) + &quot;-&quot; + jedis.get(&quot;age&quot;) + &quot;-&quot; + jedis.get(&quot;qq&quot;));//执行结果：tom-24-123456789        jedis.disconnect();    }</code></pre><p>②　连接池</p><pre><code>public class RedisUtils {    private static JedisPool jedisPool = null;    static {        try {            JedisPoolConfig config = new JedisPoolConfig();            //可用连接实例的最大数目，默认值为8 如果赋值为-1，则表示不限制            config.setMaxTotal(1024);            //控制一个pool最多有多少个状态为idle（空闲的）的jedis实例，默认值也是8            config.setMaxIdle(200);            //等待可用连接的最大时间，单位毫秒，默认值为-1，表示永不超时            config.setMaxWaitMillis(10000);            //在borrow一个jedis实例时，是否提前进行validate操作，如果为true，则得到的jedis实例均是可用的            config.setTestOnBorrow(true);            jedisPool = new JedisPool(config, &quot;192.168.116.121&quot;);        }catch (Exception e) {            e.printStackTrace();        }    }    public synchronized static Jedis getJedis(){        try {            if (jedisPool != null)                return jedisPool.getResource();        }catch (Exception e){            e.printStackTrace();        }        return null;    }    public static void returnResource(final Jedis jedis){        if (jedis != null)            jedisPool.returnResource(jedis);    }}</code></pre><p>③　使用Redis实现分布式锁<br>使用Maven搭建工程：</p><pre><code>&lt;dependency&gt;    &lt;groupId&gt;redis.clients&lt;/groupId&gt;    &lt;artifactId&gt;jedis&lt;/artifactId&gt;    &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt;</code></pre><h4 id="4、Redis的事务：不是真正的事务，是一种模拟"><a href="#4、Redis的事务：不是真正的事务，是一种模拟" class="headerlink" title="4、Redis的事务：不是真正的事务，是一种模拟"></a>4、Redis的事务：不是真正的事务，是一种模拟</h4><p>1）复习：事务（关系型数据库）<br>（*）什么是事务？<br>事务有一组DML(Data Manipulation Language)语句组成。DML 插入更新删除操作</p><p>（*）事务的特点<br>要么都成功，要么都失败</p><p>（*）Oracle中事务的本质：将事务的DML操作写入日志。日志写入成功，则事务执行成功</p><p>2）Redis事务的本质：将一组操作放入队列中，一次执行（批处理）</p><p>3）对比Oracle和Redis事务的区别</p><table><thead><tr><th align="center">操作</th><th align="center">Oracle</th><th align="center">Redis</th></tr></thead><tbody><tr><td align="center">开启事务</td><td align="center">自动开启</td><td align="center">命令：multi</td></tr><tr><td align="center">执行语句</td><td align="center">DML</td><td align="center">Redis命令</td></tr><tr><td align="center">提交事务</td><td align="center">显式提交：commit；隐式提交:DDL语句（create table）</td><td align="center">命令：exec 执行放在multi里面的操作</td></tr><tr><td align="center">回滚事务</td><td align="center">显式回滚：rollback；隐式回滚：系统掉电，客户端退出</td><td align="center">命令：discard 把队列中的操作废弃掉</td></tr></tbody></table><p><strong>注意</strong>：不是真正的事务，只是一种模拟</p><p>4）举例：模拟银行转账<br>set tom 1000<br>set mike 1000<br>tom –&gt; mike 转账操作必须在事务中，要么都成功，要么都不成功<br>multi<br>decrby tom 100<br>incrby mike 100<br>exec</p><p>127.0.0.1:6379&gt; set tom 1000<br>OK<br>127.0.0.1:6379&gt; set mike 1000<br>OK<br>127.0.0.1:6379&gt; multi<br>OK<br>127.0.0.1:6379&gt; decrby tom 100<br>QUEUED<br>127.0.0.1:6379&gt; incrby mike 100<br>QUEUED<br>127.0.0.1:6379&gt; exec</p><p>1) (integer) 900<br>2) (integer) 1100</p><p>5）举例：买票<br>set tom 1000<br>set ticket 1<br>multi<br>decrby tom 500<br>decr ticket<br>exec</p><p>在exec前，从另一个窗口，decr ticket<br>127.0.0.1:6379&gt; set tom 1000<br>OK<br>127.0.0.1:6379&gt; set ticket 1<br>OK<br>127.0.0.1:6379&gt; multi<br>OK<br>127.0.0.1:6379&gt; decrby tom 500<br>QUEUED<br>127.0.0.1:6379&gt; decr ticket<br>QUEUED<br>127.0.0.1:6379&gt; exec</p><p>1) (integer) 500<br>2) (integer) -1</p><h4 id="5、Redis锁机制"><a href="#5、Redis锁机制" class="headerlink" title="5、Redis锁机制"></a>5、Redis锁机制</h4><p>执行事务操作的时候，如果监视的值发生了变化，则提交失败<br>命令：<strong>watch</strong></p><p>举例：<strong>买票</strong><br>set tom 1000<br>set ticket 1<br>watch ticket —–&gt; 相当于给ticket加了锁。认为在下面执行事务的时候，值不会变<br>multi<br>decrby tom 500<br>decr ticket<br>exec</p><p>127.0.0.1:6379&gt; set tom 1000<br>OK<br>127.0.0.1:6379&gt; set ticket 1<br>OK<br>127.0.0.1:6379&gt; watch ticket<br>OK<br>127.0.0.1:6379&gt; multi<br>OK<br>127.0.0.1:6379&gt; decrby tom 500<br>QUEUED<br>127.0.0.1:6379&gt; decr ticket<br>QUEUED<br>127.0.0.1:6379&gt; exec<br>(nil)<br>127.0.0.1:6379&gt; get tom<br>“1000”</p><p><code>nil</code> 代表操作没有执行或者执行失败</p><p><strong>Java应用程序中的事务和锁</strong><br>①　事务</p><pre><code>@Testpublic void testTransaction(){    Jedis jedis = new Jedis(&quot;192.168.116.121&quot;,6379);    Transaction tc = null;    try{        //开启事务        tc = jedis.multi();        tc.decrBy(&quot;tom&quot;, 100);        tc.incrBy(&quot;mike&quot;, 100);        //提交事务        tc.exec();    }catch (Exception e){        e.printStackTrace();        //回滚事务        tc.discard();    }    jedis.disconnect();}</code></pre><p>②　锁</p><pre><code> @Test    public void testLock(){        Jedis jedis = new Jedis(&quot;192.168.116.121&quot;,6379);        //对ticket加锁，如果在事务执行过程中，该值有变化，则抛出异常        jedis.watch(&quot;ticket&quot;);        Transaction tc = null;        try{            //开启事务            tc = jedis.multi();            tc.decr(&quot;ticket&quot;);//车票数量减一            Thread.sleep(5000);            tc.decrBy(&quot;tom&quot;, 100);//扣tom 100块钱买票的钱            //提交事务            tc.exec();        }catch (Exception e){            e.printStackTrace();            //回滚事务            tc.discard();        }        jedis.disconnect();    }</code></pre><h4 id="6、Redis的消息机制：消息系统"><a href="#6、Redis的消息机制：消息系统" class="headerlink" title="6、Redis的消息机制：消息系统"></a>6、Redis的消息机制：消息系统</h4><p>1）消息的类型<br>（<em>）Queue消息：队列，点对点<br>（</em>）Topic消息：主题，群发：发布消息，订阅消息</p><p>2）Redis消息机制<br>只支持Topic消息</p><p>命令：发布消息 publish     格式：publish channel名称 “消息内容”</p><p>订阅：subscribe     格式：subscribe channel名称</p><p>psubscribe 订阅消息  —— 可以用通配符来订阅消息<br>    格式：psubscribe channel*名称</p><p>3）常用的消息系统：<br>Redis 只支持 Topic<br>Kafka 只支持Topic 需要Zookeeper支持<br>JMS Java Messging Service java消息服务标准。支持Queue Topic<br>产品：Weblogic</p><p>例子：<br>窗口1（发）：<br>127.0.0.1:6379&gt; PUBLISH c1 hello<br>(integer) 2<br>127.0.0.1:6379&gt; PUBLISH c1 test<br>(integer) 2</p><p>窗口2（订）：<br>127.0.0.1:6379&gt; SUBSCRIBE c1<br>Reading messages… (press Ctrl-C to quit)</p><p>1) “subscribe”<br>2) “c1”<br>3) (integer) 1</p><p>1) “message”<br>2) “c1”<br>3) “hello”</p><p>1) “message”<br>2) “c1”<br>3) “test”</p><p>窗口3（订）：<br>127.0.0.1:6379&gt; SUBSCRIBE c1<br>Reading messages… (press Ctrl-C to quit)</p><p>1) “subscribe”<br>2) “c1”<br>3) (integer) 1</p><p>1) “message”<br>2) “c1”<br>3) “hello”</p><p>1) “message”<br>2) “c1”<br>3) “test”</p><p><strong>通过通配符订阅</strong><br>窗口1（发）：<br>127.0.0.1:6379&gt; PUBLISH c2 hello<br>(integer) 1<br>127.0.0.1:6379&gt; PUBLISH c4 hello<br>(integer) 1</p><p>窗口2（发）：<br>127.0.0.1:6379&gt; PUBLISH c1 dfg<br>(integer) 1</p><p>窗口3（订）：<br>127.0.0.1:6379&gt; PSUBSCRIBE c*<br>Reading messages… (press Ctrl-C to quit)</p><p>1) “psubscribe”<br>2) “c*”<br>3) (integer) 1</p><p>1) “pmessage”<br>2) “c*”<br>3) “c2”<br>4) “hello”</p><p>1) “pmessage”<br>2) “c*”<br>3) “c4”<br>4) “hello”</p><p>1) “pmessage”<br>2) “c*”<br>3) “c1”<br>4) “dfg”</p><p><strong>使用Java程序实现消息的发布与订阅</strong><br>需要继承JedisPubSub类</p><pre><code>@Test    public void testMessage(){        Jedis jedis = new Jedis(&quot;192.168.116.121&quot;, 6379);        //subscribe和psubcribe不能同时订阅        jedis.subscribe(new MyListener(), &quot;channel&quot;);        //jedis.psubscribe(new MyListener(), &quot;channel*&quot;);    }    class MyListener extends JedisPubSub{        public void onMessage(String channel, String message){            System.out.println(&quot;onMessage channel is &quot; + channel + &quot; message is &quot; + message);        }        public void onPMessage(String pattern, String channel, String message){            System.out.println(&quot;onPMessage channel is &quot; + pattern);            System.out.println(&quot;onPMessage channel is &quot; + channel);            System.out.println(&quot;onPMessage message is &quot; + message);        }        public void onPSubscribe(String arg0, int arg1){}        public void onPUnsubscribe(String arg0, int arg1){}        public void onSubscribe(String arg0, int arg1){}        public void onUnsubscribe(String arg0, int arg1){}    }</code></pre><h4 id="7、Redis持久化"><a href="#7、Redis持久化" class="headerlink" title="7、Redis持久化"></a>7、Redis持久化</h4><p>本质：<strong>备份和恢复</strong><br>1）<strong>RDB快照</strong>：默认<br>（*）看成一种快照，备份。每隔段时间，将内存汇总的数据保存到硬盘上。产生RDB文件</p><p>（<em>）*</em>RDB 生成策略**<br><strong>redis.conf中</strong><br>147 save 900 1       900秒内，有1个key发生变化，执行RDB<br>148 save 300 10      300内，如果有10个key发生变化，执行RDB<br>149 save 60 10000    60秒内，如果有10000个key发生变化，执行RDB</p><p>save —- 时间 —– 发生变化的key的个数</p><p>（*）其他参数<br>164 stop-writes-on-bgsave-error yes  当后台写进程出错时，禁止写入新的数据</p><p>170 rdbcompression yes      是否压缩。如果看重性能，设置成no<br>    压缩会节省空间，但会影响备份和恢复性能</p><p>182 dbfilename dump.rdb  RDB的文件名字<br>192 dir ./  RDB的文件地址</p><p>（*）RDB的优点和缺点<br>优点：快，恢复速度快<br>缺点：在两次RDB之间，可能会造成数据的丢失<br>解决：AOF</p><p>2）<strong>AOF日志</strong><br>客户端在操作Redis时，把操作记录到文件中，如果发生崩溃，读取日志，把操作完全执行一遍</p><p>（*）默认是禁用<br>509 appendonly no  参数修改成yes</p><p>（*）AOF记录策略<br>538 # appendfsync always           每条操作都记录日志：优点安全 缺点：慢<br>539 appendfsync everysec    每秒写入一次<br>540 # appendfsync no            由操作系统来决定记录日志的方式。不会用的到。</p><p>（*）AOF日志重写：overwrite<br>举例：<br>set money 0<br>incr money<br>..100次</p><p>set money 100</p><p> ./redis-benchmark -n 100000<br> 模拟客户端100000次请求</p><p>（<em>）参数设置<br>561 no-appendfsync-on-rewrite *</em>no**   执行重写的时候，不写入新的aof日志<br>//561 no-appendfsync-on-rewrite yes  生成rdb的时候，是否不写入aof<br>580 auto-aof-rewrite-percentage 100 aof文件比上次重写时，超过的百分比<br>581 auto-aof-rewrite-min-size 64mb  执行重写的文件大小。到64M触发重写</p><p>3）当两个同时存在时，优先执行哪个？<br>504 # If the AOF is enabled on startup Redis will load the AOF, that is the file<br>505 # with the better durability guarantees.</p><p><strong>AOF开启时，优先使用AOF</strong></p><h4 id="8、Redis的主从复制"><a href="#8、Redis的主从复制" class="headerlink" title="8、Redis的主从复制"></a>8、Redis的主从复制</h4><p>1）<strong>Redis主从复制集群</strong><br>作用：<br>主从复制，主从备份，防止主节点down机<br>任务分离：分摊主节点压力。读写分离</p><p>Memcacached：主主复制<br>Redis：主从复制</p><p>Redis集群两种部署方式<br><strong>星型模型</strong>：<br>优点：效率高，两个slave地位一样，可以直接从主节点取出信息<br>缺点：HA比较麻烦</p><p><strong>线性模型</strong>：<br>优点：HA简单，master宕机后，可以直接切换到slave1<br>缺点：效率不如星型模型</p><p>cp redis.conf redis6379.conf<br>cp redis.conf redis6380.conf<br>cp redis.conf redis6381.conf </p><p><strong>主节点</strong>（redis6379.conf ）：关闭rdb aof<br>509 appendonly no<br>147 #save 900 1<br>148 #save 300 10<br>149 #save 60 10000</p><p><strong>从节点</strong>（redis6380.conf redis6381.conf）<br>不同机器有的可以不改<br>改端口号<br>50 port 6380<br>改aof rdb文件名：<br>182 dbfilename dump6380.rdb<br>211 slaveof 192.168.116.121 6379<br>513 appendfilename “appendonly6380.aof”</p><p><strong>改端口号</strong><br>50 port 6381<br>改aof rdb文件名：<br>182 dbfilename dump6381.rdb<br>211 slaveof 192.168.116.121 6379<br>513 appendfilename “appendonly6381.aof”</p><p>[root@hsiehchou121 redis]# ./bin/redis-server ./conf/redis6379.conf<br>[root@hsiehchou121 redis]# ./bin/redis-server ./conf/redis6380.conf<br>[root@hsiehchou121 redis]# ./bin/redis-server ./conf/redis6381.conf </p><p>[root@hsiehchou121 redis]# ps -ef | grep redis<br>root       6371      1  1 16:31 ?        00:00:00 ./bin/redis-server *:6379<br>root       6375      1  4 16:31 ?        00:00:01 ./bin/redis-server *:6380<br>root       6381      1  0 16:31 ?        00:00:00 ./bin/redis-server *:6381<br>root       6395   5432  0 16:31 pts/0    00:00:00 grep –color=auto redis</p><p>[root@hsiehchou121 redis]# ./bin/redis-cli -p 6379<br>127.0.0.1:6379&gt; set tom 10000<br>OK<br>127.0.0.1:6379&gt; quit<br>[root@hsiehchou121 redis]# ./bin/redis-cli -p 6380<br>127.0.0.1:6380&gt; get tom<br>“10000”</p><p>默认情况下，从节点只读，不可以写入数据</p><p>注意：一次性启动从节点不要太多</p><p><img src="/medias/Redis%20%E4%B8%BB%E4%BB%8E%E6%9C%8D%E5%8A%A1%E7%9A%84%E9%80%9A%E4%BF%A1%E5%8E%9F%E7%90%86.PNG" alt="Redis 主从服务的通信原理">            </p><p>2）<strong>Redis的分片</strong><br>多个从节点分摊读的压力</p><p>客户端代理分片工具：Twemproxy</p><p><strong>解压</strong><br>[root@hsiehchou121 nutcracker-0.3.0]# ./configure <code>--prefix</code>=/root/hd/nutcracker<br>[root@hsiehchou121 nutcracker-0.3.0]# make<br>[root@hsiehchou121 nutcracker-0.3.0]# make install</p><p>cp /root/hd/nutcracker-0.3.0/conf/nutcracker.yml ./conf/</p><p><strong>修改server信息</strong></p><pre><code>alpha:  listen: 127.0.0.1:22121  hash: fnv1a_64  distribution: ketama  auto_eject_hosts: true  redis: true  server_retry_timeout: 2000  server_failure_limit: 1  servers:   - 192.168.116.121:6380:1   - 192.168.116.121:6381:1</code></pre><p><strong>启动redis</strong><br>[root@hsiehchou121 redis]# ./bin/redis-server ./conf/redis6379.conf<br>[root@hsiehchou121 redis]# ./bin/redis-server ./conf/redis6380.conf<br>[root@hsiehchou121 redis]# ./bin/redis-server ./conf/redis6381.conf </p><p><strong>检查配置文件是否正确</strong><br>[root@hsiehchou121 nutcracker]# ./sbin/nutcracker -t conf/nutcracker.yml</p><p><strong>启动代理分片</strong><br>[root@hsiehchou121 nutcracker]# ./sbin/nutcracker -d -c conf/nutcracker.yml</p><p>[root@hsiehchou121 redis]# ./bin/redis-cli -p 22121 访问</p><h4 id="9、Redis的HA（哨兵机制）"><a href="#9、Redis的HA（哨兵机制）" class="headerlink" title="9、Redis的HA（哨兵机制）"></a>9、Redis的HA（哨兵机制）</h4><p><strong>主从结构，存在单点故障问题</strong></p><p>redis2.4版本之后有</p><p>redis-sentinel 就是哨兵</p><p>vi redis6380.conf<br>211 slaveof 192.168.116.121 6379</p><p>vi redis6381.conf<br>211 slaveof 192.168.116.121 6379</p><p><strong>配置</strong>：<br>cp /root/hd/redis-3.0.5/sentinel.conf  ./conf/</p><p>vim sentinel.conf </p><p>53 sentinel monitor mymaster 192.168.116.121 6379 1</p><p>[root@hsiehchou121 redis]# ./bin/redis-sentinel conf/sentinel.conf</p><p>看日志：<br>3085:X 23 Apr 17:04:17.522 # +monitor master mymaster 192.168.116.121 6379 quorum 1<br>3085:X 23 Apr 17:04:18.524 * +slave slave 192.168.116.121:6380 192.168.116.121 6380 @ mymaster 192.168.116.121 6379<br>3085:X 23 Apr 17:04:18.526 * +slave slave 192.168.116.121:6381 192.168.116.121 6381 @ mymaster 192.168.116.121 6379</p><p>kill master 检测到<br>看日志：<br>try-failover master mymaster 192.168.109.134 6379<br>检测到6379挂了</p><p>3085:X 23 Apr 17:05:14.647 # +selected-slave slave 192.168.116.121:6381 192.168.116.121 6381 @ mymaster 192.168.116.121 6379<br>select slave 选举新的主节点</p><p>3085:X 23 Apr 17:05:16.830 * +slave slave 192.168.116.121:6380 192.168.116.121 6380 @ mymaster 192.168.116.121 6381<br>把其他的从节点连接到主节点上</p><p><strong>注意</strong>:一定要按步骤来，一步一步配置</p><p><strong>亲测排坑</strong><br><strong>划重点</strong><br>此处的Redis的HA高可用的redis主节点和从节点的变化会导致<strong>sentinel monitor mymaster</strong>（sentinel.conf的第53行）和<strong>slaveof</strong>一起变化（从节点的第211行）。而且这个过程是不可逆的，就是更新了变只有自己手动去修改下。</p><p>所以，如果停止重新运行，便会报错，需要自己自行修改这些内容。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> 内存数据库 </tag>
            
            <tag> MemCached </tag>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark MLlib和Spark GraphX</title>
      <link href="/2019/04/11/spark-mllib-he-spark-graphx/"/>
      <url>/2019/04/11/spark-mllib-he-spark-graphx/</url>
      
        <content type="html"><![CDATA[<p><strong>Spark MLlib</strong></p><p>MLlib 是 Spark 可以扩展的机器学习库</p><p>MLlib is Apache Spark’s scalable machine learning library.</p><h3 id="一、MLlib概述"><a href="#一、MLlib概述" class="headerlink" title="一、MLlib概述"></a>一、MLlib概述</h3><p>MLlib 是 Spark 可以扩展的机器学习库</p><p>Spark在机器学习方面具有得天独厚的有事，有以下几个原因：</p><h4 id="1、机器学习算法"><a href="#1、机器学习算法" class="headerlink" title="1、机器学习算法"></a>1、机器学习算法</h4><p>一般都有多个步骤迭代计算，需要在多次迭代后，获得足够小的误差或者收敛才会停止</p><pre><code>double wucha = 1.0while(wucha&gt;=0.00001){    建模  wucha -= 某个值}</code></pre><p>模型计算完毕</p><p>当迭代使用Hadoop的MapReduce计算框架时，每次都要读写硬盘以及任务启动工作，导致很大的IO开销</p><p>而Spark基于内存的计算模型天生擅长迭代计算。只有在必要时，才会读写硬盘</p><p>所以Spark是机器学习比较理想的平台</p><h4 id="2、通信"><a href="#2、通信" class="headerlink" title="2、通信"></a>2、通信</h4><p>Hadoop的MapReduce计算框架，通过heartbeat方式来进行通信和传递数据，执行速度慢</p><p>spark 有高效的 Akka 和 Netty 的通信系统，通行效率高</p><p>Spark MLlib 是Spark 对常用的机器学习算法的实现库，同时包括相关测试和数据生成器</p><h3 id="二、什么是机器学习"><a href="#二、什么是机器学习" class="headerlink" title="二、什么是机器学习"></a>二、什么是机器学习</h3><h4 id="1、机器学习的定义"><a href="#1、机器学习的定义" class="headerlink" title="1、机器学习的定义"></a>1、机器学习的定义</h4><p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P,<br>if its performance at tasks in T, as measured by P, improves with experience E</p><p>三个关键词：算法、经验、模型评价</p><p>在数据的基础上，通过算法构建出模型，并进行评价<br>如果达到要求，则用该模型测试其他数据<br>如果不达到要求，要调整算法来重新建立模型，再次进行评估<br>循环往复，知道获得满意的经验</p><p>应用：金融反欺诈、语音识别、自然语言处理、翻译、模式识别、智能控制等等</p><h4 id="2、基于大数据的机器学习"><a href="#2、基于大数据的机器学习" class="headerlink" title="2、基于大数据的机器学习"></a>2、基于大数据的机器学习</h4><p>传统的机器学习算法，由于技术和单机存储的现值，只能在少量数据上使用<br>即，依赖于数据抽样<br>问题：很难做好随机，导致学习的模型不准确</p><p>在大数据上进行机器学习，直接处理全量数据并进行大量迭代计算</p><p>Spark本身计算优势，适合机器学习</p><p>另外 spark-shell pyspark 都可以提供及时查询工具</p><h4 id="3、MLlib"><a href="#3、MLlib" class="headerlink" title="3、MLlib"></a>3、MLlib</h4><p>MLlib是Spark机器学习库，简化机器学习的工程实践工作，方便扩展到更大规模<br>集成了通用的学习算法：分类、回归、聚类、协同过滤、降维等等</p><p>另外，MLlib本身在Spark中，数据清洗、SQL、建模放在一起</p><p><strong>sample_linear_regression_data.txt</strong><br>1 1:1.9<br>2 1:3.1<br>3 1:4<br>3.5 1:4.45<br>4 1:5.02<br>9 1:9.97<br>-2 1:-0.98</p><pre><code>package day7import org.apache.spark.sql.SparkSessionimport org.apache.spark.ml.regression.LinearRegression/* * 1.3850645873427236 1:0.14476184437006356 2:-0.11280617018445871 3:-0.4385084538142101 4:-0.5961619435136434 5:0.419554626795412 6:-0.5047767472761191 7:0.457180284958592 8:-0.9129360314541999 9:-0.6320022059786656 10:-0.44989608519659363 *  */object Demo1 {  def main(args: Array[String]): Unit = {    val spark = SparkSession.builder().appName(&quot;Demo1&quot;).master(&quot;local&quot;).getOrCreate()    val data_path = &quot;H:\\sample_linear_regression_data.txt&quot;    //读取训练数据    val trainning = spark.read.format(&quot;libsvm&quot;).load(data_path)    //定义模型    val lr = new LinearRegression().setMaxIter(10000)    //训练模型    val lrModel = lr.fit(trainning)    //获取模型训练结果    val trainningSummary = lrModel.summary    //获取预测值    trainningSummary.predictions.show()    //获取误差    print(trainningSummary.rootMeanSquaredError)    spark.stop()  }}</code></pre><h2 id="Spark-Graphx"><a href="#Spark-Graphx" class="headerlink" title="Spark Graphx"></a>Spark Graphx</h2><h3 id="一、Spark-Graphx-是什么？"><a href="#一、Spark-Graphx-是什么？" class="headerlink" title="一、Spark Graphx 是什么？"></a>一、Spark Graphx 是什么？</h3><p>1、是Spark 的一个模块，主要用于进行以图为核心的计算，还有分布式图计算</p><p>2、Graphx 底层基于RDD计算，和RDD共用一种存储形态。在展示形态上，可以用数据集来表示，也可以用图来表示</p><h3 id="二、Spark-GraphX-有哪些抽象？"><a href="#二、Spark-GraphX-有哪些抽象？" class="headerlink" title="二、Spark GraphX 有哪些抽象？"></a>二、Spark GraphX 有哪些抽象？</h3><h4 id="1、顶点"><a href="#1、顶点" class="headerlink" title="1、顶点"></a>1、顶点</h4><p>RDD[(VertexId,VD)]表示<br>VertexId 代表了顶点的ID，是Long类型<br>VD 是顶点的属性，可以是任何类型</p><h4 id="2、边"><a href="#2、边" class="headerlink" title="2、边"></a>2、边</h4><p>RDD[Edge[ED]]表示<br>Edge表示一个边<br>包含一个ED类型参数来设定属性<br>另外，边还包含了源顶点ID和目标顶点ID</p><h4 id="3、三元组"><a href="#3、三元组" class="headerlink" title="3、三元组"></a>3、三元组</h4><p>三元组结构用RDD[EdgeTriplet[VD,ED]]表示<br>三元组包含一个边、边的属性、源顶点ID、源顶点属性、目标顶点ID、目标顶点属性</p><h4 id="4、图"><a href="#4、图" class="headerlink" title="4、图"></a>4、图</h4><p>Graph表示，通过顶点和边来构建</p><pre><code>package day7import org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.graphx.Edgeimport org.apache.spark.graphx.Graphobject Demo2 {  def main(args: Array[String]): Unit = {    val conf = new SparkConf().setAppName(&quot;Demo2&quot;).setMaster(&quot;local&quot;)    //创建Spark Context对象    val sc = new SparkContext(conf)    //定义点    val users = sc.parallelize(Array((3L,(&quot;TIme&quot;,&quot;student&quot;)),(5L,(&quot;Andy&quot;,&quot;student&quot;)),        (7L,(&quot;Mary&quot;,&quot;student&quot;)),(2L,(&quot;Lily&quot;,&quot;post&quot;))))    //定义边    val relationship = sc.parallelize(Array(Edge(3L,7L,&quot;col&quot;),Edge(5L,3L,&quot;ad&quot;),Edge(2L,5L,&quot;col&quot;),Edge(5L,7L,&quot;heh&quot;)))     //构建图    val graph = Graph(users, relationship)    //图的操作    val post_count = graph.vertices.filter{ case (id,(name,pos)) =&gt; pos==&quot;post&quot;}.count    println(&quot;post count is &quot; + post_count)    val edges_count = graph.edges.filter(e =&gt; e.srcId &gt; e.dstId).count()    println(&quot;the value is &quot; + edges_count)  }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 调优</title>
      <link href="/2019/04/07/spark-diao-you/"/>
      <url>/2019/04/07/spark-diao-you/</url>
      
        <content type="html"><![CDATA[<p><strong>Spark 调优</strong>    </p><p>问题：只要会用就可以，为什么还要精通内核源码与调优？<br>Spark 性能优化概览：<br>Spark的计算本质是，分布式计算<br>所以，Spark程序的性能可能因为集群中的任何因素出现瓶颈：CPU、网络带宽、或者内存</p><p>CPU、网络带宽，是运维来维护的<br>聚焦点：内存</p><p>如果内存能够容纳下所有的数据，那就不需要调优了<br>如果内存比较紧张，不足以放下所有数据（10亿量级—500G）,需要对内存的使用进行性能优化<br>比如：使用某些方法减少内存的消耗</p><p>Spark性能优化，主要针对在内存的使用调优</p><p>Spark性能优化的技术：<br>1、使用高性能序列化类库<br>2、优化数据结构<br>3、对于多次使用的RDD进行持久化、checkpoint<br>4、持久化级别：MEMORY_ONLY  —&gt;  MEMORY_ONLY_SER 序列化<br>5、Java虚拟机垃圾回收调优<br>6、Shuffle调优，1.x版本中，90%的性能问题，都是由于Shuffle导致的。</p><p>其他性能优化：<br>1、提高并行度<br>2、广播共享数据<br>等等。。。</p><h3 id="一、诊断Spark内存使用"><a href="#一、诊断Spark内存使用" class="headerlink" title="一、诊断Spark内存使用"></a>一、诊断Spark内存使用</h3><p>首先要看到内存使用情况，才能进行针对性的优化</p><h4 id="1、内存花费"><a href="#1、内存花费" class="headerlink" title="1、内存花费"></a>1、内存花费</h4><p>（1）每个Java对象，都有一个对象头，占用16字节，包含一些对象的元信息，比如指向他的类的指针<br>如果对象本身很小，比如int，但是他的对象头比对象自己还大</p><p>（2）Java的String对象，会比他内存的原始数据，多出40个字节<br>String内部使用的char数组来保存内部的字符串序列，并且还要保存诸如输出长度之类的信息<br>char使用的是UTF-16编码，每个字符会占2个字节。比如，包含10个字符的String，2*10+40=60字节</p><p>（3）Java中的集合类型，比如HashMap和LinkedList，内部使用链表数据结构<br>链表中的每个数据，使用Entry对象包装<br>Entry对象，不光有对象头，还有指向下一个Entry的指针，占用8字节</p><p>（4）元素类型为原始数据类型（int），内部通常会使用原始数据类型的包装类型（Integer）来存储元素</p><h4 id="2、如何判断Spark程序消耗内存情况？"><a href="#2、如何判断Spark程序消耗内存情况？" class="headerlink" title="2、如何判断Spark程序消耗内存情况？"></a>2、如何判断Spark程序消耗内存情况？</h4><p>预估<br>（1）设置RDD的并行度<br>两种方法创建RDD，parallelize()  textFile() 在这两个方法中，传入第二个参数，设置RDD的partition数量<br>在SparkConfig中设置一个参数：<br>spark.default.parallelism<br>可以统一设置这个application中所有RDD的partition数量</p><p>（2）将RDD缓存 cache()</p><p>（3）观察日志：<br>driver的日志<br>/root/hd/spark-2.1.0-bin-hadoop2.7/work</p><p>Master和Worker的日志<br>/root/hd/spark-2.1.0-bin-hadoop2.7/logs</p><pre><code>scala&gt; val rdd1 = sc.textFile(&quot;/root/hd/tmp_files/test_Cache.txt&quot;)rdd1: org.apache.spark.rdd.RDD[String] = /root/hd/tmp_files/test_Cache.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:24scala&gt; rdd1.cacheres1: rdd1.type = /root/hd/tmp_files/test_Cache.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:24scala&gt; rdd1.countres2: Long = 921911                                                             scala&gt; rdd1.countres3: Long = 921911</code></pre><p>/root/hd/spark-2.1.0-bin-hadoop2.7/work<br>19/04/17 17:02:10 INFO MemoryStore: Block rdd_1_1 stored as values in memory (estimated size 22.9 MB, free 343.1 MB)<br>19/04/17 17:02:10 INFO MemoryStore: Block rdd_1_0 stored as values in memory (estimated size 22.9 MB, free 320.2 MB)</p><p>（4）将这个内存信息相加，就是RDD内存占用量<br>22.9MB+22.9MB=45.8MB</p><h3 id="二、使用高性能序列化类库"><a href="#二、使用高性能序列化类库" class="headerlink" title="二、使用高性能序列化类库"></a>二、使用高性能序列化类库</h3><h4 id="1、数据序列化概述"><a href="#1、数据序列化概述" class="headerlink" title="1、数据序列化概述"></a>1、数据序列化概述</h4><p>数据序列化，就是将对象或者数据结构，转换成特定的格式，使其可在网络中传输，或存储在内存或文件中<br>反序列化，是相反的操作，将对象从序列化数据中还原出来<br>序列化后的数据格式，可以是二进制，xml，Json等任何格式<br>对象、数据序列化的重点在于数据的交换与传输</p><p>在任何分布式系统中，序列化都是扮演着一个重要的角色<br>如果使用的序列化技术，操作很慢，或者序列化后的数据量还是很大，会让分布式系统应用程序性能下降很多<br>所以，Spark性能优化的第一步，就是进行序列化的性能优化</p><p>Spark自身默认会在一些地方对数据进行序列化，比如Shuffle。另外，我们使用了外部数据（自定义类型），也要让其课序列化</p><p>Spark本身对序列化的便捷性和性能进行了取舍<br>默认情况下：Spark倾向于序列化的便捷性，使用了Java自身提供的序列化机制，很方便使用</p><p>但是，Java序列化机制性能不高，序列化速度慢，序列化后数据较大，比较占用内存空间</p><h4 id="2、Kryo"><a href="#2、Kryo" class="headerlink" title="2、Kryo"></a>2、Kryo</h4><p>Spark支持使用Kryo类库来进行序列化<br>速度快，占用空间更小，比Java序列化数据占用空间小10倍</p><h4 id="3、如何使用kryo序列化机制"><a href="#3、如何使用kryo序列化机制" class="headerlink" title="3、如何使用kryo序列化机制"></a>3、如何使用kryo序列化机制</h4><p>（1）设置Spark Conf<br>bin/spark-submit will also read configuration options from </p><p><strong>conf/spark-defaults.conf</strong>,<br>in which each line consists of a key and a value separated by whitespace. For example:</p><p>spark.master            spark://5.6.7.8:7077<br>spark.executor.memory   4g<br>spark.eventLog.enabled  true<br>spark.serializer        org.apache.spark.serializer.KryoSerializer</p><p>（2）使用kryo是，要求需要序列化的类，要提前注册，以获得高性能<br>conf.registerKryoClasses(Array(classOf[Count],……))<br>conf.registerKryoClasses(Array(classOf[类], classOf[类], ……))</p><h4 id="4、kryo类库的优化"><a href="#4、kryo类库的优化" class="headerlink" title="4、kryo类库的优化"></a>4、kryo类库的优化</h4><p>（1）优化缓存大小<br>如果注册的自定义类型，本身特别大（100个字段），会导致要序列化的对象太大<br>此时需要对kyro本身进行优化。因为kryo内部的缓存，可能不能存放这么大的class对象<br>spark.kryoserializer.buffer.max  设置这个参数，将其调大</p><p>（2）预先注册自定义类型<br>虽然不注册自定义类型，kryo也可以正常工作，但会保存一份他的全限定类名，耗费内存<br>推荐预先注册要序列化的自定义类型</p><h3 id="三、优化数据结构"><a href="#三、优化数据结构" class="headerlink" title="三、优化数据结构"></a>三、优化数据结构</h3><h4 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h4><p>要减少内存的消耗，除了使用高效的序列化类库外，还要优化数据结构<br>避免Java语法特性中所导致的额外内存开销</p><p>核心：优化算子函数内部使用到的局部数据或算子函数外部的数据<br>目的：减少对内存的消耗和占用</p><h4 id="2、如何做"><a href="#2、如何做" class="headerlink" title="2、如何做"></a>2、如何做</h4><p>（1）优先使用数组以及字符串，而不是集合类。即：优先使用Array，而不是ArrayList、LinkedList、HashMap<br>使用int[] 会比List<code>&lt;Integer&gt;</code> 节省内存</p><p>（2）将对象转换成字符串<br>企业中，将HashMap、List这种数据，统一用String拼接成特殊格式的字符串</p><p>Map&lt;Integer,Person&gt; persons = new HashMap&lt;Integer,Person&gt;()</p><p>可以优化为：<br>“id:name,address”<br>String persons = “1:Andy,Beijing|2:Tom,Tianjin….”</p><p>（3）避免使用多层嵌套对象结构<br>举例：<br>下面的例子不好，因为Teacher类的内部又嵌套了大量的小的Student对象<br>public class Teacher{ private …..; privage List<code>&lt;Student&gt;</code> students = new ArrayList()}</p><p>解决：转换成字符串进行处理<br>{“teacherId”: 1, “students”:[{“stuId”:1…..},{}]}</p><p>（4）对于能够避免的场景，尽量使用int代替String<br>虽然String比List效率高，但int类型占用更少内存</p><p>比如：数据库主键，id，推荐使用自增的id，而不是uuid</p><h3 id="四、rdd-cache-checkpoint"><a href="#四、rdd-cache-checkpoint" class="headerlink" title="四、rdd.cache checkpoint"></a>四、rdd.cache checkpoint</h3><h3 id="五、持久化级别"><a href="#五、持久化级别" class="headerlink" title="五、持久化级别"></a>五、持久化级别</h3><p>MEMORY_ONLY  —&gt;  MEMORY_ONLY_SER 序列化</p><h3 id="六、Java虚拟机的调优"><a href="#六、Java虚拟机的调优" class="headerlink" title="六、Java虚拟机的调优"></a>六、Java虚拟机的调优</h3><h4 id="1、概述-1"><a href="#1、概述-1" class="headerlink" title="1、概述"></a>1、概述</h4><p>如果在持久化RDD的时候，持久化了大量的数据，那么Java虚拟机的垃圾回收就可能成为一个瓶颈</p><p>Java虚拟机会定期进行垃圾回收，此时会追踪所有Java对象，并且在垃圾回收时，找到那些已经不再使用的对象</p><p>清理旧对象，给新对象腾出空间</p><p>垃圾回收的性能开销，是与内存中的对象数量成正比</p><p>在做Java虚拟机调优之前，必须先做好上面的调优工作，这样才有意义</p><p>必须注意顺序（先进行完上面的调优，再进行JVM调优）</p><h4 id="2、Spark-GC原理"><a href="#2、Spark-GC原理" class="headerlink" title="2、Spark GC原理"></a>2、Spark GC原理</h4><p><strong>垃圾回收期GC</strong></p><p>垃圾回收器，寻找那些对象已经不再使用，将其清除出去</p><p>GC对性能的影响在于，如果内存中数据量较大，会很频繁地造成内存空间不够，导致GC频繁发生，而GC本身是有性能消耗的，如果频繁发生，对性能影响严重</p><p>此外，如果数据量过大，每次要回收的数据量也很大，导致GC慢</p><p>另外GC发生的时候，GC是一个线程，我们Task运行时线程叫做工作线程。GC运行时会让工作线程停下来，让GC单独运行，影响Spark应用程序的运行速度，降低了性能</p><p><strong>核心：不让GC频繁发生</strong></p><p><img src="/medias/sparkGC%E5%8E%9F%E7%90%86.PNG" alt="sparkGC原理"></p><h4 id="3、监测垃圾回收"><a href="#3、监测垃圾回收" class="headerlink" title="3、监测垃圾回收"></a>3、监测垃圾回收</h4><p>我们可以进行监测，比如多久进行一次垃圾回收以及耗费的时间等等。</p><p>spark-submit脚本中，添加一个配置<br>–conf “spark.executor.extraJavaOptions=-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimesStamps”</p><p>注意：这个是输出到worker日志中，而不是driver日志<br>/root/hd/spark-2.1.0-bin-hadoop2.7/logs  worker日志<br>/root/hd/spark-2.1.0-bin-hadoop2.7/work  driver日志</p><h4 id="4、优化Executor内存比例"><a href="#4、优化Executor内存比例" class="headerlink" title="4、优化Executor内存比例"></a>4、优化Executor内存比例</h4><p>目的：减少GC次数</p><p>对于GC调优来说，最重要的就是调节，RDD的缓存占用的内存空间与算子执行时创建对象所占用的内存空间的比例</p><p>Executor:Task=3:2  （Executor 占60%给RDD缓存，Task占40%）<br>对于默认情况，Spark使用每个Executor 60% 的内存空间来缓存RDD，在task运行期间所创建的对象，只有40%内存空间来存放</p><p>在默认情况下，很可能发生的事情，分配给Task的内存不够，<br>导致新创建对象时，很快占满内存，GC启动，找到不再使用的对象，清楚内存</p><p>所以，如果Task分配内存过小，可能会导致GC频繁发生，工作线程停止</p><p>可以通过调节比例，将RDD缓存空间占比调节到40%，降低Task GC频率</p><p>需要配合其他优化：<br>kryo优化<br>持久化级别优化<br>数据结构优化</p><p>使用：conf.set(“spark.storage.memoryFraction”,0.5)</p><p><img src="/medias/spark%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D.PNG" alt="spark内存分配."></p><h4 id="5、Java-GC-调优-（-）"><a href="#5、Java-GC-调优-（-）" class="headerlink" title="5、Java GC 调优 （-）"></a>5、Java GC 调优 （-）</h4><p>让GC更快的处理完</p><h3 id="七、shuffle原理"><a href="#七、shuffle原理" class="headerlink" title="七、shuffle原理"></a>七、shuffle原理</h3><h4 id="1、优化前"><a href="#1、优化前" class="headerlink" title="1、优化前"></a>1、优化前</h4><p>假设一个节点上，有4个shufflemaptask，有两个CPU core</p><p>每个MapTask会为每个ReduceTask创建一份bucket缓存，以及对应的ShuffleBlockFile磁盘文件</p><p>问题：假设有100个MapTask，100个ReduceTask，会产生100*100即10000个文件，磁盘io过多，影响性能</p><p>假设另外一个节点上，运行了4个ReduceTask</p><p>每个ReduceTask拉取过来的数据，其实会组成内部的RDD，叫ShuffledRDD，优先放入内存，如果不够，写入磁盘</p><p>每个ReduceTask针对数据进行聚合，最后生成MapPartitionsRDD，执行reduceByKey操作希望的到那个RDD<br><img src="/medias/shuffle%E4%BC%98%E5%8C%96%E5%89%8D.PNG" alt="Shuffle优化前"></p><h4 id="2、优化后"><a href="#2、优化后" class="headerlink" title="2、优化后"></a>2、优化后</h4><p>在Spark新版本中，引入了consolidation的机制，提出了ShuffleGroup概念</p><p>一个ShuffleMapTask执行完后，写入本地文件不会变，但是，下一个ShuffleSMapTask运行的时候，可以直接将数据写入之前的本地文件</p><p>相当于，多个ShuffleMapTask的输出进行了合并，大大减少文件数量</p><p>1和2可以乘坐一组ShuffleGroup，每个文件中，都存储了多个ShuffleMapTask的数量，每个ShuffleMapTask的数据叫做segment，通过外部索引，来标记每个ShuffleMapTask的数据以及偏移量，对不同的ShuffleMapTask的数据进行区分<br><img src="/medias/shuffle%E4%BC%98%E5%8C%96%E5%90%8E.PNG" alt="shuffle优化后"></p><h3 id="八、其他调优"><a href="#八、其他调优" class="headerlink" title="八、其他调优"></a>八、其他调优</h3><h4 id="1、提高并行度"><a href="#1、提高并行度" class="headerlink" title="1、提高并行度"></a>1、提高并行度</h4><h4 id="2、广播共享数据"><a href="#2、广播共享数据" class="headerlink" title="2、广播共享数据"></a>2、广播共享数据</h4>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Streaming</title>
      <link href="/2019/04/03/spark-streaming-ji-chu/"/>
      <url>/2019/04/03/spark-streaming-ji-chu/</url>
      
        <content type="html"><![CDATA[<p><strong>Spark Streaming</strong><br>流式计算框架，类似于Storm</p><p>常用的实时计算引擎（流式计算）<br>1、Apache Storm：真正的流式计算</p><p>2、Spark Streaming ：严格上来说，不是真正的流式计算（实时计算）<br>把连续的流式数据，当成不连续的RDD<br>本质：是一个离散计算（不连续）</p><p>3、Apache Flink：真正的流式计算。与Spark Streaming相反<br>把离散的数据，当成流式数据来处理</p><p>4、JStorm</p><h3 id="一、Spark-Streaming基础"><a href="#一、Spark-Streaming基础" class="headerlink" title="一、Spark Streaming基础"></a>一、Spark Streaming基础</h3><h4 id="1、什么是-Spark-Streaming"><a href="#1、什么是-Spark-Streaming" class="headerlink" title="1、什么是 Spark Streaming"></a>1、什么是 Spark Streaming</h4><p>Spark Streaming makes it easy to build scalable fault-tolerant streaming applications.<br>易于构建灵活的、高容错的流式系统</p><p>Spark Streaming是核心Spark API的扩展，可实现可扩展、高吞吐量、可容错的实时数据流处理。数据可以从诸如Kafka，Flume，Kinesis或TCP套接字等众多来源获取，并且可以使用由高级函数（如map，reduce，join和window）开发的复杂算法进行流数据处理。最后，处理后的数据可以被推送到文件系统，数据库和实时仪表板。而且，您还可以在数据流上应用Spark提供的机器学习和图处理算法</p><p>特点：<br>1、易用，已经集成到Spark中<br>2、容错性：底层RDD，RDD本身具有容错机制<br>3、支持多种语言：Java Scala Python</p><p>Spark Streaming将连续的数据流抽象为discretizedstream或DStream。在内部，DStream 由一个RDD序列表示</p><h4 id="2、演示官方的Demo"><a href="#2、演示官方的Demo" class="headerlink" title="2、演示官方的Demo"></a>2、演示官方的Demo</h4><p>往Spark Streaming中发送字符串，Spark 接收到以后，进行计数<br>使用消息服务器 netcat Linux自带<br>yum install nc.x86_64</p><p>nc -l 1234</p><p>注意：总核心数 大于等于2。一个核心用于接收数据，另一个用于处理数据</p><p>在netcat中写入数据 Spark Streaming可以取到</p><pre><code>[root@hsiehchou121 spark-2.1.0-bin-hadoop2.7]# ./bin/run-example streaming.NetworkWordCount localhost 1234</code></pre><h4 id="3、开发自己的NetWorkWordCount程序"><a href="#3、开发自己的NetWorkWordCount程序" class="headerlink" title="3、开发自己的NetWorkWordCount程序"></a>3、开发自己的NetWorkWordCount程序</h4><p>和Spark Core类似</p><p><strong>代码</strong></p><pre><code>package day5import org.apache.spark.streaming.StreamingContextimport org.apache.spark.SparkConfimport org.apache.spark.streaming.Secondsimport org.apache.spark.storage.StorageLevelimport org.apache.log4j.Loggerimport org.apache.log4j.Level/** * 开发自己的流式计算程序 *  * 知识点 * 1、创建一个StreamingContext对象  ----》核心：创建一个DStream *  * 2、DStream的表现形式：就是一个RDD *  * 3、使用DStream把连续的数据流变成不连续的RDD *  * Spark Streaming 最核心的内容 */object MyNetworkWordCount {  def main(args: Array[String]): Unit = {    //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建一个StreamingContext对象    //local[2]表示开启了两个线程    val conf = new SparkConf().setAppName(&quot;MyNetworkWordCount&quot;).setMaster(&quot;local[2]&quot;)    //Seconds(3)表示采样时间间隔     val ssc = new StreamingContext(conf, Seconds(3))    //创建DStream，从netcat服务器上接收数据    val lines = ssc.socketTextStream(&quot;192.168.116.121&quot;, 1234, StorageLevel.MEMORY_ONLY)    //lines中包含了netcat服务器发送过来的数据    //分词操作    val words = lines.flatMap(_.split(&quot; &quot;))    //计数    val wordCount = words.map((_,1)).reduceByKey(_+_)    //打印结果    wordCount.print()    //启动StreamingContext进行计算    ssc.start()    //等待任务结束    ssc.awaitTermination()  }}</code></pre><p><strong>程序中的几点说明</strong></p><p>appName参数是应用程序在集群UI上显示的名称</p><p>master是Spark，Mesos或YARN集群的URL，或者一个特殊的“local [*]”字符串来让程序以本地模式运行</p><p>当在集群上运行程序时，不需要在程序中硬编码master参数，而是使用spark-submit提交应用程序并将master的URL以脚本参数的形式传入。但是，对于本地测试和单元测试，您可以通过“local[*]”来运行Spark Streaming程序（请确保本地系统中的cpu核心数够用）</p><p>StreamingContext会内在的创建一个SparkContext的实例（所有Spark功能的起始点），你可以通过ssc.sparkContext访问到这个实例</p><p>批处理的时间窗口长度必须根据应用程序的延迟要求和可用的集群资源进行设置</p><p><strong>请务必记住以下几点</strong></p><p>一旦一个StreamingContext开始运作，就不能设置或添加新的流计算</p><p>一旦一个上下文被停止，它将无法重新启动</p><p>同一时刻，一个JVM中只能有一个StreamingContext处于活动状态</p><p>StreamingContext上的stop()方法也会停止SparkContext。 要仅停止StreamingContext（保持SparkContext活跃），请将stop() 方法的可选参数stopSparkContext设置为false</p><p>只要前一个StreamingContext在下一个StreamingContext被创建之前停止（不停止SparkContext），SparkContext就可以被重用来创建多个StreamingContext</p><p>问题：Hello Hello<br>Hello World</p><p>现在现象：（Hello,2）<br>    (Hello , 1) (World , 1)</p><p>能不能累加起来？保存记录下以前的状态？<br>能，能<br>通过Spark Streaming提供的算子来实现</p><h3 id="二、高级特性"><a href="#二、高级特性" class="headerlink" title="二、高级特性"></a>二、高级特性</h3><h4 id="1、什么是DStream？离散流"><a href="#1、什么是DStream？离散流" class="headerlink" title="1、什么是DStream？离散流"></a>1、什么是DStream？离散流</h4><p>把连续的数据变成不连续的RDD<br>因为DStream的特性，导致Spark Streaming不是真正的流式计算</p><p><strong>离散流</strong>（DStreams）：Discretized Streams<br>DiscretizedStream或DStream 是Spark Streaming对流式数据的基本抽象。它表示连续的数据流，这些连续的数据流可以是从数据源接收的输入数据流，也可以是通过对输入数据流执行转换操作而生成的经处理的数据流。在内部，DStream由一系列连续的RDD表示</p><p>举例分析：<br>在之前的MyNetworkWordCount 的例子中，我们将一行行文本组成的流转换为单词流，具体做法为：将flatMap操作应用于名为lines的 DStream中的每个RDD上，以生成words DStream的RDD</p><p><strong>DStream中的转换操作（transformation）</strong></p><table><thead><tr><th align="center">Transformation</th><th align="center">Meaning</th></tr></thead><tbody><tr><td align="center">map(func)</td><td align="center">利用函数func处理DStreamd的每个元素，返回一个新的DStream</td></tr><tr><td align="center">flatMap(func)</td><td align="center">于map相似，但是每个输入项可被映射为0个或者多个输出项</td></tr><tr><td align="center">filter(func)</td><td align="center">返回一个新的DStream，它仅仅包含源DStream中满足函数func的项</td></tr><tr><td align="center">repartition(numPartitions)</td><td align="center">通过创建更多或者更少的partition改变这个DStream的并行级别（level of parallelism）</td></tr><tr><td align="center">union(otherStream)</td><td align="center">返回一个新的DStream，它包含源DStream和otherStream的联合元素</td></tr><tr><td align="center">count()</td><td align="center">通过计算源DStream中每个RDD的元素数量，返回一个包含单元素（single-element）RDDs的新DStream</td></tr><tr><td align="center">reduce(func)</td><td align="center">利用函数func聚焦源DStream中每个RDD的元素，返回一个包含单元素（single-element）RDDs的新DStream，函数应该是相关联的，以使计算可以并行化</td></tr><tr><td align="center">countByValue()</td><td align="center">这个算子应用于元素类型为K的DStream上，返回一个（K,long）对的新DStream，每个键的值实在原DStream的每个RDD中的频率</td></tr><tr><td align="center">reduceByKey(func,[numTasks])</td><td align="center">当在一个由（K,V）对组成的DStream上调用这个算子，返回一个新的由（K,V）对组成的DStream，每一个key的值均由给定的reduce函数聚集起来。注意：在默认情况下，这个算子利用了Spark默认的并发任务数去分组，你可以用numTasks参数设置不同的任务数</td></tr><tr><td align="center">join(otherStream,[numTasks])</td><td align="center">当应用于两个DStream（一个包含（K,V）对，一个包含（K,W）对），返回一个包含（K,（V,W））对的新DStream</td></tr><tr><td align="center">cogroup(otherStream,[numTasks])</td><td align="center">当应用于两个DStream（一个包含(K,V)对，一个包含（K,W）对），返回一个包含（K,Seq[v],Seq[W]）的元组</td></tr><tr><td align="center">transform(func)</td><td align="center">通过对源DStream的每个RDD应用RDD-to-RDD函数，创建一个新的DStream，这个可以在DStream中的任何RDD操作中使用</td></tr><tr><td align="center">updateStateByKey(func)</td><td align="center">利用给定的函数更新DStream的状态，返回一个新”state”的DStream</td></tr></tbody></table><h4 id="2、重点算子讲解"><a href="#2、重点算子讲解" class="headerlink" title="2、重点算子讲解"></a>2、重点算子讲解</h4><p>（1）updateStateByKey(func)<br>默认情况下，Spark Streaming不记录之前的状态，每次发数据，都会从0开始<br>现在使用本算子，实现累加操作</p><p>操作允许不断用新信息更新它的同时保持任意状态<br>定义状态-状态可以是任何的数据类型<br>定义状态更新函数-怎样利用更新前的状态和从输入流里面获取的新值更新状态</p><p>重写MyNetworkWordCount程序，累计每个单词出现的频率（注意：累计）</p><pre><code>package day5import org.apache.spark.streaming.StreamingContextimport org.apache.spark.SparkConfimport org.apache.spark.streaming.Secondsimport org.apache.spark.storage.StorageLevelimport org.apache.log4j.Loggerimport org.apache.log4j.Levelimport javax.swing.text.DefaultEditorKit.PreviousWordAction/** * 实现累加操作 */object MyTotalNetworkWordCount {  def main(args: Array[String]): Unit = {    //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建一个StreamingContext对象    //local[2]表示开启了两个线程    val conf = new SparkConf().setAppName(&quot;MyTotalNetworkWordCount &quot;).setMaster(&quot;local[2]&quot;)    //Seconds(3)表示采样时间间隔     val ssc = new StreamingContext(conf, Seconds(3))    //设置检查点目录，保存之前的状态信息    ssc.checkpoint(&quot;hdfs://hsiehchou121:9000/tmp_files/chkp&quot;)    //创建DStream 从netcat服务器上接收数据    val lines = ssc.socketTextStream(&quot;192.168.116.121&quot;, 1234, StorageLevel.MEMORY_ONLY)    val words = lines.flatMap(_.split(&quot; &quot;))    val wordPair = words.map((_,1))    /**     * 定义一个值函数，进行累加运算     * 1、当前值是多少（参数1）     * 2、之前的结果是多少（参数2）     */    val addFunc = (currentValues:Seq[Int], previousValues:Option[Int]) =&gt;{      //进行累加运算      //1、把当前的序列进行累加      val currentTotal = currentValues.sum      //2、在之前的值上再累加      Some(currentTotal + previousValues.getOrElse(0))    }    //进行累加运算    val total = wordPair.updateStateByKey(addFunc)    total.print()    ssc.start()    ssc.awaitTermination()  }}</code></pre><p>我在执行过程中遇到访问权限问题<br>解决如下：<br>在hadoop的etc/hadoop/下的hdfs-site.xml中增加如下内容即可</p><pre><code>    &lt;property&gt;         &lt;name&gt;dfs.permissions&lt;/name&gt;         &lt;value&gt;false&lt;/value&gt;    &lt;/property&gt;     &lt;property&gt;        &lt;name&gt;dfs.safemode.threshold.pct&lt;/name&gt;        &lt;value&gt;0f&lt;/value&gt;    &lt;/property&gt;  </code></pre><p>（2）transform(func)<br>通过RDD-to-RDD函数作用于源DStream中的各个RDD，可以是任意的RDD操作，从而返回一个新的RDD</p><pre><code>package day5import org.apache.spark.streaming.StreamingContextimport org.apache.spark.SparkConfimport org.apache.spark.streaming.Secondsimport org.apache.spark.storage.StorageLevelimport org.apache.log4j.Loggerimport org.apache.log4j.Level/** * 开发自己的流式计算程序 *  * 知识点 * 1、创建一个StreamingContext对象  ----》核心：创建一个DStream *  * 2、DStream的表现形式：就是一个RDD *  * 3、使用DStream把连续的数据流变成不连续的RDD *  * Spark Streaming 最核心的内容 */object MyNetworkWordCount {  def main(args: Array[String]): Unit = {    //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建一个StreamingContext对象    //local[2]表示开启了两个线程    val conf = new SparkConf().setAppName(&quot;MyNetworkWordCount &quot;).setMaster(&quot;local[2]&quot;)    //Seconds(3)表示采样时间间隔     val ssc = new StreamingContext(conf, Seconds(3))    //创建DStream，从netcat服务器上接收数据    val lines = ssc.socketTextStream(&quot;192.168.116.121&quot;, 1234, StorageLevel.MEMORY_ONLY)    //lines中包含了netcat服务器发送过来的数据    //分词操作    val words = lines.flatMap(_.split(&quot; &quot;))    //计数    val wordPair = words.transform(x =&gt; x.map(x =&gt; (x, 1)))    //打印结果    wordPair.print()    //启动StreamingContext进行计算    ssc.start()    //等待任务结束    ssc.awaitTermination()  }}</code></pre><h4 id="3、窗口操作"><a href="#3、窗口操作" class="headerlink" title="3、窗口操作"></a>3、窗口操作</h4><p>窗口：对落在窗口内的数据进行处理，也是一个DStream，RDD</p><pre><code>package day5import org.apache.spark.streaming.StreamingContextimport org.apache.spark.SparkConfimport org.apache.spark.streaming.Secondsimport org.apache.spark.storage.StorageLevelimport org.apache.log4j.Loggerimport org.apache.log4j.Level/** * 窗口操作： * 需求：每10秒钟，把过去30秒的数据读取进来 */object MyNetworkWordCountByWindow {  def main(args: Array[String]): Unit = {    //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建一个StreamingContext对象    //local[2]表示开启了两个线程    val conf = new SparkConf().setAppName(&quot;MyNetworkWordCountByWindow&quot;).setMaster(&quot;local[2]&quot;)    //Seconds(3)表示采样时间间隔     val ssc = new StreamingContext(conf, Seconds(1))    //创建DStream 从netcat服务器上接收数据    val lines = ssc.socketTextStream(&quot;192.168.116.121&quot;, 1234, StorageLevel.MEMORY_ONLY)    //lines中包含了netcat服务器发送过来的数据    //分词操作 给每个单词记一次数    val words = lines.flatMap(_.split(&quot; &quot;)).map((_,1))    /*     * reduceByKeyAndWindow 函数的三个参数     * 1、需要进行什么操作     * 2、窗口的大小30秒     * 3、窗口滑动的距离10秒     */    val result = words.reduceByKeyAndWindow((x:Int,y:Int)=&gt;(x+y),Seconds(30),Seconds(10))    result.print()    ssc.start()    ssc.awaitTermination()    /*     * The slide duration of windowed DStream (10000 ms) must be a multiple of the slide      * duration of parent DStream (3000 ms)     *      * 注意：窗口滑动距离必须是采样时间的整数倍     */  }}</code></pre><p>举例：每10秒钟把过去30秒的数据采集过来<br>注意：先启动nc  再启动程序 local[2]</p><h4 id="4、集成Spark-SQL-使用SQL语句来处理流式数据"><a href="#4、集成Spark-SQL-使用SQL语句来处理流式数据" class="headerlink" title="4、集成Spark SQL: 使用SQL语句来处理流式数据"></a>4、集成Spark SQL: 使用SQL语句来处理流式数据</h4><pre><code>package day5import org.apache.spark.streaming.StreamingContextimport org.apache.spark.SparkConfimport org.apache.spark.streaming.Secondsimport org.apache.spark.storage.StorageLevelimport org.apache.log4j.Loggerimport org.apache.log4j.Levelimport org.apache.spark.sql.SparkSession/** * 集成Spark SQL : 在Spark Streaming中使用SQL语句 */object MyNetworkWordCountWithSQL {   def main(args: Array[String]): Unit = {     //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建一个StreamingContext对象    //local[2]表示开启了两个线程    val conf = new SparkConf().setAppName(&quot;MyNetworkWordCountByWindow&quot;).setMaster(&quot;local[2]&quot;)    //Seconds(3)表示采样时间间隔     val ssc = new StreamingContext(conf, Seconds(10))    //创建DStream 从netcat服务器上接收数据    val lines = ssc.socketTextStream(&quot;192.168.116.121&quot;, 1234, StorageLevel.MEMORY_ONLY)    //进行单词计数    val words = lines.flatMap(_.split(&quot; &quot;))    //集成Spark SQL 使用SQL语句实现WordCount    words.foreachRDD(rdd =&gt;{      //创建一个Spark Session对象      //通过ssc.sparkContext.getConf 直接获取此session的conf      val spark = SparkSession.builder().config(ssc.sparkContext.getConf).getOrCreate()      //把RDD转换成DataFrame  需要用到隐式转换      import spark.implicits._      val df1 = rdd.toDF(&quot;word&quot;)//表df1 只有一个列名 名字叫word      //创建视图      df1.createOrReplaceTempView(&quot;words&quot;)      //执行SQL  通过SQL实现wordcount      spark.sql(&quot;select word,count(1) from words group by word&quot;).show      }    )    ssc.start()    ssc.awaitTermination()   }}</code></pre><h4 id="5、缓存和持久化：和RDD一样"><a href="#5、缓存和持久化：和RDD一样" class="headerlink" title="5、缓存和持久化：和RDD一样"></a>5、缓存和持久化：和RDD一样</h4><p>与RDD类似，DStreams还允许开发人员将流数据保留在内存中。也就是说，在DStream上调用persist() 方法会自动将该DStream的每个RDD保留在内存中。如果DStream中的数据将被多次计算（例如，相同数据上执行多个操作），这个操作就会很有用。对于基于窗口的操作，如reduceByWindow和reduceByKeyAndWindow以及基于状态的操作，如updateStateByKey，数据会默认进行持久化。 因此，基于窗口的操作生成的DStream会自动保存在内存中，而不需要开发人员调用persist()</p><p>对于通过网络接收数据（例如Kafka，Flume，sockets等）的输入流，默认持久化级别被设置为将数据复制到两个节点进行容错</p><p>注意，与RDD不同，DStreams的默认持久化级别将数据序列化保存在内存中</p><h4 id="6、支持检查点：和RDD一样"><a href="#6、支持检查点：和RDD一样" class="headerlink" title="6、支持检查点：和RDD一样"></a>6、支持检查点：和RDD一样</h4><p>流数据处理程序通常都是全天候运行，因此必须对应用中逻辑无关的故障（例如，系统故障，JVM崩溃等）具有弹性。为了实现这一特性，Spark Streaming需要checkpoint足够的信息到容错存储系统，以便可以从故障中恢复</p><p>①　一般会对两种类型的数据使用检查点：<br>1）元数据检查点（Metadatacheckpointing） - 将定义流计算的信息保存到容错存储中（如HDFS）。这用于从运行streaming程序的driver程序的节点的故障中恢复。元数据包括以下几种：<br>    配置（Configuration） - 用于创建streaming应用程序的配置信息</p><p>    DStream操作（DStream operations） - 定义streaming应用程序的DStream操作集合</p><p>    不完整的batch（Incomplete batches） - jobs还在队列中但尚未完成的batch</p><p>2）数据检查点（Datacheckpointing） - 将生成的RDD保存到可靠的存储层。对于一些需要将多个批次之间的数据进行组合的stateful变换操作，设置数据检查点是必需的。在这些转换操作中，当前生成的RDD依赖于先前批次的RDD，这导致依赖链的长度随时间而不断增加，由此也会导致基于血统机制的恢复时间无限增加。为了避免这种情况，stateful转换的中间RDD将定期设置检查点并保存到到可靠的存储层（例如HDFS）以切断依赖关系链</p><p>总而言之，元数据检查点主要用于从driver程序故障中恢复，而数据或RDD检查点在任何使用stateful转换时是必须要有的</p><p>②　何时启用检查点：<br>对于具有以下任一要求的应用程序，必须启用检查点：<br>1）使用状态转：如果在应用程序中使用updateStateByKey或reduceByKeyAndWindow（具有逆函数），则必须提供检查点目录以允许定期保存RDD检查点<br>2）从运行应用程序的driver程序的故障中恢复：元数据检查点用于使用进度信息进行恢复</p><p>③　如何配置检查点：<br>可以通过在一些可容错、高可靠的文件系统（例如，HDFS，S3等）中设置保存检查点信息的目录来启用检查点。这是通过使用streamingContext.checkpoint(checkpointDirectory)完成的。设置检查点后，您就可以使用上述的有状态转换操作。此外，如果要使应用程序从驱动程序故障中恢复，您应该重写streaming应用程序以使程序具有以下行为：<br>1）当程序第一次启动时，它将创建一个新的StreamingContext，设置好所有流数据源，然后调用start()方法。<br>2）当程序在失败后重新启动时，它将从checkpoint目录中的检查点数据重新创建一个StreamingContext。<br>使用StreamingContext.getOrCreate可以简化此行为</p><p>④　改写之前的WordCount程序，使得每次计算的结果和状态都保存到检查点目录下<br>hdfs dfs -ls /spark_checkpoint</p><h3 id="三、数据源"><a href="#三、数据源" class="headerlink" title="三、数据源"></a>三、数据源</h3><p>Spark Streaming是一个流式计算引擎，就需要从外部数据源来接收数据</p><h4 id="1、基本的数据源"><a href="#1、基本的数据源" class="headerlink" title="1、基本的数据源"></a>1、基本的数据源</h4><p>文件流：监控文件系统的变化，如果文件有增加，读取文件中的内容</p><p>希望Spark Streaming监控一个文件夹，如果有变化，则把变化采集过来</p><p><strong>此功能为</strong>修改文件里面的内容，并修改文件名，才能检测到，单修改一个是不起作用的</p><pre><code>package day5import org.apache.spark.streaming.StreamingContextimport org.apache.spark.SparkConfimport org.apache.spark.streaming.Secondsimport org.apache.spark.storage.StorageLevelimport org.apache.log4j.Loggerimport org.apache.log4j.Level/** * 测试文件流 */object FileStreaming {  def main(args: Array[String]): Unit = {    //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建一个StreamingContext对象    //local[2]表示开启了两个线程    val conf = new SparkConf().setAppName(&quot;MyNetworkWordCountByWindow&quot;).setMaster(&quot;local[2]&quot;)    //Seconds(3)表示采样时间间隔     val ssc = new StreamingContext(conf, Seconds(1))    //直接监控某个目录，如果有新文件产生，就读取出来    val lines = ssc.textFileStream(&quot;H:\\other\\test_file_stream&quot;)    lines.print()    ssc.start()    ssc.awaitTermination()  }}</code></pre><p>RDD队列流：可以从队列中获取数据（不常用）</p><pre><code>package day5import org.apache.spark.streaming.StreamingContextimport org.apache.spark.SparkConfimport org.apache.spark.streaming.Secondsimport org.apache.spark.storage.StorageLevelimport org.apache.log4j.Loggerimport org.apache.log4j.Levelimport org.apache.spark.sql.SparkSessionimport org.apache.spark.rdd.RDDimport scala.collection.mutable.Queue/** * RDD队列流 */object RDDQueueStream {  def main(args: Array[String]): Unit = {     //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建一个StreamingContext对象    //local[2]表示开启了两个线程    val conf = new SparkConf().setAppName(&quot;RDDQueueStream&quot;).setMaster(&quot;local[2]&quot;)    //Seconds(3)表示采样时间间隔     val ssc = new StreamingContext(conf, Seconds(3))    //需要先创建一个队列RDD[Int]    val rddQueue = new Queue[RDD[Int]]()    //往队列里面添加数据 ----&gt; 创建数据源    for(i &lt;- 1 to 3){      rddQueue += ssc.sparkContext.makeRDD(1 to 10)      //为了便于观察      Thread.sleep(1000)    }    //从队列中接收数据，创建DStream    val inputDStream = ssc.queueStream(rddQueue)    //处理数据    val result = inputDStream.map(x =&gt; (x, x*2))    result.print()    ssc.start()    ssc.awaitTermination()   }}</code></pre><p>套接字流：socketTextStream</p><h4 id="2、高级数据源"><a href="#2、高级数据源" class="headerlink" title="2、高级数据源"></a>2、高级数据源</h4><p>（1）Flume<br>Spark SQL 对接flume有多种方式：<br>push方式：flume将数据推送给Spark Streaming<br>flume/myagent/a4.conf</p><pre><code># bin/flume-ng agent -n a4 -f myagent/a4.conf -c conf -Dflume.root.logger=INFO.console# 定义agent名，source、channel、sink的名称a4.sources = r1a4.channels = c1a4.sinks = k1# 具体定义sourcea4.sources.r1.type = spooldira4.sources.r1.spoolDir = /root/hd/tmp_files/logs# 具体定义channela4.channels.c1.type = memorya4.channels.c1.capacity = 10000a4.channels.c1.transactionCapacity = 100# 具体定义sinka4.sinks = k1a4.sinks.k1.type = avroa4.sinks.k1.channel = c1a4.sinks.k1.hostname = 192.168.116.1a4.sinks.k1.port = 1234# 组装 source、channel、sinka4.sources.r1.channels = c1a4.sinks.k1.channel = c1</code></pre><pre><code>package day5import org.apache.spark.streaming.StreamingContextimport org.apache.spark.SparkConfimport org.apache.spark.streaming.Secondsimport org.apache.log4j.Loggerimport org.apache.log4j.Levelimport org.apache.spark.streaming.flume.FlumeUtilsobject MyFlumeStream {  def main(args: Array[String]): Unit = {    //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建一个StreamingContext对象    //local[2]表示开启了两个线程    val conf = new SparkConf().setAppName(&quot;MyFlumeStream&quot;).setMaster(&quot;local[2]&quot;)    //Seconds(3)表示采样时间间隔     val ssc = new StreamingContext(conf, Seconds(3))    //对象flume    //创建一个flumeEvent  从flume中接收push来的数据，也是一个DStream    //flume将数据push到&quot;192.168.116.1&quot;,1234  Spark Streaming在这里监听    val flumeEventDStream = FlumeUtils.createStream(ssc, &quot;192.168.116.1&quot;, 8888)    //将FlumeEvent中的事件转换成字符串    val lineDStream = flumeEventDStream.map(e =&gt; {       new String(e.event.getBody.array)     })    //输出结果    lineDStream.print()    ssc.start()    ssc.awaitTermination()  }}</code></pre><p>custom sink 模式：比第一种有更好的健壮性和容错性。使用这种方式，flume配置一个sink<br>a1.conf</p><pre><code>#bin/flume-ng agent -n a1 -f myagent/a1.conf -c conf -Dflume.root.logger=INFO,consolea1.channels = c1a1.sinks = k1a1.sources = r1a1.sources.r1.type = spooldira1.sources.r1.spoolDir = /root/hd/tmp_files/logsa1.channels.c1.type = memorya1.channels.c1.capacity = 100000a1.channels.c1.transactionCapacity = 100000a1.sinks.k1.type = org.apache.spark.streaming.flume.sink.SparkSinka1.sinks.k1.channel = c1a1.sinks.k1.hostname = 192.168.116.121a1.sinks.k1.port = 1234#组装source、channel、sinka1.sources.r1.channels = c1a1.sinks.k1.channel = c1</code></pre><p>使用官方提供的spark sink组件</p><p>需要把 spark-streaming-flume-sink_2.10-2.1.0.jar 拷贝到flume lib下<br>需要把 spark-streaming-flume-sink_2.10-2.1.0.jar 拷贝到IDE的lib下添加到build path中</p><p>（2）Kafka<br>在讲Kafka时，举例</p><h3 id="四、性能优化的参数"><a href="#四、性能优化的参数" class="headerlink" title="四、性能优化的参数"></a>四、性能优化的参数</h3><p>性能优化：<br>    spark submit的时候，程序报OOM错误<br>    程序跑的很慢</p><h4 id="1、减少批数据的执行时间"><a href="#1、减少批数据的执行时间" class="headerlink" title="1、减少批数据的执行时间"></a>1、减少批数据的执行时间</h4><p>在Spark中有几个优化可以减少批处理的时间：<br>①　数据接收的并行水平<br>通过网络(如kafka，flume，socket等)接收数据需要这些数据反序列化并被保存到Spark中。如果数据接收成为系统的瓶颈，就要考虑并行地接收数据。注意，每个输入DStream创建一个receiver（运行在worker机器上）接收单个数据流。创建多个输入DStream并配置它们可以从源中接收不同分区的数据流，从而实现多数据流接收。例如，接收两个topic数据的单个输入DStream可以被切分为两个kafka输入流，每个接收一个topic。这将在两个worker上运行两个receiver，因此允许数据并行接收，提高整体的吞吐量。多个DStream可以被合并生成单个DStream，这样运用在单个输入DStream的transformation操作可以运用在合并的DStream上</p><p>②　数据处理的并行水平<br>如果运行在计算stage上的并发任务数不足够大，就不会充分利用集群的资源。默认的并发任务数通过配置属性来确定spark.default.parallelism</p><p>③　数据序列化<br>可以通过改变序列化格式来减少数据序列化的开销。在流式传输的情况下，有两种类型的数据会被序列化：<br>1）输入数据<br>2）由流操作生成的持久RDD<br>在上述两种情况下，使用Kryo序列化格式可以减少CPU和内存开销</p><h4 id="2、设置正确的批容量"><a href="#2、设置正确的批容量" class="headerlink" title="2、设置正确的批容量"></a>2、设置正确的批容量</h4><p>为了Spark Streaming应用程序能够在集群中稳定运行，系统应该能够以足够的速度处理接收的数据（即处理速度应该大于或等于接收数据的速度）。这可以通过流的网络UI观察得到。批处理时间应该小于批间隔时间</p><p>根据流计算的性质，批间隔时间可能显著的影响数据处理速率，这个速率可以通过应用程序维持。可以考虑WordCountNetwork这个例子，对于一个特定的数据处理速率，系统可能可以每2秒打印一次单词计数（批间隔时间为2秒），但无法每500毫秒打印一次单词计数。所以，为了在生产环境中维持期望的数据处理速率，就应该设置合适的批间隔时间(即批数据的容量)</p><p>找出正确的批容量的一个好的办法是用一个保守的批间隔时间（5-10,秒）和低数据速率来测试你的应用程序</p><h4 id="3、内存调优"><a href="#3、内存调优" class="headerlink" title="3、内存调优"></a>3、内存调优</h4><p>在这一节，我们重点介绍几个强烈推荐的自定义选项，它们可以减少Spark Streaming应用程序垃圾回收的相关暂停，获得更稳定的批处理时间</p><p>1）Default persistence level of DStreams：和RDDs不同的是，默认的持久化级别是序列化数据到内存中（DStream是StorageLevel.MEMORY_ONLY_SER，RDD是StorageLevel.MEMORY_ONLY）。即使保存数据为序列化形态会增加序列化/反序列化的开销，但是可以明显的减少垃圾回收的暂停</p><p>2）Clearing persistent RDDs：默认情况下，通过Spark内置策略（LUR），Spark Streaming生成的持久化RDD将会从内存中清理掉。如果spark.cleaner.ttl已经设置了，比这个时间存在更老的持久化RDD将会被定时的清理掉。正如前面提到的那样，这个值需要根据Spark Streaming应用程序的操作小心设置。然而，可以设置配置选项spark.streaming.unpersist为true来更智能的去持久化（unpersist）RDD。这个配置使系统找出那些不需要经常保有的RDD，然后去持久化它们。这可以减少Spark RDD的内存使用，也可能改善垃圾回收的行为</p><p>3）Concurrent garbage collector：使用并发的标记-清除垃圾回收可以进一步减少垃圾回收的暂停时间。尽管并发的垃圾回收会减少系统的整体吞吐量，但是仍然推荐使用它以获得更稳定的批处理时间</p><p>方法：调整spark参数<br>    conf.set…</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark SQL</title>
      <link href="/2019/03/31/spark-sql/"/>
      <url>/2019/03/31/spark-sql/</url>
      
        <content type="html"><![CDATA[<p>Spark SQL 类似于Hive</p><h3 id="一、Spark-SQL-基础"><a href="#一、Spark-SQL-基础" class="headerlink" title="一、Spark SQL 基础"></a>一、Spark SQL 基础</h3><h4 id="1、什么是Spark-SQL"><a href="#1、什么是Spark-SQL" class="headerlink" title="1、什么是Spark SQL"></a>1、什么是Spark SQL</h4><p>Spark SQL is Apache Spark’s module for working with structured data.<br>Spark SQL 是spark 的一个模块。来处理 结构化 的数据<br>不能处理非结构化的数据</p><p>特点： </p><p><strong>1）容易集成</strong> </p><p>不需要单独安装</p><p><strong>2）统一的数据访问方式</strong> </p><p>结构化数据的类型：JDBC JSon Hive parquer文件 都可以作为Spark SQL 的数据源<br>对接多种数据源，且使用方式类似</p><p><strong>3）完全兼容hive</strong> </p><p>把Hive中的数据，读取到Spark SQL中运行</p><p><strong>4）支持标准的数据连接</strong></p><p>JDBC</p><h4 id="2、为什么学习Spark-SQL"><a href="#2、为什么学习Spark-SQL" class="headerlink" title="2、为什么学习Spark SQL"></a>2、为什么学习Spark SQL</h4><p>执行效率比Hive高</p><p>hive 2.x 执行引擎可以使用 Spark</p><h4 id="3、核心概念：表（DataFrame-DataSet）"><a href="#3、核心概念：表（DataFrame-DataSet）" class="headerlink" title="3、核心概念：表（DataFrame DataSet）"></a>3、核心概念：表（DataFrame DataSet）</h4><p>mysql中的表：表结构、数据<br>DataFrame：Schema、RDD（数据）</p><p>DataSet 在spark1.6以后，对DataFrame做了一个封装</p><h4 id="4、创建DataFrame"><a href="#4、创建DataFrame" class="headerlink" title="4、创建DataFrame"></a>4、创建DataFrame</h4><p>（<em>）测试数据：员工表、部门表<br>第一种方式：使用case class<br>*</em>1）定义Schema**<br>样本类来定义Schema</p><p>case class 特点：<br>可以支持模式匹配，使用case class建立表结构</p><p>7521, WARD, SALESMAN,7698, 1981/2/22, 1250, 500, 30</p><p>case class Emp(empno:Int,ename:String,job:String,mgr:Int,hiredate:String,sal:Int,comm:Int,deptno:Int)</p><p><strong>2）读取文件</strong><br>val lines = sc.textFile(“/root/hd/tmp_files/emp.csv”).map(_.split(“,”))</p><p><strong>3）把每行数据，映射到Emp上</strong><br>val allEmp = lines.map(x =&gt; Emp(x(0).toInt,x(1),x(2),x(3).toInt,x(4),x(5).toInt,x(6).toInt,x(7).toInt))</p><p><strong>4）生成DataFrame</strong><br>val df1 = allEmp.toDF</p><p>df1.show</p><p><strong>第二种方式 使用Spark Session</strong><br>（1）什么是Spark Session<br><strong>Spark session available as ‘spark’.</strong><br>2.0以后引入的统一访问方式。可以访问所有的Spark组件</p><p>def createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame</p><p>（2）使用StructType来创建Schema</p><p>val struct =<br>StructType(<br>StructField(“a”, IntegerType, true) ::<br>StructField(“b”, LongType, false) ::<br>StructField(“c”, BooleanType, false) :: Nil)</p><p>case class Emp(<br>empno:Int,<br>ename:String,<br>job:String,<br>mgr:Int,<br>hiredate:String,<br>sal:Int,<br>comm:Int,<br>deptno:Int)</p><p>—————–分割———————-<br>import org.apache.spark.sql.types._</p><p>val myschema = StructType(<br>List(<br>StructField(“empno”,DataTypes.IntegerType),<br>StructField(“ename”,DataTypes.StringType),<br>StructField(“job”,DataTypes.StringType),<br>StructField(“mgr”,DataTypes.IntegerType),<br>StructField(“hiredate”,DataTypes.StringType),<br>StructField(“sal”,DataTypes.IntegerType),<br>StructField(“comm”,DataTypes.IntegerType),<br>StructField(“deptno”,DataTypes.IntegerType)<br>))</p><p>准备数据 RDD[Row]<br>import org.apache.spark.sql.Row</p><p>val allEmp = lines.map(x =&gt; Row(x(0).toInt,x(1),x(2),x(3).toInt,x(4),x(5).toInt,x(6).toInt,x(7).toInt))</p><p>val df2 = spark.createDataFrame(allEmp,myschema)</p><p>df2.show</p><p><strong>第三种方式</strong> </p><p>直接读取一个带格式的文件<br>在/root/hd/spark-2.1.0-bin-hadoop2.7/examples/src/main/resources有现成的json代码</p><p>val df3 = spark.read 读文件，默认是Parquet文件<br>val df3 = spark.read.json(“/uroot/hd/tmp_files/people.json”)</p><p>df3.show</p><p>val df4 = spark.read.format(“json”).load(“/root/hd/tmp_files/people.json”)</p><p>df4.show</p><h4 id="5、操作DataFrame"><a href="#5、操作DataFrame" class="headerlink" title="5、操作DataFrame"></a>5、操作DataFrame</h4><p><strong>1）DSL语句</strong><br>mybatis Hibernate</p><p>df1.printSchema</p><p>df1.select(“ename”,”sal”).show</p><p>df1.select($”ename”,$”sal”,$”sal”+100).show<br>$”sal” 可以看做是一个变量</p><p>查询薪水大于2000的员工<br>df1.filter($”sal” &gt; 2000).show</p><p>求每个部门的员工人数<br>df1.groupBy($”deptno”).count.show</p><p>相当于select deptno,count(1) from emp group by deptno</p><p><strong>2）SQL语句</strong> </p><p>注意：不能直接执行SQL，需要生成一个视图，再执行sql</p><p>scala&gt; df1.create<br>createGlobalTempView createOrReplaceTempView createTempView</p><p>一般用到 createOrReplaceTempView createTempView<br>视图：类似于表，但不保存数据</p><p>df1.createOrReplaceTempView(“emp”)</p><p>操作：<br>spark.sql(“select * from emp”).show</p><p>查询薪水大于2000的员工<br>spark.sql(“select * from emp where sal &gt; 2000”).show</p><p>求每个部门的员工人数<br>spark.sql(“select deptno,count(1) from emp group by deptno”).show</p><p><strong>3）多表查询</strong> </p><p>10,ACCOUNTING,NEW YORK</p><p>case class Dept(deptno:Int,dname:String,loc:String)<br>val lines = sc.textFile(“/root/hd/tmp_files/dept.csv”).map(_.split(“,”))<br>val allDept = lines.map(x=&gt;Dept(x(0).toInt,x(1),x(2)))</p><p>df5.createOrReplaceTempView(“dept”)</p><p>spark.sql(“select dname,ename from emp,dept where emp.deptno=dept.deptno”).show</p><h4 id="6、操作DataSet"><a href="#6、操作DataSet" class="headerlink" title="6、操作DataSet"></a>6、操作DataSet</h4><p>Dataset是一个分布式的数据收集器。这是在Spark1.6之后新加的一个接口，兼顾了RDD的优点（强类型，可以使用功能强大的lambda）以及Spark SQL的执行器高效性的优点。所以可以把DataFrames看成是一种特殊的Datasets，即：Dataset(Row)</p><p>Dataset跟DataFrame类似，是一套新的接口，是高级的Dataframe</p><p>举例： </p><p><strong>1）创建DataSet</strong></p><p>（1）使用序列来创建DataSet<br>定义一个case class<br>case class MyData(a:Int,b:String)</p><p>生成序列，并创建DataSet<br>val ds = Seq(MyData(1,”Tom”),MyData(2,”Merry”)).toDS</p><p>.toDS 生成DataSet</p><p>ds.show</p><p>（2）使用JSON数据来创建DataSet</p><p>定义case class<br>case class Person(name:String,age:BigInt)</p><p>通过Json数据来生成DataFrame<br>val df = spark.read.format(“json”).load(“/root/hd/tmp_files/people.json”)</p><p>将DataFrame转换成DataSet<br>df.as[Person].show</p><p>df.as[Person] 就是一个DataSet</p><p>（3）使用其他数据<br>RDD操作和DataFrame操作相结合 —&gt; DataSet</p><p>读取数据，创建DataSet<br>val linesDS = spark.read.text(“/root/hd/tmp_files/test_WordCount.txt”).as[String]</p><p>对DataSet进行操作：<br>val words = linesDS.flatMap(.split(” “)).filter(.length &gt; 3)</p><p>words.show<br>words.collect</p><p>执行一个WordCount程序<br>val result = linesDS.flatMap(.split(” “)).map((,1)).groupByKey( x =&gt; x._1).count<br>result.show</p><p>排序：</p><pre><code>result.orderBy($&quot;value&quot;).showresult.orderBy($&quot;count(1)&quot;).show</code></pre><p><strong>2）DataSet操作案例</strong> </p><p>使用emp.json 生成一个DataFrame<br>val empDF = spark.read.json(“/root/hd/tmp_files/emp.json”)</p><p>查询工资大于3000的员工<br>empDF.where($”sal” &gt;= 3000).show</p><p>创建case class</p><p>case class Emp(empno:BigInt,ename:String,job:String,mgr:String,hiredate:String,sal:BigInt,comm:String,deptno:BigInt)</p><p>生成DataSet<br>val empDS = empDF.as[Emp]</p><p>查询工资大于3000的员工<br>empDS.filter(_.sal &gt; 3000).show</p><p>查询10号部门的员工<br>empDS.filter(_.deptno == 10).show</p><p><strong>3）多表查询</strong> </p><p>（1）创建部门表<br>val deptRDD = sc.textFile(“/root/hd/tmp_files/dept.csv”).map(_.split(“,”))<br>case class Dept(deptno:Int,dname:String,loc:String)</p><p>val deptDS = deptRDD.map( x=&gt; Dept(x(0).toInt,x(1),x(2))).toDS</p><p>（2）创建员工表<br>case class Emp(empno:Int,ename:String,job:String,mgr:Int,hiredate:String,sal:Int,comm:Int,deptno:Int)<br>val empRDD = sc.textFile(“/root/hd/tmp_files/emp.csv”).map(_.split(“,”))</p><p>7369,SMITH,CLERK,7902,1980/12/17,800,0,20<br>val empDS = empRDD.map(x=&gt; Emp(x(0).toInt,x(1),x(2),x(3).toInt,x(4),x(5).toInt,x(6).toInt,x(7).toInt)).toDS</p><p>（3）执行多表查询：等值连接<br>val result = deptDS.join(empDS,”deptno”)<br>result.show<br>result.printSchema</p><p>val result1 = deptDS.joinWith(empDS, deptDS(“deptno”) === empDS(“deptno”) )<br>result1.show<br>result1.printSchema</p><p>join 和 joinWith 区别：连接后schema不同</p><p>join ：将两张表展开成一张更大的表<br>joinWith ：把两张表的数据分别做成一列，然后直接拼在一起</p><p><strong>4）多表连接后再筛选</strong> </p><p>deptDS.join(empDS,”deptno”).where(“deptno == 10”).show</p><p>result.explain：执行计划</p><h4 id="7、Spark-SQL-中的视图"><a href="#7、Spark-SQL-中的视图" class="headerlink" title="7、Spark SQL 中的视图"></a>7、Spark SQL 中的视图</h4><p>视图是一个虚表，不存储数据<br>两种类型： </p><p><strong>1）普通视图（本地视图）</strong></p><p>只在当前Session中有效createOrReplaceTempView createTempView</p><p><strong>2）全局视图</strong> </p><p>createGlobalTempView<br>在不同的Session中都有用，把全局视图创建在命名空间中：global_temp中。类似于一个库</p><p>scala&gt; df1.create<br>createGlobalTempView createOrReplaceTempView createTempView</p><p>举例：<br>创建一个新session，读取不到emp视图，报错<br>df1.createOrReplaceTempView(“emp”)<br>spark.sql(“select * from emp”).show<br>spark.newSession.sql(“select * from emp”)</p><p>以下两种方式均可读到全局视图中的数据<br>df1.createGlobalTempView(“emp1”)</p><p>spark.newSession.sql(“select * from global_temp.emp1”).show</p><p>spark.sql(“select * from global_temp.emp1”).show</p><h3 id="二、使用数据源"><a href="#二、使用数据源" class="headerlink" title="二、使用数据源"></a>二、使用数据源</h3><p>在Spark SQL中，可以使用各种各样的数据源来操作。 结构化</p><h4 id="1、使用load函数、save函数"><a href="#1、使用load函数、save函数" class="headerlink" title="1、使用load函数、save函数"></a>1、使用load函数、save函数</h4><p>load函数是加载数据，save是存储数据</p><p>注意：使用load 或 save时，默认是Parquet文件。列式存储文件</p><p>举例:<br>读取 users.parquet 文件<br>val userDF = spark.read.load(“/root/hd/tmp_files/users.parquet”)</p><p>userDF.printSchema<br>userDF.show</p><p>val userDF = spark.read.load(“/root/hd/tmp_files/emp.json”)</p><p>保存parquet文件</p><pre><code>userDF.select($&quot;name&quot;,$&quot;favorite_color&quot;).write.save(&quot;/root/hd/tmp_files/parquet&quot;)</code></pre><p>读取刚刚写入的文件：<br>val userDF1 = spark.read.load(“/root/hd/tmp_files/parquet/part-00000-f9a3d6bb-d481-4fc9-abf6-5f20139f97c5.snappy.parquet”)—&gt; 不推荐</p><p>生产中直接读取存放的目录即可：<br>val userDF2 = spark.read.load(“/root/hd/tmp_files/parquet”)</p><p>读json文件 必须format<br>val userDF = spark.read.format(“json”).load(“/root/hd/tmp_files/emp.json”)<br>val userDF3 = spark.read.json(“/root/hd/tmp_files/emp.json”)</p><p>关于<strong>save函数</strong>： </p><p>调用save函数的时候，可以指定存储模式，追加、覆盖等等<br>userDF.write.save(“/root/hd/tmp_files/parquet”)</p><p>userDF.write.save(“/root/hd/tmp_files/parquet”)<br>org.apache.spark.sql.AnalysisException: path file:/root/hd/tmp_files/parquet already exists.;</p><p>save的时候覆盖<br>userDF.write.mode(“overwrite”).save(“/root/hd/tmp_files/parquet”)</p><p>将结果保存成表<br>userDF.select($”name”).write.saveAsTable(“table1”)</p><p>scala&gt; userDF.select($”name”).write.saveAsTable(“table1”)</p><p>scala&gt; spark.sql(“select * from table1”).show<br>+——+<br>| name|<br>+——+<br>|Alyssa|<br>| Ben|<br>+——+</p><h4 id="2、Parquet文件"><a href="#2、Parquet文件" class="headerlink" title="2、Parquet文件"></a>2、Parquet文件</h4><p>列式存储文件，是Spark SQL 默认的数据源<br>就是一个普通的文件</p><p>举例：<br>1）把其他文件，转换成Parquet文件<br>调用save函数<br>把数据读进来，再写出去，就是Parquet文件</p><p>val empDF = spark.read.json(“/root/hd/tmp_files/emp.json”)<br>empDF.write.mode(“overwrite”).save(“/root/hd/tmp_files/parquet”)<br>empDF.write.mode(“overwrite”).parquet(“/root/hd/tmp_files/parquet”)</p><p>val emp1 = spark.read.parquet(“/root/hd/tmp_files/parquet”)<br>emp1.createOrReplaceTempView(“emp1”)<br>spark.sql(“select * from emp1”)</p><p>2）支持Schema的合并<br>项目开始 表结构简单 schema简单<br>项目越来越大 schema越来越复杂</p><p>举例：<br>通过RDD来创建DataFrame<br>val df1 = sc.makeRDD(1 to 5).map( i =&gt; (i,i*2)).toDF(“single”,”double”)<br>“single”,”double” 是表结构<br>df1.show</p><p>df1.write.mode(“overwrite”).save(“/root/hd/tmp_files/test_table/key=1”)</p><p>val df2 = sc.makeRDD(6 to 10).map( i =&gt; (i,i*3)).toDF(“single”,”triple”)<br>df2.show<br>df2.write.mode(“overwrite”).save(“/root/hd/tmp_files/test_table/key=2”)</p><p>合并两个部分<br>val df3 = spark.read.parquet(“/root/hd/tmp_files/test_table”)</p><p>val df3 = spark.read.option(“mergeSchema”,true).parquet(“/root/hd/tmp_files/test_table”)</p><p><strong>key是可以随意取名字的，两个key需要一致，不然合并会报错</strong></p><p>通过RDD来创建DataFrame<br>val df1 = sc.makeRDD(1 to 5).map( i =&gt; (i,i*2)).toDF(“single”,”double”)<br>“single”,”double” 是表结构<br>df1.show</p><p>df1.write.mode(“overwrite”).save(“/root/hd/tmp_files/test_table/kt=1”)</p><p>val df2 = sc.makeRDD(6 to 10).map( i =&gt; (i,i*3)).toDF(“single”,”triple”)<br>df2.show<br>df2.write.mode(“overwrite”).save(“/root/hd/tmp_files/test_table/kt=2”)</p><p>合并两个部分<br>val df3 = spark.read.parquet(“/root/hd/tmp_files/test_table”)</p><p>val df3 = spark.read.option(“mergeSchema”,true).parquet(“/root/hd/tmp_files/test_table”)</p><h4 id="3、json文件"><a href="#3、json文件" class="headerlink" title="3、json文件"></a>3、json文件</h4><p>读取Json文件，生成DataFrame<br>val peopleDF = spark.read.json(“/root/hd/tmp_files/people.json”)</p><p>peopleDF.printSchema</p><p>peopleDF.createOrReplaceTempView(“peopleView”)</p><p>spark.sql(“select * from peopleView”).show</p><p>Spark SQL 支持统一的访问接口。对于不同的数据源，读取进来，生成DataFrame后，操作完全一样</p><h4 id="4、JDBC"><a href="#4、JDBC" class="headerlink" title="4、JDBC"></a>4、JDBC</h4><p>使用JDBC操作关系型数据库，加载到Spark中进行分析和处理</p><p>方式一：</p><pre><code>./spark-shell --master spark://hsiehchou121:7077 --jars /root/hd/tmp_files/mysql-connector-java-8.0.12.jar --driver-class-path /root/hd/tmp_files/mysql-connector-java-8.0.12.jar </code></pre><pre><code>val mysqlDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;,&quot;jdbc:mysql://192.168.116.1/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;).option(&quot;driver&quot;,&quot;com.mysql.cj.jdbc.Driver&quot;).option(&quot;user&quot;,&quot;root&quot;).option(&quot;password&quot;,&quot;123456&quot;).option(&quot;dbtable&quot;,&quot;emp&quot;).load</code></pre><pre><code>val mysqlDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;,&quot;jdbc:mysql://192.168.116.1/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;).option(&quot;driver&quot;,&quot;com.mysql.cj.jdbc.Driver&quot;).option(&quot;user&quot;,&quot;root&quot;).option(&quot;password&quot;,&quot;123456&quot;).option(&quot;dbtable&quot;,&quot;emp&quot;).loadmysqlDF.show</code></pre><p><strong>问题解决</strong></p><p>如果遇到下面问题，就是你本机的mysql数据库没有权限给你虚拟机访问<br>java.sql.SQLException: null, message from server: “Host ‘hsiehchou121’ is not allowed to connect to this MySQL server”</p><p><strong>解决方案</strong></p><p>1）进入你本机的数据库<br>mysql -u root -p<br>2）use mysql;<br>3）修改root用户前面的Host，改为%，意思是全部IP都能访问<br>4）flush privileges;</p><p><strong>方式二</strong><br>定义一个Properties类<br>import java.util.Properties<br>val mysqlProps = new Properties()<br>mysqlProps.setProperty(“driver”,”com.mysql.cj.jdbc.Driver”)<br>mysqlProps.setProperty(“user”,”root”)<br>mysqlProps.setProperty(“password”,”123456”)</p><p>val mysqlDF1 = spark.read.jdbc(“jdbc:mysql://192.168.116.1:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8”,”emp”,mysqlProps)</p><p>mysqlDF1.show</p><h4 id="5、使用Hive"><a href="#5、使用Hive" class="headerlink" title="5、使用Hive"></a>5、使用Hive</h4><p>比较常见<br>（<em>）spark SQL 完全兼容hive<br>（</em>）需要进行配置<br>拷贝一下文件到spark/conf目录下：<br>Hive 配置文件： hive-site.xml<br>Hadoop 配置文件：core-site.xml hdfs-site.xml</p><p>配置好后，重启spark</p><p><strong>在hive的lib下和spark的jars下面增加mysql-connector-java-8.0.12.jar这边连接数据库的jar包</strong></p><p>启动Hadoop ：start-all.sh<br>启动 hive：</p><pre><code>hsiehchou121cd hive/bin/./hive --service metastorehsiehchou122cd hive/bin./hive</code></pre><p><strong>hsiehchou121启动问题</strong></p><p>java.sql.SQLSyntaxErrorException: Table ‘hive.version’ doesn’t exist<br>解决：去mysql数据库中的hive库下面创建version表<br>这里需要给本地的hive库创建下hive所必须用的表</p><p>我们去/root/hd/hive/scripts/metastore/upgrade/mysql这里面找到hive-schema-1.2.0.mysql.sql，将里面的sql语句在hive库中执行</p><p>hive-txn-schema-0.14.0.mysql.sql，这个也做好执行下，用于事务管理</p><p><strong>显示当前所在库名字</strong> </p><p>set hive.cli.print.current.db=true;</p><p>j将emp.csv上传到hdfs中的/tmp_files/下面<br>hdfs dfs -put emp.csv /tmp_files</p><p>在hive中创建emp_default表</p><pre><code>hive (default)&gt; create table emp(empno int,ename string,job string,mgr int,hiredate string,sal int,comm int,deptno int)              &gt; row format              &gt; delimited fields              &gt; terminated by &quot;,&quot;;hive (default)&gt; load data inpath &#39;/tmp_files/emp.csv&#39; into table emp;Time taken: 1.894 secondshive (default)&gt; show tables;hive (default)&gt; select * from emp;</code></pre><p>hdfs dfs -put /root/hd/tmp_files/emp.csv /tmp_files</p><pre><code>[root@hsiehchou121 bin]# ./spark-shell --master spark://hsiehchou121:7077</code></pre><p>启动Spatk时，如果出现如下错误<br>java.sql.SQLSyntaxErrorException: Table ‘hive.partitions’ doesn’t exist<br>在MySQL数据库里面创建partitions表</p><p>scala&gt; spark.sql(“select * from emp_default”).show<br>scala&gt; spark.sql(“select * from default.emp_default”).show</p><p>spark.sql(“create table company.emp_4(empno Int,ename String,job String,mgr String,hiredate String,sal Int,comm String,deptno Int)row format delimited fields terminated by ‘,’”)<br>spark.sql(“load data local inpath ‘/root/hd/tmp_files/emp.csv’ overwrite into table company.emp_4”)</p><h3 id="三、在IDE中开发Spark-SQL"><a href="#三、在IDE中开发Spark-SQL" class="headerlink" title="三、在IDE中开发Spark SQL"></a>三、在IDE中开发Spark SQL</h3><h4 id="1、创建DataFrame-StructType方式"><a href="#1、创建DataFrame-StructType方式" class="headerlink" title="1、创建DataFrame StructType方式"></a>1、创建DataFrame StructType方式</h4><pre><code>package day4import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.types.StructTypeimport org.apache.spark.sql.types.StructFieldimport org.apache.spark.sql.types.IntegerTypeimport org.apache.spark.sql.types.StringTypeimport org.apache.spark.sql.Rowimport org.apache.log4j.Loggerimport org.apache.log4j.Level/** * 创建DataFrame StructType方式 */object Demo1 {  def main(args: Array[String]): Unit = {    //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建Spark Session对象    val spark = SparkSession.builder().master(&quot;local&quot;).appName(&quot;Demo1&quot;).getOrCreate()    //从指定的地址创建RDD对象    val personRDD = spark.sparkContext.textFile(&quot;H:\\other\\students.txt&quot;).map(_.split(&quot;\t&quot;))    //通过StructType方式指定Schema    val schema = StructType(      List(        StructField(&quot;id&quot;, IntegerType),        StructField(&quot;name&quot;, StringType),        StructField(&quot;age&quot;, IntegerType)))    //将RDD映射到rowRDD上，映射到Schema上    val rowRDD = personRDD.map(p =&gt; Row(p(0).toInt,p(1),p(2).toInt))    val personDataFrame = spark.createDataFrame(rowRDD, schema)    //注册视图    personDataFrame.createOrReplaceTempView(&quot;t_person&quot;)    //执行SQL语句  desc降序   asc 升序    val df = spark.sql(&quot;select * from t_person order by age desc&quot;)    df.show    spark.stop()  }}</code></pre><h4 id="2、使用case-class来创建DataFrame"><a href="#2、使用case-class来创建DataFrame" class="headerlink" title="2、使用case class来创建DataFrame"></a>2、使用case class来创建DataFrame</h4><pre><code>package day4import org.apache.log4j.Loggerimport org.apache.log4j.Levelimport org.apache.spark.sql.SparkSession/** * 使用case class来创建DataFrame */object Demo2 {  def main(args: Array[String]): Unit = {    //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建Spark Session对象    val spark = SparkSession.builder().master(&quot;local&quot;).appName(&quot;Demo1&quot;).getOrCreate()    //从指定的地址创建RDD对象    val lineRDD = spark.sparkContext.textFile(&quot;H:\\other\\students.txt&quot;).map(_.split(&quot;\t&quot;))    //把数据与case class做匹配    val studentRDD = lineRDD.map(x =&gt; Student(x(0).toInt,x(1),x(2).toInt))    //生成DataFrame    import spark.sqlContext.implicits._    val studentDF = studentRDD.toDF()    //注册视图,执行SQL    studentDF.createOrReplaceTempView(&quot;student&quot;)    spark.sql(&quot;select * from student&quot;).show    spark.stop()  }}//定义case classcase class Student(stuId:Int, stuName:String, stuAge:Int)</code></pre><h4 id="3、写入MySQL"><a href="#3、写入MySQL" class="headerlink" title="3、写入MySQL"></a>3、写入MySQL</h4><pre><code>package day4import org.apache.log4j.Loggerimport org.apache.log4j.Levelimport org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.types.IntegerTypeimport org.apache.spark.sql.types.StringTypeimport org.apache.spark.sql.Rowimport org.apache.spark.sql.types.StructTypeimport org.apache.spark.sql.types.StructFieldimport java.util.Properties/** * 写入mysql */object Demo3 {  def main(args: Array[String]): Unit = {    //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建Spark Session对象    val spark = SparkSession.builder().master(&quot;local&quot;).appName(&quot;Demo1&quot;).getOrCreate()    //从指定的地址创建RDD对象    val lineRDD = spark.sparkContext.textFile(&quot;H:\\other\\students.txt&quot;).map(_.split(&quot;\t&quot;))    //通过StructType方式指定Schema    val schema = StructType(      List(        StructField(&quot;personID&quot;, IntegerType),        StructField(&quot;personName&quot;, StringType),        StructField(&quot;personAge&quot;, IntegerType)))    //将RDD映射到rowRDD上，映射到Schema上    val rowRDD = lineRDD.map(p =&gt; Row(p(0).toInt,p(1),p(2).toInt))    val personDataFrame = spark.createDataFrame(rowRDD, schema)    personDataFrame.createOrReplaceTempView(&quot;myperson&quot;)    val result = spark.sql(&quot;select * from myperson&quot;)    result.show    //把结果存入mysql中    val props = new Properties()    props.setProperty(&quot;user&quot;, &quot;root&quot;)    props.setProperty(&quot;password&quot;, &quot;123456&quot;)    props.setProperty(&quot;driver&quot;, &quot;com.mysql.cj.jdbc.Driver&quot;)    result.write.mode(&quot;append&quot;).jdbc(&quot;jdbc:mysql://localhost:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;, &quot;student&quot;, props)    spark.stop()  }}</code></pre><h4 id="4、使用Spark-SQL-读取Hive中的数据，将计算结果存入MySQL"><a href="#4、使用Spark-SQL-读取Hive中的数据，将计算结果存入MySQL" class="headerlink" title="4、使用Spark SQL 读取Hive中的数据，将计算结果存入MySQL"></a>4、使用Spark SQL 读取Hive中的数据，将计算结果存入MySQL</h4><pre><code>package day4import org.apache.spark.sql.SparkSessionimport java.util.Properties/** * 使用Spark SQL 读取Hive中的数据，将计算结果存入mysql */object Demo4 {  def main(args: Array[String]): Unit = {    //创建SparkSession    val spark = SparkSession.builder().appName(&quot;Demo4&quot;).enableHiveSupport().getOrCreate()    //执行SQL    val result = spark.sql(&quot;select deptno,count(1) from company.emp group by deptno&quot;)    //将结果保存到mysql中     val props = new Properties()    props.setProperty(&quot;user&quot;, &quot;root&quot;)    props.setProperty(&quot;password&quot;, &quot;123456&quot;)    props.setProperty(&quot;driver&quot;, &quot;com.mysql.cj.jdbc.Driver&quot;)    result.write.jdbc(&quot;jdbc:mysql://192.168.116.1:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;, &quot;emp_stat&quot;, props)    spark.stop()  } }</code></pre><p><strong>提交任务</strong></p><pre><code>[root@hsiehchou121 bin]# ./spark-submit --master spark://hsiehchou121:7077 --jars /root/hd/tmp_files/mysql-connector-java-8.0.12.jar --driver-class-path /root/hd/tmp_files/mysql-connector-java-8.0.12.jar --class day4.Demo4 /root/hd/tmp_files/Demo4.jar</code></pre><h3 id="四、性能优化"><a href="#四、性能优化" class="headerlink" title="四、性能优化"></a>四、性能优化</h3><p>与RDD类似</p><h4 id="1、把内存中缓存表的数据"><a href="#1、把内存中缓存表的数据" class="headerlink" title="1、把内存中缓存表的数据"></a>1、把内存中缓存表的数据</h4><p>直接读取内存的值，来提高性能</p><p>RDD中如何缓存：<br>rdd.cache 或者 rdd.persist</p><p>在Spark SQL中，使用SparkSession.sqlContext.cacheTable</p><p>spark中所有context对象<br>1）sparkContext ： SparkCore<br>2）sql Context ： SparkSQL<br>3）Streaming Context ：SparkStreaming</p><p>统一起来：SparkSession</p><p>操作mysql，启动spark shell 时，需要：<br>./spark-shell <code>--master</code> spark://hsiehchou121:7077 <code>--jars</code> /root/hd/tmp_files/mysql-connector-java-8.0.12.jar <code>--driver-class-path</code> /root/hd/tmp_files/mysql-connector-java-8.0.12.jar</p><p>val mysqlDF = spark.read.format(“jdbc”).option(“driver”,”com.mysql.cj.jdbc.Driver”).option(“url”,”jdbc:mysql://192.168.116.1:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8”).option(“user”,”root”).option(“password”,”123456”).option(“dbtable”,”emp”).load</p><p>mysqlDF.show<br>mysqlDF.createOrReplaceTempView(“emp”)</p><p>spark.sqlContext.cacheTable(“emp”) —-&gt; 标识这张表可以被缓存，数据还没有真正被缓存<br>spark.sql(“select * from emp”).show —-&gt; 依然读取mysql<br>spark.sql(“select * from emp”).show —-&gt; 从缓存中读取数据</p><p>spark.sqlContext.clearCache</p><p>清空缓存后，执行查询，会触发查询mysql数据库</p><h4 id="2、了解性能优化的相关参数"><a href="#2、了解性能优化的相关参数" class="headerlink" title="2、了解性能优化的相关参数"></a>2、了解性能优化的相关参数</h4><p>将数据缓存到内存中的相关优化参数<br>spark.sql.inMemoryColumnarStorage.compressed<br>默认为 true<br>Spark SQL 将会基于统计信息自动地为每一列选择一种压缩编码方式</p><p>spark.sql.inMemoryColumnarStorage.batchSize<br>默认值：10000<br>缓存批处理大小。缓存数据时, 较大的批处理大小可以提高内存利用率和压缩率，但同时也会带来 OOM（Out Of Memory）的风险</p><p>其他性能相关的配置选项（不过不推荐手动修改，可能在后续版本自动的自适应修改）<br>spark.sql.files.maxPartitionBytes<br>默认值：128 MB<br>读取文件时单个分区可容纳的最大字节数</p><p>spark.sql.files.openCostInBytes<br>默认值：4M<br>打开文件的估算成本, 按照同一时间能够扫描的字节数来测量。当往一个分区写入多个文件的时候会使用。高估更好, 这样的话小文件分区将比大文件分区更快 (先被调度)</p><p>spark.sql.autoBroadcastJoinThreshold<br>默认值：10M<br>用于配置一个表在执行 join 操作时能够广播给所有 worker 节点的最大字节大小。通过将这个值设置为 -1 可以禁用广播。注意，当前数据统计仅支持已经运行了 ANALYZE TABLE COMPUTE STATISTICS noscan 命令的 Hive Metastore 表</p><p>spark.sql.shuffle.partitions<br>默认值：200<br>用于配置 join 或聚合操作混洗（shuffle）数据时使用的分区数</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Core</title>
      <link href="/2019/03/29/spark-core/"/>
      <url>/2019/03/29/spark-core/</url>
      
        <content type="html"><![CDATA[<p>Spark生态圈：<br>Spark Core ： RDD（弹性分布式数据集）<br>Spark SQL<br>Spark Streaming<br>Spark MLLib ：协同过滤，ALS，逻辑回归等等 –&gt; 机器学习<br>Spark Graphx ： 图计算</p><h3 id="一、Spark-Core"><a href="#一、Spark-Core" class="headerlink" title="一、Spark Core"></a>一、Spark Core</h3><h4 id="1、什么是Spark？特点"><a href="#1、什么是Spark？特点" class="headerlink" title="1、什么是Spark？特点"></a>1、什么是Spark？特点</h4><p><a href="https://spark.apache.org/" target="_blank" rel="noopener">https://spark.apache.org/</a><br>Apache Spark™ is a unified analytics engine for large-scale data processing.<br>特点：快、易用、通用性、兼容性（完全兼容Hadoop）</p><p>快：快100倍（Hadoop 3 之前）<br>易用：支持多种语言开发<br>通用性：生态系统全<br>易用性：兼容Hadoop</p><h3 id="二、安装和部署Spark、Spark-的-HA"><a href="#二、安装和部署Spark、Spark-的-HA" class="headerlink" title="二、安装和部署Spark、Spark 的 HA"></a>二、安装和部署Spark、Spark 的 HA</h3><h4 id="1、Spark体系结构"><a href="#1、Spark体系结构" class="headerlink" title="1、Spark体系结构"></a>1、Spark体系结构</h4><p>Spark的运行方式</p><p>Yarn</p><p>Standalone：本机调试（Demo）</p><p>Worker：从节点。每个服务器上，资源和任务的管理者。只负责管理一个节点</p><p>执行过程：<br>一个Worker 有多个 Executor。 Executor是任务的执行者，按阶段（stage）划分任务。————&gt; RDD</p><p>客户端：Driver Program 提交任务到集群中<br>1）spark-submit<br>2）spark-shell</p><h4 id="2、Spark的搭建"><a href="#2、Spark的搭建" class="headerlink" title="2、Spark的搭建"></a>2、Spark的搭建</h4><p>1）准备工作：JDK 配置主机名 免密码登录</p><p>2）伪分布式模式<br>在一台虚拟机上模拟分布式环境（Master和Worker在一个节点上）<br>配置spark-env.sh<br>vi spark-env.sh</p><pre><code>export JAVA_HOME=/root/hd/jdk1.8.0_192export SPARK_MASTER_HOST=hsiehchou121export SPARK_MASTER_PORT=7077</code></pre><p>配置slaves<br>vi slaves<br>hsiehchou121</p><p>浏览器访问hsiehchou121:8080</p><p>在Spark中使用Scala语言</p><pre><code>[root@hsiehchou121 bin]# ./spark-shell --master spark://hsiehchou121:7077</code></pre><p>3）全分布式环境<br>修改slave文件 拷贝到其他三台服务器 启动</p><h4 id="3、Spark的-HA"><a href="#3、Spark的-HA" class="headerlink" title="3、Spark的 HA"></a>3、Spark的 HA</h4><p>回顾HA（高可用）<br>（<em>）HDFS Yarn Hbase Spark 主从结构<br>（</em>）单点故障</p><p>（1）基于文件目录的单点恢复<br>主要用于开发或测试环境。当spark提供目录保存spark Application和worker的注册信息，并将他们的恢复状态写入该目录中，这时，一旦Master发生故障，就可以通过重新启动Master进程（sbin/start-master.sh），恢复已运行的spark Application和worker的注册信息</p><p>基于文件系统的单点恢复，主要是在spark-en.sh里对SPARK_DAEMON_JAVA_OPTS设置</p><table><thead><tr><th align="center">配置参数</th><th align="center">参考值</th></tr></thead><tbody><tr><td align="center">spark.deploy.recoveryMode</td><td align="center">设置为FILESYSTEM开启单点恢复功能，默认值：NONE</td></tr><tr><td align="center">spark.deploy.recoveryDirectory</td><td align="center">Spark 保存恢复状态的目录</td></tr></tbody></table><p><strong>参考</strong>：<br>export SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=FILESYSTEM -Dspark.deploy.recoveryDirectory=/root/hd/spark-2.1.0-bin-hadoop2.7/recovery”</p><p>（*）本质：还是只有一个主节点Master，创建了一个恢复目录，保存集群状态和任务的信息<br>当Master挂掉，重新启动时，会从恢复目录下读取状态信息，恢复出来原来的状态</p><p>用途：这个只用于开发和测试，但是生产使用用ZooKeeper</p><p>export SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=FILESYSTEM -Dspark.deploy.recoveryDirectory=/root/hd/spark-2.1.0-bin-hadoop2.7/recovery”</p><p>（2）基于ZooKeeper ：和Hadoop类似<br>ZooKeeper提供了一个Leader Election机制，利用这个机制可以保证虽然集群存在多个Master，但是只有一个是Active的，其他的都是Standby。当Active的Master出现故障时，另外的一个Standby Master会被选举出来。由于集群的信息，包括Worker， Driver和Application的信息都已经持久化到ZooKeeper，因此在切换的过程中只会影响新Job的提交，对于正在进行的Job没有任何的影响</p><table><thead><tr><th align="center">配置参数</th><th align="center">参考值</th></tr></thead><tbody><tr><td align="center">spark.deploy.recoveryMode</td><td align="center">设置为ZOOKEEPER开启单点恢复功能，默认值：NONE</td></tr><tr><td align="center">spark.deploy.zookeeper.url</td><td align="center">ZooKeeper集群的地址</td></tr><tr><td align="center">spark.deploy.zookeeper.dir</td><td align="center">Spark信息在ZK中的保存目录，默认：/spark</td></tr></tbody></table><p><strong>参考</strong>：<br>export SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181,hsiehchou124:2181 -Dspark.deploy.zookeeper.dir=/spark”</p><p>（*）复习一下zookeeper：<br>相当于一个数据库，把一些信息存放在zookeeper中，比如集群的信息<br>数据同步功能，选举功能，分布式锁功能</p><p>数据同步：给一个节点中写入数据，可以同步到其他节点</p><p>选举：Zookeeper中存在不同的角色，Leader Follower。如果Leader挂掉，重新选举Leader</p><p>分布式锁：秒杀。以目录节点的方式来保存数据</p><p>修改 spark-env.sh</p><pre><code>export JAVA_HOME=/root/hd/jdk1.8.0_192#export SPARK_MASTER_HOST=hsiehchou121#export SPARK_MASTER_PORT=7077#export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=FILESYSTEM -Dspark.deploy.recoveryDirectory=/root/hd/spark-2.1.0-bin-hadoop2.7/recovery&quot;export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181,hsiehchou124:2181 -Dspark.deploy.zookeeper.dir=/spark&quot;</code></pre><p>同步到其他三台服务器<br>[root@hsiehchou121 spark-2.1.0-bin-hadoop2.7]# scp conf/spark-env.sh hsiehchou122:/root/hd/spark-2.1.0-bin-hadoop2.7/conf<br>[root@hsiehchou121 spark-2.1.0-bin-hadoop2.7]# scp conf/spark-env.sh hsiehchou123:/root/hd/spark-2.1.0-bin-hadoop2.7/conf<br>[root@hsiehchou121 spark-2.1.0-bin-hadoop2.7]# scp conf/spark-env.sh hsiehchou124:/root/hd/spark-2.1.0-bin-hadoop2.7/conf</p><p>在hsiehchou121 start-all hsiehchou121 master hsiehchou122 Worker hsiehchou123 Worker hsiehchou124 Worker<br>在hsiehchou121 start-master hsiehchou121 master hsiehchou122 master（standby） hsiehchou122 Worker hsiehchou123 Worker hsiehchou124 Worker</p><p>在hsiehchou121 上kill master<br>hsiehchou122 master（Active） hsiehchou122 Worker hsiehchou123 Worker hsiehchou124 Worker</p><p>在网页<a href="http://192.168.116.122:8080/" target="_blank" rel="noopener">http://192.168.116.122:8080/</a> 可以看到相应信息</p><h3 id="三、执行Spark的任务：两个工具"><a href="#三、执行Spark的任务：两个工具" class="headerlink" title="三、执行Spark的任务：两个工具"></a>三、执行Spark的任务：两个工具</h3><h4 id="1、spark-submit：用于提交Spark的任务"><a href="#1、spark-submit：用于提交Spark的任务" class="headerlink" title="1、spark-submit：用于提交Spark的任务"></a>1、spark-submit：用于提交Spark的任务</h4><p>任务：jar</p><p>举例：蒙特卡洛求PI（圆周率）</p><p>./spark-submit <code>--master</code> spark://hsiehchou121:7077 <code>--class</code><br><code>--class</code>指明主程序的名字</p><pre><code>[root@hsiehchou121 /]#cd /root/hd/spark-2.1.0-bin-hadoop2.7/bin[root@hsiehchou121 bin]# ./spark-submit --master spark://hsiehchou121:7077 --class org.apache.spark.examples.SparkPi /root/hd/spark-2.1.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.1.0.jar 100</code></pre><p>其中100指定执行的次数</p><h4 id="2、spark-shell-相当于REPL"><a href="#2、spark-shell-相当于REPL" class="headerlink" title="2、spark-shell 相当于REPL"></a>2、spark-shell 相当于REPL</h4><p>spark-shell是Spark自带的交互式Shell程序，方便用户进行交互式编程，用户可以在该命令行下用scala编写spark程序<br>（*）启动Spark Shell：spark-shell<br>也可以使用以下参数：<br>参数说明：<br><code>--master</code> spark://hsiehchou121:7077 指定Master的地址<br><code>--executor-memory</code> 2g 指定每个worker可用内存为2G<br><code>--total-executor-cores</code> 2 指定整个集群使用的cup核数为2个<br>例如：</p><p>spark-shell <code>--master</code> spark://hsiehchou121:7077 <code>--executor-memory</code> 2g <code>--total-executor-cores</code> 2<br>注意：<br>如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系</p><p>作为一个独立的Application运行<br>两种模式：<br>（1）<strong>本地模式</strong><br>spark-shell 后面不接任何参数，代表本地模式<br>./spark-shell<br>Spark context available as ‘sc’ (master = local[<em>], app id = local-1554372019995).<br>sc 是 SparkContext 对象名。 local[</em>] 代表本地模式，不提交到集群中运行</p><p>（2）<strong>集群模式</strong><br>[root@hsiehchou121 bin]# ./spark-shell <code>--master</code> spark://hsiehchou121:7077<br>提交到集群运行<br>Spark context available as ‘sc’ (master = spark://hsiehchou121:7077, app id = app-20190404190030-0000).</p><p>master = spark://hsiehchou121:7077<br>Spark session available as ‘spark’<br>Spark Session 是 2.0 以后提供的，利用 SparkSession 可以访问spark所有组件</p><p>示例：<br><strong>WordCount程序</strong> </p><p>程序如下：</p><pre><code>sc.textFile(&quot;hdfs://192.168.116.121:9000/data.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;hdfs://192.168.116.121:9000/output/wc&quot;)</code></pre><p>说明：<br>sc是SparkContext对象，该对象时提交spark程序的入口<br>textFile(“hdfs://192.168.116.121:9000/data.txt”)是hdfs中读取数据<br>flatMap(<em>.split(” “))先map在压平<br>map((</em>,1))将单词和1构成元组<br>reduceByKey(+)按照key进行reduce，并将value累加<br>saveAsTextFile(“hdfs://192.168.116.121:9000/output/wc”)将结果写入到hdfs中</p><p>（*）处理本地文件，把结果打印到屏幕上<br>vi /root/hd/tmp_files/test_WordCount.txt<br>I love China<br>I love Jiangsu<br>Jiangsu is a beautiful place in China</p><pre><code>scala&gt; sc.textFile(&quot;/root/hd/tmp_files/test_WordCount.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).collectres0: Array[(String, Int)] = Array((is,1), (love,2), (China,2), (a,1), (Jiangsu,2), (I,2), (in,1), (place,1), (beautiful,1))</code></pre><p>（*）处理HDFS文件，结果保存在HDFS上</p><pre><code>[root@hsiehchou121 tmp_files]# hdfs dfs -mkdir /tmp_files[root@hsiehchou121 tmp_files]# hdfs dfs -copyFromLocal ~/hd/tmp_files/test_WordCount.txt /tmp_files</code></pre><pre><code>scala&gt; sc.textFile(&quot;hdfs://hsiehchou121:9000/tmp_files/test_WordCount.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;hdfs://hsiehchou121:9000/out/0404/test_WordCount&quot;)</code></pre><p>-rw-r–r– 3 root supergroup 0 2019-04-04 19:12 /out/0404/test_WordCount/_SUCCESS<br>-rw-r–r– 3 root supergroup 16 2019-04-04 19:12 /out/0404/test_WordCount/part-00000<br>-rw-r–r– 3 root supergroup 65 2019-04-04 19:12 /out/0404/test_WordCount/part-00001</p><p>_SUCCESS 代表程序执行成功</p><p>part-00000 part-00001 结果文件，分区。里面内容不重复</p><p>（*）单步运行WordCount —-&gt; RDD<br>scala&gt; val rdd1 = sc.textFile(“/root/hd/tmp_files/test_WordCount.txt”)<br>rdd1: org.apache.spark.rdd.RDD[String] = /root/hd/tmp_files/test_WordCount.txt MapPartitionsRDD[12] at textFile at <code>&lt;console&gt;:</code>24</p><p>scala&gt; rdd1.collect<br>res5: Array[String] = Array(I love China, I love Jiangsu, Jiangsu is a beautiful place in China)</p><p>scala&gt; val rdd2 = rdd1.flatMap(_.split(“ “))<br>rdd2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[14] at flatMap at <code>&lt;console&gt;</code>:26</p><p>scala&gt; rdd2.collect<br>res6: Array[String] = Array(I, love, China, I, love, Jiangsu, Jiangsu, is, a, beautiful, place, in, China)</p><p>scala&gt; val rdd3 = rdd2.map((_,1))<br>rdd3: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[15] at map at <code>&lt;console&gt;</code>:28</p><p>scala&gt; rdd3.collect<br>res7: Array[(String, Int)] = Array((I,1), (love,1), (China,1), (I,1), (love,1), (Jiangsu,1), (Jiangsu,1), (is,1), (a,1), (beautiful,1), (place,1), (in,1), (China,1))</p><p>scala&gt; val rdd4 = rdd3.reduceByKey(<em>+</em>)<br>rdd4: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[16] at reduceByKey at <code>&lt;console&gt;</code>:30</p><p>scala&gt; rdd4.collect<br>res8: Array[(String, Int)] = Array((is,1), (love,2), (China,2), (a,1), (Jiangsu,2), (I,2), (in,1), (place,1), (beautiful,1))</p><p><strong>RDD 弹性分布式数据集</strong> </p><p>（1）依赖关系 ： 宽依赖和窄依赖<br>（2）算子：<br>函数：<br>Transformation ： 延时计算 map flatMap textFile<br>Action ： 立即触发计算 collect</p><p>说明：</p><p><strong>scala复习</strong> </p><p>（*）flatten：把嵌套的结果展开<br>scala&gt; List(List(2,4,6,8,10),List(1,3,5,7,9)).flatten<br>res21: List[Int] = List(2, 4, 6, 8, 10, 1, 3, 5, 7, 9)</p><p>（*）flatmap : 相当于一个 map + flatten<br>scala&gt; var myList = List(List(2,4,6,8,10),List(1,3,5,7,9))<br>myList: List[List[Int]] = List(List(2, 4, 6, 8, 10), List(1, 3, 5, 7, 9))</p><p>scala&gt; myList.flatMap(x=&gt;x.map(_*2))<br>res22: List[Int] = List(4, 8, 12, 16, 20, 2, 6, 10, 14, 18)</p><p>myList.flatMap(x=&gt;x.map(_*2))</p><p>执行过程：<br>（1）将 List(2, 4, 6, 8, 10), List(1, 3, 5, 7, 9) 调用 map(_*2) 方法。x 代表一个List<br>（2）flatten<br>（3）在IDE中开发scala版本和Java版本的WorkCount</p><h3 id="四、WordCount（Scala版本和Java版本）"><a href="#四、WordCount（Scala版本和Java版本）" class="headerlink" title="四、WordCount（Scala版本和Java版本）"></a>四、WordCount（Scala版本和Java版本）</h3><h4 id="1、Scala版本的WordCount"><a href="#1、Scala版本的WordCount" class="headerlink" title="1、Scala版本的WordCount"></a>1、Scala版本的WordCount</h4><p>新建一个工程，把jar引入到工程中</p><pre><code>package day1import org.apache.spark.SparkConfimport org.apache.spark.SparkContextobject WordCount {  def main(args: Array[String]): Unit = {    //创建一个Spark的配置文件    val conf = new SparkConf().setAppName(&quot;My Scala WordCount 0404&quot;).setMaster(&quot;local&quot;)    //创建SparkContext对象    val sc = new SparkContext(conf)    //1.从本地模式运行 .setMaster(&quot;local&quot;)   //val result = sc.textFile(&quot;hdfs://hsiehchou121:9000/tmp_files/test_WordCount.txt&quot;)      //.flatMap(_.split(&quot; &quot;))      //.map((_,1))      //.reduceByKey(_+_)    //result.foreach(println)    //2、在集群模式运行    val result = sc.textFile(args(0))      .flatMap(_.split(&quot; &quot;))      .map((_, 1))      .reduceByKey(_ + _)      .saveAsTextFile(args(1))    sc.stop()  }}</code></pre><p>export Demo1.jar 点击下一步，把jar包上传到服务器上/root/hd/tmp_files/下</p><p>在spark里面的bin目录下输入</p><p>[root@hsiehchou121 bin]# ./spark-submit <code>--master</code> spark://hsiehchou121:7077 <code>--class</code> day1.WordCount /root/hd/tmp_files/Demo1.jar hdfs://hsiehchou121:9000/tmp_files/test_WordCount.txt hdfs://hsiehchou121:9000/out/0405/Demo1</p><h4 id="2、Java版本的WordCount"><a href="#2、Java版本的WordCount" class="headerlink" title="2、Java版本的WordCount"></a>2、Java版本的WordCount</h4><pre><code>package day1;import java.util.Arrays;import java.util.Iterator;import java.util.List;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import scala.Tuple2;public class JavaWordCount {    public static void main(String[] args) {        SparkConf conf = new SparkConf().setAppName(&quot;JavaWordCount&quot;).setMaster(&quot;local&quot;);        //创建SparkContext对象        JavaSparkContext sc = new JavaSparkContext(conf);        //读入数据        JavaRDD&lt;String&gt; lines = sc.textFile(&quot;hdfs://192.168.116.121:9000/tmp_files/test_WordCount.txt&quot;);        //分词，第一个参数表示读进来的每一句话，第二个参数表示返回值        JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String,String&gt;(){            @Override            public Iterator&lt;String&gt; call(String input) throws Exception {                return Arrays.asList(input.split(&quot; &quot;)).iterator();            }        });        //每一个单词记一个数        JavaPairRDD&lt;String,Integer&gt; ones = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() {            @Override            public Tuple2&lt;String, Integer&gt; call(String input) throws Exception {                return new Tuple2&lt;String, Integer&gt;(input,1);            }        });        //执行reduce操作        JavaPairRDD&lt;String, Integer&gt; counts = ones.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() {            @Override            public Integer call(Integer arg0, Integer arg1) throws Exception {                return arg0 + arg1;            }        });        List&lt;Tuple2&lt;String,Integer&gt;&gt; output = counts.collect();        for(Tuple2&lt;String, Integer&gt; tuple:output) {            System.out.println(tuple._1 + &quot;:&quot; + tuple._2);        }        sc.stop();    }}</code></pre><p>[root@hsiehchou121 bin]# ./spark-submit <code>--master</code> spark://hsiehchou121:7077 <code>--class</code> day1.JavaWordCount /root/hd/tmp_files/Demo2.jar</p><h3 id="五、分析Spark的任务流程"><a href="#五、分析Spark的任务流程" class="headerlink" title="五、分析Spark的任务流程"></a>五、分析Spark的任务流程</h3><h4 id="1、分析WordCount程序处理过程"><a href="#1、分析WordCount程序处理过程" class="headerlink" title="1、分析WordCount程序处理过程"></a>1、分析WordCount程序处理过程</h4><p><img src="/medias/WordCount%E7%A8%8B%E5%BA%8F%E5%88%86%E6%9E%90.PNG" alt="WordCount程序分析"></p><h4 id="2、Spark调度任务的过程"><a href="#2、Spark调度任务的过程" class="headerlink" title="2、Spark调度任务的过程"></a>2、Spark调度任务的过程</h4><p>提交到及群众运行任务时，spark执行任务调度<br><img src="/medias/spark%E7%9A%84%E8%B0%83%E7%94%A8%E4%BB%BB%E5%8A%A1%E8%BF%87%E7%A8%8B.PNG" alt="spark的调用任务过程"></p><h3 id="六、RDD和RDD特性、RDD的算子"><a href="#六、RDD和RDD特性、RDD的算子" class="headerlink" title="六、RDD和RDD特性、RDD的算子"></a>六、RDD和RDD特性、RDD的算子</h3><h4 id="1、RDD：弹性分布式数据集"><a href="#1、RDD：弹性分布式数据集" class="headerlink" title="1、RDD：弹性分布式数据集"></a>1、RDD：弹性分布式数据集</h4><p>（<em>）Spark中最基本的数据抽象<br>（</em>）RDD的特性</p><p>Internally, each RDD is characterized by five main properties:<br>*</p><p>A list of partitions<br>1）是一组分区<br>RDD由分区组成，每个分区运行在不同的Worker上，通过这种方式来实现分布式计算</p><p><img src="/medias/RDD.PNG" alt="RDD"></p><p>A function for computing each split<br>在RDD中，提供算子处理每个分区中的数据</p><p>-A list of dependencies on other RDDs<br>RDD存在依赖关系：宽依赖和窄依赖</p><p>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)<br>可以自定义分区规则来创建RDD</p><p>Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)<br>优先选择离文件位置近的节点来执行</p><p><strong>如何创建RDD</strong><br>（1）通过SparkContext.parallelize方法来创建</p><p>scala&gt; val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8),3)<br>rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[32] at parallelize at <code>&lt;console&gt;</code>:29</p><p>scala&gt; rdd1.partitions.length<br>res35: Int = 3</p><p>scala&gt; val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8),2)<br>rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[33] at parallelize at <code>&lt;console&gt;</code>:29</p><p>scala&gt; rdd1.partitions.length<br>res36: Int = 2</p><p>（2）通过外部数据源来创建<br>sc.textFile()</p><p>scala&gt; val rdd2 = sc.textFile(“/root/hd/tmp_files/test_WordCount.txt”)<br>rdd2: org.apache.spark.rdd.RDD[String] = /usr/local/tmp_files/test_WordCount.txt MapPartitionsRDD[35] at textFile at <code>&lt;console&gt;</code>:29</p><h4 id="2、-算子"><a href="#2、-算子" class="headerlink" title="2、 算子"></a>2、 算子</h4><p><strong>1）Transformation</strong><br>map(func)：相当于for循环，返回一个新的RDD</p><p>filter(func)：过滤<br>flatMap(func)：flat+map 压平</p><p>mapPartitions(func)：对RDD中的每个分区进行操作<br>mapPartitionsWithIndex(func)：对RDD中的每个分区进行操作，可以取到分区号</p><p>sample(withReplacement, fraction, seed)：采样</p><p><strong>集合运算</strong><br>union(otherDataset)：对源RDD和参数RDD求并集后返回一个新的RDD<br>intersection(otherDataset)：对源RDD和参数RDD求交集后返回一个新的RDD</p><p>distinct([numTasks]))：去重</p><p><strong>聚合操作</strong>：<strong>group by</strong><br>groupByKey([numTasks]) ：在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD<br>reduceByKey(func, [numTasks])：在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置<br>aggregateByKey(zeroValue)(seqOp,combOp,[numTasks])：按照key进行聚合</p><p><strong>排序</strong><br>sortByKey([ascending], [numTasks])：在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD<br>sortBy(func,[ascending], [numTasks])：与sortByKey类似，但是更灵活</p><p>join(otherDataset, [numTasks])：在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD<br>cogroup(otherDataset, [numTasks])：在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable,Iterable))类型的RDD<br>cartesian(otherDataset)<br>pipe(command, [envVars])<br>coalesce(numPartitions)</p><p><strong>重分区</strong>：<br>repartition(numPartitions)<br>repartitionAndSortWithinPartitions(partitioner)</p><p>举例：<br>（1）创建一个RDD，每个元素乘以2，再排序<br>scala&gt; val rdd1 = sc.parallelize(Array(3,4,5,100,79,81,6,8))<br>rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <code>&lt;console&gt;</code>:24</p><p>scala&gt; val rdd2 = rdd1.map(_*2)<br>rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at <code>&lt;console&gt;</code>:26</p><p>scala&gt; rdd2.collect<br>res0: Array[Int] = Array(6, 8, 10, 200, 158, 162, 12, 16)                       </p><p>scala&gt; rdd2.sortBy(x=&gt;x,true).collect<br>res1: Array[Int] = Array(6, 8, 10, 12, 16, 158, 162, 200)</p><p>scala&gt; rdd2.sortBy(x=&gt;x,false).collect<br>res2: Array[Int] = Array(200, 162, 158, 16, 12, 10, 8, 6)     </p><p>def sortBy[K](f: (T) ⇒ K, ascending: Boolean = true)<br>过滤出大于20的元素：</p><p>scala&gt; val rdd3 = rdd2.filter(_&gt;20)<br>rdd3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[53] at filter at <code>&lt;console&gt;</code>:33+</p><p>scala&gt; rdd3.collect<br>res3: Array[Int] = Array(200, 158, 162)   </p><p>（2）字符串（字符）类型的RDD<br>scala&gt; val rdd4 = sc.parallelize(Array(“a b c”,”d e f”,”g h i”))<br>rdd4: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[28] at parallelize at <code>&lt;console&gt;</code>:24</p><p>scala&gt; rdd4.flatMap(_.split(“ “)).collect<br>res4: Array[String] = Array(a, b, c, d, e, f, g, h, i)</p><h4 id="3、RDD的集合运算"><a href="#3、RDD的集合运算" class="headerlink" title="3、RDD的集合运算"></a>3、RDD的集合运算</h4><p>scala&gt; val rdd5 = sc.parallelize(List(1,2,3,6,7,8,100))<br>rdd5: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24</p><p>scala&gt; val rdd6 = sc.parallelize(List(1,2,3,4))<br>rdd6: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24</p><p>scala&gt; val rdd7 = rdd5.union(rdd6)<br>rdd7: org.apache.spark.rdd.RDD[Int] = UnionRDD[2] at union at <console>:28</p><p>scala&gt; rdd7.collect<br>res5: Array[Int] = Array(1, 2, 3, 6, 7, 8, 100, 1, 2, 3, 4)                     </p><p>scala&gt; rdd7.distinct.collect<br>res6: Array[Int] = Array(100, 4, 8, 1, 6, 2, 3, 7)</p><h4 id="4、分组操作：reduceByKey"><a href="#4、分组操作：reduceByKey" class="headerlink" title="4、分组操作：reduceByKey"></a>4、分组操作：reduceByKey</h4><key value>scala> val rdd1 = sc.parallelize(List(("Time",1800),("Dadi",2400),("Giu",1600)))rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[6] at parallelize at <console>:24<p>scala&gt; val rdd2 = sc.parallelize(List((“Dadi”,1300),(“Time”,2900),(“Mi”,600)))<br>rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[7] at parallelize at <console>:24</p><p>scala&gt; val rdd3 = rdd1 union rdd2<br>rdd3: org.apache.spark.rdd.RDD[(String, Int)] = UnionRDD[8] at union at <console>:28</p><p>scala&gt; rdd3.collect<br>res3: Array[(String, Int)] = Array((Time,1800), (Dadi,2400), (Giu,1600), (Dadi,1300), (Time,2900), (Mi,600))</p><p>scala&gt; val rdd4 = rdd3.groupByKey<br>rdd4: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[9] at groupByKey at <console>:30</p><p>scala&gt; rdd4.collect<br>res4: Array[(String, Iterable[Int])] = Array((Mi,CompactBuffer(600)),<br>      (Time,CompactBuffer(1800, 2900)),<br>      (Dadi,CompactBuffer(2400, 1300)),<br>      (Giu,CompactBuffer(1600)))</p><p>scala&gt; rdd3.reduceByKey(<em>+</em>).collect<br>res5: Array[(String, Int)] = Array((Mi,600), (Time,4700), (Dadi,3700), (Giu,1600))<br>reduceByKey will provide much better performance.<br>官方不推荐使用 groupByKey 推荐使用 reduceByKey</p><h4 id="5、cogroup"><a href="#5、cogroup" class="headerlink" title="5、cogroup"></a>5、cogroup</h4><p>在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable,Iterable))类型的RDD</p><p>对两个RDD中的KV元素，每个RDD中相同key中的元素分别聚合成一个集合。与reduceByKey不同的是针对两个RDD中相同的key的元素进行合并，与groupByKey返回值上与区别</p><p>scala&gt; val rdd1 = sc.parallelize(List((“Tim”,1),(“Tim”,2),(“Jert”,3),(“kiy”,2)))<br>rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[11] at parallelize at <console>:24</p><p>scala&gt; val rdd1 = sc.parallelize(List((“Tim”,1),(“Tim”,2),(“Jert”,3),(“Kiy”,2)))<br>rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[12] at parallelize at <console>:24</p><p>scala&gt; val rdd2 = sc.parallelize(List((“Jert”,2),(“Tim”,1),(“Sun”,2)))<br>rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[13] at parallelize at <console>:24</p><p>scala&gt; val rdd3 = rdd1.cogroup(rdd2)<br>rdd3: org.apache.spark.rdd.RDD[(String, (Iterable[Int], Iterable[Int]))] = MapPartitionsRDD[15] at cogroup at <console>:28</p><p>scala&gt; rdd3.collect<br>res6: Array[(String, (Iterable[Int], Iterable[Int]))] = Array(<br>(Tim,(CompactBuffer(1, 2),CompactBuffer(1))),<br>(Sun,(CompactBuffer(),CompactBuffer(2))),<br>(Kiy,(CompactBuffer(2),CompactBuffer())),<br>(Jert,(CompactBuffer(3),CompactBuffer(2))))</p><h4 id="6、reduce操作（Action）"><a href="#6、reduce操作（Action）" class="headerlink" title="6、reduce操作（Action）"></a>6、reduce操作（Action）</h4><p><strong>聚合操作</strong></p><p>scala&gt; val rdd1 = sc.parallelize(List(1,2,3,4,5))<br>rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[16] at parallelize at <console>:24</p><p>scala&gt; rdd1.reduce(<em>+</em>)<br>res7: Int = 15</p><h4 id="7、需求：按照value排序"><a href="#7、需求：按照value排序" class="headerlink" title="7、需求：按照value排序"></a>7、需求：按照value排序</h4><p>做法：<br>1）交换，把key 和 value交换，然后调用sortByKey方法<br>2）再次交换</p><p>scala&gt; val rdd1 = sc.parallelize(List((“tim”,1),(“jery”,3),(“kef”,2),(“sun”,2)))<br>rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[17] at parallelize at <console>:24</p><p>scala&gt; val rdd2 = sc.parallelize(List((“jery”,1),(“tim”,3),(“sun”,5),(“kef”,1)))<br>rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[18] at parallelize at <console>:24</p><p>scala&gt; val rdd3 = rdd1.union(rdd2)<br>rdd3: org.apache.spark.rdd.RDD[(String, Int)] = UnionRDD[19] at union at <console>:28</p><p>scala&gt; val rdd4 = rdd3.reduceByKey(<em>+</em>)<br>rdd4: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[20] at reduceByKey at <console>:30</p><p>scala&gt; rdd4.collect<br>res8: Array[(String, Int)] = Array((tim,4), (kef,3), (sun,7), (jery,4))</p><p>scala&gt; val rdd5 = rdd4.map(t=&gt;(t._2,t._1)).sortByKey(false).map(t=&gt;(t._2,t._1))<br>rdd5: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[25] at map at <console>:32</p><p>scala&gt; rdd5.collect<br>res10: Array[(String, Int)] = Array((sun,7), (tim,4), (jery,4), (kef,3))</p><p>（2）Action<br>reduce(func)：通过func函数聚集RDD中的所有元素，这个功能必须是课交换且可并联的</p><p>collect()：在驱动程序中，以数组的形式返回数据集的所有元素<br>count()：返回RDD的元素个数<br>first()：返回RDD的第一个元素（类似于take(1)）<br>take(n)：返回一个由数据集的前n个元素组成的数组</p><p>takeSample(withReplacement,num, [seed])：返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子</p><p>takeOrdered(n, [ordering])：takeOrdered和top类似，只不过以和top相反的顺序返回元素</p><p>saveAsTextFile(path)：将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本</p><p>saveAsSequenceFile(path) ：将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统</p><p>saveAsObjectFile(path) ：saveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中</p><p>countByKey()：针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数</p><p>foreach(func)：在数据集的每一个元素上，运行函数func进行更新。<br>与map类似，没有返回值</p><p>3）特性<br>（1）<strong>RDD的缓存机制</strong><br>RDD通过persist方法或cache方法可以将前面的计算结果缓存，但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用</p><p>通过查看源码发现cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在object StorageLevel中定义的</p><p>缓存有可能丢失，或者存储存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition</p><p>（<em>）作用：提高性能<br>（</em>）使用：标识RDD可以被缓存 persist cache<br>（*）可以缓存的位置：</p><p>val NONE = new StorageLevel(false, false, false, false)<br>val DISK_ONLY = new StorageLevel(true, false, false, false)<br>val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)<br>val MEMORY_ONLY = new StorageLevel(false, true, false, true)<br>val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)<br>val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)<br>val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)<br>val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)<br>val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)<br>val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)<br>val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)<br>val OFF_HEAP = new StorageLevel(true, true, true, false, 1)</p><p>/**</p><ul><li>Persist this RDD with the default storage level (<code>MEMORY_ONLY</code>).</li><li>/<br>def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)</li></ul><p>/**</p><ul><li>Persist this RDD with the default storage level (<code>MEMORY_ONLY</code>).</li><li>/<br>def cache(): this.type = persist()<br>举例：测试数据，92万条<br>进入spark-shell命令</li></ul><p>./spark-shell <code>--master</code> spark://hsiehchou121:7077</p><p>scala&gt; val rdd1 = sc.textFile(“hdfs://192.168.116.121:9000/tmp_files/test_Cache.txt”)<br>rdd1: org.apache.spark.rdd.RDD[String] = hdfs://192.168.116.121:9000/tmp_files/test_Cache.txt MapPartitionsRDD[3] at textFile at <code>&lt;console&gt;</code>:24</p><p>scala&gt; rdd1.count  –&gt; 直接出发计算<br>res0: Long = 921911                                                       </p><p>scala&gt; rdd1.cache  –&gt; 标识RDD可以被缓存，不会触发计算<br>res1: rdd1.type = hdfs://192.168.116.121:9000/tmp_files/test_Cache.txt MapPartitionsRDD[3] at textFile at <code>&lt;console&gt;</code>:24</p><p>scala&gt; rdd1.count   –&gt; 和第一步一样，触发计算，但是，把结果进行缓存<br>res2: Long = 921911                                                          </p><p>scala&gt; rdd1.count   –&gt;  从缓存中直接读出结果<br>res3: Long = 921911</p><p>（2）<strong>RDD的容错机制：通过检查点来实现</strong><br>检查点（本质是通过将RDD写入Disk做检查点）是为了通过lineage（血统）做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销</p><p>设置checkpoint的目录，可以是本地的文件夹、也可以是HDFS。一般是在具有容错能力，高可靠的文件系统上(比如HDFS, S3等)设置一个检查点路径，用于保存检查点数据</p><p>/**</p><p>Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint</p><p>directory set with SparkContext#setCheckpointDir and all references to its parent</p><p>RDDs will be removed. This function must be called before any job has been</p><p>executed on this RDD. It is strongly recommended that this RDD is persisted in</p><p>memory, otherwise saving it on a file will require recomputation.<br>*/</p><p>（*）复习检查点：<br>HDFS中的检查点：有SecondaryNamenode来实现日志的合并</p><p>（*）RDD的检查点：容错<br>概念：血统 Lineage<br>理解：表示任务执行的生命周期<br>WordCount textFile —&gt; redceByKey</p><p>如果血统越长，越容易出错</p><p>假如有检查点，可以从最近的一个检查点开始，往后面计算。不用重头计算</p><p>（*）RDD检查点的类型<br>（1）基于本地目录：需要将Spark shell 或者任务运行在本地模式上（setMaster(“local”)）<br>开发和测试</p><p>（2）HDFS目录：用于生产<br>sc.setCheckPointDir(目录)</p><p>举例：<strong>设置检查点</strong><br>scala&gt; var rdd1 = sc.textFile(“hdfs://192.168.116.121:9000/tmp_files/test_Cache.txt”)<br>rdd1: org.apache.spark.rdd.RDD[String] = hdfs://192.168.116.121:9000/tmp_files/test_Cache.txt MapPartitionsRDD[1] at textFile at <code>&lt;console&gt;</code>:24</p><p>设置检查点目录：<br>scala&gt; sc.setCheckpointDir(“hdfs://192.168.116.121:9000/sparkchkpt”)</p><p>标识rdd1可以执行检查点操作<br>scala&gt; rdd1.checkpoint</p><p>scala&gt; rdd1.count<br>res2: Long = 921911</p><p>（3）*<em>依赖关系：宽依赖，窄依赖 *</em></p><p><strong>Stage是每一个job处理过程要分为的几个阶段</strong></p><p><img src="/medias/Stage%E5%88%92%E5%88%86.PNG" alt="Stage划分"></p><p>划分任务执行的stage<br>RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）</p><p><strong>窄依赖</strong>指的是每一个父RDD的Partition最多被子RDD的一个Partition使用（一（父）对一（子））<br>总结：<strong>窄依赖</strong>我们形象的比喻为<strong>独生子女</strong></p><p><strong>宽依赖</strong>指的是多个子RDD的Partition会依赖同一个父RDD的Partition（一（父）对多（子））<br>总结：<strong>宽依赖</strong>我们形象的比喻为<strong>超生</strong></p><p>DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此<strong>宽依赖是划分Stage的依据</strong></p><h3 id="七、RDD的高级算子"><a href="#七、RDD的高级算子" class="headerlink" title="七、RDD的高级算子"></a>七、RDD的高级算子</h3><h4 id="1、mapPartitionsWithIndex"><a href="#1、mapPartitionsWithIndex" class="headerlink" title="1、mapPartitionsWithIndex"></a>1、mapPartitionsWithIndex</h4><p>对RDD中的每个分区（带有下标）进行操作，下标用index表示<br>通过这个算子，我们可以获取分区号</p><p>def mapPartitionsWithIndex&lt;a href=”<br>f: %28Int, Iterator%5bT%5d%29 ⇒ Iterator%5bU%5d,<br>preservesPartitioning: Boolean = false”&gt;U(implicit arg0: ClassTag[U]): RDD[U]</p><p>通过将函数应用于此RDD的每个分区来返回新的RDD，同时跟踪原始分区的索引</p><p>preservesPartitioning指输入函数是否保留分区器，除非是一对RDD并且输入函数不修改keys，否则应该是false</p><p>参数：f是个函数参数 f 中第一个参数是Int，代表分区号，第二个Iterator[T]代表分区中的元素</p><p>举例：把分区中的元素，包括分区号，都打印出来</p><p>scala&gt; val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8),3)<br>rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[3] at parallelize at <code>&lt;console&gt;</code>:24</p><p>scala&gt; def fun1(index:Int, iter:Iterator[Int]) : Iterator[String] = {<br>     | iter.toList.map(x =&gt; “[partId: “+ index +” , value = “ + x + “ ]”).iterator<br>     | }<br>fun1: (index: Int, iter: Iterator[Int])Iterator[String]</p><p>scala&gt; rdd1.mapPartitionsWithIndex(fun1).collect<br>res3: Array[String] = Array(<br>[partId: 0 , value = 1 ], [partId: 0 , value = 2 ],<br>[partId: 1 , value = 3 ], [partId: 1 , value = 4 ], [partId: 1 , value = 5 ],<br>[partId: 2 , value = 6 ], [partId: 2 , value = 7 ], [partId: 2 , value = 8 ])</p><h4 id="2、aggregate"><a href="#2、aggregate" class="headerlink" title="2、aggregate"></a>2、aggregate</h4><p>聚合操作。类似于分组<br>（*）先对局部进行聚合操作，再对全局进行聚合操作</p><p><strong>调用聚合操作</strong><br>scala&gt; val rdd2 = sc.parallelize(List(1,2,3,4,5),2)<br>rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at <code>&lt;console&gt;</code>:24</p><p>scala&gt; rdd2.mapPartitionsWithIndex(fun1).collect<br>res4: Array[String] = Array(<br>[partId : 0 , value = 1 ], [partId : 0 , value = 2 ],<br>[partId : 1 , value = 3 ], [partId : 1 , value = 4 ], [partId : 1 , value = 5 ])</p><p>scala&gt; import scala.math._<br>import scala.math._</p><pre><code>scala&gt; rdd2.aggregate(0)(max(_,_),_+_)res6: Int = 7</code></pre><p>说明：aggregate</p><pre><code>(0) 初始值是 0 (max(_,_) 局部操作的函数,   _+_   全局操作的函数)</code></pre><pre><code>scala&gt; rdd2.aggregate(100)(max(_,_),_+_)res8: Int = 300</code></pre><p>分析结果：初始值是100，代表每个分区多了一个100<br>全局操作，也多了一个100<br>100+100+100 = 300</p><p>对RDD中的元素进行求和<br>RDD.map</p><p><strong>聚合操作（效率大于map）</strong></p><pre><code>scala&gt; rdd2.aggregate(0)(_+_,_+_)res9: Int = 15</code></pre><p>相当于MapReduce 的 Combiner</p><pre><code>scala&gt; rdd2.aggregate(10)(_+_,_+_)res10: Int = 45</code></pre><p>（*）对字符串操作</p><p>scala&gt; val rdd2 = sc.parallelize(List(“a”,”b”,”c”,”d”,”e”,”f”),2)<br>rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[7] at parallelize at <code>&lt;console&gt;</code>:27</p><pre><code>scala&gt; rdd2.aggregate(&quot;&quot;)(_+_,_+_)res11: String = abcdefscala&gt; rdd2.aggregate(&quot;*&quot;)(_+_,_+_)res12: String = **def*abc</code></pre><p>结果分析:<br>*abc *def</p><p>*<em>def</em>abc</p><p>（*）复杂的例子<br>1）<br>scala&gt; val rdd3 = sc.parallelize(List(“12”,”23”,”345”,”4567”),2)<br>rdd3: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[8] at parallelize at <code>&lt;console&gt;</code>:27</p><p>scala&gt; def fun1(index:Int, iter:Iterator[String]) : Iterator[String] = {<br> | iter.toList.map(x =&gt; “[partId : “ + index + “ , value = “ + x + “ ]”).iterator<br> | }</p><p>scala&gt; rdd3.mapPartitionsWithIndex(fun1).collect<br>res17: Array[String] = Array(<br>[partId : 0 , value = 12 ], [partId : 0 , value = 23 ],<br>[partId : 1 , value = 345 ], [partId : 1 , value = 4567 ])</p><p>scala&gt; rdd3.aggregate(“”)((x,y)=&gt; math.max(x.length,y.length).toString,(x,y)=&gt;x+y)<br>res13: String = 42<br>执行过程：<br>第一个分区：<br>第一次比较： “” “12” 长度最大值 2 2–&gt;”2”<br>第二次比较： “2” “23” 长度最大值 2 2–&gt;”2”</p><p>第二个分区：<br>第一次比较： “” “345” 长度最大值 3 3–&gt;”3”<br>第二次比较： “3” “4567” 长度最大值 4 4–&gt;”4”<br>结果：24 或者42</p><p>2）<br>scala&gt; rdd3.aggregate(“”)((x,y)=&gt; math.min(x.length,y.length).toString,(x,y)=&gt;x+y)<br>res18: String = 11<br>执行过程：<br>第一个分区：<br>第一次比较： “” “12” 长度最小值 0 0–&gt;”0”<br>第二次比较： “0” “23” 长度最小值 1 1–&gt;”1”</p><p>第二个分区：<br>第一次比较： “” “345” 长度最小值 0 0–&gt;”0”<br>第二次比较： “0” “4567” 长度最小值 1 1–&gt;”1”</p><p>val rdd3 = sc.parallelize(List(“12”,”23”,”345”,””),2)<br>rdd3.aggregate(“”)((x,y)=&gt; math.min(x.length,y.length).toString,(x,y)=&gt;x+y)</p><p>scala&gt; val rdd3 = sc.parallelize(List(“12”,”23”,”345”,””),2)<br>rdd3: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[10] at parallelize at <console>:27</p><p>scala&gt; rdd3.aggregate(“”)((x,y)=&gt; math.min(x.length,y.length).toString,(x,y)=&gt;x+y)<br>res19: String = 10</p><p>scala&gt; rdd3.aggregate(“”)((x,y)=&gt; math.min(x.length,y.length).toString,(x,y)=&gt;x+y)<br>res20: String = 01<br>3）aggregateByKey：类似于aggregate，区别：操作的是 key value 的数据类型</p><p>scala&gt; val pairRDD = sc.parallelize(List((“cat”,2),(“cat”,5),(“mouse”,4),(“cat”,12),(“dog”,12),(“mouse”,2)),2)<br>pairRDD: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at parallelize at <code>&lt;console&gt;</code>:24</p><p>scala&gt; def fun3(index:Int, iter:Iterator[(String,Int)]) : Iterator[String] = {<br>     | iter.toList.map(x=&gt;”partId : “ + index + “ , value = “ + x + “ ]”).iterator<br>     | }<br>fun3: (index: Int, iter: Iterator[(String, Int)])Iterator[String]</p><p>scala&gt; pairRDD.mapPartitionsWithIndex(fun3).collect<br>res0: Array[String] = Array(<br>partId : 0 , value = (cat,2) ], partId : 0 , value = (cat,5) ], partId : 0 , value = (mouse,4) ],<br>partId : 1 , value = (cat,12) ], partId : 1 , value = (dog,12) ], partId : 1 , value = (mouse,2) ])</p><p>1.将每个动物园（分区）中，动物数最多的动物，进行求和<br>动物园0<br>[partId : 0 , value = (cat,2) ], [partId : 0 , value = (cat,5) ], [partId : 0 , value = (mouse,4) ],</p><p>动物园1<br>[partId : 1 , value = (cat,12) ], [partId : 1 , value = (dog,12) ], [partId : 1 , value = (mouse,2) ])</p><pre><code>pairRDD.aggregateByKey(0)(math.max(_,_),_+_)scala&gt; pairRDD.aggregateByKey(0)(math.max(_,_),_+_).collectres1: Array[(String, Int)] = Array((dog,12), (cat,17), (mouse,6))   </code></pre><p>2.将所有动物求和</p><pre><code>pairRDD.aggregateByKey(0)(_+_,_+_).collectscala&gt; pairRDD.reduceByKey(_+_).collectres27: Array[(String, Int)] = Array((dog,12), (cat,19), (mouse,6))</code></pre><p>aggregateByKey效率更高</p><p>4）<strong>coalesce与repartition</strong><br>与分区有关<br>都是对RDD进行重分区</p><p>区别：<br>coalesce 默认不会进行Shuffle 默认 false 如需修改分区，需置为true</p><p>repartition 会进行Shuffle</p><p>scala&gt; val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9),2)<br>rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at <code>&lt;console&gt;</code></p><p>scala&gt; val rdd2 = rdd1.repartition(3)<br>rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[9] at repartition at <code>&lt;console&gt;</code>:26</p><p>scala&gt; rdd2.partitions.length<br>res4: Int = 3</p><p>scala&gt; val rdd3 = rdd1.coalesce(3,true)<br>rdd3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[13] at coalesce at <code>&lt;console&gt;</code>:26</p><p>scala&gt; rdd3.partitions.length<br>res5: Int = 3</p><p>scala&gt; val rdd4 = rdd1.coalesce(4)<br>rdd4: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[14] at coalesce at <code>&lt;console&gt;</code>:26</p><p>scala&gt; rdd4.partitions.length<br>res6: Int = 2</p><p>5）<strong>其他高级算子</strong><br>比较好的高级算子的博客（推荐）<br><a href="http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html" target="_blank" rel="noopener">http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html</a></p><h3 id="八、编程案例"><a href="#八、编程案例" class="headerlink" title="八、编程案例"></a>八、编程案例</h3><h4 id="1、分析日志"><a href="#1、分析日志" class="headerlink" title="1、分析日志"></a>1、分析日志</h4><p>需求：找到访问量最高的两个网页<br>（<em>）第一步：对网页的访问量求和<br>（</em>）第二步：排序，降序 </p><p><strong>日志数据</strong><br>192.168.88.1 - - [30/Jul/2017:12:53:43 +0800] “GET /MyDemoWeb/ HTTP/1.1” 200 259<br>192.168.88.1 - - [30/Jul/2017:12:53:43 +0800] “GET /MyDemoWeb/head.jsp HTTP/1.1” 200 713<br>192.168.88.1 - - [30/Jul/2017:12:53:43 +0800] “GET /MyDemoWeb/body.jsp HTTP/1.1” 200 240<br>192.168.88.1 - - [30/Jul/2017:12:54:37 +0800] “GET /MyDemoWeb/oracle.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:38 +0800] “GET /MyDemoWeb/hadoop.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:38 +0800] “GET /MyDemoWeb/java.jsp HTTP/1.1” 200 240<br>192.168.88.1 - - [30/Jul/2017:12:54:40 +0800] “GET /MyDemoWeb/oracle.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:40 +0800] “GET /MyDemoWeb/hadoop.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:41 +0800] “GET /MyDemoWeb/mysql.jsp HTTP/1.1” 200 241<br>192.168.88.1 - - [30/Jul/2017:12:54:41 +0800] “GET /MyDemoWeb/hadoop.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:42 +0800] “GET /MyDemoWeb/web.jsp HTTP/1.1” 200 239<br>192.168.88.1 - - [30/Jul/2017:12:54:42 +0800] “GET /MyDemoWeb/oracle.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:52 +0800] “GET /MyDemoWeb/oracle.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:52 +0800] “GET /MyDemoWeb/hadoop.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:53 +0800] “GET /MyDemoWeb/oracle.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:54 +0800] “GET /MyDemoWeb/mysql.jsp HTTP/1.1” 200 241<br>192.168.88.1 - - [30/Jul/2017:12:54:54 +0800] “GET /MyDemoWeb/hadoop.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:54 +0800] “GET /MyDemoWeb/hadoop.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:56 +0800] “GET /MyDemoWeb/web.jsp HTTP/1.1” 200 239<br>192.168.88.1 - - [30/Jul/2017:12:54:56 +0800] “GET /MyDemoWeb/java.jsp HTTP/1.1” 200 240<br>192.168.88.1 - - [30/Jul/2017:12:54:57 +0800] “GET /MyDemoWeb/oracle.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:57 +0800] “GET /MyDemoWeb/java.jsp HTTP/1.1” 200 240<br>192.168.88.1 - - [30/Jul/2017:12:54:58 +0800] “GET /MyDemoWeb/oracle.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:58 +0800] “GET /MyDemoWeb/hadoop.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:59 +0800] “GET /MyDemoWeb/oracle.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:59 +0800] “GET /MyDemoWeb/hadoop.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:55:43 +0800] “GET /MyDemoWeb/mysql.jsp HTTP/1.1” 200 241<br>192.168.88.1 - - [30/Jul/2017:12:55:43 +0800] “GET /MyDemoWeb/oracle.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:55:43 +0800] “GET /MyDemoWeb/web.jsp HTTP/1.1” 200 239<br>192.168.88.1 - - [30/Jul/2017:12:55:43 +0800] “GET /MyDemoWeb/hadoop.jsp HTTP/1.1” 200 242</p><pre><code>package day2import org.apache.spark.SparkConfimport org.apache.spark.SparkContextobject MyTomcatLogCount {  def main(args: Array[String]): Unit = {    val conf  = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;MyTomcatLogCount&quot;)    val sc = new SparkContext(conf)    /**     * 读入日志解析     *      * 192.168.88.1 - - [30/Jul/2017:12:54:52 +0800] &quot;GET /MyDemoWeb/oracle.jsp HTTP/1.1&quot; 200 242     *      */    val rdd1 = sc.textFile(&quot;H:\\other\\localhost_access_log.txt&quot;)      .map(       line =&gt; {         //解析字符串， 得到jsp的名字         //1.解析两个引号之间的字符串         val index1 = line.indexOf(&quot;\&quot;&quot;)         val index2 = line.lastIndexOf(&quot;\&quot;&quot;)         val line1 = line.substring(index1+1,index2)//GET /MyDemoWeb/oracle.jsp HTTP/1.1         //得到两个空格的位置         val index3 = line1.indexOf(&quot; &quot;)         val index4 = line1.lastIndexOf(&quot; &quot;)         val line2 = line1.substring(index3+1,index4)///MyDemoWeb/oracle.jsp         //得到jsp的名字         val jspName = line2.substring(line2.lastIndexOf(&quot;/&quot;))//oracle.jsp         (jspName,1)       }      )    //统计出每个jsp的次数               val rdd2 = rdd1.reduceByKey(_+_)                               //使用value排序    val rdd3 = rdd2.sortBy(_._2, false)    rdd3.take(2).foreach(println)    sc.stop()  }}</code></pre><p>结果：<br>(/hadoop.jsp,9)<br>(/oracle.jsp,9)</p><h4 id="2、创建自定义分区"><a href="#2、创建自定义分区" class="headerlink" title="2、创建自定义分区"></a>2、创建自定义分区</h4><p>根据jsp文件的名字，将各自的访问日志放入到不同的分区文件中</p><pre><code>package day2import org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.Partitionerimport scala.collection.mutable.HashMapobject MyTomcatLogPartitioner {  def main(args: Array[String]): Unit = {    System.setProperty(&quot;hadoop.home.dir&quot;, &quot;E:\\hadoop-2.7.3&quot;)    val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;MyTomcatLogPartitioner&quot;)    val sc = new SparkContext(conf)     /**     * 读入日志解析     *      * 192.168.88.1 - - [30/Jul/2017:12:54:52 +0800] &quot;GET /MyDemoWeb/oracle.jsp HTTP/1.1&quot; 200 242     *      */    val rdd1 = sc.textFile(&quot;H:\\other\\localhost_access_log.txt&quot;)      .map(       line =&gt; {         //解析字符串， 得到jsp的名字         //1.解析两个引号之间的字符串         val index1 = line.indexOf(&quot;\&quot;&quot;)         val index2 = line.lastIndexOf(&quot;\&quot;&quot;)         val line1 = line.substring(index1+1,index2)//GET /MyDemoWeb/oracle.jsp HTTP/1.1         //得到两个空格的位置         val index3 = line1.indexOf(&quot; &quot;)         val index4 = line1.lastIndexOf(&quot; &quot;)         val line2 = line1.substring(index3+1,index4)///MyDemoWeb/oracle.jsp         //得到jsp的名字         val jspName = line2.substring(line2.lastIndexOf(&quot;/&quot;))//oracle.jsp         (jspName,line)       }      )                              //定义分区规则    //得到不重复的jsp的名字    val rdd2 = rdd1.map(_._1).distinct().collect()    //创建分区规则    val myPartitioner = new MyWebPartitioner(rdd2)    val rdd3 = rdd1.partitionBy(myPartitioner)    //将rdd3 输出    rdd3.saveAsTextFile(&quot;H:\\other\\test_partition&quot;)      sc.stop()  }}class MyWebPartitioner(jspList : Array[String]) extends Partitioner{  //定义一个集合来保存分区条件， String 代表jsp的名字， Int 代表序号  val partitionMap = new HashMap[String,Int]()  var partID = 0 //初始分区号  for (jsp &lt;- jspList){    partitionMap.put(jsp, partID)    partID += 1  }  //定义有多少个分区  def numPartitions : Int = partitionMap.size  //根据jsp，返回对应的分区  def getPartition(key : Any) : Int = partitionMap.getOrElse(key.toString(),0)}</code></pre><h4 id="3、使用JDBCRDD-操作数据库"><a href="#3、使用JDBCRDD-操作数据库" class="headerlink" title="3、使用JDBCRDD 操作数据库"></a>3、使用JDBCRDD 操作数据库</h4><p>将RDD的数据保存到mysql数据库中</p><pre><code>package day2import java.sql.DriverManagerimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.rdd.JdbcRDD/** * 需求找出工资小于等于2000大于900的员工 * select * from emp where sal &gt; ? and sal &lt;= ? */object MyMysqlDemo {  val connection = () =&gt; {    Class.forName(&quot;com.mysql.cj.jdbc.Driver&quot;).newInstance()    DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;,&quot;root&quot;,&quot;123456&quot;)  }  def main(args: Array[String]): Unit = {    System.setProperty(&quot;hadoop.home.dir&quot;, &quot;E:\\hadoop-2.7.3&quot;)    val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;MyMysqlDemo&quot;)    val sc = new SparkContext(conf)    val mysqlRDD = new JdbcRDD(sc, connection, &quot;select * from emp where sal &gt; ? and sal &lt;= ?&quot;, 900, 2000, 2, r =&gt; {      val ename = r.getString(2)      val sal = r.getInt(4)      (ename, sal)    })    val result = mysqlRDD.collect()    println(result.toBuffer)    sc.stop()      }}</code></pre><p>mysql的company的emp数据<br>1 Tom 10 2400<br>2 Alis 11 1900<br>3 Kei 12 1500<br>4 Mi 11 900<br>结果<br>ArrayBuffer((Alis,1900), (Kei,1500))</p><p>JdbcRDD参数说明</p><table><thead><tr><th align="center">参数名称</th><th align="center">类型</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">sc</td><td align="center">org.apache.spark.SparkContext</td><td align="center">Spark Context对象</td></tr><tr><td align="center">getConnection</td><td align="center">scala.Function0[java.sql.Connection]</td><td align="center">得到一个数据库Connection</td></tr><tr><td align="center">sql</td><td align="center">scala.Predef.String</td><td align="center">执行的SQL语句</td></tr><tr><td align="center">lowerBound</td><td align="center">scala.Long</td><td align="center">下边界值，即：SQL的第一个参数</td></tr><tr><td align="center">upperBound</td><td align="center">scala.Long</td><td align="center">上边界值，即：SQL的第二个参数</td></tr><tr><td align="center">numPartitions</td><td align="center">scala.Int</td><td align="center">分区的个数，即：启动多少个Executor</td></tr><tr><td align="center">mapRow</td><td align="center">scala.Function1[java.sql.ResultSet, T]</td><td align="center">得到的结果集</td></tr></tbody></table><p>JdbcRDD的缺点：从上面的参数说明可以看出，JdbcRDD有以下两个缺点：<br>（1）执行的SQL必须有两个参数，并类型都是Long<br>（2）得到的结果是ResultSet，即：只支持select操作</p><h4 id="4、操作数据库：把结果存放到数据库中"><a href="#4、操作数据库：把结果存放到数据库中" class="headerlink" title="4、操作数据库：把结果存放到数据库中"></a>4、操作数据库：把结果存放到数据库中</h4><pre><code>package day3import org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport java.sql.Connectionimport java.sql.DriverManagerimport java.sql.PreparedStatement/** * 把Spark结果存放到mysql数据库中 */object MyTomcatLogCountToMysql {  def main(args: Array[String]): Unit = {    //创建SparkContext    val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;MyTomcatLogCountToMysql&quot;)    val sc = new SparkContext(conf)     /**     * 读入日志解析     *      * 192.168.88.1 - - [30/Jul/2017:12:54:52 +0800] &quot;GET /MyDemoWeb/oracle.jsp HTTP/1.1&quot; 200 242     *      */    val rdd1 = sc.textFile(&quot;H:\\other\\localhost_access_log.txt&quot;)      .map(       line =&gt; {         //解析字符串， 得到jsp的名字         //1.解析两个引号之间的字符串         val index1 = line.indexOf(&quot;\&quot;&quot;)         val index2 = line.lastIndexOf(&quot;\&quot;&quot;)         val line1 = line.substring(index1+1,index2)//GET /MyDemoWeb/oracle.jsp HTTP/1.1         //得到两个空格的位置         val index3 = line1.indexOf(&quot; &quot;)         val index4 = line1.lastIndexOf(&quot; &quot;)         val line2 = line1.substring(index3+1,index4)///MyDemoWeb/oracle.jsp         //得到jsp的名字         val jspName = line2.substring(line2.lastIndexOf(&quot;/&quot;))//oracle.jsp         (jspName,1)     }    )      //存入数据库//    var conn : Connection = null//    var pst : PreparedStatement = null//    //    try{//        /**//         * create table mydata(jspname varchar(50), countNumber Int);//         * //         * foreach 没有返回值 ， 在本需求中，只需要写数据库，不需要返回新的RDD，所以用foreach即可//         * //         * 运行Task not serializable//         *///        conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;,&quot;root&quot;,&quot;123456&quot;) //        pst = conn.prepareStatement(&quot;insert into mydata values (?,?)&quot;)//    //        rdd1.foreach(f =&gt; {//          pst.setString(1, f._1)//          pst.setInt(2, f._2)//          //          pst.executeUpdate()//        })//    }catch{//      case t : Throwable =&gt; t.printStackTrace()//    }finally{//      if(pst != null) pst.close()//      if(conn != null)  conn.close()//    }//    sc.stop()    //第一种修改方式    //存入数据库//    var conn : Connection = null//    var pst : PreparedStatement = null//    //    try{//      rdd1.foreach(f =&gt; {//        conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;,&quot;root&quot;,&quot;123456&quot;) //        pst = conn.prepareStatement(&quot;insert into mydata values (?,?)&quot;)//  //        pst.setString(1, f._1)//        pst.setInt(2, f._2)//        //        pst.executeUpdate()//      })//    }catch{//      case t : Throwable =&gt; t.printStackTrace()//    }finally{//      if(pst != null) pst.close()//      if(conn != null)  conn.close()//    }//    sc.stop()      /*     * 第一种修改方式功能上可以实现，但每条数据都会创建连接，对数据库造成很大压力     *      * 针对分区来操作：一个分区建立一个连接即可     */     rdd1.foreachPartition(saveToMysql)      sc.stop()  }  def saveToMysql(it : Iterator[(String, Int)]) = {      var conn : Connection = null      var pst : PreparedStatement = null      try{        conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;,&quot;root&quot;,&quot;123456&quot;)         pst = conn.prepareStatement(&quot;insert into mydata values (?,?)&quot;)        it.foreach(f =&gt; {          pst.setString(1, f._1)          pst.setInt(2, f._2)          pst.executeUpdate()        })      }catch{        case t : Throwable =&gt; t.printStackTrace()      }finally{        if(pst != null) pst.close()        if(conn != null)  conn.close()      }  }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Akka练习</title>
      <link href="/2019/03/27/akka-lian-xi/"/>
      <url>/2019/03/27/akka-lian-xi/</url>
      
        <content type="html"><![CDATA[<h4 id="Actor并发模型"><a href="#Actor并发模型" class="headerlink" title="Actor并发模型"></a>Actor并发模型</h4><p><strong>Java中的并发开发</strong><br>Java的并发编程是基于 共享数据 和 加锁 的一种机制。锁的是共享数据<br>synchronized</p><p><strong>Scala中的并发开发</strong><br>不共享数据。依赖于 消息传递 的一种并发编程模式</p><p>如果 Actor A 和 Actor B要相互沟通<br>1、A要给B传递一个消息，B有一个收件箱，B轮询自己的收件箱<br>2、如果B看到A的消息，解析A的消息并执行相应操作<br>3、有可能会回复 A 消息</p><p><strong>Actor示例</strong></p><pre><code>package day6import akka.actor.{Actor, ActorSystem, Props}/**  * Actor示例  */class HelloActor extends Actor{  override def receive: Receive = {    case &quot;Hello&quot; =&gt; println(&quot;Hello Receive&quot;)    case _ =&gt; println(&quot;aaa&quot;)  }}object Demo1 extends App {  //新建一个ActorSystem  val system = ActorSystem(&quot;HelloSystem&quot;)  //构造函数  val helloActor = system.actorOf(Props[HelloActor],&quot;helloactor&quot;)  //发消息  helloActor ! &quot;Hello&quot;  helloActor ! &quot;Hello2234&quot;}</code></pre><p><strong>建立两个Actor 相互传递消息</strong></p><pre><code>package day6import akka.actor.{Actor, ActorRef, ActorSystem, Props}/**  * 建立两个Actor 相互传递消息  *  * 定义消息：样本类、区分 消息的不同  *///消息的定义case object PingMessagecase object PongMessagecase object StartMessagecase object StopMessageclass Ping(pong : ActorRef) extends Actor{  var count = 0  def incrementAndPing {    count += 1;    println(&quot;Ping&quot;)  }  override def receive: Receive = {    case StartMessage =&gt;      incrementAndPing      pong ! PingMessage    case PongMessage =&gt;      if(count &gt; 9){        sender() ! StopMessage      }else {        incrementAndPing        pong ! PingMessage      }  }}class Pong extends Actor{  override def receive: Receive = {    case PingMessage =&gt;    println(&quot;pong&quot;)    //给ping回复消息    sender ! PongMessage    case StopMessage =&gt;      println(&quot;pong Stop&quot;)      context.stop(self)      //context.system.finalize()  }}object Demo2 extends App{  val system = ActorSystem(&quot;PingPongSystem&quot;)  val pong =  system.actorOf(Props[Pong],name=&quot;pong&quot;)  val ping = system.actorOf(Props(new Ping(pong)),name=&quot;ping&quot;)  ping ! StartMessage}</code></pre><p>AKKA 负责来回传递消息</p><p><strong>Scala项目</strong></p><h4 id="实现一个主从管理系统"><a href="#实现一个主从管理系统" class="headerlink" title="实现一个主从管理系统"></a>实现一个主从管理系统</h4><p><img src="/medias/NewAkkaSystem.PNG" alt="NewAkkaSystem"></p><p>Worker类<br>Master类</p><p>ActorMessage类<br>WorkerInfo类</p><p>ActorMessage 类：定义消息 5种消息</p><p><strong>WorkerInfo.scala</strong></p><pre><code>package akka/**  * 保存worker的基本信息  */class WorkerInfo(val id : String, val workerHost : String, val memory : String, val cores : String) {  //保存心跳信息  var lastHeartBeat : Long = System.currentTimeMillis()  override def toString : String = s&quot;WorkerInfo($id, $workerHost, $memory, $cores)&quot;}</code></pre><p><strong>ActorMessage.scala</strong></p><pre><code>package akka/**  * 样本类，保存所有信息  *///worker ----&gt; master注册节点case class RegisterWorker(val id : String, val workerHost : String, val memory : String, val cores : String)//worker ----&gt; master 发送心跳信号case class HeartBeat(val workerId : String)//master ----&gt; worker 注册完成 ACKcase class RegisteredWorker(val workerHost : String)//master ----&gt; master 检查超时节点case class CheckTimeOutWorker()//worker ----&gt; worker 提醒自己发送心跳信号case class SendHeartBeat()</code></pre><p><strong>Worker.scala</strong></p><pre><code>package akkaimport java.util.UUIDimport akka.actor._import com.typesafe.config.ConfigFactoryimport scala.concurrent.duration._import scala.concurrent.ExecutionContext.Implicits.globalclass Worker extends Actor {  //Worker端持有Master端的引用（代理对象）  //因为worker会给Master发送信息，所以才要这个对象  var master : ActorSelection = null  ////生成一个UUID，作为Worker的标识  val id = UUID.randomUUID().toString  //构造方法执行完执行一次  override def preStart(): Unit = {    //Worker向MasterActorSystem发送建立连接请求    master = context.system.actorSelection(&quot;akka.tcp://MasterActorSystem@localhost:8881/user/Master&quot;)    //Worker向Master发送注册消息    master ! RegisterWorker(id, &quot;localhost&quot;, &quot;10240&quot;, &quot;8&quot;)  }  //该方法会被反复执行，用于接收消息，通过case class模式匹配接收消息  override def receive : Receive = {    //Master向Worker的反馈信息    case RegisteredWorker(masterURL) =&gt; {      //启动定时任务，向Master发送心跳      context.system.scheduler.schedule(0 millis, 5000 millis, self, SendHeartBeat)    }    case SendHeartBeat =&gt; {      println(&quot;worker send hearbeat&quot;)      master ! HeartBeat(id)    }  }}object Worker extends App{  val clientPort = 8803  //创建ActorSystem的必要参数  val configStr =    s&quot;&quot;&quot;       |akka.actor.provider = &quot;akka.remote.RemoteActorRefProvider&quot;       |akka.remote.netty.tcp.port = $clientPort       &quot;&quot;&quot;.stripMargin  val conf = ConfigFactory.parseString(configStr)  //创建ActorSystem  val actorSystem = ActorSystem(&quot;MasterActorSystem&quot;,conf)  //启动Actor，Master会被实例化，生命周期方法会被调用  actorSystem.actorOf(Props[Worker],&quot;Worker&quot;)}</code></pre><p><strong>Master.scala</strong></p><pre><code>package akkaimport akka.actor.{Actor, ActorSystem, Props}import com.typesafe.config.ConfigFactoryimport scala.collection.mutableimport scala.concurrent.duration._import scala.concurrent.ExecutionContext.Implicits.globalclass Master extends Actor {  //保存WorkerId 和 Worker信息的 map  val idToWorker = new mutable.HashMap[String, WorkerInfo]  //保存所有worker信息的Set  val workers = new mutable.HashSet[WorkerInfo]  //Worker超时时间  val WORKER_TIMEOUT = 10 * 1000  //构造方法执行完执行一次  override def preStart(): Unit = {    //启动定时器，定时执行    //设置在5毫秒之后，间隔10秒，给自己发一个CheckOfTimeOutWorker    context.system.scheduler.schedule(0 millis, 5000 millis, self, CheckTimeOutWorker)  }  //该方法会被反复执行，用于接收消息，通过case class模式匹配接收消息  override def receive: Receive = {    //Worker向Master发送的注册消息    case RegisterWorker(id, workerHost, memory, cores) =&gt; {        if(!idToWorker.contains(id)){          val worker = new WorkerInfo(id,workerHost,memory,cores)          workers.add(worker)          idToWorker(id) = worker          println(&quot;nrew register worker: &quot; + worker)          sender ! RegisteredWorker(worker.id)        }      }    //Worker向Master发送的心跳消息    case HeartBeat(workerId) =&gt; {      val workerInfo = idToWorker(workerId)      println(&quot;get heartbeat message from: &quot;+ workerInfo)      workerInfo.lastHeartBeat = System.currentTimeMillis()    }    //Master自己向自己发送的定期检查超时Worker的消息    case CheckTimeOutWorker =&gt; {      //检查超时的worker      val currentTime = System.currentTimeMillis()      val toRemove = workers.filter( w =&gt; currentTime - w.lastHeartBeat &gt; WORKER_TIMEOUT).toArray      for(worker &lt;- toRemove){        workers -= worker        idToWorker.remove(worker.id)      }      println(&quot;Worker size: &quot; + workers.size)    }  }}object Master extends App {  val host = &quot;localhost&quot;  val port = 8881  //创建ActorSystem的必要参数  val configStr =    s&quot;&quot;&quot;       |akka.actor.provider = &quot;akka.remote.RemoteActorRefProvider&quot;       |akka.remote.netty.tcp.hostname = &quot;$host&quot;       |akka.remote.netty.tcp.port = &quot;$port&quot;     &quot;&quot;&quot;.stripMargin  val conf = ConfigFactory.parseString(configStr)  //创建ActorSystem  val actorSystem = ActorSystem(&quot;MasterActorSystem&quot;,conf)  //启动Actor Master会被实例化 生命周期的方法会被调用  actorSystem.actorOf(Props[Master],&quot;Master&quot;)}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Akka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala编程</title>
      <link href="/2019/03/25/scala-bian-cheng/"/>
      <url>/2019/03/25/scala-bian-cheng/</url>
      
        <content type="html"><![CDATA[<h3 id="一、Scala函数式编程"><a href="#一、Scala函数式编程" class="headerlink" title="一、Scala函数式编程"></a>一、Scala函数式编程</h3><p>多范式：面向对象，函数式编程（程序实现起来简单）</p><p>举例：WordCount<br>sc 是 SparkContext , 非常重要</p><p>一行：</p><pre><code>var result = sc.textFile(&quot;hdfs://xxxx/xxx/data.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).collect</code></pre><h4 id="1、复习函数"><a href="#1、复习函数" class="headerlink" title="1、复习函数"></a>1、复习函数</h4><p>关键字 def</p><h4 id="2、匿名函数"><a href="#2、匿名函数" class="headerlink" title="2、匿名函数"></a>2、匿名函数</h4><p>没有名字的函数 </p><p><strong>举例</strong></p><pre><code>scala&gt; var myarray = Array(1,2,3)myarray: Array[Int] = Array(1, 2, 3)scala&gt; def fun1(x:Int):Int = x*3fun1: (x: Int)Intscala&gt; (x:Int) =&gt; x*3res0: Int =&gt; Int = &lt;function1&gt;</code></pre><p>问题：怎么去调用？高阶函数</p><pre><code>scala&gt; fun1(3)res1: Int = 9scala&gt; myarray.foreach(println)123//调用匿名函数scala&gt; myarray.map((x:Int) =&gt; x*3)res3: Array[Int] = Array(3, 6, 9)</code></pre><p><code>(_,1)  (_+_)</code> 都是匿名函数</p><h4 id="3、高阶函数（带有函数参数的函数）"><a href="#3、高阶函数（带有函数参数的函数）" class="headerlink" title="3、高阶函数（带有函数参数的函数）"></a>3、高阶函数（带有函数参数的函数）</h4><p>把一个函数作为另外一个函数的参数值</p><p>定义一个高阶函数：<br>对10做某种运算</p><pre><code>scala&gt; def someAction(f:(Double)=&gt;(Double)) = f(10)someAction: (f: Double =&gt; Double)Double</code></pre><p><strong>解释</strong></p><p>(Double)=&gt;(Double) 代表了f 的类型：入参是double，返回值也是double的函数</p><pre><code>import scala.math._scala&gt; someAction(sqrt)res5: Double = 3.1622776601683795scala&gt; someAction(sin)res6: Double = -0.5440211108893698scala&gt; someAction(cos)res7: Double = -0.8390715290764524scala&gt; someAction(println)&lt;console&gt;:16: error: type mismatch;found   : () =&gt; Unitrequired: Double =&gt; Double   someAction(println)                ^def someAction(f:(Double)=&gt;(Double)) = f(10)someAction(sqrt) = sqrt(10)</code></pre><h4 id="4、高阶函数的实例"><a href="#4、高阶函数的实例" class="headerlink" title="4、高阶函数的实例"></a>4、高阶函数的实例</h4><p>scala中提供了常用的高阶函数</p><p>（1）map : 相当于一个循环，对某个集合中的每个元素都进行操作（接收一个函数），返回一个新的集合</p><p>scala&gt; var numbers = List(1,2,3,4,5,6,7,8,9,10)<br>numbers: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</p><p>scala&gt; numbers.map((i:Int)=&gt;i*2)<br>res2: List[Int] = List(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)</p><p>scala&gt; numers.map(_*2)<br>res3: List[Int] = List(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)</p><p>说明：<br>(i:Int)=&gt;i<em>2 与 _</em>2 等价的</p><p>(i:Int,j:Int)=&gt;i+j   <code>_+_</code><br>scala&gt; numbers<br>res4: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)<br>不改变numbers本身的值</p><p>（2）foreach：相当于一个循环，对某个集合中的每个元素都进行操作（接收一个函数），不返回结果<br>scala&gt; numbers.foreach(println)<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10</p><p>numbers.foreach(_*2)<br>没有返回值</p><p>（3） filter ：过滤，选择满足的数据<br>举例：查询能被2整除的数字</p><p>scala&gt; numbers.filter((i:Int)=&gt;i%2==0)<br>res5: List[Int] = List(2, 4, 6, 8, 10)</p><p>filter函数，参数要求：要求一个返回 bool 值的函数，筛选出所有为true的数据</p><p>（4）zip操作：合并两个集合<br>scala&gt; List(1,2,3).zip(List(4,5,6))<br>res6: List[(Int, Int)] = List((1,4), (2,5), (3,6))</p><p>//少的话匹配不上就不合并<br>scala&gt; List(1,2,3).zip(List(4,5))<br>res7: List[(Int, Int)] = List((1,4), (2,5))</p><p>scala&gt; List(1).zip(List(4,5))<br>res8: List[(Int, Int)] = List((1,4))</p><p>（5）partition ： 根据断言（就是某个条件，匿名函数）的结果，来进行分区<br>举例：<br>能被2整除的分成一个区，不能被2整除的分成另外一个区<br>scala&gt; numbers.partition((i:Int)=&gt;i%2==0)<br>res9: (List[Int], List[Int]) = (List(2, 4, 6, 8, 10),List(1, 3, 5, 7, 9))</p><p>（6）find ： 查找第一个满足条件的元素<br>scala&gt; numbers.find(_%3==0)<br>res10: Option[Int] = Some(3)</p><p>_%3==0 (i:Int)=&gt;i%3==0 一样</p><p>（7）flatten：把嵌套的结果展开<br>scala&gt; List(List(2,4,6,8,10),List(1,3,5,7,9)).flatten<br>res11: List[Int] = List(2, 4, 6, 8, 10, 1, 3, 5, 7, 9)</p><p>（8）flatmap : 相当于一个 map + flatten<br>scala&gt; var myList = List(List(2,4,6,8,10),List(1,3,5,7,9))<br>myList: List[List[Int]] = List(List(2, 4, 6, 8, 10), List(1, 3, 5, 7, 9))</p><p>scala&gt; myList.flatMap(x=&gt;x.map(_*2))<br>res22: List[Int] = List(4, 8, 12, 16, 20, 2, 6, 10, 14, 18)</p><p>myList.flatMap(x=&gt;x.map(_*2))</p><p>执行过程：<br>1、将 List(2, 4, 6, 8, 10), List(1, 3, 5, 7, 9) 调用 map(_*2) 方法。x 代表一个List<br>2、flatten</p><pre><code>package day4/**  * 对比flatmap与map  */object Demo2 {  /**    * flatmap执行分析    * 1 List(1*2)   List(2)    * 2 List(2*2)   List(4)    * 3 List(&#39;a&#39;,&#39;b&#39;)    *    * List(List(2),List(4),List(&#39;a&#39;,&#39;b&#39;)).flatten    * List(2,4,&#39;a&#39;,&#39;b&#39;)    */  def flatMap(){    val li = List(1,2,3)    val res = li.flatMap(x=&gt;      x match{      case 3=&gt;List(&#39;a&#39;,&#39;b&#39;)      case _=&gt;List(x*2)    })    println(res)  }  /**    * map过程解析    *     * 1 2  ----&gt;List(2,2,3)    * 2 4  ----&gt;List(2,4,5)    * 3 List(&#39;a&#39;,&#39;b&#39;)----&gt;List(2,4,List(&#39;a&#39;,&#39;b&#39;))    */  def map(){    val li = List(1,2,3)    val res = li.map(x=&gt;      x match{      case 3=&gt;List(&#39;a&#39;,&#39;b&#39;)      case _=&gt;x*2    })    println(res)  }  def main(args: Array[String]): Unit = {    flatMap()    map()  }}</code></pre><h4 id="5、概念：闭包、柯里化"><a href="#5、概念：闭包、柯里化" class="headerlink" title="5、概念：闭包、柯里化"></a>5、概念：闭包、柯里化</h4><p>（1）闭包：就是函数的嵌套<br>在一个函数的里面，包含了另一个函数的定义<br>可以在内函数中访问外函数的变量</p><p>举例：</p><pre><code>def mulBy(factor:Double) = (x:Double)=&gt;x*factor        外                           内</code></pre><p>乘以三：</p><pre><code>scala&gt; def mulBy(factor:Double) = (x:Double)=&gt;x*factormulBy: (factor: Double)Double =&gt; Doublescala&gt; var triple = mulBy(3)triple: Double =&gt; Double = &lt;function1&gt;相当于 triple(x:Double) = x*3scala&gt; triple(10)res1: Double = 30.0scala&gt; triple(20)res2: Double = 60.0scala&gt; var half = mulBy(0.5)half: Double =&gt; Double = &lt;function1&gt;scala&gt; half(10)res3: Double = 5.0</code></pre><p>引入柯里化：<br>scala&gt; mulBy(3)(10)<br>res4: Double = 30.0</p><p>（2）柯里化<br>概念：柯里化函数：是把具有多个参数的函数，转化为一个函数链，每个节点上都是单一函数</p><p>def add(x:Int,y:Int) = x+y</p><p>def add(x:Int)(y:Int) = x+y</p><p>转化步骤：</p><p>原始：def add(x:Int,y:Int) = x+y</p><p>闭包：def add(x:Int) = (y:Int) =&gt; x+y</p><p>简写：def add(x:Int)(y:Int) = x+y</p><p>scala&gt; def add(x:Int)(y:Int) = x+y<br>add: (x: Int)(y: Int)Int</p><p>scala&gt; add(1)(2)<br>res5: Int = 3</p><h3 id="二、Scala集合"><a href="#二、Scala集合" class="headerlink" title="二、Scala集合"></a>二、Scala集合</h3><h4 id="1、可变集合和不可变集合（Map）"><a href="#1、可变集合和不可变集合（Map）" class="headerlink" title="1、可变集合和不可变集合（Map）"></a>1、可变集合和不可变集合（Map）</h4><p>immutable mutable<br>举例：<br>scala&gt; def math = scala.collection.immutable.Map(“Tom”-&gt;80,”Lily”-&gt;20)<br>math: scala.collection.immutable.Map[String,Int]</p><p>scala&gt; def math = scala.collection.mutable.Map(“Tom”-&gt;80,”Lily”-&gt;20,”Mike”-&gt;95)<br>math: scala.collection.mutable.Map[String,Int]</p><p>集合的操作：<br>获取集合中的值<br>scala&gt; math.get(“Tom”)<br>res1: Option[Int] = Some(80)</p><p>scala&gt; math(“Tom”)<br>res2: Int = 80</p><p>scala&gt; math(“Tom123”)<br>java.util.NoSuchElementException: key not found: Tom123<br>at scala.collection.MapLike$class.default(MapLike.scala:228)<br>at scala.collection.AbstractMap.default(Map.scala:59)<br>at scala.collection.mutable.HashMap.apply(HashMap.scala:65)<br>… 32 elided</p><p>scala&gt; math.get(“Tom123”)<br>res3: Option[Int] = None</p><p>scala&gt; math.contains(“Tom123”)<br>res4: Boolean = false</p><p>scala&gt; math.getOrElse(“Tom123”,-1)<br>res5: Int = -1</p><p>更新集合中的值：注意：必须是可变集合<br>scala&gt; math<br>res6: scala.collection.mutable.Map[String,Int] = Map(Mike -&gt; 95, Tom -&gt; 80, Lily -&gt; 20)</p><p>scala&gt; math(“Tom”)=0<br>scala&gt; math<br>res7: scala.collection.mutable.Map[String,Int] = Map(Mike -&gt; 95, Tom -&gt; 80, Lily -&gt; 20)</p><p>造成上述现象的原因，没有import包，如果import以后，问题解决：<br>scala&gt; import scala.collection.mutable._<br>import scala.collection.mutable._</p><p>scala&gt; var math = Map(“Tom”-&gt;80,”Lily”-&gt;20,”Mike”-&gt;95)<br>math: scala.collection.mutable.Map[String,Int] = Map(Mike -&gt; 95, Tom -&gt; 80, Lily -&gt; 20)</p><p>scala&gt; math(“Tom”)=0<br>scala&gt; math<br>res8: scala.collection.mutable.Map[String,Int] = Map(Mike -&gt; 95, Tom -&gt; 0, Lily -&gt; 20)</p><p>添加新的元素<br>scala&gt; math(“Tom”)=80<br>scala&gt; math += “Bob”-&gt;85<br>res9: scala.collection.mutable.Map[String,Int] = Map(Bob -&gt; 85, Mike -&gt; 95, Tom -&gt; 80, Lily -&gt; 20)</p><p>移出一个元素<br>scala&gt; math -= “Bob”<br>res10: scala.collection.mutable.Map[String,Int] = Map(Mike -&gt; 95, Tom -&gt; 80, Lily -&gt; 20)</p><h4 id="2、列表：可变列表，不可变列表"><a href="#2、列表：可变列表，不可变列表" class="headerlink" title="2、列表：可变列表，不可变列表"></a>2、列表：可变列表，不可变列表</h4><p>不可变列表 List<br>scala&gt; var myList=List(1,2,3)<br>myList: List[Int] = List(1, 2, 3)</p><p>scala&gt; val nullList:List[Nothing] = List()<br>nullList: List[Nothing] = List()</p><p>//二维列表<br>scala&gt; val dim : List[List[Int]] = List(List(1,2,3),List(4,5,6))<br>dim: List[List[Int]] = List(List(1, 2, 3), List(4, 5, 6))</p><p>scala&gt; myList.head<br>res11: Int = 1</p><p>scala&gt; myList.tail<br>res12: List[Int] = List(2, 3)</p><p>注意：tail 是除了第一个元素外，其他的元素</p><p>可变列表：LinedList 在 scala.collection.mutable 包中</p><p>scala&gt; var myList = scala.collection.mutable.LinkedList(1,2,3,4)<br>warning: there was one deprecation warning; re-run with -deprecation for details<br>myList: scala.collection.mutable.LinkedList[Int] = LinkedList(1, 2, 3, 4)</p><p>需求：把上面列表中，每一个元素都乘以2</p><p>游标，指向列表的开始</p><pre><code>var cur = myList//Nil意思为空while(cur != Nil ){    //把当前元素乘以2    cur.elem = cur.elem*2    //移动指针到下一个元素    cur = cur.next}</code></pre><p>scala&gt; var cur = myList<br>cur: scala.collection.mutable.LinkedList[Int] = LinkedList(1, 2, 3, 4)</p><p>scala&gt; while(cur != Nil ){<br>| cur.elem = cur.elem*2<br>| cur = cur.next<br>| }</p><p>scala&gt; myList<br>res13: scala.collection.mutable.LinkedList[Int] = LinkedList(2, 4, 6, 8)</p><p>scala&gt; myList.map(_*2)<br>warning: there was one deprecation warning; re-run with -deprecation for details<br>res14: scala.collection.mutable.LinkedList[Int] = LinkedList(4, 8, 12, 16)</p><h4 id="3、序列"><a href="#3、序列" class="headerlink" title="3、序列"></a>3、序列</h4><p>（*）数据库中也有序列：sequence 、 auto increment<br>（1）作为主键，实现自动增长<br>（2）提高性能，序列在Oracle是在内存中的</p><p>（*）Vector Range<br>举例：<br>Vector 是一个带下标的序列，我们可以通过下标来访问Vector中的元素<br>scala&gt; var v = Vector(1,2,3,4,5,6)<br>v: scala.collection.immutable.Vector[Int] = Vector(1, 2, 3, 4, 5, 6)</p><p>Range ： 是一个整数的序列<br>scala&gt; Range(0,5)<br>res15: scala.collection.immutable.Range = Range(0, 1, 2, 3, 4)</p><p>从0开始，到5 ，但不包括5</p><p>scala&gt; println(0 until 5)<br>Range(0, 1, 2, 3, 4)</p><p>scala&gt; println(0 to 5)<br>Range(0, 1, 2, 3, 4, 5)</p><p>Range可以相加<br>scala&gt; (‘0’ to ‘9’) ++ (‘A’ to ‘Z’)<br>res16: scala.collection.immutable.IndexedSeq[Char] = Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z)</p><p>把Range转换成list<br>scala&gt; 1 to 5 toList<br>warning: there was one feature warning; re-run with -feature for details<br>res17: List[Int] = List(1, 2, 3, 4, 5)</p><h4 id="4、集（Set）"><a href="#4、集（Set）" class="headerlink" title="4、集（Set）"></a>4、集（Set）</h4><p>不重复元素的集合，默认是HashSet，与java类似<br>scala&gt; var s1 = Set(1,2,10,8)<br>s1: scala.collection.immutable.Set[Int] = Set(1, 2, 10, 8)</p><p>scala&gt; s1 + 10<br>res18: scala.collection.immutable.Set[Int] = Set(1, 2, 10, 8)</p><p>scala&gt; s1 + 7<br>res19: scala.collection.immutable.Set[Int] = Set(10, 1, 2, 7, 8)</p><p>scala&gt; s1<br>res20: scala.collection.immutable.Set[Int] = Set(1, 2, 10, 8)</p><p>创建一个可排序的Set SortedSet<br>scala&gt; var s2 = scala.collection.mutable.SortedSet(1,2,3,10,8)<br>s2: scala.collection.mutable.SortedSet[Int] = TreeSet(1, 2, 3, 8, 10)</p><p>判断元素是否存在<br>scala&gt; s2.contains(1)<br>res21: Boolean = true</p><p>scala&gt; s2.contains(1231231)<br>res22: Boolean = false</p><p>集的运算：union并集 intersect 交集 diff 差集<br>scala&gt; var s1 = Set(1,2,3,4,5,6)<br>s1: scala.collection.immutable.Set[Int] = Set(5, 1, 6, 2, 3, 4)</p><p>scala&gt; var s2 = Set(5,6,7,8,9,10)<br>s2: scala.collection.immutable.Set[Int] = Set(5, 10, 6, 9, 7, 8)</p><p>scala&gt; s1 union s2<br>res23: scala.collection.immutable.Set[Int] = Set(5, 10, 1, 6, 9, 2, 7, 3, 8, 4)</p><p>scala&gt; s1 intersect s2<br>res24: scala.collection.immutable.Set[Int] = Set(5, 6)</p><p>scala&gt; s1 diff s2<br>res25: scala.collection.immutable.Set[Int] = Set(1, 2, 3, 4)</p><p>数据库里面的union操作，要求：<br>列数一样<br>列的类型一样</p><pre><code>select A,B from ****union select C,D from *****</code></pre><p>函数的定义+返回值<br>def sum():Int =</p><p>python数据分析<br>pyspark</p><h4 id="5、模式匹配"><a href="#5、模式匹配" class="headerlink" title="5、模式匹配"></a>5、模式匹配</h4><p>相当于java中的switch case语句 但是 功能更强大</p><pre><code>package day5/**  * 模式匹配  */object Demo1 {  def main(args: Array[String]): Unit = {    //1、相当于java switch case    var chi = &#39;-&#39;    var sign = 0    chi match {      case &#39;+&#39; =&gt; sign = 1      case &#39;-&#39; =&gt; sign = -1      case _ =&gt; sign = 0    }    println(sign)    /**      * 2、scala中的守卫，case _ if 匹配某种类型的所有值      * 需求：匹配所有的数字      */    var ch2 = &#39;5&#39;    var result : Int = -1    ch2 match {      case &#39;+&#39; =&gt; println(&quot;这是一个加号&quot;)      case &#39;-&#39; =&gt; println(&quot;这是一个减号&quot;)      //这里的10表示转换成1十进制      case _ if Character.isDigit(ch2) =&gt; result=Character.digit(ch2,10)      case _ =&gt; println(&quot;其他&quot;)    }    println(result)    /**      * 3、在模式匹配中使用变量      * 如果改成var mystr = &quot;Hello W+rld&quot;      * 打印：加号      *      * 匹配中，则相当于break      */    var mystr = &quot;Hello World&quot;    //取出某个字符，赋给模式匹配的变量    mystr(7) match {      case &#39;+&#39; =&gt; println(&quot;加号&quot;)      case &#39;-&#39; =&gt; println(&quot;减号&quot;)      //case 语句中使用变量 ch代表传递进来的字符      case ch =&gt; println(ch)    }    /**      * 4、匹配类型 instance of      * 用法：case x : Int =&gt;      *      * Any : 表示任何类型，相当于java中的Object      * Unit : 表示没有值， void      * Nothing : 表示在函数抛出异常时，返回值就是Nothing      *           是scala类层级中的最低端，是任何其他类型的子类型      * Null : 表示引用类型的子类，值：null      *      * 特殊类型      * Option : 表示一个值是可选的（有值或者无值）      * Some : 如果值存在，Option[T] 就是一个Some[T]      * None : 如果值不存在，Option[T] 就是一个None      *      * scala&gt; var myMap = Map(&quot;Time&quot;-&gt;96)      * myMap: scala.collection.immutable.Map[String,Int] = Map(Time -&gt; 96)      *      * scala&gt; myMap.get(&quot;Time&quot;)      * res0: Option[Int] = Some(96)      *      * scala&gt; myMap.get(&quot;Time12342&quot;)      * res1: Option[Int] = None      *      * Nil : 空的List      *      * 四个N总结：None Nothing Null Nil      * None : 如果值不存在，Option[T] 就是一个None      * Nothing : 如果方法抛出异常时，则异常的返回值类型就是Nothing      * Null : 可以赋值给所以的引用类型，但是不能赋值给值类型      *       class Student      *       var s1 = new Student      *       s1 = null      * Nil : 空的List      */    var v4 : Any = 100    v4 match {      case x : Int =&gt; println(&quot;这是一个整数&quot;)      case s : String =&gt; println(&quot;这是一个字符串&quot;)      case _ =&gt; println(&quot;这是其他类型&quot;)    }    //5、匹配数组和列表    var myArray = Array(1,2,3)    myArray match {      case Array(0) =&gt; println(&quot;数组中只有一个0&quot;)      case Array(x,y) =&gt; println(&quot;数组中包含两个元素&quot;)      case Array(x,y,z) =&gt; println(&quot;数组中包含三个元素&quot;)      case Array(x,_*) =&gt; println(&quot;这是一个数组，包含多个元素&quot;)    }    var myList = List(1,2,3)    myList match {      case List(0) =&gt; println(&quot;列表中只有一个0&quot;)      case List(x,y) =&gt; println(&quot;列表中包含两个元素，和是&quot; + (x+y))      case List(x,y,z) =&gt; println(&quot;列表中包含三个元素，和是&quot; + (x+y+z))      case List(x,_*) =&gt; println(&quot;列表中包含多个元素，和是&quot; + myList.sum)    }  }}</code></pre><h4 id="6、样本类"><a href="#6、样本类" class="headerlink" title="6、样本类"></a>6、样本类</h4><p>定义： case class</p><pre><code>package day5/**  * 使用case class 来实现模式匹配  */class Vehiclecase class Car(name:String) extends  Vehiclecase class Bike(name:String) extends  Vehicleobject Demo2 {  def main(args: Array[String]): Unit = {    var aCar : Vehicle = new Car(&quot;Car&quot;)    aCar match {      case Car(name) =&gt; println(&quot;汽车 &quot; + name)      case Bike(name) =&gt; println(&quot;自行车 &quot; + name)      case _ =&gt; println(&quot;其他&quot;)    }  }}</code></pre><p>作用：<br>（1）支持模式匹配，instanceof<br>（2）定一个 Spark SQL 中的 schema ： 表结构</p><pre><code>scala&gt; class Fruitdefined class Fruitscala&gt; class Banana(name:String) extends Fruitdefined class Bananascala&gt; class Apple(name:String) extends Fruitdefined class Applescala&gt; var a = new Apple(&quot;Apple&quot;)a: Apple = Apple@572e6fd9scala&gt; println(a.isInstanceOf[Fruit])truescala&gt; println(a.isInstanceOf[Banana])&lt;console&gt;:16: warning: fruitless type test: a value of type Apple cannot also be a Banana       println(a.isInstanceOf[Banana])                             ^false</code></pre><h3 id="三、Scala高级特性"><a href="#三、Scala高级特性" class="headerlink" title="三、Scala高级特性"></a>三、Scala高级特性</h3><h4 id="1、泛型"><a href="#1、泛型" class="headerlink" title="1、泛型"></a>1、泛型</h4><p>和java类似 T</p><p><strong>1）泛型类</strong><br>定义类的时候，可以带有一个泛型的参数<br>例子：</p><pre><code>package day5/**  * 泛型类  *///需求：操作一个整数class GenericClassInt{  //定义一个整数的变量  private var content : Int = 10  //定义set get  def set(value : Int) = content = value  def get() : Int = content}//需求：操作一个字符串class GenericClassString{  //定义一个空字符串  private var content : String = &quot;&quot;  //定义set get  def set(value : String) = content = value  def get() : String = content}class GenericClass[T]{  //定义变量  //注意：初始值用_来表示  private var content : T = _  //定义set get  def set(value : T) = content = value  def get() : T = content}object Demo3{  def main(args: Array[String]): Unit = {    //定义一个Int 类型    var v1 = new GenericClass[Int]    v1.set(1000)    println(v1.get())    //定义一个String 类型    var v2 = new GenericClass[String]    v2.set(&quot;Ni&quot;)    println(v2.get())  }}</code></pre><p><strong>2）泛型函数</strong></p><p>定义一个函数，可以带有一个泛型的参数</p><p>scala&gt; def mkIntArray(elem:Int<em>)=`Array[Int](elem:_</em>)`<br>mkIntArray: (elem: Int*)Array[Int]</p><p>scala&gt; mkIntArray(1,2,3)<br>res5: Array[Int] = Array(1, 2, 3)</p><p>scala&gt; mkIntArray(1,2,3,4,5)<br>res6: Array[Int] = Array(1, 2, 3, 4, 5)</p><p>scala&gt; def mkStringArray(elem:String<em>)=`Array[String](elem:_</em>)`<br>mkStringArray: (elem: String*)Array[String]</p><p>scala&gt; mkStringArray(“a”,”b”)<br>res7: Array[String] = Array(a, b)</p><p>scala&gt; def mkArray[T:ClassTag]</p><p>ClassTag ： 表示scala在运行时候的状态信息，这里表示调用时候数据类型</p><p>scala&gt; import scala.reflect.ClassTag<br>import scala.reflect.ClassTag</p><pre><code>scala&gt; def mkArray[T:ClassTag](elem:T*)= Array[T](elem:_*)mkArray: [T](elem: T*)(implicit evidence$1: scala.reflect.ClassTag[T])Array[T]</code></pre><p>scala&gt; mkArray(1,2)<br>res8: Array[Int] = Array(1, 2)</p><p>scala&gt; mkArray(“Hello”,”aaa”)<br>res9: Array[String] = Array(Hello, aaa)</p><p>scala&gt; mkArray(“Hello”,1)<br>res10: Array[Any] = Array(Hello, 1)<br>泛型：但凡有重复的时候，考虑使用泛型</p><p><strong>3）上界和下界</strong></p><p>Int x<br>规定x的取值范围 100 &lt;= x &lt;=1000</p><p>泛型的取值范围：<br>T</p><p>类的继承关系 A —&gt; B —&gt; C —&gt; D 箭头指向子类</p><p>定义T的取值范围 D &lt;: T &lt;: B</p><p>T 的 取值范围 就是 B C D</p><p>&lt;: 就是上下界的表示方法</p><p>概念<br>上界 S &lt;： T 规定了 S的类型必须是 T的子类或本身<br>下界 U &gt;： T 规定了 U的类型必须是 T的父类或本身</p><p>例子：</p><pre><code>package day5/**  * 主界  *///定义父类class Vehicle{  //函数：驾驶  def drive() = println(&quot;Driving&quot;)}//定义两个子类class Car extends Vehicle{  override def drive() : Unit = println(&quot;Car Driving&quot;)}//class Bike extends Vehicle{//  override def drive(): Unit = println(&quot;Bike Driving&quot;)//}class Bike{  def drive(): Unit = println(&quot;Bike Driving&quot;)}object ScalaUpperBoud {  //定义驾驶交通工具的函数  def takeVehicle[T &lt;: Vehicle](v:T) = v.drive()  def main(args: Array[String]): Unit = {    //定义交通工具    var v : Vehicle = new Vehicle    takeVehicle(v)    var c : Car = new Car    takeVehicle(c)    //因为没有继承Vehicle，所以运行报错    var b : Bike = new Bike    takeVehicle(b)  }}</code></pre><pre><code>scala&gt; def addTwoString[T &lt;: String](x:T,y:T) = x +&quot; ********* &quot; + yaddTwoString: [T &lt;: String](x: T, y: T)Stringscala&gt; addTwoString(&quot;Hello&quot;,&quot;World&quot;)res11: String = Hello ********* Worldscala&gt; addTwoString(1,2)&lt;console&gt;:14: error: inferred type arguments [Int] do not conform to method addTwoString&#39;s type parameter bounds [T &lt;: String]       addTwoString(1,2)       ^&lt;console&gt;:14: error: type mismatch; found   : Int(1) required: T       addTwoString(1,2)                    ^&lt;console&gt;:14: error: type mismatch; found   : Int(2) required: T       addTwoString(1,2)                      ^scala&gt; addTwoString(1.toString,2.toString)res13: String = 1 ********* 2</code></pre><p><strong>4）视图界定 View bounds</strong></p><p>就是上界和下界的扩展</p><p>除了可以接收上界和下界规定的类型以外，还可以接收能够通过隐式转换过去的类型</p><p>用 % 来表示</p><pre><code>scala&gt;  def addTwoString[T &lt;% String](x:T,y:T) = x +&quot; ********* &quot; + yaddTwoString: [T](x: T, y: T)(implicit evidence$1: T =&gt; String)Stringscala&gt; addTwoString(1,2)&lt;console&gt;:14: error: No implicit view available from Int =&gt; String.       addTwoString(1,2)                   ^//定义隐式转换函数scala&gt; implicit def int2String(n:Int):String = n.toStringwarning: there was one feature warning; re-run with -feature for detailsint2String: (n: Int)Stringscala&gt; addTwoString(1,2)res14: String = 1 ********* 2</code></pre><p><strong>执行过程</strong> </p><p>1、调用了 int2String Int =&gt; String<br>2、addTwoString(“1”,”2”)</p><p><strong>5）协变和逆变（概念）</strong></p><p>协变：表示在类型参数前面加上 + 。泛型变量的值，可以是本身类型或者其子类类型<br>例子：</p><pre><code>package day5/**  * 协变：表示在类型参数前面加上 + 。泛型变量的值，可以是本身类型或者其子类类型  */class Animalclass Bird extends Animalclass Sparrow extends Bird//定义第四个类，吃东西的类，协变，有继承关系了class EatSomething[+T](t:T)object Demo4 {  def main(args: Array[String]): Unit = {    //定义一个鸟吃东西的对象    var c1 : EatSomething[Bird] =new EatSomething[Bird](new Bird)    //定义一个动物吃东西的对象    var c2 : EatSomething[Animal] = c1    /**      * 问题：能否把c1 赋给c2      * c1 c2都是EatSomething      * c1 c2 没有继承关系      *      * class EatSomething[T](t:T)      * var c2 : EatSomething[Animal] = c1  报错      * 原因 ： EatSomething[Bird] 并没有继承EatSomething[Animal]      *      * class EatSomething[+T](t:T)      * 报错消失      *      * 协变      */    var c3 : EatSomething[Sparrow] = new EatSomething[Sparrow](new Sparrow)    var c4 : EatSomething[Animal] = c3  }}</code></pre><p>逆变：表示在类型参数前面加上 - 。泛型变量的值，可以是本身类型或者其父类类型<br>例子：</p><pre><code>package day5/**  * 逆变：表示在类型参数前面加上 - 。泛型变量的值，可以是本身类型或者其父类类型  */class Animalclass Bird extends Animalclass Sparrow extends Bird//定义第四个类，吃东西的类，逆变class EatSomething[-T](t:T)object Demo5 {  def main(args: Array[String]): Unit = {    //定义一个鸟吃东西的对象    var c1 : EatSomething[Bird] =new EatSomething[Bird](new Bird)    //定义一个动物吃东西的对象    var c2 : EatSomething[Sparrow] = c1  }}</code></pre><h4 id="2、隐式转换"><a href="#2、隐式转换" class="headerlink" title="2、隐式转换"></a>2、隐式转换</h4><p><strong>1）隐式转换函数： implicit</strong></p><pre><code>package day5/**  * 隐式转换  *  * 定义一个隐式转换函数  */class Fruit(name:String){  def getFruitName() : String = name}class Monkey(f:Fruit){  def say()  = println(&quot;Monkey like &quot; + f.getFruitName())}object ImplicitDemo {  def main(args: Array[String]): Unit = {    //定义一个水果对象    var f : Fruit = new Fruit(&quot;Banana&quot;)    f.say()  }  implicit def fruit2Monkey(f:Fruit) : Monkey = {    new Monkey(f)  }}</code></pre><p><strong>2）隐式参数：使用implicit 修饰的函数参数</strong></p><p>定义一个带有隐式参数的函数：</p><pre><code>scala&gt; def testPara(implicit name:String) = println(&quot;The value is &quot; + name)testPara: (implicit name: String)Unitscala&gt; testPara(&quot;AAAA&quot;)The value is AAAAscala&gt; implicit val name : String = &quot;*****&quot;name: String = *****scala&gt; testParaThe value is *****</code></pre><p>定义一个隐式参数，找到两个值中比较小的那个值<br>100 23 –&gt;23<br>“Hello” “ABC” –&gt; ABC</p><pre><code>scala&gt; def smaller[T](a:T,b:T)(implicit order : T =&gt; Ordered[T]) = if(a&lt;b) a else bsmaller: [T](a: T, b: T)(implicit order: T =&gt; Ordered[T])Tscala&gt; smaller(1,2)res18: Int = 1scala&gt; smaller(&quot;Hello&quot;,&quot;ABC&quot;)res19: String = ABC</code></pre><p>解释：<br>order 就是一个隐式参数，我们使用Scala中的 Ordered 类，表示该值可以被排序，也就是可以被比较</p><p>作用：扩充了属性的功能</p><p><strong>3）隐式类 在类名前 加 implicit 关键字</strong> </p><p>作用：扩充类的功能</p><pre><code>package day5/**  * 隐式类  */object Demo6 {  def main(args: Array[String]): Unit = {    //执行两个数字的求和    println(&quot;两个数字的和是： &quot;+1.add(2))    /**      * 定义一个隐式类，类增强1的功能      *      * Calc(x:Int)      * 1是Int类型，所以就会传递进来      *      * 执行过程：      * 1---&gt;Calc类      * var a = new Calc(1)      * 在调用Calc add方法      * a.add(2)      *      */    implicit class Calc(x:Int){      def add(y: Int) : Int = x + y    }  }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala基础</title>
      <link href="/2019/03/23/scala-ji-chu/"/>
      <url>/2019/03/23/scala-ji-chu/</url>
      
        <content type="html"><![CDATA[<p>1、Scala编程语言<br>2、Spark Core ： Spark内核 ，最重要的一个部分<br>3、Spark SQL : 类似于 Hive 和Pig。数据分析引擎。sql语句提交到Spark集群中运行<br>4、Spark Streaming ：类似于 Storm，用于流式计算、实时计算。本质：一个离线计算</p><h3 id="一、Scala基础"><a href="#一、Scala基础" class="headerlink" title="一、Scala基础"></a>一、Scala基础</h3><h4 id="1、Scala简介"><a href="#1、Scala简介" class="headerlink" title="1、Scala简介"></a>1、Scala简介</h4><p>1）Scala是一个多范式的编程语言（支持多种方式的编程）<br>（1）使用面向对象编程：封装、继承、多态<br>（2）使用函数式编程：最大的特定<br>优点：代码非常简洁<br>缺点：可读性太差，尤其是隐式类、隐式函数、隐式参数</p><p>2）安装和配置Scala<br>（1）基于JDK，先安装JDK<br>（2）Scala：2.11.8（Spark 2.1.0）<br>（3）配置环境变量：SCALA_HOME<br>（4）%SCALA_HOME%\bin 配置到path中<br>下载地址：<a href="https://www.scala-lang.org" target="_blank" rel="noopener">https://www.scala-lang.org</a><br>文档地址：<a href="https://www.scala-lang.org/api/2.11.8/#scala.math.package" target="_blank" rel="noopener">https://www.scala-lang.org/api/2.11.8/#scala.math.package</a></p><p>3）开发环境<br>REPL命令行<br>IDEA ： 需要安装Scala插件</p><h4 id="2、Scala中的数据类型和变量常量"><a href="#2、Scala中的数据类型和变量常量" class="headerlink" title="2、Scala中的数据类型和变量常量"></a>2、Scala中的数据类型和变量常量</h4><p>1）注意一点：Scala中所有的数据，都是对象<br>举例：1 Java int 。在Scala中，1 就是一个对象</p><p>2）基本数据类型<br>Byte ：8位有符号数字<br>Short ：16位有符号数字<br>Int ：32位有符号数字<br>Long ： 64位有符号数字<br>Float ：32位有符号数字<br>Double ：64位有符号数字<br>Char ：8位有符号数字<br>Boolean ： 1位有符号数字</p><p>字符串类型<br>String </p><p>字符<br>Char ：8位有符号数字</p><p>Scala中字符串的插值操作：就是相当于字符串的拼接</p><pre><code>scala&gt; var s1 : String = &quot;Hello &quot;s1: String = &quot;Hello &quot;scala&gt; &quot;My name is Tom and ${s1}&quot;res1: String = My name is Tom and ${s1}</code></pre><p>插值操作时，需要加入 s</p><pre><code>scala&gt; s&quot;My name is Tom and ${s1}&quot;res2: String = &quot;My name is Tom and Hello &quot;</code></pre><p>3）变量var和常量val</p><p>scala&gt; val s2 :String = “Hello all”<br>s2: String = Hello all</p><p>scala&gt; s2 = “Hello everyone”<br><console>:12: error: reassignment to val<br>   s2 = “Hello everyone”</p><p>4）Unit类型和Nothing类型<br>（1）Unit类型，就是java中的void，没有返回值</p><pre><code>scala&gt; val f = ()f: Unit = ()</code></pre><p>返回值 Unit类型<br>( ) 代表了一个函数，这个函数没有返回值</p><p>（2）Nothing类型，在执行过程中，产生了异常Exception<br>举例：<br>scala函数：scala中函数非常重要，是scala的头等公民<br>用法很多：函数式编程、高阶函数</p><p>def myFunction = 函数的实现</p><pre><code>scala&gt; def myFun = throw new Exception(&quot;Some Error&quot;)myFun: Nothing</code></pre><h4 id="3、函数（头等公民）"><a href="#3、函数（头等公民）" class="headerlink" title="3、函数（头等公民）"></a>3、函数（头等公民）</h4><p>（1）scala内置函数，可以直接使用的函数</p><pre><code>scala&gt; max(1,2)&lt;console&gt;:12: error: not found: value max       max(1,2)       ^scala&gt; import scala.math   final package mathscala&gt; import scala.math._import scala.math._</code></pre><p>_ 就相当于Java中的 * 代表包内所有东西</p><pre><code>scala&gt; max(1,2)res4: Int = 2</code></pre><p>res4: Int = 2<br>定义了一个变量 res4 ，接收了 max 函数的返回值。Scala中支持类型的推导。<br>res4 = “”</p><p>（2） 自定义函数<br>语法：<br>def 函数名称（[参数名称：参数类型]*） : 返回值类型 = {<br>函数的实现<br>}</p><p>举例：<br>1）求和</p><pre><code>scala&gt; def sum(x:Int,y:Int):Int = x + ysum: (x: Int, y: Int)Intscala&gt; sum(1,2)res5: Int = 3</code></pre><p>2）求阶乘，5！= 5 * 4* 3 2 1 </p><p><strong>递归</strong></p><pre><code>scala&gt; def myFactor(x:Int):Int = {     | if(x&lt;=1)     | 1     | else     | x*myFactor(x-1)     | }myFactor: (x: Int)Intscala&gt; myFactor(5)res6: Int = 120</code></pre><p>注意：没有return语句<br>函数的最后一句话，就是函数的返回值</p><p>3）求输入的年份是否是闰年<br>闰年：<br>普通闰年：可以被4整除但是不能被100整除的年份<br>世纪闰年：可以被400整除的年份</p><pre><code>scala&gt; def isLeapYear(x:Int) = {     | if(( x%4 == 0 &amp;&amp; x%100 != 0) || (x%400==0)) true     | else false     | }isLeapYear: (x: Int)Booleanscala&gt; isLeapYear(2019)res7: Boolean = falsescala&gt; isLeapYear(2008)res8: Boolean = true</code></pre><p><strong>注意</strong></p><p>1、( x%4 == 0 &amp;&amp; x%100 != 0) || (x%400==0)<br>2、函数定义的时候，可以不写返回值，因为scala支持类型推导</p><h4 id="4、循环语句"><a href="#4、循环语句" class="headerlink" title="4、循环语句"></a>4、循环语句</h4><p>1）类似于Java的用法 while dowhile for</p><pre><code>/**      * for循环      *      * 定义一个集合      */    var list = List(&quot;dfg&quot;,&quot;Agddg&quot;,&quot;Fd&quot;)    println(&quot;------------for循环中的第一种写法-------------&quot;)    for( s &lt;- list ) println(s)    println(&quot;------------for循环中的第二种写法-------------&quot;)    //打印长度大于3的名字    for{      s &lt;- list      if(s.length &gt; 3)    }println(s)    println(&quot;------------for循环中的第三种写法-------------&quot;)    //对第二种进一步简化    for( s&lt;- list if s.length &lt;= 3) println(s)    println(&quot;------------for循环中的第四种写法-------------&quot;)    /**      * 1、把list中所有元素都变成大写      * s &lt;- list      * s1 = s.toUpperCase      *      * 2、返回一个新的集合      * yield(s1)      *      */    var newList = for {      s &lt;- list      s1 = s.toUpperCase    }yield(s1)    for(s &lt;- newList) println(s)    println(&quot;------------while循环-------------&quot;)    //定义循环变量    var i = 0    while( i &lt; list.length){      println(list(i))      /**        * 自增        *        * 注意scala中没有i++        */      i += 1    }    println(&quot;------------do while循环-------------&quot;)    //定义循环变量    var j = 0    do{      println(list(j))      j += 1    }while( j &lt; list.length)</code></pre><p>2）foreach循环（Spark算子）</p><pre><code>  println(&quot;------------foreach循环-------------&quot;)    /**      * foreach scala里面有 spark里面      * map      *      * 没有返回值，map有返回值      * foreach是list的一个方法      */    list.foreach(println)    /**      * foreach 说明      * list.foreach(println)      *      *  我们把一个函数传入了foreach      *  高阶函数（函数式编程）      */    /**      * 判断101-200之间有多少个素数      *      * 判断素数的方法：      * x%2 -----x%sqrt(根号)x      * 当都不能被整除的时候就是素数      *      *      * 16      * sqrt(16) = 4      * 2 3 4      *      * 16%2 == 0 ?      * 16%3 == 0 ?      * 16%4 == 0 ?      *      * 编程思路：      * 两层循环：      *   第一层：101-200      *     第二层：2--sqrt第一层      *      */    println(&quot;---------循环嵌套-------------&quot;)    var count : Int = 0 //保存结果    var index_outer = 0    var index_inner = 0    for(index_outer &lt;- 101 until 200){      index_inner = 2      var b = false //标识是否能被整除      breakable{        while(index_inner &lt;= sqrt(index_outer)){          if(index_outer % index_inner ==0) {            b = true            break          }          index_inner += 1        }      }      if(!b) count += 1    }    println(&quot;个数为：&quot; + count)    /**    * 算法分析：    * 1、比较相邻的元素。如果第一个比第二个大，就交换他们两个。    * 2、对每一对相邻元素都做上述工作，循环完第一次后，最后的元素，就是最大的元素。    * 3、针对剩下的元素，重复上面工作（除了最后一个元素）    *    * 程序分析：    * 1、两层循环    * 2、外层循环控制比较的次数    * 3、内层循环控制到达的位置，就是 结束比较 的位置    **/    println(&quot;---------冒泡排序-------------&quot;)    var a = Array(12,3,6,3,6,7,3,8,34,3)    println(&quot;---------排序前---------------&quot;)    a.foreach(println)    for(i &lt;- 0 until a.length - 1){      for(j &lt;- 0 until a.length - i - 1){        if(a(j) &gt; a(j+1)){          //交换          var tmp = a(j)          a(j) = a(j+1)          a(j+1) = tmp        }      }    }    println(&quot;---------排序后---------------&quot;)    a.foreach(println)</code></pre><h4 id="5、Scala的函数参数"><a href="#5、Scala的函数参数" class="headerlink" title="5、Scala的函数参数"></a>5、Scala的函数参数</h4><p>1）函数参数的求值策略<br>（1）call by value :<br>对函数的实参求值，并且只求一次</p><p>（2）call by name : =&gt;<br>函数实参在函数体内部用到的时候，才会被求值</p><p>举例：</p><pre><code>scala&gt; def test1(x:Int,y:Int) = x + xtest1: (x: Int, y: Int)Intscala&gt; test1(3+4,8)res9: Int = 14scala&gt; def test2(x : =&gt; Int,y : =&gt; Int) = x+xtest2: (x: =&gt; Int, y: =&gt; Int)Intscala&gt; test2(3+4,8)res10: Int = 14</code></pre><p>执行过程对比：<br>test1 —&gt; test1(3+4,8) —&gt; test1(7,8) —&gt; 7+7 —&gt; 14<br>test2 —&gt; test2(3+4,8) —&gt; (3+4) + (3+4) —&gt; 14</p><p>（3）复杂的例子<br>def bar(x:Int,y : =&gt; Int) : Int = 1<br>x 是 value y 是 name</p><p>定义一个死循环：<br>def loop() : Int = loop</p><p>调用bar函数的时候：<br>1、bar(1,loop)<br>2、bar(loop,1)–&gt;产生死循环</p><p>哪个方式会产生死循环？</p><pre><code>scala&gt; def bar(x:Int,y : =&gt; Int) : Int = 1bar: (x: Int, y: =&gt; Int)Intscala&gt; def loop() : Int = looploop: ()Intscala&gt; bar(1,loop)res11: Int = 1scala&gt; bar(loop,1)</code></pre><p><strong>解析</strong></p><p>1、虽然 y 是 name, 每次调用的时候会被求值。但是，函数体内，没有调用到y.<br>2、x 是 value，对函数参数求值，并且只求一次。虽然后面没有用到x，但求值时产生了死循环</p><p>2、Scala中函数参数的类型<br>（1）默认参数<br>当你没有给参数值赋值的时候，就会使用默认值</p><pre><code>def fun1(name:String=&quot;Ti&quot;) :String = &quot;Hello &quot; + namescala&gt; def fun1(name:String=&quot;Ti&quot;) :String = &quot;Hello &quot; + namefun1: (name: String)Stringscala&gt; fun1(&quot;An&quot;)res0: String = Hello Anscala&gt; fun1()res1: String = Hello Ti</code></pre><p>（2）代名参数<br>当有多个默认参数的时候，通过代名参数可以确定给哪个函数参数赋值。</p><pre><code>def fun2(str:String = &quot;Hello &quot; , name:String = &quot; Ti &quot; ,age:Int = 20) = str + name + &quot; age is &quot; +agescala&gt; def fun2(str:String = &quot;Hello &quot; , name:String = &quot; Ti &quot; ,age:Int = 20) = str + name + &quot; age is &quot; +agefun2: (str: String, name: String, age: Int)Stringscala&gt; fun2()res2: String = Hello  Ti  age is 20scala&gt; fun2(&quot;An&quot;)res3: String = An Ti  age is 20scala&gt; fun2(name=&quot;An&quot;)res4: String = Hello An age is 20</code></pre><p>（3）可变参数<br>类似于Java中的可变参数，即 参数数量不固定</p><pre><code>scala&gt; def sum(args:Int*)= { | var result = 0 | for(s&lt;-args) result +=s | result | }sum: (args: Int*)Intscala&gt; sum(1,2,3,4)res5: Int = 10scala&gt; sum(1,2,3,4,3,4)res6: Int = 17</code></pre><h4 id="6、懒值（lazy）"><a href="#6、懒值（lazy）" class="headerlink" title="6、懒值（lazy）"></a>6、懒值（lazy）</h4><p>铺垫：Spark的核心是 RDD（数据集合），操作数据集合的数据，使用算子来操作RDD（函数、方法）</p><p>算子：<br>Transformation ： 延时加载，不会触发计算<br>Action ： 会立刻触发计算</p><p>定义：常量如果是lazy的，他的初始化会被延迟，推迟到第一次使用该常量的时候<br>举例：<br>scala&gt; var x : Int = 10<br>x: Int = 10</p><p>scala&gt; val y : Int = x+1<br>y: Int = 11</p><p>y 的值是x+1 定义后会立即进行计算</p><pre><code>scala&gt; lazy val z : Int = x+1z: Int = &lt;lazy&gt;</code></pre><p>z的初始化会被延迟<br>scala&gt; z<br>res0: Int = 11</p><p>当我们第一次使用z的时候，才会触发计算</p><p>读文件：<br>scala&gt; val words = scala.io.Source.fromFile(“E:\student.txt”).mkString<br>words: String =<br>1 Tom 12<br>2 Mary 13<br>3 Lily 15</p><p>scala&gt; lazy val words = scala.io.Source.fromFile(“E:\student.txt”).mkString<br>words: String = <code>&lt;lazy&gt;</code><br>scala&gt; words<br>res1: String =<br>1 Tom 12<br>2 Mary 13<br>3 Lily 15</p><p>scala&gt; lazy val words = scala.io.Source.fromFile(“E:\student121.txt”).mkString<br>words: String = <code>&lt;lazy&gt;</code></p><p>定义成lazy后，初始化被延迟，所以不会抛异常<br>scala&gt; val words = scala.io.Source.fromFile(“E:\student121.txt”).mkString<br>java.io.FileNotFoundException: E:\student121.txt (系统找不到指定的文件。)<br>at java.io.FileInputStream.open0(Native Method)<br>at java.io.FileInputStream.open(FileInputStream.java:195)<br>at java.io.FileInputStream.(FileInputStream.java:138)<br>at scala.io.Source.fromFile(Source.scala:76)<br>at scala.io.Source$.fromFile(Source.scala:54)<br>… 32 elided</p><h4 id="7、例外：Exception"><a href="#7、例外：Exception" class="headerlink" title="7、例外：Exception"></a>7、例外：Exception</h4><p>类似于Java，还是有一些变化<br>文件操作</p><pre><code>scala&gt; var words = scala.io.Source.fromFile(&quot;E:\\server.xml&quot;).mkStringwords: String =&quot;&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--  Licensed to the Apache Software Foundation (ASF) under one or more  contributor license agreements.  See the NOTICE file distributed with  this work for additional information regarding copyright ownership....</code></pre><pre><code>scala&gt; val words = scala.io.Source.fromFile(&quot;E:\\212.txt&quot;).mkStringjava.io.FileNotFoundException: E:\\212.txt(系统找不到指定的文件。) at java.io.FileInputStream.open0(Native Method) at java.io.FileInputStream.open(FileInputStream.java:195) at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:138) at scala.io.Source$.fromFile(Source.scala:91) at scala.io.Source$.fromFile(Source.scala:76) at scala.io.Source$.fromFile(Source.scala:54) ... 32 elided</code></pre><p><strong>try catch finally练习</strong></p><pre><code>   /**      * 1、采用 try catch finally 来捕获异常和处理异常      * 试验一下，scala中文件读取      *      */    try{      // try 代码块里面写 可能抛出异常的函数      println(&quot;------------try catch finally------------------&quot;)      var words = scala.io.Source.fromFile(&quot;E:\\server.xml&quot;).mkString      println(words)    }catch {      case ex: FileNotFoundException =&gt; {        println(&quot;File Not Found Exception&quot;)      }      case ex: IllegalArgumentException =&gt;{        println(&quot;Illegal Argument Exception&quot;)      }      case _:Exception =&gt;{        println(&quot;This is an Exception&quot;)      }    }finally {      println(&quot;This is finally&quot;)    }  /**   * 当没有抛出异常时：   * try  ---&gt;  finally   * 打印：   * This is finally   *   * 当抛出异常时：   * try  --&gt; catch --&gt; finally   * File Not Found Exception   * This is finally   */   /**    * 2、如果一个函数返回值类型是nothing，表示：在函数执行的过程中，产生了异常    *    * scala&gt; def fun1() = throw new Exception(&quot;Exception&quot;)    *  fun1: ()Nothing    */</code></pre><h4 id="8、数组"><a href="#8、数组" class="headerlink" title="8、数组"></a>8、数组</h4><p>1）数组的类型<br>（1）定长数组：Array</p><pre><code>scala&gt; val a = new Array[Int](10)   -----&gt;  (10) 就是数组的长度a: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)scala&gt; val a = new Array[String](10)a: Array[String] = Array(null, null, null, null, null, null, null, null, null, null)</code></pre><p><strong>初始化赋给默认值</strong></p><pre><code>scala&gt; val c : Array[String] = Array(&quot;Tom&quot;,&quot;Lily&quot;)c: Array[String] = Array(Tom, Lily)scala&gt; val c : Array[String] = Array(&quot;Tom&quot;,&quot;Lily&quot;,1)&lt;console&gt;:11: error: type mismatch; found   : Int(1) required: String       val c : Array[String] = Array(&quot;Tom&quot;,&quot;Lily&quot;,1)</code></pre><p>不能往数组中添加不同类型的元素</p><p>（2）变长数组：ArrayBuffer</p><pre><code>scala&gt; val d = ArrayBuffer[Int]()&lt;console&gt;:11: error: not found: value ArrayBuffer       val d = ArrayBuffer[Int]()       ^</code></pre><p>scala&gt; import scala.collection.mutable._<br>import scala.collection.mutable._</p><p><strong>mutable（可变的）</strong></p><p>scala&gt; val d = ArrayBufferInt<br>d: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer()</p><p>scala&gt; d += 1<br>res2: d.type = ArrayBuffer(1)</p><p>scala&gt; d += 2<br>res3: d.type = ArrayBuffer(1, 2)</p><p>scala&gt; d += (1,2,3,4)<br>res4: d.type = ArrayBuffer(1, 2, 1, 2, 3, 4)</p><p>scala&gt; d.<br>++ combinations groupBy mapResult reverse to<br>++: companion grouped max reverseIterator toArray<br>++= compose hasDefiniteSize maxBy reverseMap toBuffer<br>++=: contains hashCode min runWith toIndexedSeq<br>+: containsSlice head minBy sameElements toIterable<br>+= copyToArray headOption mkString scan toIterator<br>+=: copyToBuffer indexOf nonEmpty scanLeft toList </p><ul><li>corresponds indexOfSlice orElse scanRight toMap<br>– count indexWhere padTo segmentLength toSeq<br>–= diff indices par seq toSet </li><li>= distinct init partition size toStream<br>/: drop inits patch sizeHint toString<br>:+ dropRight insert permutations sizeHintBounded toTraversable<br>:\ dropWhile insertAll prefixLength slice toVector<br>&lt;&lt; endsWith intersect prepend sliding transform<br>WithFilter equals isDefinedAt prependAll sortBy transpose<br>addString exists isEmpty product sortWith trimEnd<br>aggregate filter isTraversableAgain readOnly sorted trimStart<br>andThen filterNot iterator reduce span union<br>append find last reduceLeft splitAt unzip<br>appendAll flatMap lastIndexOf reduceLeftOption startsWith unzip3<br>apply flatten lastIndexOfSlice reduceOption stringPrefix update<br>applyOrElse fold lastIndexWhere reduceRight sum updated<br>canEqual foldLeft lastOption reduceRightOption tail view<br>clear foldRight length reduceToSize tails withFilter<br>clone forall lengthCompare remove take zip<br>collect foreach lift repr takeRight zipAll<br>collectFirst genericBuilder map result takeWhile zipWithIndex</li></ul><p>举例：去掉数组中，最后两个元素</p><pre><code>scala&gt; dres5: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(1, 2, 1, 2, 3, 4)scala&gt; d.trimEnd(2)scala&gt; dres7: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(1, 2, 1, 2)</code></pre><p><strong>遍历数组</strong><br>for循环、foreach：</p><pre><code>scala&gt; var a = Array(&quot;Tom&quot;,&quot;Lily&quot;,&quot;Andy&quot;)a: Array[String] = Array(Tom, Lily, Andy)scala&gt; for(s &lt;- a ) println(s)TomLilyAndyscala&gt; a.foreach(println)TomLilyAndy</code></pre><p>数组的常见操作举例：</p><pre><code>scala&gt; val myarray = Array(1,2,7,8,10,3,6)myarray: Array[Int] = Array(1, 2, 7, 8, 10, 3, 6)scala&gt; myarray.maxres10: Int = 10scala&gt; myarray.minres11: Int = 1scala&gt; myarray.sortWith(_&gt;_)res12: Array[Int] = Array(10, 8, 7, 6, 3, 2, 1)scala&gt; myarray.sortWith(_&lt;_)res13: Array[Int] = Array(1, 2, 3, 6, 7, 8, 10)</code></pre><p>解释：(<em>&gt;</em>)<br>完整 ： sortWith函数里面，参数也是一个函数  –&gt; 高阶函数</p><pre><code>_&gt;_   函数def comp(a:Int,b:Int) = {if(a&gt;b) true else false}(a,b) =&gt; {if(a&gt;b) true else false}(a,b) =&gt; {if(a&gt;b) true else false}   ----&gt;  _&gt;_</code></pre><p><em>&gt;</em> 是一个函数，传入两个参数，返回值是Bool （布尔型）</p><p>2）多维数组<br>和Java类似，通过数组的数组来实现</p><pre><code>scala&gt; var matrix = Array.ofDim[Int](3,4)matrix: Array[Array[Int]] = Array(Array(0, 0, 0, 0), Array(0, 0, 0, 0), Array(0, 0, 0, 0))    Array(0, 0, 0, 0)     Array(0, 0, 0, 0)    Array(0, 0, 0, 0)</code></pre><p>三行四列的数组</p><p>scala&gt; matrix(1)(2)=10</p><p>scala&gt; matrix<br>res15: Array[Array[Int]] = Array(Array(0, 0, 0, 0), Array(0, 0, 10, 0), Array(0, 0, 0, 0))</p><p>数组下标是从0开始的</p><p>例子：<br>定义一个二维数组，其中每个元素是一个一维数组，并且长度不固定</p><pre><code>scala&gt; var triangle = new Array[Array[Int]](10)triangle: Array[Array[Int]] = Array(null, null, null, null, null, null, null, null, null, null)</code></pre><p>初始化：</p><pre><code>scala&gt; for(i &lt;- 0 until triangle.length){     | triangle(i) = new Array[Int](i+1)     | }scala&gt; triangle res17: Array[Array[Int]] = Array(Array(0), Array(0, 0), Array(0, 0, 0), Array(0, 0, 0, 0), Array(0, 0, 0, 0, 0), Array(0, 0, 0, 0, 0, 0), Array(0, 0, 0, 0, 0, 0, 0), Array(0, 0, 0, 0, 0, 0, 0, 0), Array(0, 0, 0, 0, 0, 0, 0, 0, 0), Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0))</code></pre><p>二维数组，如果使用 ArrayArray[Int] 声明时：<br>1、首先指定的是外层数据的长度<br>2、初始化内层数组的时候，再指定内层数组的长度</p><h4 id="9、映射-lt-key-value-gt-Map"><a href="#9、映射-lt-key-value-gt-Map" class="headerlink" title="9、映射 &lt;key,value&gt; Map"></a>9、映射 &lt;key,value&gt; Map</h4><p>举例：<br>创建一个map，来保存学生的成绩</p><p>scala&gt; val scores = Map(“Tom” -&gt; 80,”Andy”-&gt;70,”Mike”-&gt;90)<br>scores: scala.collection.mutable.Map[String,Int] = Map(Mike -&gt; 90, Tom -&gt; 80, Andy -&gt; 70)</p><p>1）Map[String,Int] key String value Int<br>2）scala.collection.mutable</p><p>scala中，映射是有两种，一种是可变map，一种是不可变map<br>scala.collection.mutable —&gt; 可变<br>scala.collection.immutable —&gt; 不可变</p><pre><code>scala&gt; val scores2 = scala.collection.immutable.Map(“Tom” -&gt; 80,”Andy”-&gt;70,”Mike”-&gt;90) scores2: scala.collection.immutable.Map[String,Int] = Map(Tom -&gt; 80, Andy -&gt; 70, Mike -&gt; 90)</code></pre><p><strong>映射的初始化</strong></p><pre><code>scala&gt; val scores2 = scala.collection.mutable.Map((“Tom”,80),(“Andy”,70)) scores2: scala.collection.mutable.Map[String,Int] = Map(Tom -&gt; 80, Andy -&gt; 70)scala&gt; scores2=1&lt;console&gt;:15: error: reassignment to val       scores2=1</code></pre><p><strong>映射的操作</strong><br>1）获取映射中的值</p><pre><code>scala&gt; val chinese = scala.collection.mutable.Map((&quot;Tom&quot;,80),(&quot;Andy&quot;,70))chinese: scala.collection.mutable.Map[String,Int] = Map(Tom -&gt; 80, Andy -&gt; 70)scala&gt; chinese(&quot;Tom&quot;)res18: Int = 80scala&gt; chinese.get(&quot;Andy&quot;)res19: Option[Int] = Some(70)scala&gt; chinese(&quot;aaaa&quot;)java.util.NoSuchElementException: key not found: aaaa  at scala.collection.MapLike$class.default(MapLike.scala:228)  at scala.collection.AbstractMap.default(Map.scala:59)  at scala.collection.mutable.HashMap.apply(HashMap.scala:65)  ... 32 elidedscala&gt; chinese.get(&quot;Andy1231312&quot;)res21: Option[Int] = Nonechinese(&quot;aaaa&quot;) get(&quot;Andy1231312&quot;)需求：判断key是否存在，若不存在，返回默认值scala&gt; if(chinese.contains(&quot;aaa&quot;)){     | chinese(&quot;aaa&quot;)     | }else{     | -1     | }res22: Int = -1scala&gt; chinese.getOrElse(&quot;aaaa&quot;,-1)res23: Int = -1</code></pre><p>2）更新映射中的值<br>注意：必须是可变映射</p><pre><code>scala&gt; chineseres24: scala.collection.mutable.Map[String,Int] = Map(Tom -&gt; 80, Andy -&gt; 70)scala&gt; chinese(&quot;Andy&quot;)=20scala&gt; chineseres26: scala.collection.mutable.Map[String,Int] = Map(Tom -&gt; 80, Andy -&gt; 20)</code></pre><p>3）映射的迭代<br>for foreach</p><pre><code>scala&gt; chineseres27: scala.collection.mutable.Map[String,Int] = Map(Tom -&gt; 80, Andy -&gt; 20)scala&gt; for(s&lt;-chinese) println(s)(Tom,80)(Andy,20)scala&gt; chinese.foreach(println)(Tom,80)(Andy,20)</code></pre><p>foreach  高阶函数</p><h4 id="10、元组-Tuple"><a href="#10、元组-Tuple" class="headerlink" title="10、元组 : Tuple"></a>10、元组 : Tuple</h4><p>scala 中的tuple ： 是不同类型值的集合</p><pre><code>scala&gt; val t1 = Tuple(&quot;Tom&quot;,&quot;Lily&quot;,1)&lt;console&gt;:14: error: not found: value Tuple       val t1 = Tuple(&quot;Tom&quot;,&quot;Lily&quot;,1)                ^</code></pre><p>—–Tuple需要指明不同类型值的个数</p><pre><code>scala&gt; val t1 = Tuple3(&quot;Tom&quot;,&quot;Lily&quot;,1)t1: (String, String, Int) = (Tom,Lily,1)</code></pre><p>Tuple3 代表 Tuple中有三个元素</p><pre><code>scala&gt; val t1 = Tuple2(&quot;Lily&quot;,1)t1: (String, Int) = (Lily,1)scala&gt; val t2 = (1,2,4,&quot;Hello&quot;)t2: (Int, Int, Int, String) = (1,2,4,Hello)</code></pre><p><strong>tuple操作</strong></p><p>访问tuple中的元素</p><pre><code>scala&gt; val t1 = Tuple3(&quot;Tom&quot;,&quot;Lily&quot;,1)t1: (String, String, Int) = (Tom,Lily,1)scala&gt; t1._1   _3         copy     hashCode   productArity     productIterator   toString   zipped_2   canEqual   equals   invert     productElement   productPrefix     xscala&gt; t1._1res30: String = Tomscala&gt; t1._3res31: Int = 1</code></pre><p>如何遍历Tuple中的元素<br>    注意：Tuple并没有提供一个foreach函数，我们使用productIterator</p><p>遍历分为两步：<br>1、使用 productIterator 生成一个迭代器<br>2、遍历</p><pre><code>scala&gt; t1.productIterator.!=                copyToBuffer   forall               min                reduceRightOption   toIterable##                corresponds    foreach              minBy              sameElements        toIterator+                 count          formatted            mkString           scanLeft            toList++                drop           getClass             ne                 scanRight           toMap-&gt;                dropWhile      grouped              next               seq                 toSeq/:                duplicate      hasDefiniteSize      nonEmpty           size                toSet:\                ensuring       hasNext              notify             slice               toStream==                eq             hashCode             notifyAll          sliding             toStringGroupedIterator   equals         indexOf              padTo              span                toTraversableaddString         exists         indexWhere           partition          sum                 toVectoraggregate         filter         isEmpty              patch              synchronized        waitasInstanceOf      filterNot      isInstanceOf         product            take                withFilterbuffered          find           isTraversableAgain   reduce             takeWhile           zipcollect           flatMap        length               reduceLeft         to                  zipAllcollectFirst      fold           map                  reduceLeftOption   toArray             zipWithIndexcontains          foldLeft       max                  reduceOption       toBuffer            →copyToArray       foldRight      maxBy                reduceRight        toIndexedSeqscala&gt; t1.productIterator.foreach(println)TomLily1</code></pre><h4 id="11、scala中的文件操作"><a href="#11、scala中的文件操作" class="headerlink" title="11、scala中的文件操作"></a>11、scala中的文件操作</h4><p>类似于java的IO<br>举例：<br>1、读取文件<br>2、读取二进制文件<br>3、从url中获取信息<br>4、写入文件<br>5、Scala中调用Java的类库</p><p><strong>代码练习</strong></p><pre><code>//读取文件中的行var source = fromFile(&quot;E:\\server.xml&quot;)/**  * 1、将整个文件作为字符串输出  *  * 2、将文件的每一行读入输出  */println(&quot;--------mkString------------------&quot;)//println(source.mkString)println(&quot;----------lines-------------------&quot;)//var lines = source.getLines()//lines.foreach(println)println(&quot;-----------读取字符----------------&quot;)//for (c &lt;- source) println(c)println(&quot;-----------读取字URL----------------&quot;)var source2 = fromURL(&quot;https://hsiehchou.com&quot;,&quot;UTF-8&quot;)println(source2.mkString)/**  * 注意:scala中并不支持直接读取二进制文件  *  * 通过调用java的InputStream来实现  */println(&quot;-----------读取二进制文件----------------&quot;)//var file = new File(&quot;E:\\hsiehchou.war&quot;)//构造一个inputstream//var in = new FileInputStream(file)//构造一个buffer// var buffer = new Array[Byte](file.length().toInt)//读取//in.read(buffer)//println(buffer.length)//关闭//in.close()/**  * 写文件  */println(&quot;---------Write File----------------&quot;)var out = new PrintWriter(&quot;E:\\insert.txt&quot;)for(i &lt;- 0 until 10)  out.println(i)out.close()</code></pre><h3 id="二、Scala面向对象"><a href="#二、Scala面向对象" class="headerlink" title="二、Scala面向对象"></a>二、Scala面向对象</h3><p>Scala是一个多范式的编程语言（支持多种方式的编程）<br>类似于Java 有区别</p><h4 id="1、面向对象的概念"><a href="#1、面向对象的概念" class="headerlink" title="1、面向对象的概念"></a>1、面向对象的概念</h4><p>1、封装 ： 把属性和操作属性的方法，写在了一起。class<br>2、继承<br>3、多态</p><p>Java中面向对象的概念，也是用与Scala</p><h4 id="2、定义类：class"><a href="#2、定义类：class" class="headerlink" title="2、定义类：class"></a>2、定义类：class</h4><p>举例：创建一个学生类</p><pre><code>package day3/**  * 学生  */class Student1 {  //定义学生的属性  private var stuId:Int =0  private var stuName:String = &quot;Time&quot;  private var age:Int = 20  //定义方法（函数）get set  def getStuName():String = stuName  def setStuName(newName:String) = this.stuName = newName  def getStuAge():Int = age  def setStuAge(age:Int) = this.age = age}/**  * 注意object 和 class名字可以不一样  *  * 如果一样的话，这个object就叫作class的伴生对象  */object Student1{  def main(args: Array[String]): Unit = {    //测试    //创建一个学生对象    var s1 = new Student1    //访问他的属性并输出    println(s1.getStuName()+&quot;\t&quot;+s1.getStuAge())    //访问set方法    s1.setStuName(&quot;Hsieh&quot;)    s1.setStuAge(23)    println(s1.getStuName()+&quot;\t&quot;+s1.getStuAge())    //直接访问私有属性    println(&quot;-------访问私有属性----------&quot;)    println(s1.stuId+&quot;\t&quot;+s1.stuName+&quot;\t&quot;+s1.age)    /**      * 为什么我们可以访问私有成员      *      * s1.stuId      *      * 属性的set get 方法      * 1、当一个属性是private属性的时候，scala会自动为其生成set get方法      *      * s1.stuId   .stuId调用了get方法  get 方法的名字就叫stuId      *      * 2、如果只希望生成get方法而不生成set方法，可以定义成常量      *      * 3、如果希望属性不能被外部访问，使用private[this]关键字      */  }}</code></pre><h4 id="3、内部类（嵌套类）"><a href="#3、内部类（嵌套类）" class="headerlink" title="3、内部类（嵌套类）"></a>3、内部类（嵌套类）</h4><p>在一个类的内部，定义了另外一个类</p><pre><code>package day3import scala.collection.mutable.ArrayBuffer/**  * 需求：定义一个学生类，同时要保存学生的成绩信息  */class Student2 {  //定义学生的属性  private var stuName : String = &quot;Time&quot;  private var stuAge : Int = 23  //定义一个数组，来保存学生的课程成绩信息  private var courseList = new ArrayBuffer[Course]()  //定义一个函数，用于添加学生课程成绩  def addNewCourse(cname:String, grade:Int): Unit = {    //创建课程成绩信息    var c = new Course(cname,grade)    //添加到学生    courseList += c  }  //定义课程类  class Course(var courseName:String, var grade : Int){  }}object Student2{  def main(args: Array[String]): Unit = {    //创建学生对象    var s = new Student2    //给学生添加课程信息    s.addNewCourse(&quot;Chinese&quot;,78)    s.addNewCourse(&quot;English&quot;,80)    s.addNewCourse(&quot;Math&quot;,90)    println(s.stuName+&quot;\t&quot;+s.stuAge)    println(&quot;-----------课程信息------------&quot;)    for(c&lt;-s.courseList) println(c.courseName+&quot;\t&quot;+c.grade)  }}</code></pre><h4 id="4、类的构造器：两种"><a href="#4、类的构造器：两种" class="headerlink" title="4、类的构造器：两种"></a>4、类的构造器：两种</h4><p>1）主构造器 ： 和类的声明在一起，并且一个类只能有一个主构造器<br>class Course(var courseName:String,var grade : Int)</p><p>2）辅助构造器 ： 一个类可以有多个辅助构造器，通过this来实现</p><h4 id="5、Object对象"><a href="#5、Object对象" class="headerlink" title="5、Object对象"></a>5、Object对象</h4><p>相当于Java中的static<br>1）Object 对象中的内容都是静态的<br>2）如果和类名相同，则成为伴生对象<br>3）Scala中没有static关键字<br>4）举例<br>（1）使用Object来实现单例模式：一个类里面只有一个对象<br>在Java中，把类的构造器定义成private的，并且提供一个getInstance,返回对象</p><p>在Scala中，使用Object实现 </p><p><strong>例子</strong></p><pre><code>package day3/**  * 实现单例模式  */object CreditCard {  //定义一个变量来保存信用卡卡号  private [this] var creditCardNumber : Long = 0  //定义一个函数产生卡号  def generateNum : Long = {    creditCardNumber += 1    creditCardNumber  }  def main(args: Array[String]): Unit = {    println(CreditCard.generateNum)    println(CreditCard.generateNum)    println(CreditCard.generateNum)    println(CreditCard.generateNum)  }}</code></pre><p>（2）使用App对象：应用程序对象<br>好处：可以省略main方法 </p><p><strong>例子</strong></p><pre><code>package day3object HelloWorld extends App{//  def main(args: Array[String]): Unit = {//    println(&quot;Hello World!&quot;)//  }  println(&quot;Hello World!&quot;)  if(args.length&gt;0){    println(&quot;有参数&quot;)  }else{    println(&quot;没有参数&quot;)  }}</code></pre><h4 id="6、apply方法"><a href="#6、apply方法" class="headerlink" title="6、apply方法"></a>6、apply方法</h4><p>val t1 = Tuple3(“Tom”,”Lily”,1)<br>没有new关键字，但是也创建出来对象，用了apply方法</p><pre><code>package day3class Student4(var stuName:String)/**  * 定义Student4的apply方法  */object Student4 {  def apply(name:String) = {    println(&quot;调用apply方法&quot;)    new Student4(name)  }  def main(args: Array[String]): Unit = {    //通过主构造器来创建学生对象    var s1 = new Student4(&quot;Time&quot;)    println(s1.stuName)    //通过apply方法来创建学生对象，省略new 关键字    var s2 = Student4(&quot;Hsiehchou&quot;)    println(s2.stuName)  }}</code></pre><p>注意：apply方法必须写在伴生对象中</p><h4 id="7、继承"><a href="#7、继承" class="headerlink" title="7、继承"></a>7、继承</h4><p>1）extends 和java一样<br>object HelloWorld extends App</p><pre><code>package day3/**  * extends 继承  *  * 父类：Person 人  * 子类：Employee员工  *///定义父类class Person(val name:String,val age:Int){  //定义函数  def sayHello():String = &quot;Hello &quot;+name+&quot; and the age is &quot;+age}//定义子类class Employee(override val name:String,override val age:Int,salary:Int) extends Person(name,age){  //重写父类中的函数  override def sayHello(): String = &quot;子类中的sayHello&quot;}object Demo1 extends App {  //创建Person  var p1 = new Person(&quot;Tim&quot;,23)  println(p1.name+&quot;\t&quot;+p1.age)  println(p1.sayHello())  //创建一个子类对象  var p2:Person = new Employee(&quot;Nike&quot;,35,1000)  println(p2.sayHello())  //匿名子类  var p3:Person = new Person(&quot;Jike&quot;,32){    //在匿名子类中重写sayHello方法    override def sayHello(): String = &quot;匿名子类中的sayHello&quot;  }  println(p3.sayHello())}</code></pre><p>2）抽象类</p><pre><code>package day3/**  * 抽象类：只能用于继承的类，可以包含抽象方法  *///父类：交通工具类abstract class Vehicle {  //定义抽象方法，没有实现的方法  def checkType():String}//子类：自行车、汽车class Car extends Vehicle{  def checkType : String = &quot;I am a car&quot;}class Bike extends Vehicle{  def checkType : String = &quot;I am a bike&quot;}object Demo2{  def main(args: Array[String]): Unit = {    //多态    var v1 : Vehicle = new Car    println(v1.checkType())    var v2 : Vehicle = new Bike    println(v2.checkType())  }}</code></pre><p>3）抽象字段</p><pre><code>package day3/**  * 抽象字段 抽象属性  *  * 定义：没有初始值的字段  */abstract class Person1{  //定义抽象字段  val id:Int  val name:String}//如果不加abstract 报错abstract class Employee1 extends Person1{}//下面两种方式均不会报错class Employee2() extends Person1{  val id:Int = 1  val name:String = &quot;Time&quot;}class Employee3(val id:Int,val name:String) extends Person1{}object Demo3 {}</code></pre><h4 id="8、特质（trait）"><a href="#8、特质（trait）" class="headerlink" title="8、特质（trait）"></a>8、特质（trait）</h4><p>抽象类，支持多重继承<br>本质：scala 的一个抽象类<br>trait</p><pre><code>package day4/**  * trait特质  *  * 定义两个父类，就是两个trait  *  * 父类：人、动作  *  * 子类：学生  */trait Human{  //抽象字段  val id:Int  val name:String}//动作trait Action{  //定义一个抽象函数  def getActionName():String}class Student1(val id:Int,val name:String) extends Human with Action{  override def getActionName(): String = &quot;Action is running&quot;  /**    * 实现多重继承的方式 extends Human with Action    */}object Demo1 {  def main(args: Array[String]): Unit = {    //创建一个学生对象    var s1 = new Student1(1,&quot;Time&quot;)    println(s1.id+&quot;\t&quot;+s1.name)    println(s1.getActionName())  }}</code></pre><p>关键字：extends Human with Action<br>`</p><h4 id="9、包和包对象"><a href="#9、包和包对象" class="headerlink" title="9、包和包对象"></a>9、包和包对象</h4><p>package<br>package object</p><p><strong>Scala中包的定义和使用</strong></p><p><strong>包的定义</strong> </p><p>1）首先是Scala中的包可以像Java一样使用，例如：</p><pre><code>package com.my.ioclass XXX</code></pre><p>2）可以像C#的namespace一样使用package语句，例如：</p><pre><code>package com.my.io{    class XXX}</code></pre><p>3）package也是可以嵌套的，例如：</p><pre><code>package com.my.io{    class XXX    package test{        class T    }}</code></pre><p><strong>包的引入</strong><br>Scala中依然使用import作为引用包的关键字，例如<br>import com.my.io.XXX //可以不写XXX的全路径<br>import com.my.io._ //引用import com.my.io下的所有类型<br>import com.my.io.XXX._ //引用import com.my.io.XXX的所有成员</p><p>而且<strong>Scala中的import可以写在任意地方</strong></p><pre><code>def method(fruit:Fruit){    import fruit._    println(name)}</code></pre><p><strong>包对象</strong><br>包可以包含类、对象和特质，但不能包含函数或者变量的定义。很不幸，这是Java虚拟机的局限</p><p>把工具函数或者常量添加到包而不是某个Utils对象，这是更加合理的做法。Scala中，包对象的出现正是为了解决这个局限</p><p>Scala中的包对象：常量，变量，方法，类，对象，trait（特质）</p><pre><code>package class4/**  * Scala中的包对象：常量、变量、方法、类、对象、trait（特质）  *///定义一个包对象package object MyPackageObject{    //常量    val x:Int = 0    //变量    var y:String = &quot;Hello World&quot;    //方法    def sayHelloWorld():String = &quot;Hello World&quot;    //类    class MyTestClass{    }    //对象object    object MyTestObject{    }    //trait（特质）    trait MyTestTrait{    }}class Demo3{    //测试    def method1() = {        //导入需要的包对象        import class4.MyPackageObject._        //定义MyTestClass的一个对象        var a = new MyTestClass    }   }</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ElasticSearch（二）</title>
      <link href="/2019/03/20/elasticsearch-er/"/>
      <url>/2019/03/20/elasticsearch-er/</url>
      
        <content type="html"><![CDATA[<h3 id="一、Java-API操作"><a href="#一、Java-API操作" class="headerlink" title="一、Java API操作"></a>一、Java API操作</h3><p>Elasticsearch的Java客户端非常强大；它可以建立一个嵌入式实例并在必要时运行管理任务</p><p>运行一个Java应用程序和Elasticsearch时，有两种操作模式可供使用。该应用程序可在Elasticsearch集群中扮演更加主动或更加被动的角色。在更加主动的情况下（称为Node Client），应用程序实例将从集群接收请求，确定哪个节点应处理该请求，就像正常节点所做的一样。（应用程序甚至可以托管索引和处理请求。）另一种模式称为Transport Client，它将所有请求都转发到另一个Elasticsearch节点，由后者来确定最终目标</p><h4 id="1-API基本操作"><a href="#1-API基本操作" class="headerlink" title="1. API基本操作"></a>1. API基本操作</h4><p><strong>1.1 操作环境准备</strong><br>1）创建maven工程<br>2）添加pom文件</p><pre><code>&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;junit&lt;/groupId&gt;        &lt;artifactId&gt;junit&lt;/artifactId&gt;        &lt;version&gt;4.10&lt;/version&gt;        &lt;scope&gt;test&lt;/scope&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;        &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;        &lt;version&gt;6.1.1&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;        &lt;artifactId&gt;transport&lt;/artifactId&gt;        &lt;version&gt;6.1.1&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;        &lt;artifactId&gt;log4j-core&lt;/artifactId&gt;        &lt;version&gt;2.9.0&lt;/version&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;</code></pre><p>3）等待依赖的jar包下载完成<br>当直接在ElasticSearch 建立文档对象时，如果索引不存在的，默认会自动创建，映射采用默认方式</p><p><strong>1.2 获取Transport Client</strong><br>（1）ElasticSearch服务默认端口9300<br>（2）Web管理平台端口9200</p><pre><code>private TransportClient client;@SuppressWarnings(&quot;unchecked&quot;)@Beforepublic void getClient() throws Exception {    // 1 设置连接的集群名称    Settings settings = Settings.builder().put(&quot;cluster.name&quot;, &quot;my-application&quot;).build();    // 2 连接集群    client = new PreBuiltTransportClient(settings);    client.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;hsiehchou121&quot;), 9300));    // 3 打印集群名称    System.out.println(client.toString());}</code></pre><p>（3）显示log4j2报错，在resource目录下创建一个文件命名为log4j2.xml并添加如下内容</p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;Configuration status=&quot;warn&quot;&gt;    &lt;Appenders&gt;        &lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt;            &lt;PatternLayout pattern=&quot;%m%n&quot;/&gt;        &lt;/Console&gt;    &lt;/Appenders&gt;    &lt;Loggers&gt;        &lt;Root level=&quot;INFO&quot;&gt;            &lt;AppenderRef ref=&quot;Console&quot;/&gt;        &lt;/Root&gt;    &lt;/Loggers&gt;&lt;/Configuration&gt;</code></pre><p><strong>1.3 创建索引</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void createIndex_blog(){    // 1 创建索引    client.admin().indices().prepareCreate(&quot;blog2&quot;).get();    // 2 关闭连接    client.close();}</code></pre><p><strong>1.4 删除索引</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void deleteIndex(){    // 1 删除索引    client.admin().indices().prepareDelete(&quot;blog2&quot;).get();    // 2 关闭连接    client.close();}</code></pre><p><strong>1.5 新建文档（源数据json串）</strong><br>当直接在ElasticSearch建立文档对象时，如果索引不存在的，默认会自动创建，映射采用默认方式<br><strong>源代码</strong></p><pre><code>@Testpublic void createIndexByJson() throws UnknownHostException {    // 1 文档数据准备    String json = &quot;{&quot; + &quot;\&quot;id\&quot;:\&quot;1\&quot;,&quot; + &quot;\&quot;title\&quot;:\&quot;基于Lucene的搜索服务器\&quot;,&quot;            + &quot;\&quot;content\&quot;:\&quot;它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口\&quot;&quot; + &quot;}&quot;;    // 2 创建文档    IndexResponse indexResponse = client.prepareIndex(&quot;blog&quot;, &quot;article&quot;, &quot;1&quot;).setSource(json).execute().actionGet();    // 3 打印返回的结果    System.out.println(&quot;index:&quot; + indexResponse.getIndex());    System.out.println(&quot;type:&quot; + indexResponse.getType());    System.out.println(&quot;id:&quot; + indexResponse.getId());    System.out.println(&quot;version:&quot; + indexResponse.getVersion());    System.out.println(&quot;result:&quot; + indexResponse.getResult());    // 4 关闭连接    client.close();}</code></pre><p><strong>1.6 新建文档（源数据map方式添加json）</strong><br><strong>源代码</strong> </p><pre><code>@Test public void createIndexByMap() {    // 1 文档数据准备    Map&lt;String, Object&gt; json = new HashMap&lt;String, Object&gt;();    json.put(&quot;id&quot;, &quot;2&quot;);    json.put(&quot;title&quot;, &quot;基于Lucene的搜索服务器&quot;);    json.put(&quot;content&quot;, &quot;它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口&quot;);    // 2 创建文档    IndexResponse indexResponse = client.prepareIndex(&quot;blog&quot;, &quot;article&quot;, &quot;2&quot;).setSource(json).execute().actionGet();    // 3 打印返回的结果    System.out.println(&quot;index:&quot; + indexResponse.getIndex());    System.out.println(&quot;type:&quot; + indexResponse.getType());    System.out.println(&quot;id:&quot; + indexResponse.getId());    System.out.println(&quot;version:&quot; + indexResponse.getVersion());    System.out.println(&quot;result:&quot; + indexResponse.getResult());    // 4 关闭连接    client.close();}</code></pre><p><strong>1.7 新建文档（源数据es构建器添加json）</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void createIndex() throws Exception {    // 1 通过es自带的帮助类，构建json数据    XContentBuilder builder = XContentFactory.jsonBuilder().startObject().field(&quot;id&quot;, 3).field(&quot;title&quot;, &quot;基于Lucene的搜索服务器&quot;).field(&quot;content&quot;, &quot;它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。&quot;)            .endObject();    // 2 创建文档    IndexResponse indexResponse = client.prepareIndex(&quot;blog&quot;, &quot;article&quot;, &quot;3&quot;).setSource(builder).get();    // 3 打印返回的结果    System.out.println(&quot;index:&quot; + indexResponse.getIndex());    System.out.println(&quot;type:&quot; + indexResponse.getType());    System.out.println(&quot;id:&quot; + indexResponse.getId());    System.out.println(&quot;version:&quot; + indexResponse.getVersion());    System.out.println(&quot;result:&quot; + indexResponse.getResult());    // 4 关闭连接    client.close();}</code></pre><p><strong>1.8 搜索文档数据（单个索引）</strong><br><strong>源代码</strong> </p><pre><code>@Test public void getData() throws Exception {    // 1 查询文档    GetResponse response = client.prepareGet(&quot;blog&quot;, &quot;article&quot;, &quot;1&quot;).get();    // 2 打印搜索的结果    System.out.println(response.getSourceAsString());    // 3 关闭连接    client.close();}</code></pre><p><strong>1.9 搜索文档数据（多个索引）</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void getMultiData() {    // 1 查询多个文档    MultiGetResponse response = client.prepareMultiGet().add(&quot;blog&quot;, &quot;article&quot;, &quot;1&quot;).add(&quot;blog&quot;, &quot;article&quot;, &quot;2&quot;, &quot;3&quot;).add(&quot;blog&quot;, &quot;article&quot;, &quot;2&quot;).get();    // 2 遍历返回的结果    for(MultiGetItemResponse itemResponse:response){        GetResponse getResponse = itemResponse.getResponse();        // 如果获取到查询结果        if (getResponse.isExists()) {            String sourceAsString = getResponse.getSourceAsString();            System.out.println(sourceAsString);        }    }    // 3 关闭资源    client.close();}</code></pre><p><strong>1.10 更新文档数据（update）</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void updateData() throws Throwable {    // 1 创建更新数据的请求对象    UpdateRequest updateRequest = new UpdateRequest();    updateRequest.index(&quot;blog&quot;);    updateRequest.type(&quot;article&quot;);    updateRequest.id(&quot;3&quot;);    updateRequest.doc(XContentFactory.jsonBuilder().startObject()            // 对没有的字段添加, 对已有的字段替换            .field(&quot;title&quot;, &quot;基于Lucene的搜索服务器&quot;)            .field(&quot;content&quot;,&quot;它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。大数据前景无限&quot;)            .field(&quot;createDate&quot;, &quot;2017-8-22&quot;).endObject());    // 2 获取更新后的值    UpdateResponse indexResponse = client.update(updateRequest).get();    // 3 打印返回的结果    System.out.println(&quot;index:&quot; + indexResponse.getIndex());    System.out.println(&quot;type:&quot; + indexResponse.getType());    System.out.println(&quot;id:&quot; + indexResponse.getId());    System.out.println(&quot;version:&quot; + indexResponse.getVersion());    System.out.println(&quot;create:&quot; + indexResponse.getResult());    // 4 关闭连接    client.close();}</code></pre><p><strong>1.11 更新文档数据（upsert）</strong><br>设置查询条件, 查找不到则添加IndexRequest内容，查找到则按照UpdateRequest更新</p><pre><code>@Testpublic void testUpsert() throws Exception {    // 设置查询条件, 查找不到则添加    IndexRequest indexRequest = new IndexRequest(&quot;blog&quot;, &quot;article&quot;, &quot;5&quot;)            .source(XContentFactory.jsonBuilder().startObject().field(&quot;title&quot;, &quot;搜索服务器&quot;).field(&quot;content&quot;,&quot;它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。&quot;).endObject());    // 设置更新, 查找到更新下面的设置    UpdateRequest upsert = new UpdateRequest(&quot;blog&quot;, &quot;article&quot;, &quot;5&quot;)            .doc(XContentFactory.jsonBuilder().startObject().field(&quot;user&quot;, &quot;李四&quot;).endObject()).upsert(indexRequest);    client.update(upsert).get();    client.close();}</code></pre><p><strong>1.12 删除文档数据（prepareDelete）</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void deleteData() {    // 1 删除文档数据    DeleteResponse indexResponse = client.prepareDelete(&quot;blog&quot;, &quot;article&quot;, &quot;5&quot;).get();    // 2 打印返回的结果    System.out.println(&quot;index:&quot; + indexResponse.getIndex());    System.out.println(&quot;type:&quot; + indexResponse.getType());    System.out.println(&quot;id:&quot; + indexResponse.getId());    System.out.println(&quot;version:&quot; + indexResponse.getVersion());    System.out.println(&quot;found:&quot; + indexResponse.getResult());    // 3 关闭连接    client.close();}</code></pre><h4 id="2-条件查询QueryBuilder"><a href="#2-条件查询QueryBuilder" class="headerlink" title="2. 条件查询QueryBuilder"></a>2. 条件查询QueryBuilder</h4><p><strong>2.1 查询所有（matchAllQuery）</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void matchAllQuery() {    // 1 执行查询    SearchResponse searchResponse = client.prepareSearch(&quot;blog&quot;).setTypes(&quot;article&quot;)            .setQuery(QueryBuilders.matchAllQuery()).get();    // 2 打印查询结果    SearchHits hits = searchResponse.getHits(); // 获取命中次数，查询结果有多少对象    System.out.println(&quot;查询结果有：&quot; + hits.getTotalHits() + &quot;条&quot;);    for (SearchHit hit : hits) {       System.out.println(hit.getSourceAsString());//打印出每条结果    }    // 3 关闭连接    client.close();}</code></pre><p><strong>2.2 对所有字段分词查询（queryStringQuery）</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void query() {    // 1 条件查询    SearchResponse searchResponse = client.prepareSearch(&quot;blog&quot;).setTypes(&quot;article&quot;)            .setQuery(QueryBuilders.queryStringQuery(&quot;全文&quot;)).get();    // 2 打印查询结果    SearchHits hits = searchResponse.getHits(); // 获取命中次数，查询结果有多少对象    System.out.println(&quot;查询结果有：&quot; + hits.getTotalHits() + &quot;条&quot;);    for (SearchHit hit : hits) {       System.out.println(hit.getSourceAsString());//打印出每条结果    }    // 3 关闭连接    client.close();}</code></pre><p><strong>2.3 通配符查询（wildcardQuery）</strong></p><p>：表示多个字符（0个或多个字符）<br>？：表示单个字符<br>源代码</p><pre><code>@Testpublic void wildcardQuery() {    // 1 通配符查询    SearchResponse searchResponse = client.prepareSearch(&quot;blog&quot;).setTypes(&quot;article&quot;)            .setQuery(QueryBuilders.wildcardQuery(&quot;content&quot;, &quot;*全*&quot;)).get();    // 2 打印查询结果    SearchHits hits = searchResponse.getHits(); // 获取命中次数，查询结果有多少对象    System.out.println(&quot;查询结果有：&quot; + hits.getTotalHits() + &quot;条&quot;);    for (SearchHit hit : hits) {       System.out.println(hit.getSourceAsString());//打印出每条结果    }    // 3 关闭连接    client.close();}</code></pre><p><strong>2.4 词条查询（TermQuery）</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void termQuery() {    // 1 第一field查询    SearchResponse searchResponse = client.prepareSearch(&quot;blog&quot;).setTypes(&quot;article&quot;)            .setQuery(QueryBuilders.termQuery(&quot;content&quot;, &quot;全文&quot;)).get();    // 2 打印查询结果    SearchHits hits = searchResponse.getHits(); // 获取命中次数，查询结果有多少对象    System.out.println(&quot;查询结果有：&quot; + hits.getTotalHits() + &quot;条&quot;);    for (SearchHit hit : hits) {       System.out.println(hit.getSourceAsString());//打印出每条结果    }    // 3 关闭连接    client.close();}</code></pre><p><strong>2.5 模糊查询（fuzzy）</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void fuzzy() {    // 1 模糊查询    SearchResponse searchResponse = client.prepareSearch(&quot;blog&quot;).setTypes(&quot;article&quot;)            .setQuery(QueryBuilders.fuzzyQuery(&quot;title&quot;, &quot;lucene&quot;)).get();    // 2 打印查询结果    SearchHits hits = searchResponse.getHits(); // 获取命中次数，查询结果有多少对象    System.out.println(&quot;查询结果有：&quot; + hits.getTotalHits() + &quot;条&quot;);    Iterator&lt;SearchHit&gt; iterator = hits.iterator();    while (iterator.hasNext()) {        SearchHit searchHit = iterator.next(); // 每个查询对象        System.out.println(searchHit.getSourceAsString()); // 获取字符串格式打印    }    // 3 关闭连接    client.close();}</code></pre><h4 id="3-映射相关操作"><a href="#3-映射相关操作" class="headerlink" title="3. 映射相关操作"></a>3. 映射相关操作</h4><p><strong>源代码</strong></p><pre><code>@Testpublic void createMapping() throws Exception {    // 1设置mapping    XContentBuilder builder = XContentFactory.jsonBuilder()            .startObject()                .startObject(&quot;article&quot;)                    .startObject(&quot;properties&quot;)                        .startObject(&quot;id1&quot;)                            .field(&quot;type&quot;, &quot;string&quot;)                            .field(&quot;store&quot;, &quot;yes&quot;)                        .endObject()                        .startObject(&quot;title2&quot;)                            .field(&quot;type&quot;, &quot;string&quot;)                            .field(&quot;store&quot;, &quot;no&quot;)                        .endObject()                        .startObject(&quot;content&quot;)                            .field(&quot;type&quot;, &quot;string&quot;)                            .field(&quot;store&quot;, &quot;yes&quot;)                        .endObject()                    .endObject()                .endObject()            .endObject();    // 2 添加mapping    PutMappingRequest mapping = Requests.putMappingRequest(&quot;blog4&quot;).type(&quot;article&quot;).source(builder);    client.admin().indices().putMapping(mapping).get();    // 3 关闭资源    client.close();}</code></pre><h3 id="二、IK分词器"><a href="#二、IK分词器" class="headerlink" title="二、IK分词器"></a>二、IK分词器</h3><p>针对词条查询（TermQuery）,查看默认中文分词器的效果:<br>curl -XGET ‘<a href="http://hsiehchou:9200/_analyze?pretty&amp;analyzer=standard’" target="_blank" rel="noopener">http://hsiehchou:9200/_analyze?pretty&amp;analyzer=standard’</a> -d ‘中华人民共和国’ </p><pre><code>{     “tokens” : [         {             “token” : “中”,             “start_offset” : 0,             “end_offset” : 1,             “type” : “”,             “position” : 0         },         {             “token” : “华”,             “start_offset” : 1,             “end_offset” : 2,             “type” : “”,             “position” : 1         },         {             “token” : “人”,             “start_offset” : 2,             “end_offset” : 3,             “type” : “”,             “position” : 2         },         {             “token” : “民”,             “start_offset” : 3,             “end_offset” : 4,             “type” : “”,             “position” : 3         },         {             “token” : “共”,             “start_offset” : 4,             “end_offset” : 5,             “type” : “”,             “position” : 4         },         {             “token” : “和”,             “start_offset” : 5,             “end_offset” : 6,             “type” : “”,             “position” : 5         },         {             “token” : “国”,             “start_offset” : 6,             “end_offset” : 7,             “type” : “”,             “position” : 6         }     ] }</code></pre><h4 id="1-IK分词器的安装"><a href="#1-IK分词器的安装" class="headerlink" title="1. IK分词器的安装"></a>1. IK分词器的安装</h4><p><strong>1.1 前期准备工作</strong><br>1）CentOS联网<br>配置CentOS能连接外网。Linux虚拟机ping <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a> 是畅通的</p><p>2）jar包准备<br>（1）elasticsearch-analysis-ik-master.zip<br>(下载地址:<a href="https://github.com/medcl/elasticsearch-analysis-ik" target="_blank" rel="noopener">https://github.com/medcl/elasticsearch-analysis-ik</a>)<br>（2）apache-maven-3.6.0-bin.tar.gz</p><p><strong>1.2 jar包安装</strong><br>1）Maven解压、配置 MAVEN_HOME和PATH。<br>tar -zxvf apache-maven-3.6.0-bin.tar.gz -C /opt/module/<br>sudo vi /etc/profile</p><p><code>#MAVEN_HOME</code><br>export MAVEN_HOME=/opt/module/apache-maven-3.6.0<br>export PATH=<code>$PATH:$MAVEN_HOME/bin</code><br>source /etc/profile<br>验证命令：mvn -version</p><p>2）Ik分词器解压、打包与配置<br><strong>ik分词器解压</strong><br>unzip elasticsearch-analysis-ik-master.zip -d ./<br>进入ik分词器所在目录</p><p>cd elasticsearch-analysis-ik-master<br>使用maven进行打包</p><p>mvn package -Pdist,native -DskipTests -Dtar<br>打包完成之后，会出现 target/releases/elasticsearch-analysis-ik-{version}.zip</p><p>pwd /opt/software/elasticsearch-analysis-ik-master/target/releases<br>对zip文件进行解压，并将解压完成之后的文件拷贝到es所在目录下的/plugins/</p><p>unzip elasticsearch-analysis-ik-6.0.0.zip<br>cp -r elasticsearch /opt/module/elasticsearch-5.6.1/plugins/</p><p>需要修改plugin-descriptor.properties文件，将其中的es版本号改为你所使用的版本号，即完成ik分词器的安装<br>vi plugin-descriptor.properties<br>修改为<br>elasticsearch.version=6.1.1<br>至此，安装完成，重启ES！</p><p>注意：需选择与es相同版本的ik分词器。<br>安装方法（2种）： </p><ol><li><p>./elasticsearch-plugin install <a href="https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.1.1/elasticsearch-analysis-ik-6.1.1.zip" target="_blank" rel="noopener">https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.1.1/elasticsearch-analysis-ik-6.1.1.zip</a></p></li><li><p>cp elasticsearch-analysis-ik-6.1.1.zip ./elasticsearch-6.1.1/plugins/<br>unzip elasticsearch-analysis-ik-6.1.1.zip -d ik-analyzer<br>elasticsearch-plugin install -f file:///usr/local/elasticsearch-analysis-ik-6.1.1.zip</p></li></ol><h4 id="2-IK分词器的使用"><a href="#2-IK分词器的使用" class="headerlink" title="2. IK分词器的使用"></a>2. IK分词器的使用</h4><p><strong>2.1 命令行查看结果</strong><br><strong>ik_smart模式</strong><br>curl -XGET ‘<a href="http://hsiehchou121:9200/_analyze?pretty&amp;analyzer=ik_smart’" target="_blank" rel="noopener">http://hsiehchou121:9200/_analyze?pretty&amp;analyzer=ik_smart’</a> -d ‘中华人民共和国’</p><p>curl -H “Content-Type:application/json” -XGET ‘<a href="http://192.168.116.121:9200/_analyze?pretty’" target="_blank" rel="noopener">http://192.168.116.121:9200/_analyze?pretty’</a> -d ‘{“analyzer”:”ik_smasysctl -prt”,”text”:”中华人民共和国”}’ </p><pre><code>{     “tokens” : [         {             “token” : “中华人民共和国”,             “start_offset” : 0,             “end_offset” : 7,             “type” : “CN_WORD”,             “position” : 0         }     ] }</code></pre><p><strong>ik_max_word模式</strong><br>curl -XGET ‘<a href="http://hadoop121:9200/_analyze?pretty&amp;analyzer=ik_max_word’" target="_blank" rel="noopener">http://hadoop121:9200/_analyze?pretty&amp;analyzer=ik_max_word’</a> -d ‘中华人民共和国’</p><p>curl -H “Content-Type:application/json” -XGET ‘<a href="http://192.168.116.124:9200/_analyze?pretty’" target="_blank" rel="noopener">http://192.168.116.124:9200/_analyze?pretty’</a> -d ‘{“analyzer”:”ik_max_word”,”text”:”中华人民共和国”}’</p><pre><code>{     “tokens” : [         {             “token” : “中华人民共和国”,             “start_offset” : 0,             “end_offset” : 7,             “type” : “CN_WORD”,             “position” : 0         },         {             “token” : “中华人民”,             “start_offset” : 0,             “end_offset” : 4,             “type” : “CN_WORD”,             “position” : 1         },         {             “token” : “中华”,             “start_offset” : 0,             “end_offset” : 2,             “type” : “CN_WORD”,             “position” : 2         },         {             “token” : “华人”,             “start_offset” : 1,             “end_offset” : 3,             “type” : “CN_WORD”,             “position” : 3         },         {             “token” : “人民共和国”,             “start_offset” : 2,             “end_offset” : 7,             “type” : “CN_WORD”,             “position” : 4         },         {             “token” : “人民”,             “start_offset” : 2,             “end_offset” : 4,             “type” : “CN_WORD”,             “position” : 5         },         {             “token” : “共和国”,             “start_offset” : 4,             “end_offset” : 7,             “type” : “CN_WORD”,             “position” : 6         },         {             “token” : “共和”,             “start_offset” : 4,             “end_offset” : 6,             “type” : “CN_WORD”,             “position” : 7         },         {             “token” : “国”,             “start_offset” : 6,             “end_offset” : 7,             “type” : “CN_CHAR”,             “position” : 8         }     ] }</code></pre><p><strong>2.2 JavaAPI操作</strong><br>1）创建索引<br>//创建索引(数据库)</p><pre><code>@Testpublic void createIndex() {    //创建索引    client.admin().indices().prepareCreate(&quot;blog4&quot;).get();    //关闭资源    client.close();}</code></pre><p>2）创建mapping<br>//创建使用ik分词器的mapping</p><pre><code>@Testpublic void createMapping() throws Exception {    // 1设置mapping    XContentBuilder builder = XContentFactory.jsonBuilder()            .startObject()                .startObject(&quot;article&quot;)                    .startObject(&quot;properties&quot;)                    .startObject(&quot;id1&quot;)                        .field(&quot;type&quot;, &quot;string&quot;)                        .field(&quot;store&quot;, &quot;yes&quot;)                        .field(&quot;analyzer&quot;,&quot;ik_smart&quot;)                    .endObject()                    .startObject(&quot;title2&quot;)                        .field(&quot;type&quot;, &quot;string&quot;)                        .field(&quot;store&quot;, &quot;no&quot;)                        .field(&quot;analyzer&quot;,&quot;ik_smart&quot;)                    .endObject()                    .startObject(&quot;content&quot;)                        .field(&quot;type&quot;, &quot;string&quot;)                        .field(&quot;store&quot;, &quot;yes&quot;)                        .field(&quot;analyzer&quot;,&quot;ik_smart&quot;)                    .endObject()                    .endObject()                .endObject()            .endObject();    // 2 添加mapping    PutMappingRequest mapping = Requests.putMappingRequest(&quot;blog4&quot;).type(&quot;article&quot;).source(builder);    client.admin().indices().putMapping(mapping).get();    // 3 关闭资源    client.close();}</code></pre><p>3）插入数据<br>//创建文档,以map形式</p><pre><code>@Testpublic void createDocumentByMap() {    HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;();    map.put(&quot;id1&quot;, &quot;2&quot;);    map.put(&quot;title2&quot;, &quot;Lucene&quot;);    map.put(&quot;content&quot;, &quot;它提供了一个分布式的web接口&quot;);    IndexResponse response = client.prepareIndex(&quot;blog4&quot;, &quot;article&quot;, &quot;3&quot;).setSource(map).execute().actionGet();    //打印返回的结果    System.out.println(&quot;结果:&quot; + response.getResult());    System.out.println(&quot;id:&quot; + response.getId());    System.out.println(&quot;index:&quot; + response.getIndex());    System.out.println(&quot;type:&quot; + response.getType());    System.out.println(&quot;版本:&quot; + response.getVersion());    //关闭资源    client.close();}</code></pre><p>4） 词条查询<br>//词条查询</p><pre><code>@Testpublic void queryTerm() {    SearchResponse response = client.prepareSearch(&quot;blog4&quot;).setTypes(&quot;article&quot;).setQuery(QueryBuilders.termQuery(&quot;content&quot;,&quot;提供&quot;)).get();    //获取查询命中结果    SearchHits hits = response.getHits();    System.out.println(&quot;结果条数:&quot; + hits.getTotalHits());    for (SearchHit hit : hits) {        System.out.println(hit.getSourceAsString());    }}</code></pre><p><strong>Store 的解释</strong><br>官方文档说 store 默认是 no ，想当然的理解为也就是说这个 field 是不会 store 的，但是查询的时候也能查询出来</p><p>经过查找资料了解到原来 store 的意思是，是否在 _source 之外在独立存储一份。这里要说一下 _source 这是源文档，当索引数据的时候， elasticsearch 会保存一份源文档到 _source 。如果文档的某一字段设置了 store 为 yes (默认为 no)，这时候会在 _source 存储之外再为这个字段独立进行存储，这么做的目的主要是针对内容比较多的字段</p><p>如果放到 _source 返回的话，因为_source 是把所有字段保存为一份文档，命中后读取只需要一次 IO，包含内容特别多的字段会很占带宽影响性能。通常我们也不需要完整的内容返回(可能只关心摘要)，这时候就没必要放到 _source 里一起返回了(当然也可以在查询时指定返回字段)</p><h3 id="三、Logstash"><a href="#三、Logstash" class="headerlink" title="三、Logstash"></a>三、Logstash</h3><h4 id="1-Logstash简介"><a href="#1-Logstash简介" class="headerlink" title="1. Logstash简介"></a>1. Logstash简介</h4><p>Logstash is a tool for managing events and logs. You can use it to collect logs, parse them, and store them for later use (like, for searching).</p><p>logstash是一个数据分析软件，主要目的是分析log日志。整一套软件可以当作一个MVC模型，logstash是controller层，Elasticsearch是一个model层，kibana是view层</p><p>首先将数据传给logstash，它将数据进行过滤和格式化（转成JSON格式），然后传给Elasticsearch进行存储、建搜索的索引，kibana提供前端的页面再进行搜索和图表可视化，它是调用Elasticsearch的接口返回的数据进行可视化。logstash和Elasticsearch是用Java写的，kibana使用node.js框架</p><p>这个软件官网有很详细的使用说明，<a href="https://www.elastic.co/，除了docs之外，还有视频教程。这篇博客集合了docs和视频里面一些比较重要的设置和使用" target="_blank" rel="noopener">https://www.elastic.co/，除了docs之外，还有视频教程。这篇博客集合了docs和视频里面一些比较重要的设置和使用</a></p><h4 id="2-Logstash-安装"><a href="#2-Logstash-安装" class="headerlink" title="2. Logstash 安装"></a>2. Logstash 安装</h4><p>直接下载官方发布的二进制包的，可以访问 <a href="https://www.elastic.co/downloads/logstash" target="_blank" rel="noopener">https://www.elastic.co/downloads/logstash</a> 页面找对应操作系统和版本，点击下载即可</p><p>在终端中，像下面这样运行命令来启动 Logstash 进程：<br>输入（读取数据）：file、es。 输出：file、es、kafka</p><p>bin/logstash -e ‘input{stdin{}}output{stdout{codec=&gt;rubydebug}}’<br>-f文件 -e命令 标准输入、输出（命令行）</p><p>注意：如果出现如下报错，请调高虚拟机内存容量<br>Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000c5330000, 986513408, 0) failed; error=’Cannot allocate memory’ (errno=12)</p><p>然后你会发现终端在等待你的输入。没问题，敲入 Hello World，回车，</p><pre><code>{     “@version” =&gt; “1”,     “host” =&gt; “*“,     “message” =&gt; “hello world”,     “@timestamp” =&gt; 2019-03-18T02:51:18.578Z }</code></pre><p>每位系统管理员都肯定写过很多类似这样的命令<br>cat randdata | awk ‘{print $2}’ | sort | uniq -c | tee sortdata</p><p>Logstash 就像管道符一样！<br>你输入(就像命令行的 cat )数据，然后处理过滤(就像 awk 或者 uniq 之类)数据，最后输出(就像 tee )到其他地方</p><h4 id="3-Logstash-配置"><a href="#3-Logstash-配置" class="headerlink" title="3. Logstash 配置"></a>3. Logstash 配置</h4><p><strong>3.1 input配置</strong><br>读取文件(File)</p><pre><code>input {    file {        path =&gt; [&quot;/var/log/*.log&quot;, &quot;/var/log/message&quot;]        type =&gt; &quot;system&quot;        start_position =&gt; &quot;beginning&quot;    }}output{stdout{codec=&gt;rubydebug}}</code></pre><p>有一些比较有用的配置项，可以用来指定 FileWatch 库的行为</p><p><strong>discover_interval</strong><br>logstash 每隔多久去检查一次被监听的 path 下是否有新文件。默认值是 15 秒</p><p><strong>exclude</strong><br>不想被监听的文件可以排除出去，这里跟 path 一样支持 glob 展开</p><p><strong>close_older</strong><br>一个已经监听中的文件，如果超过这个值的时间内没有更新内容，就关闭监听它的文件句柄。默认是 3600 秒，即一小时</p><p><strong>ignore_older</strong><br>在每次检查文件列表的时候，如果一个文件的最后修改时间超过这个值，就忽略这个文件。默认是 86400 秒，即一天</p><p><strong>sincedb_path</strong><br>如果你不想用默认的 $HOME/.sincedb(Windows 平台上在 C:\Windows\System32\config\systemprofile.sincedb)，可以通过这个配置定义 sincedb 文件到其他位置</p><p><strong>sincedb_write_interval</strong><br>logstash 每隔多久写一次 sincedb 文件，默认是 15 秒</p><p><strong>stat_interval</strong><br>logstash 每隔多久检查一次被监听文件状态（是否有更新），默认是 1 秒</p><p><strong>start_position</strong><br>logstash 从什么位置开始读取文件数据，默认是结束位置，也就是说 logstash 进程会以类似 tail -F 的形式运行。如果你是要导入原有数据，把这个设定改成 “beginning”，logstash 进程就从头开始读取，类似 less +F 的形式运行</p><p><strong>启动命令</strong>：../bin/logstash -f ./input_file.conf<br><strong>测试命令</strong>：echo ‘hehe’ &gt;&gt; test.log<br>echo ‘hehe2’ &gt;&gt; message</p><p><strong>标准输入(Stdin)</strong><br>我们已经见过好几个示例使用 stdin 了。这也应该是 logstash 里最简单和基础的插件了</p><pre><code>input {    stdin {        add_field =&gt; {&quot;key&quot; =&gt; &quot;value&quot;}        codec =&gt; &quot;plain&quot;        tags =&gt; [&quot;add&quot;]        type =&gt; &quot;std&quot;    }}output{stdout{codec=&gt;rubydebug}}</code></pre><p>用上面的新 stdin 设置重新运行一次最开始的 hello world 示例。我建议大家把整段配置都写入一个文本文件，然后运行命令：../bin/logstash -f ./input_stdin.conf。输入 “hello world” 并回车后，你会在终端看到如下输出</p><pre><code>{       &quot;message&quot; =&gt; &quot;hello world&quot;,      &quot;@version&quot; =&gt; &quot;1&quot;,    &quot;@timestamp&quot; =&gt; &quot;2014-08-08T06:48:47.789Z&quot;,          &quot;type&quot; =&gt; &quot;std&quot;,          &quot;tags&quot; =&gt; [        [0] &quot;add&quot;    ],           &quot;key&quot; =&gt; &quot;value&quot;,          &quot;host&quot; =&gt; &quot;raochenlindeMacBook-Air.local&quot;}</code></pre><p><strong>解释</strong><br>type 和 tags 是 logstash 事件中两个特殊的字段。通常来说我们会在输入区段中通过 type 来标记事件类型。而 tags 则是在数据处理过程中，由具体的插件来添加或者删除的</p><p>最常见的用法是像下面这样</p><pre><code>input {    stdin {        type =&gt; &quot;web&quot;    }}filter {    if [type] == &quot;web&quot; {        grok {            match =&gt; [&quot;message&quot;, %{COMBINEDAPACHELOG}]        }    }}output {    if &quot;_grokparsefailure&quot; in [tags] {        nagios_nsca {            nagios_status =&gt; &quot;1&quot;        }    } else {        elasticsearch {        }    }}</code></pre><p><strong>3.2 codec配置</strong><br>Codec 是 logstash 从 1.3.0 版开始新引入的概念(Codec 来自 Coder/decoder 两个单词的首字母缩写)</p><p>在此之前，logstash 只支持纯文本形式输入，然后以过滤器处理它。但现在，我们可以在输入期处理不同类型的数据，这全是因为有了 codec 设置</p><p>所以，这里需要纠正之前的一个概念。Logstash 不只是一个input | filter | output 的数据流，而是一个 input | decode | filter | encode | output 的数据流！codec 就是用来 decode、encode 事件的</p><p>codec 的引入，使得 logstash 可以更好更方便的与其他有自定义数据格式的运维产品共存，比如 graphite、fluent、netflow、collectd，以及使用 msgpack、json、edn 等通用数据格式的其他产品等</p><p>事实上，我们在第一个 “hello world” 用例中就已经用过 codec 了 —— rubydebug 就是一种 codec！虽然它一般只会用在 stdout 插件中，作为配置测试或者调试的工具</p><p><strong>采用 JSON 编码</strong><br>在早期的版本中，有一种降低 logstash 过滤器的 CPU 负载消耗的做法盛行于社区(在当时的 cookbook 上有专门的一节介绍)：直接输入预定义好的 JSON 数据，这样就可以省略掉 filter/grok 配置！</p><p>这个建议依然有效，不过在当前版本中需要稍微做一点配置变动 —— 因为现在有专门的 codec 设置</p><p><strong>配置示例</strong></p><pre><code>input {    stdin {        add_field =&gt; {&quot;key&quot; =&gt; &quot;value&quot;}        codec =&gt; &quot;json&quot;        type =&gt; &quot;std&quot;    }}output{stdout{codec=&gt;rubydebug}}</code></pre><p>输入：<br>{“simCar”:18074045598,”validityPeriod”:”1996-12-06”,”unitPrice”:9,”quantity”:19,”amount”:35,”imei”:887540376467915,”user”:”test”}</p><p>运行结果：<br>{<br>“imei” =&gt; 887540376467915,<br>“unitPrice” =&gt; 9,<br>“user” =&gt; “test”,<br>“@timestamp” =&gt; 2019-03-19T05:01:53.451Z,<br>“simCar” =&gt; 18074045598,<br>“host” =&gt; “zzc-203”,<br>“amount” =&gt; 35,<br>“@version” =&gt; “1”,<br>“key” =&gt; “value”,<br>“type” =&gt; “std”,<br>“validityPeriod” =&gt; “1996-12-06”,<br>“quantity” =&gt; 19<br>}</p><p><strong>3.3 filter配置</strong><br><strong>Grok插件</strong></p><p>logstash拥有丰富的filter插件,它们扩展了进入过滤器的原始数据，进行复杂的逻辑处理，甚至可以无中生有的添加新的 logstash 事件到后续的流程中去！Grok 是 Logstash 最重要的插件之一。也是迄今为止使蹩脚的、无结构的日志结构化和可查询的最好方式。Grok在解析 syslog logs、apache and other webserver logs、mysql logs等任意格式的文件上表现完美</p><p>这个工具非常适用于系统日志，Apache和其他网络服务器日志，MySQL日志等</p><p><strong>配置</strong></p><pre><code>input {    stdin {        type =&gt; &quot;std&quot;    }}filter {  grok {    match=&gt;{&quot;message&quot;=&gt; &quot;%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}&quot; }  }}output{stdout{codec=&gt;rubydebug}}</code></pre><p>输入：55.3.244.1 GET /index.html 15824 0.043<br>输出：<br>{<br>“@version” =&gt; “1”,<br>“host” =&gt; “zzc-203”,<br>“request” =&gt; “/index.html”,<br>“bytes” =&gt; “15824”,<br>“duration” =&gt; “0.043”,<br>“method” =&gt; “GET”,<br>“@timestamp” =&gt; 2019-03-19T05:09:55.777Z,<br>“message” =&gt; “55.3.244.1 GET /index.html 15824 0.043”,<br>“type” =&gt; “std”,<br>“client” =&gt; “55.3.244.1”<br>}</p><p>grok模式的语法如下：<br>%{SYNTAX:SEMANTIC}</p><p>SYNTAX：代表匹配值的类型,例如3.44可以用NUMBER类型所匹配,127.0.0.1可以使用IP类型匹配。<br>SEMANTIC：代表存储该值的一个变量名称,例如 3.44 可能是一个事件的持续时间,127.0.0.1可能是请求的client地址。所以这两个值可以用 %{NUMBER:duration} %{IP:client} 来匹配</p><p>你也可以选择将数据类型转换添加到Grok模式。默认情况下，所有语义都保存为字符串。如果您希望转换语义的数据类型，例如将字符串更改为整数，则将其后缀为目标数据类型。例如%{NUMBER:num:int}将num语义从一个字符串转换为一个整数。目前唯一支持的转换是int和float</p><p>Logstash附带约120个模式。你可以在这里找到它们<a href="https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns" target="_blank" rel="noopener">https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns</a></p><p><strong>自定义类型</strong><br>更多时候logstash grok没办法提供你所需要的匹配类型，这个时候我们可以使用自定义</p><p>创建自定义 patterns 文件<br>①创建一个名为patterns其中创建一个文件postfix （文件名无关紧要,随便起）,在该文件中，将需要的模式写为模式名称，空格，然后是该模式的正则表达式。例如：</p><p>POSTFIX_QUEUEID [0-9A-F]{10,11}</p><p>②然后使用这个插件中的patterns_dir设置告诉logstash目录是你的自定义模式。</p><p><strong>配置</strong></p><pre><code>input {    stdin {        type =&gt; &quot;std&quot;    }}filter {  grok {    patterns_dir =&gt; [&quot;./patterns&quot;]    match =&gt; { &quot;message&quot; =&gt; &quot;%{SYSLOGBASE} %{POSTFIX_QUEUEID:queue_id}: %{GREEDYDATA:syslog_message}&quot; }  }}output{stdout{codec=&gt;rubydebug}}</code></pre><p>输入：<br>Jan 1 06:25:43 mailserver14 postfix/cleanup[21403]: BEF25A72965: message-id=&lt;20130101142543.5828399CCAF@mailserver1</p><p>输出：<br>{<br>“queue_id” =&gt; “BEF25A72965”,<br>“message” =&gt; “Jan 1 06:25:43 mailserver14 postfix/cleanup[21403]: BEF25A72965: message-id=&lt;20130101142543.5828399CCAF@mailserver1”,<br>“pid” =&gt; “21403”,<br>“program” =&gt; “postfix/cleanup”,<br>“@version” =&gt; “1”,<br>“type” =&gt; “std”,<br>“logsource” =&gt; “mailserver14”,<br>“host” =&gt; “zzc-203”,<br>“timestamp” =&gt; “Jan 1 06:25:43”,<br>“syslog_message” =&gt; “message-id=&lt;20130101142543.5828399CCAF@mailserver1”,<br>“@timestamp” =&gt; 2019-03-19T05:31:37.405Z<br>}</p><p><strong>GeoIP 地址查询归类</strong><br>GeoIP 是最常见的免费 IP 地址归类查询库，同时也有收费版可以采购。GeoIP 库可以根据 IP 地址提供对应的地域信息，包括国别，省市，经纬度等，对于可视化地图和区域统计非常有用。</p><p><strong>配置</strong></p><pre><code>input {    stdin {        type =&gt; &quot;std&quot;    }}filter {    geoip {        source =&gt; &quot;message&quot;    }}output{stdout{codec=&gt;rubydebug}}</code></pre><p>输入：183.60.92.253<br>输出：<br>{<br>“type” =&gt; “std”,<br>“@version” =&gt; “1”,<br>“@timestamp” =&gt; 2019-03-19T05:39:26.714Z,<br>“host” =&gt; “zzc-203”,<br>“message” =&gt; “183.60.92.253”,<br>“geoip” =&gt; {<br>“country_code3” =&gt; “CN”,<br>“latitude” =&gt; 23.1167,<br>“region_code” =&gt; “44”,<br>“region_name” =&gt; “Guangdong”,<br>“location” =&gt; {<br>“lon” =&gt; 113.25,<br>“lat” =&gt; 23.1167<br>},<br>“city_name” =&gt; “Guangzhou”,<br>“country_name” =&gt; “China”,<br>“continent_code” =&gt; “AS”,<br>“country_code2” =&gt; “CN”,<br>“timezone” =&gt; “Asia/Shanghai”,<br>“ip” =&gt; “183.60.92.253”,<br>“longitude” =&gt; 113.25<br>}<br>}</p><p><strong>3.4 output配置</strong><br>标准输出(Stdout)</p><p>保存成文件(File)<br>通过日志收集系统将分散在数百台服务器上的数据集中存储在某中心服务器上，这是运维最原始的需求。Logstash 当然也能做到这点</p><p>和 LogStash::Inputs::File 不同, LogStash::Outputs::File 里可以使用 sprintf format 格式来自动定义输出到带日期命名的路径</p><p><strong>配置</strong></p><pre><code>input {    stdin {        type =&gt; &quot;std&quot;    }}output {    file {        path =&gt; &quot;../data_test/%{+yyyy}/%{+MM}/%{+dd}/%{host}.log&quot;        codec =&gt; line { format =&gt; &quot;custom format: %{message}&quot;}    }}</code></pre><p>启动后输入，可看到文件</p><p>服务器间传输文件(File)</p><p><strong>配置</strong><br>接收日志服务器配置</p><pre><code>input {  tcp {    mode =&gt; &quot;server&quot;    port =&gt; 9600    ssl_enable =&gt; false  }}filter {    json {        source =&gt; &quot;message&quot;    }}output {    file {        path =&gt; &quot;/usr/local/logstash-6.6.2/data_test/%{+YYYY-MM-dd}/%{servip}-%{filename}&quot;        codec =&gt; line { format =&gt; &quot;%{message}&quot;}    }}</code></pre><p>发送日志服务器配置</p><pre><code>input{    file {        path =&gt; [&quot;/usr/local/logstash-6.6.2/data_test/send.log&quot;]        type =&gt; &quot;ecolog&quot;        start_position =&gt; &quot;beginning&quot;    }}filter {    if [type] =~ /^ecolog/ {        ruby {            code =&gt; &quot;file_name = event.get(&#39;path&#39;).split(&#39;/&#39;)[-1]                     event.set(&#39;file_name&#39;,file_name)                     event.set(&#39;servip&#39;,&#39;接收方ip&#39;)&quot;        }        mutate {            rename =&gt; {&quot;file_name&quot; =&gt; &quot;filename&quot;}        }    }}output {    tcp {        host  =&gt; &quot;接收方ip&quot;        port  =&gt; 9600        codec =&gt; json_lines    }}</code></pre><p>从发送方发送message，接收方可以看到写出文件</p><p>写入到ES<br><strong>配置</strong></p><pre><code>input {    stdin {        type =&gt; &quot;log2es&quot;    }}output {    elasticsearch {        hosts =&gt; [&quot;192.168.109.133:9200&quot;]        index =&gt; &quot;logstash-%{type}-%{+YYYY.MM.dd}&quot;        document_type =&gt; &quot;%{type}&quot;        sniffing =&gt; true        template_overwrite =&gt; true    }}</code></pre><p>在head插件中可以看到数据<br>sniffing ： 寻找其他es节点</p><p><strong>实战举例</strong>：将错误日志写入es<br><strong>配置</strong></p><pre><code>input {    file {        path =&gt; [&quot;/usr/local/logstash-6.6.2/data_test/run_error.log&quot;]        type =&gt; &quot;error&quot;        start_position =&gt; &quot;beginning&quot;    }}output {    elasticsearch {        hosts =&gt; [&quot;192.168.109.133:9200&quot;]        index =&gt; &quot;logstash-%{type}-%{+YYYY.MM.dd}&quot;        document_type =&gt; &quot;%{type}&quot;        sniffing =&gt; true        template_overwrite =&gt; true    }}</code></pre><h3 id="四、Kibana"><a href="#四、Kibana" class="headerlink" title="四、Kibana"></a>四、Kibana</h3><p>Kibana是一个开源的分析和可视化平台，设计用于和Elasticsearch一起工作</p><p>你用Kibana来搜索，查看，并和存储在Elasticsearch索引中的数据进行交互</p><p>你可以轻松地执行高级数据分析，并且以各种图标、表格和地图的形式可视化数据</p><p>Kibana使得理解大量数据变得很容易。它简单的、基于浏览器的界面使你能够快速创建和共享动态仪表板，实时显示Elasticsearch查询的变化</p><p><strong>安装步骤</strong><br>解压：tar -zxvf kibana-6.6.2-linux-x86_64.tar.gz<br>修改 kibana.yml 配置文件： </p><pre><code>server.port: 5601 server.host: “192.168.116.121” ———-部署kinana服务器的ip elasticsearch.hosts: [“http://192.168.116.121:9200“] kibana.index: “.kibana”</code></pre><p>启动kibana，报错：<br>./bin/kibana<br><code>[error][status]</code>[plugin:<a href="mailto:remote_clusters@6.6.2">remote_clusters@6.6.2</a>] Status changed from red to red - X-Pack plugin is not installed on the [data] Elasticsearch cluster.</p><p>解决，卸载x-pack插件<br>elasticsearch-plugin remove x-pack<br>kibana-plugin remove x-pack</p><p>安装好后启动即可。页面操作</p><p>访问页面<br><a href="http://192.168.116.121:5601/" target="_blank" rel="noopener">http://192.168.116.121:5601/</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> ElasticSearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ElasticSearch（一）</title>
      <link href="/2019/03/18/elasticsearch-yi/"/>
      <url>/2019/03/18/elasticsearch-yi/</url>
      
        <content type="html"><![CDATA[<h3 id="1-全文检索技术简介"><a href="#1-全文检索技术简介" class="headerlink" title="1. 全文检索技术简介"></a>1. 全文检索技术简介</h3><h4 id="什么是搜索？"><a href="#什么是搜索？" class="headerlink" title="什么是搜索？"></a>什么是搜索？</h4><p>搜索，就是在任何场景下，找寻你想要的信息，这个时候，会输入一段你要搜索的关键字，然后就期望找到这个关键字相关的有些信息</p><h4 id="如何实现搜索？"><a href="#如何实现搜索？" class="headerlink" title="如何实现搜索？"></a>如何实现搜索？</h4><p>OA系统，比如：通过名字搜索员工等等<br>mysql :<br>select * from employee e where e.name like “%李雷%”;<br>select * from employee e where e.comment like “%好%”;<br>问题：<br>1）性能<br>2）比如搜索“优秀工”，mysql 无法支持</p><h4 id="全文检索"><a href="#全文检索" class="headerlink" title="全文检索"></a>全文检索</h4><p>全文数据库是全文检索系统的主要构成部分。所谓全文数据库是将一个完整的信息源的全部内容转化为计算机可以识别、处理的信息单元而形成的数据集合</p><p>全文数据库不仅存储了信息，而且还有对全文数据进行词、字、段落等更深层次的编辑、加工的功能</p><p>所有全文数据库无一不是海量信息数据库</p><h4 id="倒排索引"><a href="#倒排索引" class="headerlink" title="倒排索引"></a>倒排索引</h4><p>传统数据库存储：<br>| id  |  描述 |<br>| :—:| :—:|<br>|  1  |  优秀员工  |<br>|  2  |  销售冠军  |<br>|  3  |  优秀团队领导|<br>|  4  |  优秀项目  |</p><p>倒排索引处理步骤：<br>1、切词：<br>优秀员工 —— 优秀 员工<br>销售冠军 —— 销售 冠军<br>优秀团队领导 —— 优秀 团队 领导<br>优秀项目 —— 优秀 项目</p><p>2、建立倒排索引：<br>关键词 id<br>| 关键词 |  id  |<br>| :—:| :—:|<br>| 优秀 | 1,3,4 |<br>| 员工 |  1  |<br>| 销售 |  2  |<br>| 团队 |  3  |<br>|。。。| 。。。| </p><h4 id="Lucene"><a href="#Lucene" class="headerlink" title="Lucene"></a>Lucene</h4><p><strong>全文检索引擎</strong><br>Lucene 能够为文本类型的数据建立索引，所以你只要能把你要索引的数据格式转化的文本的，Lucene 就能对你的文档进行索引和搜索。比如你要对一些 HTML 文档，PDF 文档进行索引的话你就首先需要把 HTML 文档和 PDF 文档转化成文本格式的，然后将转化后的内容交给 Lucene 进行索引，然后把创建好的索引文件保存到磁盘或者内存中，最后根据用户输入的查询条件在索引文件上进行查询。不指定要索引的文档的格式也使 Lucene 能够几乎适用于所有的搜索应用程序</p><p>换句话说，使用 Lucene 可以轻松完成上述步骤</p><h4 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h4><p>Elasticsearch 是一个高度可伸缩的开源全文搜索和分析引擎。它允许你以近实时的方式快速存储、搜索和分析大量的数据。它通常被用作基础的技术来赋予应用程序复杂的搜索特性和需求</p><p>Elasticsearch ，是基于 lucene 开发的，隐藏复杂性，提供简单易用的 restful api 接口、java api 接口（还有其他语言的 api 接口）</p><h4 id="Elasticsearch-特点"><a href="#Elasticsearch-特点" class="headerlink" title="Elasticsearch 特点"></a>Elasticsearch 特点</h4><p>可以作为一个大型分布式集群（数百台服务器）技术，处理 PB 级数据，服务大公司；也可以运行在单机上，服务小公司</p><p>Elasticsearch 不是什么新技术，主要是将全文检索、数据分析以及分布式技术，合并在了一起，才形成了独一无二的 ES</p><p>对用户而言，是开箱即用的，非常简单，作为中小型的应用，直接3分钟部署一下 ES ，就可以作为生产环境的系统来使用了，数据量不大，操作不是太复杂</p><p>数据库的功能面对很多领域是不够用的（事务，还有各种联机事务型的操作）；特殊的功能，比如全文检索，同义词处理，相关度排名，复杂数据分析，海量数据的近实时处理； Elasticsearch 作为传统数据库的一个补充，提供了数据库所不能提供的很多功能</p><h4 id="Elasticsearch核心概念"><a href="#Elasticsearch核心概念" class="headerlink" title="Elasticsearch核心概念"></a>Elasticsearch核心概念</h4><p><strong>近实时</strong><br>近实时，两个意思，从写入数据到数据可以被搜索到有一个小延迟（大概1秒）；基于es执行搜索和分析可以达到秒级</p><p><strong>Cluster（集群）</strong><br>集群包含多个节点，每个节点属于哪个集群是通过一个配置（集群名称，默认是elasticsearch）来决定的，对于中小型应用来说，刚开始一个集群就一个节点很正常</p><p><strong>Node（节点）</strong><br>集群中的一个节点，节点也有一个名称（默认是随机分配的），节点名称很重要（在执行运维管理操作的时候），默认节点会去加入一个名称为“elasticsearch”的集群，如果直接启动一堆节点，那么它们会自动组成一个elasticsearch集群，当然一个节点也可以组成一个elasticsearch集群</p><p><strong>Index（索引-数据库）</strong><br>索引包含一堆有相似结构的文档数据，比如可以有一个客户索引，商品分类索引，订单索引，索引有一个名称。一个index包含很多document，一个index就代表了一类类似的或者相同的document。比如说建立一个product index，商品索引，里面可能就存放了所有的商品数据，所有的商品document</p><p><strong>Type（类型-表）</strong><br>每个索引里都可以有一个或多个type，type是index中的一个逻辑数据分类，一个type下的document，都有相同的field，比如博客系统，有一个索引，可以定义用户数据type，博客数据type，评论数据type</p><p>商品index，里面存放了所有的商品数据，商品document<br>但是商品分很多种类，每个种类的document的field可能不太一样，比如说电器商品，可能还包含一些诸如售后时间范围这样的特殊field；生鲜商品，还包含一些诸如生鲜保质期之类的特殊field</p><p>type，日化商品type，电器商品type，生鲜商品type<br>日化商品type：product_id，product_name，product_desc，category_id，category_name</p><p>电器商品type：product_id，product_name，product_desc，category_id，category_name，service_period</p><p>生鲜商品type：product_id，product_name，product_desc，category_id，category_name，eat_period</p><p>每一个type里面，都会包含一堆document</p><pre><code>{  &quot;product_id&quot;: &quot;1&quot;,  &quot;product_name&quot;: &quot;长虹电视机&quot;,  &quot;product_desc&quot;: &quot;4k高清&quot;,  &quot;category_id&quot;: &quot;3&quot;,  &quot;category_name&quot;: &quot;电器&quot;,  &quot;service_period&quot;: &quot;1年&quot;}{  &quot;product_id&quot;: &quot;2&quot;,  &quot;product_name&quot;: &quot;基围虾&quot;,  &quot;product_desc&quot;: &quot;纯天然，冰岛产&quot;,  &quot;category_id&quot;: &quot;4&quot;,  &quot;category_name&quot;: &quot;生鲜&quot;,  &quot;eat_period&quot;: &quot;7天&quot;}</code></pre><p><strong>Document（文档-行）</strong><br>文档是es中的最小数据单元，一个document可以是一条客户数据，一条商品分类数据，一条订单数据，通常用JSON数据结构表示，每个index下的type中，都可以去存储多个document</p><p><strong>Field（字段-列）</strong><br>Field是Elasticsearch的最小单位。一个document里面有多个field，每个field就是一个数据字段</p><pre><code>product document{  &quot;product_id&quot;: &quot;1&quot;,  &quot;product_name&quot;: &quot;高露洁牙膏&quot;,  &quot;product_desc&quot;: &quot;高效美白&quot;,  &quot;category_id&quot;: &quot;2&quot;,  &quot;category_name&quot;: &quot;日化用品&quot;}</code></pre><p><strong>mapping（映射-约束）</strong><br>数据如何存放到索引对象上，需要有一个映射配置，包括：数据类型、是否存储、是否分词等</p><p>这样就创建了一个名为blog的Index。Type不用单独创建，在创建Mapping 时指定就可以。Mapping用来定义Document中每个字段的类型，即所使用的 analyzer、是否索引等属性，非常关键等。创建Mapping 的代码示例如下：</p><pre><code>client.indices.putMapping({    index : &#39;blog&#39;,    type : &#39;article&#39;,    body : {        article: {            properties: {                id: {                    type: &#39;string&#39;,                    analyzer: &#39;ik&#39;,                    store: &#39;yes&#39;,                },                title: {                    type: &#39;string&#39;,                    analyzer: &#39;ik&#39;,                    store: &#39;no&#39;,                },                content: {                    type: &#39;string&#39;,                     analyzer: &#39;ik&#39;,                    store: &#39;yes&#39;,                }            }        }    }});</code></pre><h4 id="elasticsearch与数据库的类比"><a href="#elasticsearch与数据库的类比" class="headerlink" title="elasticsearch与数据库的类比"></a>elasticsearch与数据库的类比</h4><table><thead><tr><th align="center">关系型数据库（比如Mysql）</th><th align="center">非关系型数据库（Elasticsearch）</th></tr></thead><tbody><tr><td align="center">数据库Database</td><td align="center">索引Index</td></tr><tr><td align="center">表Table</td><td align="center">类型Type</td></tr><tr><td align="center">数据行Row</td><td align="center">文档Document</td></tr><tr><td align="center">数据列Column</td><td align="center">字段Field</td></tr><tr><td align="center">约束 Schema</td><td align="center">映射Mapping</td></tr></tbody></table><h4 id="ES存入数据和搜索数据机制"><a href="#ES存入数据和搜索数据机制" class="headerlink" title="ES存入数据和搜索数据机制"></a>ES存入数据和搜索数据机制</h4><p>1）索引对象（index）：存储数据的表结构 ，任何搜索数据，存放在索引对象上 </p><p>2）映射（mapping）：数据如何存放到索引对象上，需要有一个映射配置， 包括：数据类型、是否存储、是否分词等</p><p>3）文档（document）：一条数据记录，存在索引对象上 </p><p>4）文档类型（type）：一个索引对象，存放多种类型数据，数据用文档类型进行标识</p><h3 id="2-安装"><a href="#2-安装" class="headerlink" title="2. 安装"></a>2. 安装</h3><h4 id="单节点安装教程"><a href="#单节点安装教程" class="headerlink" title="单节点安装教程"></a>单节点安装教程</h4><p>java8<br>1）下载es安装包<br>curl -L -O <a href="https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.1.1.tar.gz" target="_blank" rel="noopener">https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.1.1.tar.gz</a></p><p>2）浏览器下载上传到虚拟机安装并创建和修改文件</p><p>解压elasticsearch-6.1.1.tar.gz到/opt/module目录下</p><p>在/opt/module/elasticsearch-6.1.1路径下创建data和logs文件夹<br>mkdir data<br>mkdir logs</p><p>修改配置文件/opt/module/elasticsearch-6.1.1/config/elasticsearch.yml<br>vi elasticsearch.yml</p><pre><code># ----------- Cluster ----------- cluster.name: my-application# ----------- Node ----------- node.name: node-121# ----------- Paths ----------- path.data: /opt/module/elasticsearch-6.1.1/datapath.logs: /opt/module/elasticsearch-6.1.1/logs# ----------- Memory ----------- bootstrap.memory_lock: falsebootstrap.system_call_filter: false# ----------- Network ----------- network.host: 192.168.116.121# ----------- Discovery ----------- discovery.zen.ping.unicast.hosts: [&quot;hsiehchou121&quot;]</code></pre><p>（1）cluster.name<br>如果要配置集群需要两个节点上的elasticsearch配置的cluster.name相同，都启动可以自动组成集群，这里如果不改cluster.name则默认是cluster.name=my-application</p><p>（2）nodename随意取但是集群内的各节点不能相同 </p><p>（3）修改后的每行前面不能有空格，修改后的“：”后面必须有一个空格</p><p>3）配置linux系统环境<br><strong>编辑limits.conf</strong><br>添加类似如下内容<br>sudo vi /etc/security/limits.conf<br>添加如下内容:</p><ul><li>soft nofile 65536</li><li>hard nofile 131072</li><li>soft nproc 4096</li><li>hard nproc 4096</li></ul><p><strong>进入limits.d目录下修改配置文件</strong><br>sudo vi /etc/security/limits.d/20-nproc.conf<br>修改如下内容：</p><ul><li>soft nproc 4096（修改为此参数，6版本的默认就是4096）</li></ul><p><strong>修改配置sysctl.conf</strong><br>sudo vi /etc/sysctl.conf<br>添加下面配置：<br>vm.max_map_count=655360<br>并执行命令：<br>sudo sysctl -p<br>然后，重新启动elasticsearch，即可启动成功</p><p><strong>启动（非root账户下启动）</strong><br>bin/elasticsearch<br>注意：can not run elasticsearch as root</p><p><strong>测试elasticsearch</strong><br>curl <a href="http://hsiehchou121:9200" target="_blank" rel="noopener">http://hsiehchou121:9200</a><br>curl -XGET ‘hsiehchou121:9200/_cat/health?v&amp;pretty’</p><p><strong>注：Linux中新建账户elasticsearch</strong><br>1、Linux中新建用户命令：<br>举例：我们创建一个名字叫 elasticsearch的用户<br>使用root用户操作如下命令：<br>useradd elasticsearch———–创建用户<br>passwd elasticsearch———–为用户设置密码<br>vim /etc/sudoers ———–为用户赋予sudo权限<br>添加 elasticsearch ALL=(ALL) ALL</p><p>2、修改文件夹及其子文件夹属主命令<br>chown -R elasticsearch ./elasticsearch-6.1.1/<br>修改后即可以使用elasticsearch操作此文件夹内容</p><p>4）安装elasticsearch-head.crx插件<br>页面连接按钮前面有个输入框输入：<a href="http://192.168.116.121:9200/" target="_blank" rel="noopener">http://192.168.116.121:9200/</a></p><p>5）命令行验证<br>REST API<br>curl -XGET ‘localhost:9200/_cat/health?v&amp;pretty’<br>epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent<br>1550960314 06:18:34 elasticsearch green 1 1 0 0 0 0 0 0 - 100.0%</p><p>看到status是 green，证明启动成功</p><p><strong>Green</strong> - 一切运行正常(集群功能齐全)<br><strong>Yellow</strong> - 所有数据是可以获取的，但是一些复制品还没有被分配(集群功能齐全)<br><strong>Red</strong> - 一些数据因为一些原因获取不到(集群部分功能不可用)</p><h4 id="多节点集群安装教程"><a href="#多节点集群安装教程" class="headerlink" title="多节点集群安装教程"></a>多节点集群安装教程</h4><p>1）分发Elasticsearch安装包至hsiehchou122、hsiehchou123、hsiehchou124<br>xsync elasticsearch-6.1.1/<br>或者scp -r elasticsearch-6.1.1/ hsiehchou122:/opt/module/</p><p>2）修改hsiehchou121配置信息<br>[elasticsearch@hsiehchou121 config]$ vi elasticsearch.yml<br>添加如下信息：<br>node.master: true<br>node.data: true</p><p>3）修改hsiehchou122配置信息<br>修改Elasticsearch配置信息<br>[elasticsearch@hsiehchou122 config]$ vi elasticsearch.yml<br>node.name: node-122<br>node.master: false<br>node.data: true<br>network.host: 192.168.116.122</p><p>修改Linux相关配置信息（同hsiehchou121 ）</p><p>修改hsiehchou122、hsiehchou123、hsiehchou124 配置信息</p><p>因为是scp的，所以一些跟hsiehchou121一样的配置就不需要修改了</p><p>5）分别启动三台节点的Elasticsearch<br>6）使用插件查看集群状态</p><h4 id="集群安装（详细增加或者修改内容）"><a href="#集群安装（详细增加或者修改内容）" class="headerlink" title="集群安装（详细增加或者修改内容）"></a>集群安装（详细增加或者修改内容）</h4><p>vi /opt/module/elasticsearch-6.1.1/config/elasticsearch.yml<br>hsiehchou121增加<br>node.master: true<br>node.data: true</p><p>hsiehchou121修改<br>discovery.zen.ping.unicast.hosts: [“hsiehchou121”,”hsiehchou122”,”hsiehchou123”,”hsiehchou124”]</p><p>hsiehchou122增加<br>node.master: false<br>node.data: true</p><p>hsiehchou122修改<br>node.name: node-122<br>network.host: 192.168.116.122<br>discovery.zen.ping.unicast.hosts: [“hsiehchou121”,”hsiehchou122”,”hsiehchou123”,”hsiehchou124”]</p><p>hsiehchou123增加<br>node.master: false<br>node.data: true</p><p>hsiehchou123修改<br>node.name: node-123<br>network.host: 192.168.116.123<br>discovery.zen.ping.unicast.hosts: [“hsiehchou121”,”hsiehchou122”,”hsiehchou123”,”hsiehchou124”]</p><p>hsiehchou124增加<br>node.master: false<br>node.data: true</p><p>hsiehchou124修改<br>node.name: node-124<br>network.host: 192.168.116.124<br>discovery.zen.ping.unicast.hosts: [“hsiehchou121”,”hsiehchou122”,”hsiehchou123”,”hsiehchou124”]</p><p><strong>说明</strong><br>cluster.name ：如果要配置集群需要两个节点上的 elasticsearch 配置的 cluster.name ：相同，都启动可以自动组成集群，这里如果不改 cluster.name ：则默认是 cluster.name=my-application<br>nodename ：随意取但是集群内的各节点不能相同</p><p><strong>启动es，报错：</strong><br>[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]<br>[2]: max number of threads [1024] for user [hduser] is too low, increase to at least [4096]<br>[3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]<br>[4]: system call filters failed to install; check the logs and fix your configuration or disable system</p><p>配置 linux 系统环境：<br>切换到 root 用户，编辑 limits.conf 添加类似如下内容<br>vi /etc/security/limits.conf</p><p>添加如下内容：</p><ul><li>soft nofile 65536</li><li>hard nofile 131072</li><li>soft nproc 4096</li><li>hard nproc 4096<br>进入 limits.d 目录下修改配置jian</li></ul><p>vi /etc/security/limits.d/20-nproc.conf<br>把 * soft nproc 1024 改成4096<br>es6版本的我没有要改，默认就是4096</p><p>修改配置 sysctl.conf<br>vi /etc/sysctl.conf<br>添加：<br>vm.max_map_count=655360<br>执行：<br>sysctl -p</p><p>重新登录elasticsearch用户，重新启动es<br>如果还有报错，则需重启虚拟机</p><p>查看集群状态命令：<br>curl -XGET ‘your ip:9200/_cat/health?v&amp;pretty’</p><p>查看所有数据命令：</p><pre><code>[elasticsearch@hsiehchou121 config]$ curl -XGET &#39;192.168.116.121:9200/blog/_search?pretty&#39; -H &#39;Content-Type: application/json&#39; -d&#39;&gt; {&gt;  &quot;query&quot;: { &quot;match_all&quot;: {} }&gt; }&gt; &#39;</code></pre><p>Elasticsearch head插件安装<br>node js下载插件：<a href="https://github.com/mobz/elasticsearch-head" target="_blank" rel="noopener">https://github.com/mobz/elasticsearch-head</a><br>nodejs官网下载安装包：<a href="https://nodejs.org/dist/" target="_blank" rel="noopener">https://nodejs.org/dist/</a><br>node-v6.9.2-linux-x64.tar.xz<br>拷贝<br>安装nodejs：<br>解压<br>配置环境变量：<br>export NODE_HOME=/usr/local/node-v6.9.2-linux-x64<br>export PATH=<code>$PATH:$NODE_HOME/bin</code></p><p>查看node和npm版本：<br>node -v<br>v6.9.2</p><p>npm -v<br>3.10.9</p><p>解压head插件到/opt/module目录下：<br>unzip elasticsearch-head-master.zip</p><p>查看当前head插件目录下有无node_modules/grunt目录：<br>没有的话，执行下面命令创建：<br>npm install grunt <code>--save</code> <code>--registry</code>=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><br>安装head插件：</p><p>npm install -g cnpm <code>--registry</code>=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><br>安装grunt：</p><p>npm install -g grunt-cli <code>--registry</code>=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><br>编辑Gruntfile.js<br>vim Gruntfile.js</p><p><strong>文件93行添加</strong><br><strong>hostname: ‘0.0.0.0’,</strong></p><p>检查head根目录下是否存在base文件夹<br>没有的话，将 _site下的base文件夹及其内容复制到head根目录下<br>mkdir base<br>cp base/* ../base/</p><p>启动grunt server：<br>[root@hsiehchou121 elasticsearch-head-master]# grunt server -d</p><p>如果提示grunt的模块没有安装：<br>Local Npm module “grunt-contrib-clean” not found. Is it installed?<br>Local Npm module “grunt-contrib-concat” not found. Is it installed?<br>Local Npm module “grunt-contrib-watch” not found. Is it installed?<br>Local Npm module “grunt-contrib-connect” not found. Is it installed?<br>Local Npm module “grunt-contrib-copy” not found. Is it installed?<br>Local Npm module “grunt-contrib-jasmine” not found. Is it installed?</p><p>执行以下命令：<br>npm install grunt-contrib-clean -registry=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><br>npm install grunt-contrib-concat -registry=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><br>npm install grunt-contrib-watch -registry=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><br>npm install grunt-contrib-connect -registry=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><br>npm install grunt-contrib-copy -registry=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><br>npm install grunt-contrib-jasmine -registry=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a></p><p>最后一个模块可能安装不成功，但是不影响使用</p><p>浏览器访问head插件：<br><a href="http://192.168.116.121:9100" target="_blank" rel="noopener">http://192.168.116.121:9100</a></p><p><strong>CDH上的elasticsearch的配置</strong><br>vim /etc/security/limits.conf</p><ul><li>soft nofile 65536</li><li>hard nofile 131072</li><li>soft nproc 2048</li><li>hard nproc 4096</li></ul><p>vim /etc/security/limits.d/90-nproc.conf</p><ul><li>soft    nproc     4096<br>root       soft    nproc     unlimited</li></ul><p>vim /etc/sysctl.conf</p><p><strong>添加下面配置</strong><br>vm.max_map_count=655360</p><p>并执行命令：<br>sysctl -p</p><p><a href="http://hadoop2:9200" target="_blank" rel="noopener">http://hadoop2:9200</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> ElasticSearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase 操作</title>
      <link href="/2019/03/15/hbase-cao-zuo/"/>
      <url>/2019/03/15/hbase-cao-zuo/</url>
      
        <content type="html"><![CDATA[<h4 id="1、HBase-API操作"><a href="#1、HBase-API操作" class="headerlink" title="1、HBase API操作"></a>1、HBase API操作</h4><p>1）首先将core-site.xml、hbase-site.xml、hdfs-site.xml引入maven工程的resources下面</p><p>2）配置pom.xml文件<br>增加hbase依赖</p><pre><code>&lt;dependencies&gt;   &lt;dependency&gt;       &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;       &lt;artifactId&gt;hbase-server&lt;/artifactId&gt;       &lt;version&gt;1.3.0&lt;/version&gt;   &lt;/dependency&gt;   &lt;dependency&gt;       &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;       &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;       &lt;version&gt;1.3.0&lt;/version&gt;   &lt;/dependency&gt;&lt;/dependencies&gt;</code></pre><p>3）创建HbaseTest.java</p><pre><code>package com.hsiehchou.hbase;import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.hbase.*; import org.apache.hadoop.hbase.client.*; import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException; import java.util.ArrayList; import java.util.List;public class HbaseTest {     //配置信息     public static Configuration conf;     //获取配置信息     static{         //alt + enter         conf = HBaseConfiguration.create();     }</code></pre><p><strong>判断HBase中表是否存在</strong></p><pre><code>//1.判断HBase中表是否存在public static boolean isExist(String tableName) throws IOException{    //对表操作需要用HbaseAdmin    //HBaseAdmin admin = new HBaseAdmin(conf);老版本    Connection connection = ConnectionFactory.createConnection(conf);    //管理器    HBaseAdmin admin = (HBaseAdmin) connection.getAdmin();    return  admin.tableExists(TableName.valueOf(tableName));}</code></pre><p><strong>在HBase中创建表</strong></p><pre><code>//2.在HBase中创建表public static void createTable(String tableName, String... columnFamily) throws IOException {    //1.如果对表操作需要使用管理器    Connection connection = ConnectionFactory.createConnection(conf);    HBaseAdmin admin = (HBaseAdmin)connection.getAdmin();    //2.创建描述器    HTableDescriptor hd = new HTableDescriptor(TableName.valueOf(tableName));    //3.指定多个列族    for(String cf:columnFamily){        hd.addFamily(new HColumnDescriptor(cf));    }    //4.创建表    admin.createTable(hd);    System.out.println(&quot;表已经创建成功！！！！&quot;);}</code></pre><p><strong>bin/hbase shell操作</strong><br>list<br>scan ‘ni’<br>describe ‘ni’</p><p><strong>向表中添加数据</strong></p><pre><code>//3,向表中添加数据 put   rowkey  cf:列族public static void addData(String tableName, String rowkey, String cf, String column, String value) throws IOException {  Connection connection = ConnectionFactory.createConnection(conf);  Table table = connection.getTable(TableName.valueOf(tableName));  //添加数据 put方式  Put put = new Put(Bytes.toBytes(rowkey));  //指定列族 列 值  put.addColumn(Bytes.toBytes(cf), Bytes.toBytes(column), Bytes.toBytes(value));  table.put(put);}</code></pre><p><strong>删除一行数据</strong></p><pre><code>//4.删除一行数据public static void deleteRow(String tableName, String rowkey) throws IOException {    Connection connection = ConnectionFactory.createConnection(conf);    Table table = connection.getTable(TableName.valueOf(tableName));    Delete delete = new Delete(Bytes.toBytes(rowkey));    table.delete(delete);}</code></pre><p><strong>删除多个rowkey的数据</strong></p><pre><code>//5.删除多个rowkey的数据public static void deleteMore(String tableName, String... rowkey) throws IOException {    Connection connection = ConnectionFactory.createConnection(conf);    Table table = connection.getTable(TableName.valueOf(tableName));    //封装delete    List&lt;Delete&gt; d = new ArrayList&lt;Delete&gt;();    //遍历rowkey    for(String rk:rowkey){        Delete dd = new Delete(Bytes.toBytes(rk));        d.add(dd);    }    table.delete(d);}</code></pre><p><strong>全表扫描</strong></p><pre><code>//6.全表扫描public static void scanAll(String tableName) throws IOException {    Connection connection = ConnectionFactory.createConnection(conf);    Table table = connection.getTable(TableName.valueOf(tableName));    Scan scan = new Scan();    ResultScanner rs = table.getScanner(scan);    //遍历    for(Result r:rs){        //单元格        Cell[] cells = r.rawCells();        for(Cell c:cells) {            System.out.println(&quot;rowkey为：&quot; + Bytes.toString(CellUtil.cloneRow(c)));            System.out.println(&quot;列族为：&quot; + Bytes.toString(CellUtil.cloneFamily(c)));            System.out.println(&quot;值为：&quot; + Bytes.toString(CellUtil.cloneValue(c)));        }    }}</code></pre><p><strong>删除表</strong></p><pre><code>//7.删除表public static void deleteTable(String tableName) throws IOException {    //1.如果对表操作需要使用管理器    Connection connection = ConnectionFactory.createConnection(conf);    HBaseAdmin admin = (HBaseAdmin)connection.getAdmin();    admin.disableTable(tableName);    admin.deleteTable(TableName.valueOf(tableName));}public static void main(String[] args) throws IOException {     //System.out.println(isExist(“user”));     //create ‘表名’,’列族名’     //createTable(“ni”,”info1”,”info2”,”info3”);     //addData(“ni”,”shanghai”,”info1”,”name”,”lilei”);     //deleteRow(“ni”,”shanghai”);     //deleteMore(“ni”,”shanghai1”,”shanghai2”);    //scanAll(“ni”);     deleteTable(“ni”); } }</code></pre><h4 id="2、HBase-MR"><a href="#2、HBase-MR" class="headerlink" title="2、HBase-MR"></a>2、HBase-MR</h4><p>HBase主要擅长的领域是存储数据，不擅长分析数据</p><p>HBase如果想计算的话需要结合Hadoop的MapReduce</p><p>HBase-MR所需的jar包查看<br>bin/hbase mapredcp</p><p>配置临时环境变量</p><p>export HBASE_HOME=/root/hd/hbase-1.3.0<br>export HADOOP_HOME=/root/hd/hadoop-2.8.4<br>export HADOOP_CLASSPATH=<code>${HBASE_HOME}/bin/hbase mapredcp</code><br>跑hbase-mr程序<br>bin/yarn jar /root/hd/hbase-1.3.0/lib/hbase-server-1.3.0.jar rowcounter user</p><h4 id="3、HBase的表操作"><a href="#3、HBase的表操作" class="headerlink" title="3、HBase的表操作"></a>3、HBase的表操作</h4><p><strong>场景一</strong>：<br>region分片<br>指定列的过滤<br>name age high<br>name</p><p><strong>代码实现</strong><br><strong>ReadLoveMapper.java</strong></p><pre><code>package com.hsiehchou.mr;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.CellUtil;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableMapper;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;/** * HBase -MR * mapper类进行对数据的读取操作 * key:ImmutableBytesWritable hbase中的rowkey * value:封装的一条条的数据 */public class ReadLoveMapper extends TableMapper&lt;ImmutableBytesWritable, Put&gt; {    @Override    protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException {        //1.读取数据  根据rowkey拿到数据        Put put = new Put(key.get());        //2.过滤列 Cell单元格        for (Cell c:value.rawCells()){            //拿到info列族数据  如果是info列族  取出  如果不是info 过滤掉            if(&quot;info&quot;.equals(Bytes.toString(CellUtil.cloneFamily(c)))){                //过滤列                if(&quot;name&quot;.equals(Bytes.toString(CellUtil.cloneQualifier(c)))){                    put.add(c);                }            }        }        //3.输出到reducer端        context.write(key,put);    }}</code></pre><p><strong>WriteLoveReducer .java</strong></p><pre><code>package com.hsiehchou.mr;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableReducer;import org.apache.hadoop.io.NullWritable;import java.io.IOException;/** * keyIn:ImmutableBytesWritable * valueIn:Put * keyOut:NullWritable（在put里面已经有了rowkey了，所以不需要了） */public class WriteLoveReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; {    @Override    protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException {        for (Put p:values){            context.write(NullWritable.get(),p);        }    }}</code></pre><p><strong>LoverDriver .java</strong></p><pre><code>package com.hsiehchou.mr;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Scan;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class LoverDriver implements Tool {    private Configuration conf;    public void setConf(Configuration configuration) {        this.conf = HBaseConfiguration.create(configuration);    }    public Configuration getConf() {        return this.conf;    }    public int run(String[] strings) throws Exception {        //1.创建任务        Job job = Job.getInstance(conf);        //2.指定运行的主类        job.setJarByClass(LoverDriver.class);        //3.配置job        Scan scan = new Scan();        //4.设置具体运行的mapper类        TableMapReduceUtil.initTableMapperJob(&quot;love&quot;,                    scan,                    ReadLoveMapper.class,                    ImmutableBytesWritable.class,                    Put.class,                    job                );        //5.设置具体运行的Reducer类        TableMapReduceUtil.initTableReducerJob(&quot;lovemr&quot;,                    WriteLoveReducer.class,                    job                );        //6.设置reduceTask        job.setNumReduceTasks(1);        boolean rs = job.waitForCompletion(true);        return rs?0:1;    }    public static void main(String[] args) {        try {            //状态码            int sts = ToolRunner.run(new LoverDriver(), args);            System.exit(sts);        } catch (Exception e) {            e.printStackTrace();        }    }}</code></pre><p><strong>场景二</strong></p><p>把HDFS中的数据导入到HBase表中<br>HBase-MR</p><p><strong>代码实现</strong> </p><p><strong>ReadHdfsMapper .java</strong></p><pre><code>package com.hsiehchou.mr1;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.util.Bytes;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;/** * 读取hdfs中的数据 * hdfs -&gt;hbase */public class ReadHdfsMapper extends Mapper&lt;LongWritable, Text, ImmutableBytesWritable, Put&gt; {    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        //1.读取数据        String line = value.toString();        //2.切分数据        String[] fields = line.split(&quot;\t&quot;);        //3.封装数据        byte[] rowkey = Bytes.toBytes(fields[0]);        byte[] name = Bytes.toBytes(fields[1]);        byte[] desc = Bytes.toBytes(fields[2]);        //4.封装成put        Put put = new Put(rowkey);        put.addColumn(Bytes.toBytes(&quot;info&quot;),Bytes.toBytes(&quot;name&quot;),name);        put.addColumn(Bytes.toBytes(&quot;info&quot;),Bytes.toBytes(&quot;desc&quot;),desc);        //5.输出到reducer        context.write(new ImmutableBytesWritable(rowkey),put);    }}</code></pre><p><strong>WriteHbaseReducer.java</strong></p><pre><code>package com.hsiehchou.mr1;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableReducer;import org.apache.hadoop.io.NullWritable;import java.io.IOException;public class WriteHbaseReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; {    @Override    protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException {        for(Put p:values){            context.write(NullWritable.get(),p);        }    }}</code></pre><p><strong>LoveDriver.java</strong></p><pre><code>package com.hsiehchou.mr1;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class LoveDriver implements Tool {    private Configuration conf = null;    public void setConf(Configuration configuration) {        this.conf = HBaseConfiguration.create(configuration);    }    public Configuration getConf() {        return this.conf;    }    public int run(String[] strings) throws Exception {        //1.创建job        Job job = Job.getInstance();        job.setJarByClass(LoveDriver.class);        //2.配置mapper        job.setMapperClass(ReadHdfsMapper.class);        job.setMapOutputKeyClass(ImmutableBytesWritable.class);        job.setMapOutputValueClass(Put.class);        //3.配置reducer        TableMapReduceUtil.initTableReducerJob(&quot;lovehdfs&quot;, WriteHbaseReducer.class, job);        //4.输入配置 hdfs读数据 inputformat        FileInputFormat.addInputPath(job,new Path(&quot;/lovehbase/&quot;));        //5.需要配置outputformat吗？不需要 reducer中已经指定了表        return job.waitForCompletion(true)? 0:1;    }    public static void main(String[] args) {        try {            int sts = ToolRunner.run(new LoveDriver(),args);            System.exit(sts);        } catch (Exception e) {            e.printStackTrace();        }    }}</code></pre><h4 id="4、HBase优化"><a href="#4、HBase优化" class="headerlink" title="4、HBase优化"></a>4、HBase优化</h4><p>1）预分区问题<br>region分片？表很大 bigtable</p><p>分布式？数据量大</p><p>region存储数据，如果有多个region，每个region负责维护一部分的rowkey{startrowkey, endrowkey}<br>1<del>10001<br>1</del>2001 1980<br>2001~40002</p><p>分多少片？提前规划好，提高hbase的性能<br>进行存储数据前做好rowkey的预分区优化hbase</p><p>实际操作：<br>create ‘user_p’,’info’,’partition’,SPLITS =&gt;[‘201’,’202’,’203’,’204’]</p><p><strong>Table Regions</strong></p><table><thead><tr><th align="center">Region Server</th><th align="center">Start Key</th><th align="center">End Key</th></tr></thead><tbody><tr><td align="center">hsiehchou123:16020</td><td align="center">-∞</td><td align="center">201</td></tr><tr><td align="center">hsiehchou124:16020</td><td align="center">201</td><td align="center">202</td></tr><tr><td align="center">hsiehchou124:16020</td><td align="center">202</td><td align="center">203</td></tr><tr><td align="center">hsiehchou123:16020</td><td align="center">203</td><td align="center">204</td></tr><tr><td align="center">hsiehchou122:16020</td><td align="center">204</td><td align="center">+∞</td></tr></tbody></table><p>create ‘user_pppp’,’partition’,SPLITS_FILE =&gt; ‘partitions.txt’</p><p>partitions.txt’放在hbase-shell路径下</p><p>2）rowkey如何设计<br>rowkey是数据的唯一标识，这条数据存储在哪个分区由预分区范围决定</p><p>合理设计rowkey<br>如一份数据分为5个region存储<br>但是我们需要尽可能的保持每个region中的数据量差不多</p><p>尽可能的打散数据，平均分配到每个region中即可</p><p>解决方案：<br>生成随机数、hash/散列值<br>原本的rowkey是201，hash后<br>dfgyfugpgdcjhgfd11412nod<br>202变为：<br>21dqddwdgjohfxsovbxiufq12</p><p>字符串拼接：<br>20190316_a3d4<br>20190316_g04f</p><p>反转字符串：<br>201903161-&gt;161309102<br>201903162-&gt;261309102</p><p>3）HBase基础优化<br>HBase用的HDFS存储<br>DataNode允许最大文件打开数<br>默认4096 调大<br>dfs.datanode.max.transfer.threads<br>hdfs-site.xml</p><p>优化等待时间<br>dfs.image.transfer.timeout<br>默认60000毫秒<br>调大</p><p>内存优化：<br>hadoop-env.sh设置内存的堆大小<br>30%~40%最好</p><p>2G<br>512m</p><p>export HADOOP_PORTMAP_OPTS=’-Xmx512m $HADOOP_PORTMAP_OPTS’</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase基础</title>
      <link href="/2019/03/12/hbase-ji-chu/"/>
      <url>/2019/03/12/hbase-ji-chu/</url>
      
        <content type="html"><![CDATA[<h4 id="1、hbase"><a href="#1、hbase" class="headerlink" title="1、hbase"></a>1、hbase</h4><p>google:<br>gfs –&gt; hdfs<br>mapreduce –&gt; mapreduce<br>bigtable –&gt; hbase</p><p>Apache HBase™是Hadoop数据库，是一个分布式，可扩展的大数据存储</p><p>当您需要对大数据进行随机，实时读/写访问时，请使用Apache HBase™。该项目的目标是托管非常大的表 - 数十亿行X百万列 - 在商品硬件集群上。Apache HBase是一个开源的，分布式的，版本化的非关系数据库nosql，模仿Google的Bigtable： Chang等人的结构化数据分布式存储系统。正如Bigtable利用Google文件系统提供的分布式数据存储一样，Apache HBase在Hadoop和HDFS之上提供类似Bigtable的功能</p><h4 id="2、hbase集群角色"><a href="#2、hbase集群角色" class="headerlink" title="2、hbase集群角色"></a>2、hbase集群角色</h4><p>hdfs: NameNode DataNode<br>yarn: ResourceManager NodeManager<br>zookeeper: QuorumPeerMain<br>hbase: HMaster RegionServer</p><p><strong>主从结构</strong><br><strong>HMaster</strong><br>1）对RegionServer监控<br>2）处理一些元数据的变更<br>3）对RegionServer进行故障转移<br>4）空闲时对数据进行负载均衡<br>5）对region进行管理<br>6）发布位置到客户端借助于zookeeper</p><p><strong>RegionServer</strong><br>1）存储hbase实际的数据<br>2）刷新缓存数据到hdfs<br>3）处理Region<br>4）可以进行压缩<br>5）对Hlog进行维护<br>6）对region分片</p><h4 id="3、hbase集群安装部署"><a href="#3、hbase集群安装部署" class="headerlink" title="3、hbase集群安装部署"></a>3、hbase集群安装部署</h4><p>1）需要安装好zookeeper集群</p><p>2）需要安装好hadoop集群<br>hdfs<br>yarn</p><p>3）解压hbase压缩包<br>tar -zxvf hbase-1.3.0-bin.tar.gz</p><p>4）修改配置hbase-env.sh<br>export JAVA_HOME=/root/hd/jdk1.8.0_192<br>export HBASE_MANAGES_ZK=false</p><p>5）配置hbase-site.xml<br>cd /root/hd/hbase-1.3.0<br>vi hbase-site.xml</p><pre><code>&lt;configuration&gt;    &lt;!-- 设置namenode所在位置 通过rootdir设置 也就是设置hdfs中存放的路径 --&gt;    &lt;property&gt;        &lt;name&gt;hbase.rootdir&lt;/name&gt;        &lt;value&gt;hdfs://hsiehchou121:9000/hbase&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 是否开启集群 --&gt;    &lt;property&gt;        &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 0.98 后的新变动，之前版本没有.port,默认端口为 60000 --&gt;    &lt;property&gt;        &lt;name&gt;hbase.master.port&lt;/name&gt;        &lt;value&gt;16000&lt;/value&gt;    &lt;/property&gt;    &lt;!-- zookeeper集群的位置 --&gt;    &lt;property&gt;        &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;        &lt;value&gt;hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181,hsiehchou124:2181&lt;/value&gt;    &lt;/property&gt;    &lt;!-- hbase的元数据信息存储在zookeeper的位置 --&gt;    &lt;property&gt;        &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;        &lt;value&gt;/root/hd/zookeeper-3.4.10/zkData&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>指定从节点regionservers</p><p>vi regionservers</p><p>hsiehchou122<br>hsiehchou123<br>hsiehchou124</p><p>6）解决依赖问题<br>HBase依赖于Hadoop，换成2.8.4的hadoop依赖包<br>hadoop-annotations-2.8.4.jar<br>hadoop-auth-2.8.4.jar<br>hadoop-common-2.8.4.jar<br>hadoop-hdfs-2.8.4.jar<br>hadoop-hdfs-client-2.8.4.jar<br>hadoop-mapreduce-client-app-2.8.4.jar<br>hadoop-mapreduce-client-common-2.8.4.jar<br>hadoop-mapreduce-client-core-2.8.4.jar<br>hadoop-mapreduce-client-hs-2.8.4.jar<br>hadoop-mapreduce-client-hs-plugins-2.8.4.jar<br>hadoop-mapreduce-client-jobclient-2.8.4.jar<br>hadoop-mapreduce-client-jobclient-2.8.4-tests.jar<br>hadoop-mapreduce-client-shuffle-2.8.4.jar<br>hadoop-yarn-api-2.8.4.jar<br>hadoop-yarn-applications-distributedshell-2.8.4.jar<br>hadoop-yarn-applications-unmanaged-am-launcher-2.8.4.jar<br>hadoop-yarn-client-2.8.4.jar<br>hadoop-yarn-common-2.8.4.jar<br>hadoop-yarn-server-applicationhistoryservice-2.8.4.jar<br>hadoop-yarn-server-common-2.8.4.jar<br>hadoop-yarn-server-nodemanager-2.8.4.jar<br>hadoop-yarn-server-resourcemanager-2.8.4.jar<br>hadoop-yarn-server-web-proxy-2.8.4.jar</p><p>zookeeper-3.4.10.jar</p><p>7）软连接core-site.xml hdfs-site.xml<br>ln -s /root/hd/hadoop-2.8.4/etc/hadoop/hdfs-site.xml<br>ln -s /root/hd/hadoop-2.8.4/etc/hadoop/core-site.xml</p><p>8）发送到其他机器<br>scp -r hbase-1.3.0 hsiehchou122:/root/hd<br>scp -r hbase-1.3.0 hsiehchou123:/root/hd<br>scp -r hbase-1.3.0 hsiehchou124:/root/hd</p><p>find /root/hd/hadoop-2.8.4/ -name hadoop-a*</p><p>9）访问ui界面<br><a href="http://192.168.116.121:16010/master-status" target="_blank" rel="noopener">http://192.168.116.121:16010/master-status</a></p><p>启动hbase<br>bin/hbase-daemon.sh start master<br>bin/hbase-daemon.sh start regionserver</p><p>关闭hbase<br>bin/hbase-daemon.sh stop master<br>bin/hbase-daemon.sh stop regionserver</p><h4 id="4、hbase设计架构"><a href="#4、hbase设计架构" class="headerlink" title="4、hbase设计架构"></a>4、hbase设计架构</h4><p>Rowkey行键 类似 id<br>列式存储<br>hbase操作<br>0）启动终端<br>bin/hbase shell</p><p>1）查看表操作<br>list</p><p>2）显示当前服务器状态<br>status ‘hsiehchou121’<br>1 active master, 0 backup masters, 3 servers, 0 dead, 0.5000 ave<br>rage load<br>1 active master: 1个存活的master<br>0 backup masters: 0个备份master<br>3 servers: 3个regionserver<br>0 dead: 没有挂掉的<br>0.5000 average load：平均加载</p><p>3）显示当前用户<br>whoami</p><p>4）创建表<br>create ‘表名’,’列族’<br>create ‘user’,’info1’</p><p>5）添加数据<br>put ‘表名’,’rowkey’,’列族:列’,’值’<br>put ‘user’,’1001’,’info1:name’,’xie’<br>put ‘user’,’1001’,’info1:age’,’19’</p><p><strong>删除需要ctrl+&lt;-</strong></p><p>6）全表扫描<br>scan ‘表名’<br>scan ‘user’</p><p>ROW COLUMN+CELL<br>1001 column=info1:age, timestamp=1552579563486, value=19<br>1001 column=info1:name, timestamp=1552579531260, value=xie</p><p>7）hbase没有修改，只有覆盖<br>put ‘user’,’1001’,’info1:name’,’mi’<br>只要对应上表名、rowkey、列族、列即可</p><p>8）查看表结构<br>describe ‘user’</p><p>9）变更表结构信息<br>alter ‘user’,{NAME =&gt; ‘info1’,VERSIONS=&gt;’8’}</p><p>10）查看指定的数据信息<br>指定具体的rowkey<br>get ‘user’,’1001’<br>指定具体的列<br>get ‘user’,’1001’,’info1:name’</p><p>11）清空表<br>truncate ‘user1’</p><p>12）删除表<br>需要先指定不可用<br>disable ‘表名’<br>drop ‘表名’<br>disable ‘user1’<br>drop ‘user1’</p><p>13）扫描指定范围<br>指定从某一rowkey扫描<br>scan ‘user’,{STARTROW =&gt; ‘1002’}</p><p>包含头不包含尾（1001保留，1002不扫描）<br>scan ‘user’,{STARTROW =&gt; ‘1001’, STOPROW =&gt; ‘1002’}</p><p>14）统计rowkey的个数<br>count ‘user’</p><p>15）退出hbase shell<br>quit</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban</title>
      <link href="/2019/03/08/azkaban/"/>
      <url>/2019/03/08/azkaban/</url>
      
        <content type="html"><![CDATA[<h4 id="1、Azkaban"><a href="#1、Azkaban" class="headerlink" title="1、Azkaban"></a>1、Azkaban</h4><p>官网：<a href="https://azkaban.github.io/" target="_blank" rel="noopener">https://azkaban.github.io/</a><br>Azkaban是一款开源工作流管理器</p><p>Azkaban是在LinkedIn上创建的批处理工作流作业调度程序，用于运行Hadoop作业</p><p>Azkaban通过作业依赖性解决订单，并提供易于使用的Web用户界面来维护和跟踪您的工作流程</p><p>工作流作业：<br>Flume-&gt;HDFS-&gt;MR-&gt;Hive建表-&gt;导入load data脚本<br>自动化调度</p><h4 id="2、Azkaban安装部署"><a href="#2、Azkaban安装部署" class="headerlink" title="2、Azkaban安装部署"></a>2、Azkaban安装部署</h4><p>1）解压<br>首先将压缩包放进/root/hd/azkaban里面<br>azkaban-executor-server-2.5.0.tar.gz –&gt;executor<br>azkaban-sql-script-2.5.0.tar.gz –&gt;azkaban-2.5.0<br>azkaban-web-server-2.5.0.tar.gz –&gt;server<br>tar -zxvf *.tar.gz</p><p>2）进入MySQL创建Azkaban库，然后将解压好的脚本导入<br>create database azkaban;<br>use azkaban;<br>source /root/hd/azkaban/azkaban-2.5.0/create-all-sql-2.5.0.sql;</p><p>3）生成证书（https生成器）<br>keytool -keystore keystore -alias jetty -genkey -keyalg RSA<br>（回车，不用填，到那个CN=Unknown那行下面有个判断，输入y，后面一行继续回车）<br>将keystore移动到server文件夹下</p><p>4）时间同步配置<br>任务调度，所以和本地时间保持一致<br>开启交互窗口：<br>sudo date -s ”<br>hwclock -w</p><p>5）修改server端配置文件<br>cd /root/hd/azkaban/server/conf<br>vi azkaban.properties</p><pre><code># Azkaban Personalization Settingsazkaban.name=Testazkaban.label=My Local Azkabanazkaban.color=#FF3601azkaban.default.servlet.path=/indexweb.resource.dir=web/# 时区default.timezone.id=Asia/Shanghai#Azkaban UserManager class# 用户权限管理默认类user.manager.class=azkaban.user.XmlUserManager# 用户配置user.manager.xml.file=conf/azkaban-users.xml# Loader for projects#配置文件所在位置executor.global.properties=conf/global.propertiesazkaban.project.dir=projects# azkaban目前只支持mysqldatabase.type=mysqlmysql.port=3306# 当前主机名mysql.host=hsiehchou121mysql.database=azkabanmysql.user=rootmysql.password=root# 最大连接数mysql.numconnections=100# Velocity dev modevelocity.dev.mode=false# Azkaban Jetty server properties.# 最大线程数jetty.maxThreads=25jetty.ssl.port=8443jetty.port=8081jetty.keystore=keystorejetty.password=123456jetty.keypassword=123456jetty.truststore=keystorejetty.trustpassword=123456# Azkaban Executor settingsexecutor.port=12321# mail settingsmail.sender=@qq.commail.host=smtp.qq.comjob.failure.email=job.success.email=lockdown.create.projects=falsecache.directory=cache</code></pre><p><strong>azkaban-users.xml</strong></p><pre><code>&lt;azkaban-users&gt;    &lt;user username=&quot;azkaban&quot; password=&quot;azkaban&quot; roles=&quot;admin&quot; groups=&quot;azkaban&quot; /&gt;    &lt;user username=&quot;metrics&quot; password=&quot;metrics&quot; roles=&quot;metrics&quot;/&gt;    &lt;!--增加这一行  role管理员权限:admin--&gt;    &lt;user username=&quot;admin&quot; password=&quot;admin&quot; roles=&quot;admin,metrics&quot;/&gt;     &lt;role name=&quot;admin&quot; permissions=&quot;ADMIN&quot; /&gt;    &lt;role name=&quot;metrics&quot; permissions=&quot;METRICS&quot;/&gt;&lt;/azkaban-users&gt;</code></pre><p>6）修改excutor端配置文件 </p><p><strong>azkaban.properties</strong></p><pre><code># Azkaban# 时区default.timezone.id=Asia/Shanghai# Azkaban JobTypes Plugins# 插件azkaban.jobtype.plugin.dir=plugins/jobtypes#Loader for projectsexecutor.global.properties=conf/global.propertiesazkaban.project.dir=projectsdatabase.type=mysqlmysql.port=3306mysql.host=hsiehchou121mysql.database=azkabanmysql.user=rootmysql.password=rootmysql.numconnections=100# Azkaban Executor settings# 最大线程数executor.maxThreads=50executor.port=12321executor.flow.threads=30</code></pre><h4 id="3、Azkaban实战"><a href="#3、Azkaban实战" class="headerlink" title="3、Azkaban实战"></a>3、Azkaban实战</h4><p>HDFS-&gt;Hive建表-&gt;导入</p><p>1）job1.job</p><p><strong>job1.job</strong><br>type=command<br>command=echo ‘Hello World!’<br>打包成zip包上传到azkaban，执行</p><p>2）job2(a.job和b.job) </p><p><strong>a.job</strong><br>type=command<br>command=echo ‘li’</p><p><strong>b.job</strong><br>type=command<br>dependencies=a<br>command=echo ‘666’<br>打包成zip包上传到azkaban，执行</p><p>3）startyarn.job</p><p><strong>startyarn.job</strong><br>type=command<br>command=/root/hd/hadoop-2.8.4/sbin/start-yarn.sh<br>打包成zip包上传到azkaban，执行</p><p>4）mapreduce.job</p><p><strong>mapreduce.job</strong><br>type=command<br>command=/root/hd/hadoop-2.8.4/bin/hadoop jar hadoop-mapreduce-examples-2.8.4.jar wordcount /wc /wc/out<br>打包成zip包上传到azkaban，执行</p><p>5）Hive操作 </p><p><strong>hive.sql</strong><br>use default;<br>create table azhive(id int, name string) row format delimited fields terminated by ‘\t’;<br>load data inpath ‘/hsiehchou.txt’ into table azhive;</p><p><strong>hivef.job</strong><br>type=command<br>command=/root/hd/hive/bin/hive -f ‘hive.sql’</p><p>打包成zip包上传到Azkaban，执行</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop</title>
      <link href="/2019/03/04/sqoop/"/>
      <url>/2019/03/04/sqoop/</url>
      
        <content type="html"><![CDATA[<h4 id="1、Sqoop"><a href="#1、Sqoop" class="headerlink" title="1、Sqoop"></a>1、Sqoop</h4><p>Flume数据采集 采集日志数据<br>Sqoop数据迁移 HDFS-&gt;MySQL<br>Azkaban任务调度 Flume-&gt;HDFS-&gt;Shell-&gt;Hive-&gt;SQL-&gt;BI</p><p>Sqoop数据迁移=MapReduce<br>处理离线数据<br>整个过程就是数据导入处理导出过程<br>直接使用map</p><p>Sqoop作用：简化开发<br>MySQL-&gt;HDFS<br>MapReduce<br>Sqoop!</p><h4 id="2、概述"><a href="#2、概述" class="headerlink" title="2、概述"></a>2、概述</h4><p>Apache Sqoop（TM）是一种工具，用于在Apache Hadoop和结构化数据存储（如关 系数据库）之间高效传输批量数据 。数据迁移！ </p><p>Sqoop于2012年3月成功从孵化器毕业，现在是一个顶级Apache项目： 更多信息</p><h4 id="3、Sqoop安装部署"><a href="#3、Sqoop安装部署" class="headerlink" title="3、Sqoop安装部署"></a>3、Sqoop安装部署</h4><p>1）下载</p><p>2）上传</p><p>3）解压</p><p>4）重命名<br>mv sqoop-env-template.sh sqoop-env.sh</p><p>5）添加配置信息<br>export HADOOP_COMMON_HOME=/root/hd/hadoop-2.8.4<br>export HADOOP_MAPRED_HOME=/root/hd/hadoop-2.8.4<br>export HIVE_HOME=/root/hd/hive<br>export ZOOCFGDIR=/root/hd/zookeeper-3.4.10/conf</p><p>6）启动查看版本号<br>bin/sqoop version</p><h4 id="4、Sqoop的import导入"><a href="#4、Sqoop的import导入" class="headerlink" title="4、Sqoop的import导入"></a>4、Sqoop的import导入</h4><p>import导入：MySQL-&gt;HDFS<br>export导出：HDFS-&gt;MySQL<br>MySQL-&gt;HDFS操作：<br>1）导入mysql驱动到sqoop/lib下<br>2）命令操作<br>mysql&gt; create database sqoop;<br>mysql&gt; use sqoop;<br>mysql&gt; create table user(id int primary key auto_increment,name varchar(50),addr varchar(300));</p><pre><code>[root@hsiehchou121 sqoop]# bin/sqoop import \&gt; --connect jdbc:mysql://hsiehchou121:3306/sqoop \&gt; --username root \&gt; --password root \&gt; --table user \&gt; --target-dir /sqoop/datas \&gt; --num-mappers 1 \&gt; --fields-terminated-by &quot;\t&quot;</code></pre><p><strong>注意：如果显示mysql的访问权限问题，需要设置mysql的用户权限：所在库 mysql库的user表</strong></p><p>update user set host=’%’ where host=’localhost’;<br>delete from user where Host=’127.0.0.1’;<br>delete from user where Host=’hsiehchou121’;<br>delete from user where Host=’::1’;<br>flush privileges;</p><p><strong>使用query对数据进行过滤</strong></p><pre><code>[root@hsiehchou121 sqoop]# bin/sqoop import \&gt; --connect jdbc:mysql://hsiehchou121:3306/sqoop \&gt; --username root \&gt; --password root \&gt; --target-dir /sqoop/selectimport \&gt; --num-mappers 1 \&gt; --fields-terminated-by &quot;\t&quot; \&gt; --query &#39;select * from user where id&lt;=1 and $CONDITIONS&#39;</code></pre><p><strong>直接过滤字段</strong></p><pre><code>[root@hsiehchou121 sqoop]# bin/sqoop import \&gt; --username root \&gt; --password root \&gt; --connect jdbc:mysql://hsiehchou121:3306/sqoop \&gt; --target-dir /sqoop/selectimport1 \&gt; --num-mappers 1 \&gt; --table user \&gt; --columns addr</code></pre><h4 id="5、MySQL导入到Hive"><a href="#5、MySQL导入到Hive" class="headerlink" title="5、MySQL导入到Hive"></a>5、MySQL导入到Hive</h4><p>在~/.bash_profile里面增加下面配置<br>export HADOOP_CLASSPATH=<code>$HADOOP_CLASSPATH:/root/hd/hive/lib/*</code><br>export HADOOP_USER_HOME=root</p><pre><code>[root@hsiehchou121 sqoop]# bin/sqoop import \&gt; --connect jdbc:mysql://hsiehchou121:3306/sqoop \&gt; --username root \&gt; --password root \&gt; --table user \&gt; --num-mappers 1 \&gt; --hive-import \&gt; --fields-terminated-by &quot;\t&quot; \&gt; --hive-overwrite \&gt; --hive-table user_sqoop</code></pre><h4 id="6、Sqoop的export命令"><a href="#6、Sqoop的export命令" class="headerlink" title="6、Sqoop的export命令"></a>6、Sqoop的export命令</h4><p>Hive-&gt;MySQL<br>Hive导出到MySQL<br>首先清空mysql里面的user：truncate table user;</p><pre><code>[root@hsiehchou121 sqoop]# bin/sqoop export \&gt; --connect jdbc:mysql://hsiehchou121:3306/sqoop \&gt; --username root \&gt; --password root \&gt; --table user \&gt; --num-mappers 1 \&gt; --export-dir /user/hive/warehouse/user_sqoop \&gt; --input-fields-terminated-by &quot;\t&quot;</code></pre><h4 id="7、常用参数"><a href="#7、常用参数" class="headerlink" title="7、常用参数"></a>7、常用参数</h4><p>import ：导入数据到集群<br>export ：从集群导出数据<br>create-hive-table ：创建Hive表<br>import-all-tables ：指定关系型数据库所有表到HDFS集群<br>list-databases ：列出所有数据库<br>list-tables ：列出所有数据库表<br>merge ：合并hdfs中的不同目录下的数据<br>codegen ：获取某张表数据生成JavaBean 并打包</p><pre><code>[root@hsiehchou121 sqoop]# bin/sqoop codegen \&gt; --connect jdbc:mysql://hsiehchou121:3306/sqoop \&gt; --username root \&gt; --password root \&gt; --table user \&gt; --bindir /root/sqoopjar/UserBean \&gt; --class-name UserBean \&gt; --fields-terminated-by &quot;\t&quot;</code></pre><pre><code>[root@hsiehchou121 sqoop]# bin/sqoop list-databases \&gt; --connect jdbc:mysql://hsiehchou121:3306/ \&gt; --username root \&gt; --password root</code></pre><pre><code>[root@hsiehchou121 sqoop]# bin/sqoop merge \&gt; --new-data /testmerge/new/ \&gt; --onto /testmerge/old/ \&gt; --target-dir /testmerge/merged table user \&gt; --jar-file /root/sqoopjar/UserBean/UserBean.jar \&gt; --class-name UserBean \&gt; --merge-key id</code></pre><p><strong>注意</strong>：<br>merge操作是一个新表替代旧表的操作，如果有冲突id的话新表数据替换旧表数据，如果没有冲突则是新表数据添加到旧表的数据</p><p>用户画像 merge<br>身高180 体重70 爱好 …..<br>身高180 体重90 爱好….</p><p>广告大数据 提高销量 广告推送更加精准<br>工业大数据 flink面试</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume</title>
      <link href="/2019/03/02/flume/"/>
      <url>/2019/03/02/flume/</url>
      
        <content type="html"><![CDATA[<h4 id="1、Flume"><a href="#1、Flume" class="headerlink" title="1、Flume"></a>1、Flume</h4><p><strong>概述</strong>：<br>Flume是一种分布式，可靠且可用的服务，用于有效地收集，聚合和移动大量日志数据。它具有基于流数据流的简单灵活的架构。它具有可靠的可靠性机制和许多故障转移和恢复机制，具有强大的容错性。它使用简单的可扩展数据模型，允许在线分析应用程序<br>1）数据采集（爬虫\日志数据\Flume）<br>2）数据存储（HDFS/Hive/HBase(NoSQL)）<br>3）数据计算（MapReduce/Hive/SparkSQL/SparkStreaming/Flink）<br>4）数据可视化</p><h4 id="2、Flume角色"><a href="#2、Flume角色" class="headerlink" title="2、Flume角色"></a>2、Flume角色</h4><p>1）Source<br>数据源，用户采集数据，Source产生数据流，同时会把产生的数据流传输到Channel</p><p>2）Channel<br>传输通道，用于桥接Source和Sink</p><p>3）Sink<br>下沉，用于收集Channel传输的数据，将数据源传递到目标源</p><p>4）Agent<br>在Flume中使用事件作为传输的基本单元</p><h4 id="3、Flume使用"><a href="#3、Flume使用" class="headerlink" title="3、Flume使用"></a>3、Flume使用</h4><p>简单易用，只需要写配置文件即可</p><h4 id="4、Flume安装配置"><a href="#4、Flume安装配置" class="headerlink" title="4、Flume安装配置"></a>4、Flume安装配置</h4><p>1）下载Flume<br>2）上传到Linux<br>3）解压<br>tar -zxvf apache-flume-1.6.0-bin.tar.gz -C /root/hd </p><p>4）重命名<br>mv apache-flume-1.6.0-bin/ flume<br>cp flume-env.sh.template flume-env.sh </p><p>5）修改配置<br>vi flume-env.sh<br>export JAVA_HOME=/root/hd/jdk1.8.0_192</p><h4 id="5、Flume监听端口"><a href="#5、Flume监听端口" class="headerlink" title="5、Flume监听端口"></a>5、Flume监听端口</h4><p>启动命令：<br>bin/flume-ng agent <code>--conf</code> conf/log4j.properties <code>--name</code> a1 <code>--conf-file</code> conf/flumejob_telnet.conf</p><p><strong>我已经排坑了，这里我建议–conf 后面指定的路径建议是全路径，指定到log4j.properties或，我当时老师讲的是直接conf/，我实际操作是有问题的，不能实时的反馈</strong></p><pre><code>bin/flume-ng agent 使用ng启动agent--conf conf/log4j.properties 指定配置所在的文件夹--name a1 指定的agent别名--conf-file conf/flumejob_telnet.conf 文件-Dflume.root.logger=INFO,console 日志级别的反馈**</code></pre><p><strong>flumejob_telnet.conf</strong></p><pre><code>#smple.conf: A single-node Flume configuration# Name the components on this agent 定义变量方便调用 加s可以有多个此角色a1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the source 描述source角色 进行内容定制# 此配置属于tcp source 必须是netcat类型a1.sources.r1.type = netcat a1.sources.r1.bind = localhosta1.sources.r1.port = 44444# Describe the sink 输出日志文件a1.sinks.k1.type = logger# Use a channel which buffers events in memory（file） 使用内存 总大小1000 每次传输100a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channel 一个source可以绑定多个channel # 一个sinks可以只能绑定一个channel  使用的是图二的模型a1.sources.r1.channels = c1a1.sinks.k1.channel = c1</code></pre><pre><code>[root@hsiehchou121 flume]# bin/flume-ng agent \&gt; --conf conf/ \&gt; --name a1 \&gt; --conf-file conf/flumejob_telnet.conf \&gt; -Dflume.root.logger=INFO.console</code></pre><p><strong>yum search telnet</strong> </p><p><strong>yum install telnet.x86_64</strong></p><h4 id="6、flume监听本地linux文件采集到hdfs"><a href="#6、flume监听本地linux文件采集到hdfs" class="headerlink" title="6、flume监听本地linux文件采集到hdfs"></a>6、flume监听本地linux文件采集到hdfs</h4><p>启动命令：<br>bin/flume-ng agent <code>--conf</code> conf/log4j.properties <code>--name</code> a1 <code>--conf-file</code> conf/flumejob_hdfs.conf</p><p><strong>flumejob_hdfs.conf</strong></p><pre><code># Name the components on this agent agent别名设置a1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the source  设置数据源监听本地文件配置# exec 执行一个命令的方式去查看文件 tail -F 实时查看a1.sources.r1.type = exec# 要执行的脚本command tail -F 默认10行 man tail  查看帮助a1.sources.r1.command = tail -F /tmp/root/hive.log# 执行这个command使用的是哪个脚本 -c 指定使用什么命令# whereis bash# bash: /usr/bin/bash /usr/share/man/man1/bash.1.gz a1.sources.r1.shell = /usr/bin/bash -c# Describe the sink a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://hsiehchou121:9000/flume/%Y%m%d/%H#上传文件的前缀a1.sinks.k1.hdfs.filePrefix = logs-#是否按照时间滚动文件夹a1.sinks.k1.hdfs.round = true#多少时间单位创建一个新的文件夹  秒 （默认30s）a1.sinks.k1.hdfs.roundValue = 1#重新定义时间单位（每小时滚动一个文件夹）a1.sinks.k1.hdfs.roundUnit = minute#是否使用本地时间戳a1.sinks.k1.hdfs.useLocalTimeStamp = true#积攒多少个 Event 才 flush 到 HDFS 一次a1.sinks.k1.hdfs.batchSize = 500#设置文件类型，可支持压缩a1.sinks.k1.hdfs.fileType = DataStream#多久生成一个新的文件 秒a1.sinks.k1.hdfs.rollInterval = 30#设置每个文件的滚动大小 字节（最好128M,合理）a1.sinks.k1.hdfs.rollSize = 134217700#文件的滚动与 Event 数量无关a1.sinks.k1.hdfs.rollCount = 0#最小冗余数(备份数 生成滚动功能则生效roll hadoop本身有此功能 无需配置) 1份 不冗余 hdfs已经备份3份a1.sinks.k1.hdfs.minBlockReplicas = 1# Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1</code></pre><pre><code>[root@hsiehchou121 flume]# bin/flume-ng agent \&gt; --conf conf/log4j.properties \&gt; --name a1 \&gt; --conf-file conf/flumejob_hdfs.conf</code></pre><h4 id="7、监听文件夹"><a href="#7、监听文件夹" class="headerlink" title="7、监听文件夹"></a>7、监听文件夹</h4><p><strong>flumejob_dir.conf</strong></p><pre><code># 定义别名a1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = spooldir# 监控的文件夹a1.sources.r1.spoolDir = /root/testdir# 上传成功后显示后缀名 a1.sources.r1.fileSuffix = .COMPLETED# 如论如何 加绝对路径的文件名 默认falsea1.sources.r1.fileHeader = true#忽略所有以.tmp 结尾的文件（正在被写入），不上传# ^以任何开头 出现无限次 以.tmp结尾的a1.sources.r1.ignorePattern = ([^ ]*\.tmp)# Describe the sink 下沉到hdfsa1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://hsiehchou121:9000/flume/testdir/%Y%m%d/%H#上传文件的前缀a1.sinks.k1.hdfs.filePrefix = testdir-#是否按照时间滚动文件夹a1.sinks.k1.hdfs.round = true#多少时间单位创建一个新的文件夹a1.sinks.k1.hdfs.roundValue = 1#重新定义时间单位a1.sinks.k1.hdfs.roundUnit = hour#是否使用本地时间戳a1.sinks.k1.hdfs.useLocalTimeStamp = true#积攒多少个 Event 才 flush 到 HDFS 一次a1.sinks.k1.hdfs.batchSize = 100#设置文件类型，可支持压缩a1.sinks.k1.hdfs.fileType = DataStream#多久生成一个新的文件a1.sinks.k1.hdfs.rollInterval = 600#设置每个文件的滚动大小大概是 128M a1.sinks.k1.hdfs.rollSize = 134217700#文件的滚动与 Event 数量无关a1.sinks.k1.hdfs.rollCount = 0#最小副本数a1.sinks.k1.hdfs.minBlockReplicas = 1# Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1 a1.sinks.k1.channel = c1</code></pre><p>[root@hsiehchou121 conf]# bin/flume-ng agent <code>--conf</code> conf/log4j.properties <code>--name</code> a1 <code>--conf-file</code> conf/flumejob_dir.conf</p><pre><code>[root@hsiehchou121 flume]# bin/flume-ng agent \&gt; --conf conf/log4j.properties \&gt; --name a1 \&gt; --conf-file conf/flumejob_dir.conf </code></pre><h4 id="8、多个channel-sink"><a href="#8、多个channel-sink" class="headerlink" title="8、多个channel/sink"></a>8、多个channel/sink</h4><p>需求：监控hive.log文件，用同时产生两个channel，一个channel对应的sink存储到hdfs中，另外一个channel对应的sink存储到本地 </p><p><strong>flumejob_1.conf</strong></p><pre><code># name the components on this agent 别名设置a1.sources = r1a1.sinks = k1 k2 a1.channels = c1 c2# 将数据流复制给多个 channela1.sources.r1.selector.type = replicating# Describe/configure the source a1.sources.r1.type = execa1.sources.r1.command = tail -F /tmp/root/hive.loga1.sources.r1.shell = /bin/bash -c# Describe the sink# 分两个端口发送数据 a1.sinks.k1.type = avro a1.sinks.k1.hostname = hsiehchou121 a1.sinks.k1.port = 4141a1.sinks.k2.type = avro a1.sinks.k2.hostname = hsiehchou121 a1.sinks.k2.port = 4142# Describe the channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.channels.c2.type = memory a1.channels.c2.capacity = 1000a1.channels.c2.transactionCapacity = 100# Bind the source and sink to the channel a1.sources.r1.channels = c1 c2 a1.sinks.k1.channel = c1a1.sinks.k2.channel = c2</code></pre><p>[root@hsiehchou121 flume]# bin/flume-ng agent <code>--conf</code> conf/log4j.properties <code>--name</code> a1 <code>--conf-file</code> conf/flumejob_1.conf</p><p><strong>flumejob_2.conf</strong></p><pre><code># Name the components on this agent a2.sources = r1a2.sinks = k1 a2.channels = c1# Describe/configure the sourcea2.sources.r1.type = avro # 端口抓取数据a2.sources.r1.bind = hsiehchou121a2.sources.r1.port = 4141# Describe the sink a2.sinks.k1.type = hdfsa2.sinks.k1.hdfs.path = hdfs://hsiehchou121:9000/flume2/%Y%m%d/%H#上传文件的前缀a2.sinks.k1.hdfs.filePrefix = flume2-#是否按照时间滚动文件夹a2.sinks.k1.hdfs.round = true#多少时间单位创建一个新的文件夹a2.sinks.k1.hdfs.roundValue = 1#重新定义时间单位a2.sinks.k1.hdfs.roundUnit = hour#是否使用本地时间戳a2.sinks.k1.hdfs.useLocalTimeStamp = true#积攒多少个 Event 才 flush 到 HDFS 一次a2.sinks.k1.hdfs.batchSize = 100#设置文件类型，可支持压缩a2.sinks.k1.hdfs.fileType = DataStream#多久生成一个新的文件a2.sinks.k1.hdfs.rollInterval = 600#设置每个文件的滚动大小大概是 128M a2.sinks.k1.hdfs.rollSize = 134217700#文件的滚动与 Event 数量无关a2.sinks.k1.hdfs.rollCount = 0#最小副本数a2.sinks.k1.hdfs.minBlockReplicas = 1# Describe the channel a2.channels.c1.type = memory a2.channels.c1.capacity = 1000a2.channels.c1.transactionCapacity = 100# Bind the source and sink to the channel a2.sources.r1.channels = c1a2.sinks.k1.channel = c1</code></pre><p>[root@hsiehchou121 flume]# bin/flume-ng agent <code>--conf</code> conf/log4j.properties <code>--name</code> a2 <code>--conf-file</code> conf/flumejob_1.conf</p><p><strong>flumejob_3.conf</strong></p><pre><code># Name the components on this agent a3.sources = r1a3.sinks = k1 a3.channels = c1# Describe/configure the source a3.sources.r1.type = avroa3.sources.r1.bind = hsiehchou121a3.sources.r1.port = 4142# Describe the sink a3.sinks.k1.type = file_rolla3.sinks.k1.sink.directory = /root/flume2# Describe the channel a3.channels.c1.type = memory a3.channels.c1.capacity = 1000a3.channels.c1.transactionCapacity = 100# Bind the source and sink to the channel a3.sources.r1.channels = c1a3.sinks.k1.channel = c1</code></pre><p>[root@hsiehchou121 flume]# bin/flume-ng agent <code>--conf</code> conf/log4j.properties <code>--name</code> a3 <code>--conf-file</code> conf/flumejob_1.conf</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive函数&amp;压缩</title>
      <link href="/2019/02/28/hive-han-shu-ya-suo/"/>
      <url>/2019/02/28/hive-han-shu-ya-suo/</url>
      
        <content type="html"><![CDATA[<h4 id="1、排序"><a href="#1、排序" class="headerlink" title="1、排序"></a>1、排序</h4><p>Order By:全局排序<br>1）按照员工表的奖金金额进行正序排序<br>select * from emptable order by emptable.comm asc;<br>可以省略asc</p><p>2）按照员工表的奖金金额进行倒序排序<br>select * from emptable order by emptable.comm desc;</p><p>3）按照部门和奖金进行升序排序<br>select * from emptable order by deptno,comm;</p><p>Sort By: <strong>内部排序（区内有序，全局无序）</strong><br>设置reduce个数的属性：set mapreduce.job.reduces = 3;<br>select * from dept_partitions sort by deptno desc;</p><p>Distribute By: <strong>分区排序</strong><br>1）先按照部门编号进行排序再按照地域编号进行降序排序。<br>select * from dept_partitions distribute by deptno sort by loc desc;</p><p>Cluster By: <strong>分桶排序</strong><br>1）按照部门编号进行排序<br>select * from dept_partitions cluster by deptno;</p><p><strong>注意</strong>：如果Distrbute和Sort by 是相同字段时，可以用cluster by代替</p><h4 id="2、分桶"><a href="#2、分桶" class="headerlink" title="2、分桶"></a>2、分桶</h4><p>分桶分的是文件<br>1）创建分桶表<br>clustered by(id) into 4 buckets</p><pre><code>hive&gt; set mapreduce.job.reduces=4;hive&gt; create table emptable_buck(id int, name string)    &gt; clustered by(id) into 4 buckets    &gt; row format    &gt; delimited fields    &gt; terminated by &#39;\t&#39;;</code></pre><p><strong>查看表的描述信息</strong></p><p>hive&gt; desc formatted emptable_buck;</p><p><strong>加载数据</strong></p><pre><code>hive&gt; load data local inpath &#39;/root/hsiehchou.txt&#39; into table emptable_buck;hive&gt; create table emptable_b(id int, name string)    &gt; row format    &gt; delimited fields    &gt; terminated by &#39;\t&#39;;</code></pre><p><strong>清空表</strong></p><pre><code>hive&gt; truncate table emptable_buck;</code></pre><p><strong>加载数据（桶）</strong></p><pre><code>hive&gt; load data local inpath &#39;/root/hsiehchou.txt&#39; into table emptable_b;</code></pre><p><strong>设置桶的环境变量(插入数据时分桶，不开启默认在一个桶里面)</strong></p><pre><code>hive&gt; set hive.enforce.bucketing=true;hive&gt; truncate table emptable_buck;</code></pre><p>用户需要统计一个具有代表性的结果时，并不是全部结果！抽样！<br>(bucket 1 out of 2 on id）<br>1：第一桶数据<br>2：代表拿两桶</p><pre><code>hive&gt; select * from emptable_buck  tablesample(bucket 1 out of 2 on id);</code></pre><h4 id="3、UDF自定义函数"><a href="#3、UDF自定义函数" class="headerlink" title="3、UDF自定义函数"></a>3、UDF自定义函数</h4><p><strong>查看内置函数</strong><br>show functions; </p><p><strong>查看函数的详细内容</strong><br>desc function extended upper;</p><p>UDF:一进一出<br>UDAF:聚合函数 多进一出 count /max/avg<br>UDTF:一进多出</p><p><strong>java</strong><br>导入Hive的lib下的所有jar包<br>编程java代码</p><pre><code>package com.hsiehchou;import org.apache.hadoop.hive.ql.exec.UDF;public class MyConcat extends UDF {    //将大写转换成小写    public String evaluate(String a, String b) {        return a + &quot;******&quot; + String.valueOf(b);    }   }</code></pre><p>export此文件，打包jar，放入hsiehchou121中</p><p>添加临时：<br>add jar /root/Myconcat.jar;<br>create temporary function my_cat as “com.hsiehchou.MyConcat”;</p><pre><code>&lt;!-- 注册永久：hive-site.xml --&gt;&lt;property&gt;&lt;name&gt;hive.aux.jars.path&lt;/name&gt;&lt;value&gt;file:///root/hd/hive/lib/hive.jar&lt;/value&gt;&lt;/property&gt;</code></pre><h4 id="4、Hive压缩"><a href="#4、Hive压缩" class="headerlink" title="4、Hive压缩"></a>4、Hive压缩</h4><p>存储：hdfs<br>计算：mapreduce</p><p><strong>Map输出阶段压缩方式</strong><br>开启hive中间传输数据压缩功能<br>set hive.exec.compress.intermediate=true;</p><p><strong>开启map输出压缩</strong><br>set mapreduce.map.output.compress=true;</p><p><strong>设置snappy压缩方式</strong><br>set mapreduce.map.output.compress.codec=org.apache.hadoop.io.com<br>press.SnappyCodec;</p><p><strong>Reduce输出阶段压缩方式</strong><br>设置hive输出数据压缩功能<br>set hive.exec.compress.output=true;</p><p><strong>设置mr输出数据压缩</strong><br>set mapreduce.output.fileoutputformat.compress=true;</p><p><strong>指定压缩编码</strong><br>set mapreduce.output.fileoutputformat.compress.codec=org.apache.<br>hadoop.io.compress.SnappyCodec;</p><p><strong>指定压缩类型块压缩</strong><br>set mapreduce.output.fileoutputformat.compress.type=BLOCK;</p><p><strong>测试结果</strong><br>insert overwrite local directory ‘/root/datas/rs’ select * from emptable order by sal desc;</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的SQL操作</title>
      <link href="/2019/02/27/hive-de-sql-cao-zuo/"/>
      <url>/2019/02/27/hive-de-sql-cao-zuo/</url>
      
        <content type="html"><![CDATA[<h3 id="1、分区表"><a href="#1、分区表" class="headerlink" title="1、分区表"></a>1、分区表</h3><h4 id="1）创建分区表"><a href="#1）创建分区表" class="headerlink" title="1）创建分区表"></a>1）创建分区表</h4><pre><code>hive&gt; create table dept_partitions()      &gt; partition by()      &gt; row format      &gt; delimited fields      &gt; terminated by &#39;&#39;;</code></pre><p>例：</p><pre><code>hive&gt; create table dept_partitions(deptno int, dept string, loc string)    &gt; partitioned by(day string)    &gt; row format    &gt; delimited fields    &gt; terminated by &#39;\t&#39;;hive&gt; load data local inpath &#39;/root/dept.txt&#39; into table dept_partitions    &gt; partition(day=&#39;0228&#39;);</code></pre><h4 id="2）查询"><a href="#2）查询" class="headerlink" title="2）查询"></a>2）查询</h4><p><strong>全查询</strong><br>hive&gt; select * from dept_partitions;<br>注意：此时查看的是整个分区表中的数据</p><p><strong>单分区查询</strong><br>hive&gt; select * from dept_partitions where day = ‘0228’;<br>注意：此时查看的是指定分区中的数据</p><p><strong>联合查询</strong><br>hive&gt; select * from dept_partitions where day = ‘0228’ union select * from dept_partitions where day = ‘0302’;</p><p><strong>添加单个分区</strong><br>hive&gt; alter table dept_partitions add partition(day = ‘0303’); </p><p><strong>注意</strong>：如果想一次添加多个的话 空格分割即可<br>hive&gt; alter table dept_partitions add partition(day = ‘0304’) partition(day = ‘0305’);</p><p><strong>查看分区</strong><br>hive&gt; show partitions dept_partitions;</p><p><strong>删除分区</strong><br>hive&gt; alter table dept_partitions drop partition(day=’0305’);<br>分区表在hdfs中分目录文件夹</p><p>hive&gt; dfs -mkdir -p /user/hive/warehouse/dept_partitions/day=0305;</p><p>hive&gt; dfs -put /root/dept.txt /user/hive/warehouse/dept_partitions/day=0305;</p><p>hive&gt; show partitions dept_partitions;<br>此时并没有day=0305，需要进行下面操作</p><p><strong>导入数据</strong><br>相当于修复数据：msck repair table dept_partitions;</p><h3 id="2、DML数据操作"><a href="#2、DML数据操作" class="headerlink" title="2、DML数据操作"></a>2、DML数据操作</h3><h4 id="1）数据的导入"><a href="#1）数据的导入" class="headerlink" title="1）数据的导入"></a>1）数据的导入</h4><p>hive&gt; load data [local] inpath ” into table ;</p><h4 id="2）向表中插入数据"><a href="#2）向表中插入数据" class="headerlink" title="2）向表中插入数据"></a>2）向表中插入数据</h4><p>hive&gt; insert into table student_partitions partition(age = 20)  values(1,’re’);<br>向表中插入sql查询结果数据<br>hive&gt; insert overwrite table student_partitions partition(age = 20) select * from hsiehchou where id&lt;3;</p><p>create方式：<br>hive&gt; create table if not exists student_partitions1 as select * from student_partitions where id = 2;</p><h4 id="3）创建表直接加载数据"><a href="#3）创建表直接加载数据" class="headerlink" title="3）创建表直接加载数据"></a>3）创建表直接加载数据</h4><pre><code>hive&gt; create table student_partitions3(id int,name string)      &gt; row format      &gt; delimited fields      &gt; terminated by &#39;\t&#39;      &gt; location &#39;&#39;;</code></pre><p><strong>注意</strong>：locatition路径是hdfs路径<br>关联文件时不能有多级目录！！！<br>例：</p><pre><code>hive&gt; create table student_partitions4(id int,name string)    &gt; row format    &gt; delimited fields    &gt; terminated by &#39;\t&#39;    &gt; location &#39;/wc&#39;;</code></pre><h4 id="4）把操作结果导出到本地linux"><a href="#4）把操作结果导出到本地linux" class="headerlink" title="4）把操作结果导出到本地linux"></a>4）把操作结果导出到本地linux</h4><p>hive&gt; insert overwrite local directory ‘/root/data’ select * from hsiehchou;</p><h4 id="5）把hive中表数据导出到hdfs中"><a href="#5）把hive中表数据导出到hdfs中" class="headerlink" title="5）把hive中表数据导出到hdfs中"></a>5）把hive中表数据导出到hdfs中</h4><p>hive&gt; export table hsiehchou to ‘/hsiehchou’;</p><p>把hdfs数据导入到hive中<br>hive&gt; import table hsiehchou3 from ‘/hsiehchou/’;</p><h4 id="6）清空表数据"><a href="#6）清空表数据" class="headerlink" title="6）清空表数据"></a>6）清空表数据</h4><p>hive&gt; truncate table hsiehchou3;</p><h3 id="3、查询操作"><a href="#3、查询操作" class="headerlink" title="3、查询操作"></a>3、查询操作</h3><p>基础查询<br>select * from table;全表查询<br>hive&gt; select hsiehchou.id,hsiehchou.name from table …;指定列</p><h4 id="1）指定列查询"><a href="#1）指定列查询" class="headerlink" title="1）指定列查询"></a>1）指定列查询</h4><p>hive&gt; select hsiehchou.name from hsiehchou;</p><h4 id="2）指定列查询设置别名"><a href="#2）指定列查询设置别名" class="headerlink" title="2）指定列查询设置别名"></a>2）指定列查询设置别名</h4><p>hive&gt; select hsiehchou.name as myname from hsiehchou;</p><h4 id="3）创建员工表"><a href="#3）创建员工表" class="headerlink" title="3）创建员工表"></a>3）创建员工表</h4><pre><code>hive&gt; create table hive_db.emptable(empno int, ename string , job string,mgr int, birthday string, sal double, comm double, deptno int)    &gt; row format    &gt; delimited fields    &gt; terminated by &#39;\t&#39;;hive&gt; load data local ‘/root/emp.txt’ into table hive_db.emptable;</code></pre><h4 id="4）查询员工姓名和工资-每个员工加薪1000块"><a href="#4）查询员工姓名和工资-每个员工加薪1000块" class="headerlink" title="4）查询员工姓名和工资(每个员工加薪1000块)"></a>4）查询员工姓名和工资(每个员工加薪1000块)</h4><p>hive&gt; select emptable.ename,emptable.sal+1000 salmoney from emptable;</p><h4 id="5）查看公司有多少员工"><a href="#5）查看公司有多少员工" class="headerlink" title="5）查看公司有多少员工"></a>5）查看公司有多少员工</h4><p>hive&gt; select count(1) empnumber from emptable;</p><h4 id="6）查询工资最高的工资"><a href="#6）查询工资最高的工资" class="headerlink" title="6）查询工资最高的工资"></a>6）查询工资最高的工资</h4><p>hive&gt; select max(sal) numberone from emptable;</p><h4 id="7）查询工资最小的工资"><a href="#7）查询工资最小的工资" class="headerlink" title="7）查询工资最小的工资"></a>7）查询工资最小的工资</h4><p>hive&gt; select min(sal) from emptable;</p><h4 id="8）求工资的总和"><a href="#8）求工资的总和" class="headerlink" title="8）求工资的总和"></a>8）求工资的总和</h4><p>hive&gt; select sum(sal) sal_sum from emptable;</p><h4 id="9）求该公司员工工资的平均值"><a href="#9）求该公司员工工资的平均值" class="headerlink" title="9）求该公司员工工资的平均值"></a>9）求该公司员工工资的平均值</h4><p>hive&gt; select avg(sal) sal_avg from emptable;</p><h4 id="10）查询结果只显示前多少条"><a href="#10）查询结果只显示前多少条" class="headerlink" title="10）查询结果只显示前多少条"></a>10）查询结果只显示前多少条</h4><p>hive&gt; select * from emptable limit 4;</p><h4 id="11）where语句"><a href="#11）where语句" class="headerlink" title="11）where语句"></a>11）where语句</h4><p>作用：过滤<br>使用：where子句紧接着from</p><p>求出工资大于2600的员工<br>hive&gt; select * from emptable where sal&gt;2600;</p><p>求出工资在1000~2500范围的员工<br>hive&gt; select * from emptable where sal&gt;1000 and sal&lt;2500;</p><p>或者<br>hive&gt; select * from emptable where sal between 1000 and 2500;</p><p><strong>查询工资在2000和3000这两个数的员工信息</strong><br>hive&gt; select ename from emptable where sal in(2000,3000);</p><h4 id="12）is-null与is-not-null"><a href="#12）is-null与is-not-null" class="headerlink" title="12）is null与is not null"></a>12）is null与is not null</h4><p><strong>空与非空的过滤</strong><br>空<br>hive&gt; select * from emptable where comm is null;</p><p>非空<br>hive&gt; select * from emptable where comm is not null;</p><h4 id="13）like"><a href="#13）like" class="headerlink" title="13）like"></a>13）like</h4><p><strong>模糊查询</strong><br>使用：<br>通配符% 后面零个或者多个字符<br>_代表一个字符</p><p>查询工资以1开头的员工信息<br>hive&gt; select * from emptable where sal like ‘1%’;</p><p>查询工资地第二位是1的员工信息<br>hive&gt; select * from emptable where sal like ‘_1%’;</p><p>_代表一个字符<br>查询工资中有5的员工信息<br>hive&gt; select * from emptable where sal like ‘%5%’;</p><h4 id="14）And-Not-Or"><a href="#14）And-Not-Or" class="headerlink" title="14）And/Not/Or"></a>14）And/Not/Or</h4><p>查询部门号30并且工资大于1000的员工信息<br>hive&gt; select * from emptable where sal&gt;1000 and deptno=30;</p><p>查询部门号30或者工资大于1000的员工信息<br>hive&gt; select * from emptable where sal&gt;1000 or deptno=30;</p><p>查询工资在2000和3000这两个数的员工信息<br>hive&gt; select * from emptable where sal in(2000,3000);</p><p>查询工资不在2000和3000这两个数的员工信息<br>hive&gt; select * from emptable where sal not in(2000,3000);</p><h4 id="15）分组操作"><a href="#15）分组操作" class="headerlink" title="15）分组操作"></a>15）分组操作</h4><p>Group By语句<br>通常和一些聚合函数一起使用 </p><p>求每个部门的平均工资<br>hive&gt; select avg(sal) avg_sal,deptno from emptable group by deptno;<br>having<br>where：后不可以与分组函数，而having可以</p><p>求每个部门的平均工资大于2000的部门<br>hive&gt; select deptno,avg(sal) avg_sal from emptable group by deptno hav<br>ing avg_sal&gt;2000;</p><h3 id="4、Join操作"><a href="#4、Join操作" class="headerlink" title="4、Join操作"></a>4、Join操作</h3><pre><code>hive&gt; create table dept(deptno int, dname string, loc int)      &gt; row format      &gt; delimited fields      &gt; terminated by &#39;\t&#39;;</code></pre><p>员工表中只有部门编号，并没有部门名称<br>部门表中有部门标号和部门名称</p><p><strong>等值join</strong> </p><h4 id="1）查询员工编号、员工姓名、员工所在的部门名称"><a href="#1）查询员工编号、员工姓名、员工所在的部门名称" class="headerlink" title="1）查询员工编号、员工姓名、员工所在的部门名称"></a>1）查询员工编号、员工姓名、员工所在的部门名称</h4><p>hive&gt; select emptable.empno,emptable.ename,dept.dname from emptable join dept on emptable.deptno=dept.deptno;</p><h4 id="2）查询员工编号、员工姓名、员工所在部门名称、部门所在地"><a href="#2）查询员工编号、员工姓名、员工所在部门名称、部门所在地" class="headerlink" title="2）查询员工编号、员工姓名、员工所在部门名称、部门所在地"></a>2）查询员工编号、员工姓名、员工所在部门名称、部门所在地</h4><p>内连接：只有连接的两张表中都存在与条件向匹配的数据才会被保留下来<br>hive&gt; select e.empno,e.ename,d.dname,d.loc from emptable e join dept d on e.deptno=d.deptno;</p><h4 id="3）左外连接-left-join"><a href="#3）左外连接-left-join" class="headerlink" title="3）左外连接(left join)"></a>3）左外连接(left join)</h4><p>查询员工编号，员工姓名，部门名称<br>hive&gt; select e.empno,e.ename,d.deptname from emptable e left join dept d on e.deptno=d.deptno;<br>特点：默认用的Left join 可以省略left<br>保留左表数据，右表没有join上 显示为null</p><h4 id="4）右外连接-right-join"><a href="#4）右外连接-right-join" class="headerlink" title="4）右外连接(right join)"></a>4）右外连接(right join)</h4><p>hive&gt; select e.empno,e.ename,d.dname from emptable e right join dept d on e.deptno=d.deptno;<br>特点：<br>保留右表数据，左表没有join上 显示为null</p><h4 id="5）满外连接-full-join"><a href="#5）满外连接-full-join" class="headerlink" title="5）满外连接(full join)"></a>5）满外连接(full join)</h4><p>hive&gt; select e.empno,e.ename,d.dname from emptable e full join dept d on e.deptno=d.deptno;<br>特点：结果会返回所有表中符合条件的所有记录，如果有字段没有符合条件用null值代替</p><h4 id="6）多表连接"><a href="#6）多表连接" class="headerlink" title="6）多表连接"></a>6）多表连接</h4><pre><code>hive&gt; create table location(loc int, loc_name string)      &gt; row format      &gt; delimited fields      &gt; terminated by &#39;\t&#39;;</code></pre><p><strong>加载数据</strong><br>hive&gt; load data local inpath ‘/root/location.txt’ into table location;</p><p><strong>查询员工名、部门名称、地域名称</strong><br>hive&gt; select e.ename,d.dname,l.loc_name from emptable e join dept d on<br>e.deptno=d.deptno join location l on d.loc=l.loc;</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hive SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive基础</title>
      <link href="/2019/02/25/hive-ji-chu/"/>
      <url>/2019/02/25/hive-ji-chu/</url>
      
        <content type="html"><![CDATA[<h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3><p>官网：<a href="http://hive.apache.org/" target="_blank" rel="noopener">http://hive.apache.org/</a><br>Apache Hive?数据仓库软件有助于使用SQL读取，编写和管理驻留在分布式存储中的大型数据集。可以将结构投影到已存储的数据中。提供了命令行工具和JDBC驱动程序以将用户连接到Hive</p><p>Hive提供了SQL查询功能 hdfs分布式存储</p><p>Hive本质HQL转化为MapReduce程序<br>环境前提：<br>1）启动hdfs集群<br>2）启动yarn集群<br>如果想用hive的话，需要提前安装部署好hadoop集群</p><h3 id="为什么要学习Hive"><a href="#为什么要学习Hive" class="headerlink" title="为什么要学习Hive"></a>为什么要学习Hive</h3><p>简化开发<br>easycoding!<br>高德地图使用Hive</p><p><strong>优势</strong>：<br>1）操作接口采用类sql语法，select * from stu;<br>简单、上手快！<br>2）hive可以替代mr程序，sqoop<br>3）hive可以处理海量数据<br>4）hive支持UDF，自定义函数</p><p><strong>劣势</strong>：<br>1）处理数据延迟高，慢<br>引擎：1.2.2以前版本都是用的mr引擎<br>2.x之后用的是Spark引擎 </p><p>2）HQL的表达能力有限<br>一些sql无法解决的场景，依然需要我们写MapReduce</p><h3 id="hive架构原理解析"><a href="#hive架构原理解析" class="headerlink" title="hive架构原理解析"></a>hive架构原理解析</h3><p>sql-&gt;转换-&gt;MapReduce-&gt;job</p><h3 id="hive安装部署"><a href="#hive安装部署" class="headerlink" title="hive安装部署"></a>hive安装部署</h3><h4 id="1）下载"><a href="#1）下载" class="headerlink" title="1）下载"></a>1）下载</h4><h4 id="2）上传到Linux"><a href="#2）上传到Linux" class="headerlink" title="2）上传到Linux"></a>2）上传到Linux</h4><h4 id="3）解压"><a href="#3）解压" class="headerlink" title="3）解压"></a>3）解压</h4><p>tar -zxvf apache-hive-1.2.2-bin.tar.gz -C hd/ </p><h4 id="4）重命名"><a href="#4）重命名" class="headerlink" title="4）重命名"></a>4）重命名</h4><p>mv apache-hive-1.2.2-bin/ hive</p><h4 id="5）修改配置文件"><a href="#5）修改配置文件" class="headerlink" title="5）修改配置文件"></a>5）修改配置文件</h4><p>mv hive-env.sh.template hive-env.sh<br>vi hive-env.sh<br>HADOOP_HOME=/root/hd/hadoop-2.8.4<br>export HIVE_CONF_DIR=/root/hd/hive/conf</p><h4 id="6）启动"><a href="#6）启动" class="headerlink" title="6）启动"></a>6）启动</h4><p>bin/hive </p><h3 id="配置mysql元数据库"><a href="#配置mysql元数据库" class="headerlink" title="配置mysql元数据库"></a>配置mysql元数据库</h3><h4 id="1）拷贝mysql驱动到hive-lib"><a href="#1）拷贝mysql驱动到hive-lib" class="headerlink" title="1）拷贝mysql驱动到hive/lib"></a>1）拷贝mysql驱动到hive/lib</h4><p>cp/mv hive/lib </p><h4 id="2）添加hive-site-xml"><a href="#2）添加hive-site-xml" class="headerlink" title="2）添加hive-site.xml"></a>2）添加hive-site.xml</h4><pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;&lt;value&gt;jdbc:mysql://hsiehchou121:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt;&lt;description&gt;JDBC connect string for a JDBCmetastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;description&gt;username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;</code></pre><h4 id="3）注意：重启hadoop集群"><a href="#3）注意：重启hadoop集群" class="headerlink" title="3）注意：重启hadoop集群"></a>3）注意：重启hadoop集群</h4><h4 id="4）启动hive"><a href="#4）启动hive" class="headerlink" title="4）启动hive"></a>4）启动hive</h4><p>bin/hive<br>此时mysql中创建metastore元数据库<br>hive&gt; create table hsiehchou(id int, name string)</p><p>row format<br>delimited fields<br>terminated by ‘\t’;<br>OK<br>hive&gt; load data local inpath ‘/root/hsiehchou.txt’ into table hsiehchou;<br>OK<br>hive&gt; select * from hsiehchou;<br>OK<br>1 re<br>2 mi<br>3 zk<br>4 sf<br>5 ls</p><h3 id="杀死hive进程"><a href="#杀死hive进程" class="headerlink" title="杀死hive进程"></a>杀死hive进程</h3><p>[root@hsiehchou121 hive]# ps -aux|grep hive<br>root 3649 3.7 16.9 2027072 239240 pts/0 Tl 21:37 0:31<br>root 4285 0.0 0.0 112648 948 pts/0<br>[root@hsiehchou121 hive]# kill -9 3649</p><h3 id="安装mysql5-6"><a href="#安装mysql5-6" class="headerlink" title="安装mysql5.6"></a>安装mysql5.6</h3><p>yum search libaio # 检索相关信息<br>yum install libaio # 安装依赖包<br>wget <a href="http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm" target="_blank" rel="noopener">http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm</a></p><p>添加 MySQL Yum Repository 到linux系统 repository 列表中，输入<br>yum localinstall mysql-community-release-el7-5.noarch.rpm</p><p>验证是否添加成功<br>yum repolist enabled | grep “mysql.-community.”</p><p>查看 MySQL 版本，输入<br>yum repolist all | grep mysql</p><p>可以看到 5.5， 5.7 版本是默认禁用的，因为现在最新的稳定版是 5.6<br>yum repolist enabled | grep mysql</p><p>通过 Yum 来安装 MySQL，输入<br>yum install mysql-community-server</p><p>rpm -qi mysql-community-server.x86_64 0:5.6.24-3.el7</p><p>查看 MySQL 的安装目录<br>whereis mysql</p><p>启动 MySQL Server<br>systemctl start mysqld</p><p>查看 MySQL Server 状态<br>systemctl status mysqld</p><p>关闭 MySQL Server<br>systemctl stop mysqld</p><p>测试是否安装成功<br>mysql</p><p>修改mysql密码<br>use mysql;<br>update user set password=password(‘root’) where user=’root’;<br>flush privileges;</p><h3 id="数据导入操作"><a href="#数据导入操作" class="headerlink" title="数据导入操作"></a>数据导入操作</h3><p>load data []local] inpath ‘/root/hsiehchou.txt’ into table hsiehchou; </p><p>load data:加载数据 </p><p>local:可选操作，如果加上local导入是本地linux中的数据，如果去掉local 那么 导入的是hdfs中数据</p><p>inpath:表示的是加载数据的路径 </p><p>into table:表示要加载的对应的表</p><h3 id="hive数据类型"><a href="#hive数据类型" class="headerlink" title="hive数据类型"></a>hive数据类型</h3><table><thead><tr><th align="center">Java数据类型</th><th align="center">Hive数据类型</th><th align="center">长度</th></tr></thead><tbody><tr><td align="center">byte</td><td align="center">TINYINT</td><td align="center">1byte有符号整数</td></tr><tr><td align="center">short</td><td align="center">SMALLINT</td><td align="center">2byte有符号整数</td></tr><tr><td align="center">int</td><td align="center">INT</td><td align="center">4byte有符号整数</td></tr><tr><td align="center">long</td><td align="center">GINT</td><td align="center">8byte有符号整数</td></tr><tr><td align="center">boolean</td><td align="center">BOOLEAN</td><td align="center">false/true</td></tr><tr><td align="center">float</td><td align="center">FLOAT</td><td align="center">单精度浮点</td></tr><tr><td align="center">double</td><td align="center">DOUBLE</td><td align="center">双精度浮点</td></tr><tr><td align="center">string</td><td align="center">STRING</td><td align="center">字符</td></tr><tr><td align="center"></td><td align="center">BINARY</td><td align="center">字节数组</td></tr></tbody></table><h3 id="DDL数据定义"><a href="#DDL数据定义" class="headerlink" title="DDL数据定义"></a>DDL数据定义</h3><h4 id="1）查看数据库"><a href="#1）查看数据库" class="headerlink" title="1）查看数据库"></a>1）查看数据库</h4><p>show databases;</p><h4 id="2）创建库"><a href="#2）创建库" class="headerlink" title="2）创建库"></a>2）创建库</h4><p>create database hive_db;</p><h4 id="3）创建库-标准写法"><a href="#3）创建库-标准写法" class="headerlink" title="3）创建库 标准写法"></a>3）创建库 标准写法</h4><p>create database if not exists hive_db;</p><h4 id="4）创建库指定hdfs路径"><a href="#4）创建库指定hdfs路径" class="headerlink" title="4）创建库指定hdfs路径"></a>4）创建库指定hdfs路径</h4><p>create database hive_db location ‘/hive_db’;</p><h4 id="5）创建表"><a href="#5）创建表" class="headerlink" title="5）创建表"></a>5）创建表</h4><p>如果指定了hdfs路径<br>创建的表存在于这个路径</p><h4 id="6）查看数据库结构"><a href="#6）查看数据库结构" class="headerlink" title="6）查看数据库结构"></a>6）查看数据库结构</h4><p>desc database hive_db;</p><h4 id="7）添加额外的描述信息"><a href="#7）添加额外的描述信息" class="headerlink" title="7）添加额外的描述信息"></a>7）添加额外的描述信息</h4><p>alter database hive_db set dbproperties(‘created’=’hsiehchou’);<br>注意：查询需要使用desc database extended hive_db;</p><h4 id="8）查看指定的通配库-过滤"><a href="#8）查看指定的通配库-过滤" class="headerlink" title="8）查看指定的通配库:过滤"></a>8）查看指定的通配库:过滤</h4><p>show databases like ‘h*’;</p><h4 id="9）删除空库"><a href="#9）删除空库" class="headerlink" title="9）删除空库"></a>9）删除空库</h4><p>drop database hive_db;</p><h4 id="10）删除非空库"><a href="#10）删除非空库" class="headerlink" title="10）删除非空库"></a>10）删除非空库</h4><p>drop database hive_db2 cascade;</p><h4 id="11-删除非空库标准写法"><a href="#11-删除非空库标准写法" class="headerlink" title="11) 删除非空库标准写法"></a>11) 删除非空库标准写法</h4><p>drop database if exists hive_db cascade;</p><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><p>create <code>[external]</code> table <code>[if not exists]</code> table_name(字段信息) <code>[partitioned by(字段信息)][clustered by(字段信息)]</code> [sorted by(字段信息)]row format delimited fields terminated by ‘切割符’;</p><h3 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h3><p>默认不加external创建的就是管理表，也称为内部表。<br>MANAGED_TABLE管理表<br>Table Type:MANAGED_TABLE </p><p>查看表类型：<br>desc formatted hsiehchou2;</p><h3 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h3><p>EXTERNAL_TABLE外部表<br>创建方式：<br>create external table student(id int,name string) </p><p>区别：如果是管理表删除hdfs中数据删除，如果是外部表删除hdfs数据不删除！</p><h3 id="hive命令"><a href="#hive命令" class="headerlink" title="hive命令"></a>hive命令</h3><h4 id="1）不登录hive客户端直接输入命令操作Hive"><a href="#1）不登录hive客户端直接输入命令操作Hive" class="headerlink" title="1）不登录hive客户端直接输入命令操作Hive"></a>1）不登录hive客户端直接输入命令操作Hive</h4><p>[root@hsiehchou121 hive]# bin/hive -e “select * from hsiehchou;”<br>19/02/28 03:09:23 WARN conf.HiveConf: HiveConf of name hive.cli,print.current.db does not exist<br>Logging initialized using configuration in jar:file:/root/hd/hive/lib/hive-common-1.2.2.jar!/hive-log4j.properties<br>OK<br>hsiehchou.id hsiehchou.name<br>1 re<br>2 mi<br>3 zk<br>4 sf<br>5 ls</p><h4 id="2）直接把sql写入到文件中"><a href="#2）直接把sql写入到文件中" class="headerlink" title="2）直接把sql写入到文件中"></a>2）直接把sql写入到文件中</h4><p>bin/hive -f /root/hived.sql</p><h4 id="3）查看hdfs文件"><a href="#3）查看hdfs文件" class="headerlink" title="3）查看hdfs文件"></a>3）查看hdfs文件</h4><p>dfs -ls /;<br>dfs -cat /wc/in/words.txt;</p><h4 id="4）查看历史操作"><a href="#4）查看历史操作" class="headerlink" title="4）查看历史操作"></a>4）查看历史操作</h4><p>[root@hsiehchou121 hive]# cat ~/.hivehistory</p><p>在hive/conf/hive-site.xml中增加</p><pre><code>&lt;!--是否显示当前表头--&gt;&lt;property&gt;        &lt;name&gt;hive.cli.print.header&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!--是否显示当前所在库名--&gt;&lt;property&gt;        &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;&lt;/property</code></pre><p><strong>显示效果</strong><br>hive&gt; select * from hsiehchou;<br>OK<br>hsiehchou.id hsiehchou.name<br>1 re<br>2 mi<br>3 zk<br>4 sf<br>5 ls</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ZooKeeper练习</title>
      <link href="/2019/02/23/zookeeper-lian-xi/"/>
      <url>/2019/02/23/zookeeper-lian-xi/</url>
      
        <content type="html"><![CDATA[<h3 id="命令行操作"><a href="#命令行操作" class="headerlink" title="命令行操作"></a>命令行操作</h3><p>配置环境变量：vi /etc/profile<br>export ZOOKEEPER_HOME=<code>/root/hd/zookeeper-3.4.10</code><br>export PATH=<code>$ZOOKEEPER_HOME/bin:$PATH</code></p><p>声明环境变量：source /etc/profile </p><p>发送到其他机器<br>scp /etc/profile hsiehchou122:/etc/<br>scp /etc/profile hsiehchou123:/etc/<br>scp /etc/profile hsiehchou124:/etc/ </p><p>启动zookeeper<br>zkServer.sh start </p><p>查看zookeeper状态<br>zkServer.sh status</p><h4 id="1）启动客户端"><a href="#1）启动客户端" class="headerlink" title="1）启动客户端"></a>1）启动客户端</h4><p>bin/zkCli.sh</p><h4 id="2）连接其它机器客户端操作"><a href="#2）连接其它机器客户端操作" class="headerlink" title="2）连接其它机器客户端操作"></a>2）连接其它机器客户端操作</h4><p>没有太大必要，每台机器内容都一样<br>connect hsiehchou122:2181<br>connect hsiehchou123:2181<br>connect hsiehchou124:2181</p><h4 id="3）查看历史操作记录"><a href="#3）查看历史操作记录" class="headerlink" title="3）查看历史操作记录"></a>3）查看历史操作记录</h4><p>history</p><h4 id="4）查看当前节点的内容"><a href="#4）查看当前节点的内容" class="headerlink" title="4）查看当前节点的内容"></a>4）查看当前节点的内容</h4><p>ls /</p><h4 id="5）存储：创建节点"><a href="#5）存储：创建节点" class="headerlink" title="5）存储：创建节点"></a>5）存储：创建节点</h4><p>create /hsiehchou 10(存储的数据)</p><h4 id="6）查看节点的值"><a href="#6）查看节点的值" class="headerlink" title="6）查看节点的值"></a>6）查看节点的值</h4><p>get /hsiehchou</p><p>10<br>cZxid = 0x400000004<br>ctime = Sat Feb 23 20:05:58 PST 2019<br>mZxid = 0x400000004<br>mtime = Sat Feb 23 20:05:58 PST 2019<br>pZxid = 0x400000004<br>cversion = 0<br>dataVersion = 0<br>aclVersion = 0<br>ephemeralOwner = 0x0<br>dataLength = 2<br>numChildren = 0</p><h4 id="7）创建节点的可选项"><a href="#7）创建节点的可选项" class="headerlink" title="7）创建节点的可选项"></a>7）创建节点的可选项</h4><p>create <code>[-s] [-e]</code> path data acl</p><p>[-p]永久节点–默认<br>[-e] 短暂节点<br>[-s] 带序号</p><p>create -e /re hm<br>注意：此时-e创建的是临时的短暂节点，退出客户端后消失<br>退出客户端：quit</p><p>create -s /re hm<br>注意：此时-s创建是带序号的节点，可以创建节点名相同的，序号依次累加</p><p>[zk: localhost:2181(CONNECTED) 1] create -s /mm hm<br>Created /mm0000000002<br>[zk: localhost:2181(CONNECTED) 2] create -s /mm hm<br>Created /mm0000000003<br>[zk: localhost:2181(CONNECTED) 3] create -s /mm hm<br>Created /mm0000000004<br>[zk: localhost:2181(CONNECTED) 4] create  /re hm<br>Created /re<br>[zk: localhost:2181(CONNECTED) 5] create  /re hm<br>Node already exists: /re<br>创建短暂带序号节点<br>create -e -s /tt bt</p><h4 id="8）修改节点值"><a href="#8）修改节点值" class="headerlink" title="8）修改节点值"></a>8）修改节点值</h4><p>set path data [version]<br>例如：set /re hm2 1<br>[version] 版本<br>注意：设置版本号 必须从0开始</p><h4 id="9）删除节点"><a href="#9）删除节点" class="headerlink" title="9）删除节点"></a>9）删除节点</h4><p>delete path<br>[zk: localhost:2181(CONNECTED) 12] ls /<br>[mm0000000004, re, zookeeper, mm0000000002, mm0000000003, hsiehchou]<br>[zk: localhost:2181(CONNECTED) 13] delete /mm0000000002<br>[zk: localhost:2181(CONNECTED) 14] ls /<br>[mm0000000004, re, zookeeper, mm0000000003, hsiehchou]</p><h4 id="10）创建子节点"><a href="#10）创建子节点" class="headerlink" title="10）创建子节点"></a>10）创建子节点</h4><p>create /re/pa qi</p><h4 id="11）递归删除"><a href="#11）递归删除" class="headerlink" title="11）递归删除"></a>11）递归删除</h4><p>rmr /re</p><h4 id="12）监听"><a href="#12）监听" class="headerlink" title="12）监听"></a>12）监听</h4><p>获得监听（文件）：get path watch<br>获得当前节点下增减变化（文件夹）：ls path watch</p><h4 id="13）查看当前节点的状态"><a href="#13）查看当前节点的状态" class="headerlink" title="13）查看当前节点的状态"></a>13）查看当前节点的状态</h4><p>stat /hsiehchou</p><h3 id="节点状态信息"><a href="#节点状态信息" class="headerlink" title="节点状态信息"></a>节点状态信息</h3><p>czxid：ZooKeeper事务id<br>ctime：节点创建时间<br>mZxid：最后更新的czxid<br>mtime：最后修改的时间*<br>pZxid：最后更新子节点的czxid<br>cversion：子节点的变化号、子节点修改次数<br>dataVersion：数据变化号<br>aclVersion：访问控制列表的变化号<br>ephemeralOwner：临时节点判断<br>dataLength：节点数据长度<br>numChildren：子节点个数</p><h3 id="JAVA-API-练习"><a href="#JAVA-API-练习" class="headerlink" title="JAVA-API 练习"></a>JAVA-API 练习</h3><p><strong>pom.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;    &lt;artifactId&gt;ZKTest&lt;/artifactId&gt;    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;dependencies&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;            &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;            &lt;version&gt;3.4.10&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;junit&lt;/groupId&gt;            &lt;artifactId&gt;junit&lt;/artifactId&gt;            &lt;version&gt;RELEASE&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><h3 id="练习1"><a href="#练习1" class="headerlink" title="练习1"></a>练习1</h3><p><strong>ZkClient类</strong></p><pre><code>package com.hsiehchou.zk;import org.apache.zookeeper.*;import org.apache.zookeeper.data.Stat;import org.junit.Before;import org.junit.Test;import java.io.IOException;import java.util.List;public class ZkClient {    private String conected = &quot;hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181,hsiehchou124:2181&quot;;    //毫秒    private int timeout = 2000;    ZooKeeper zkCli = null;    //连接zookeeper集群    @Before    public void init() throws IOException {        //String:连接集群的IP端口号，Int：超时设置，Watcher：监听        zkCli = new ZooKeeper(conected, timeout, new Watcher() {            //回调方法，显示/节点            public void process(WatchedEvent watchedEvent) {                List&lt;String&gt; children;                //获得节点信息 get                try {                    children = zkCli.getChildren(&quot;/&quot;,true);                } catch (KeeperException e) {                    e.printStackTrace();                } catch (InterruptedException e) {                    e.printStackTrace();                }            }        });    }    //测试 是否连通集群  创建节点    @Test    public void createNode() throws KeeperException, InterruptedException {        String p = zkCli.create(&quot;/bq&quot;, &quot;sk&quot;.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);        System.out.println(p);    }    //查看子节点    @Test    public void getChild() throws KeeperException, InterruptedException {        List&lt;String&gt; children = zkCli.getChildren(&quot;/&quot;, true);        for(String c:children){            System.out.println(c);        }    }    //删除子节点数据:delete path    @Test    public void deleteData() throws KeeperException, InterruptedException {        zkCli.delete(&quot;/da&quot;, -1);    }    //修改数据:set path data    @Test    public void setData() throws KeeperException, InterruptedException {        zkCli.setData(&quot;/hsiehchou&quot;,&quot;nihao&quot;.getBytes(),-1);        //查看/hsiehchou        byte[] data = zkCli.getData(&quot;/hsiehchou&quot;, false, new Stat());        System.out.println(new String(data));    }    //指定节点是否存在    @Test    public void testExist() throws KeeperException, InterruptedException {        Stat exists = zkCli.exists(&quot;/hsiehchou&quot;, false);        System.out.println(exists == null ? &quot;no have&quot;:&quot;have&quot;);    }}</code></pre><h3 id="练习2"><a href="#练习2" class="headerlink" title="练习2"></a>练习2</h3><p><strong>WatchDemo类</strong></p><pre><code>package com.hsiehchou.watch;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import java.io.IOException;public class WatchDemo {    public static void main(String[] args) throws IOException, KeeperException, InterruptedException {        String connected = &quot;hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181,hsiehchou124:2181,&quot;;        //毫秒        int timeout = 2000;        //1.连接zookeeper集群        ZooKeeper zkCli = new ZooKeeper(connected, timeout, new Watcher() {            //监听回调            public void process(WatchedEvent watchedEvent) {                System.out.println(&quot;正在监听中.........&quot;);            }        });        //2.监听： ls / watch    get / watch        zkCli.getChildren(&quot;/&quot;, new Watcher() {            public void process(WatchedEvent watchedEvent) {                System.out.println(&quot;此时监听的路径是：&quot;+watchedEvent.getPath());                System.out.println(&quot;此时监听的类型为：&quot;+watchedEvent.getType());                System.out.println(&quot;有人正在修改数据！！！&quot;);            }        },null);        Thread.sleep(Long.MAX_VALUE);    }}</code></pre><p><strong>WatchDemo1类</strong></p><pre><code>package com.hsiehchou.watch;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import java.io.IOException;public class WatchDemo1 {    public static void main(String[] args) throws IOException, KeeperException, InterruptedException {        ZooKeeper zkCli = new ZooKeeper(&quot;hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181,hsiehchou124:2181&quot;, 2000, new Watcher() {            public void process(WatchedEvent watchedEvent) {            }        });        byte[] data = zkCli.getData(&quot;/re&quot;, new Watcher() {            //具体监听的内容            public void process(WatchedEvent watchedEvent) {                System.out.println(&quot;此时监听的路径是：&quot; + watchedEvent.getPath());                System.out.println(&quot;此时监听的类型为：&quot; + watchedEvent.getType());                System.out.println(&quot;有人正在修改数据！！！&quot;);            }        }, null);        System.out.println(new String(data));        Thread.sleep(Long.MAX_VALUE);    }}</code></pre><h3 id="练习3"><a href="#练习3" class="headerlink" title="练习3"></a>练习3</h3><p><strong>ZkClient类</strong></p><pre><code>package com.hsiehchou.qq;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import java.io.IOException;import java.util.ArrayList;import java.util.List;/** * 实现对zookeeper / 的监听 */public class ZkClient {    public static void main(String[] args) throws IOException, KeeperException, InterruptedException {        //1.获取zookeeper的连接        ZkClient zkCli = new ZkClient();        zkCli.getConnect();        //2.指定监听的节点路径        zkCli.getServers();        //3.写业务逻辑，一直监听        zkCli.getWatch();    }    //1.获得zookeeper连接    private String connected = &quot;hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181,hsiehchou124:2181&quot;;    //毫秒    private int timeout = 2000;    ZooKeeper zkCli;    public void getConnect() throws IOException {        zkCli = new ZooKeeper(connected, timeout, new Watcher() {            public void process(WatchedEvent watchedEvent) {                List&lt;String&gt; children;                try {                    children = zkCli.getChildren(&quot;/&quot;, true);                    //服务器列表                    ArrayList&lt;String&gt; serverList = new ArrayList&lt;String&gt;();                    //获取每个节点的数据                    for (String c:children){                        byte[] data = zkCli.getData(&quot;/&quot; + c, true, null);                        serverList.add(new String(data));                    }                    //查看服务器列表                    System.out.println(serverList);                } catch (KeeperException e) {                    e.printStackTrace();                } catch (InterruptedException e) {                    e.printStackTrace();                }            }        });    }    //2.指定监听节点路径    public void getServers() throws KeeperException, InterruptedException {        List&lt;String&gt; children = zkCli.getChildren(&quot;/&quot;, true);        //存储服务器列表        ArrayList&lt;String&gt; serverList = new ArrayList&lt;String&gt;();        for (String c:children){            byte[] data = zkCli.getData(&quot;/&quot; + c, true, null);            //添加集合中            serverList.add(new String(data));        }        //打印服务器列表        System.out.println(serverList);    }    //3.一直监听    public void getWatch() throws InterruptedException {        //循环监听        Thread.sleep(Long.MAX_VALUE);    }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> ZooKeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zookeeper介绍</title>
      <link href="/2019/02/21/zookeeper-jie-shao/"/>
      <url>/2019/02/21/zookeeper-jie-shao/</url>
      
        <content type="html"><![CDATA[<h3 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h3><p>官网：<a href="http://zookeeper.apache.org/" target="_blank" rel="noopener">http://zookeeper.apache.org/</a><br>介绍：Apache ZooKeeper致力于开发和维护开源服务器，实现高度可靠的分布式协调</p><p>ZooKeeper是一种集中式服务，用于维护配置信息，命名，提供分布式同步和提供组服务。所有这些类型的服务都以分布式应用程序的某种形式使用。每次实施它们都需要做很多工作来修复不可避免的错误和竞争条件。由于难以实现这些类型的服务，应用程序最初通常会吝啬它们，这使得它们在变化的情况下变得脆弱并且难以管理。即使正确完成，这些服务的不同实现也会在部署应用程序时导致管理复杂性</p><h4 id="1、ZooKeeper工作原理"><a href="#1、ZooKeeper工作原理" class="headerlink" title="1、ZooKeeper工作原理"></a>1、ZooKeeper工作原理</h4><p>ZooKeeper功能：存储+监听</p><h4 id="2、ZooKeeper角色"><a href="#2、ZooKeeper角色" class="headerlink" title="2、ZooKeeper角色"></a>2、ZooKeeper角色</h4><p>主从结构<br>1）Leader领导者-》主<br>2）Follower追随者-》从<br>3）ZooKeeper由一个领导者多个追随者组成<br>ZK集群中只要有半数以上的节点存活，zk集群就能正常工作。所以搭建ZK集群最好搭建<br>奇数台（3,5,11）</p><h4 id="3、ZooKeeper功能"><a href="#3、ZooKeeper功能" class="headerlink" title="3、ZooKeeper功能"></a>3、ZooKeeper功能</h4><p>大数据中使用ZooKeeper业务<br>1）做统一的配置管理<br>2）做统一的命名服务<br>3）做统一的集群管理<br>4）做服务器的动态上下线感知（代码）</p><h4 id="4、单节点安装部署"><a href="#4、单节点安装部署" class="headerlink" title="4、单节点安装部署"></a>4、单节点安装部署</h4><p>1）下载安装包</p><p>2）上传安装到linux<br>alt+p</p><p>3）解压<br>tar -zxvf zookeeper-3.4.10.tar.gz -C hd/</p><p>4）修改配置文件<br>重命名：mv zoo_sample.cfg zoo.cfg</p><p>5）创建文件夹zkData<br>添加到配置文件：zoo.cfg<br>dataDir=/root/hd/zookeeper-3.4.10/zkData</p><p>6）启动ZooKeeper<br>bin/zkServer.sh start</p><p>7）启动ZooKeeper客户端<br>bin/zkCli.sh</p><h4 id="5、ZooKeeper集群安装部署"><a href="#5、ZooKeeper集群安装部署" class="headerlink" title="5、ZooKeeper集群安装部署"></a>5、ZooKeeper集群安装部署</h4><p>1）下载安装包</p><p>2）上传安装到linux<br>alt+p</p><p>3）解压<br>$ tar -zxvf zookeeper-3.4.10.tar.gz -C hd/</p><p>4）修改配置文件名<br>重命名：mv zoo_sample.cfg zoo.cfg<br>或者拷贝：cp zoo_sample.cfg zoo.cfg</p><p>5）修改配置<br>vi zookeeper-3.4.10/conf/zoo.cfg</p><p>dataDir=/root/hd/zookeeper-3.4.10/zkData</p><p>—————-zkconfig————<br>server.1=hsiehchou121:2888:3888<br>server.2=hsiehchou122:2888:3888<br>server.3=hsiehchou123:2888:3888<br>server.4=hsiehchou124:2888:3888</p><p>创建文件<strong>myid</strong> </p><p>添加服务器编号：1<br>[root@hsiehchou121 zookeeper-3.4.10]# cd zkData/<br>[root@hsiehchou121 zkData]# touch myid</p><h4 id="6）拷贝ZooKeeper到其它机器"><a href="#6）拷贝ZooKeeper到其它机器" class="headerlink" title="6）拷贝ZooKeeper到其它机器"></a>6）拷贝ZooKeeper到其它机器</h4><p> scp -r zookeeper-3.4.10/ hsiehchou122:<code>$PWD</code><br> scp -r zookeeper-3.4.10/ hsiehchou123:<code>$PWD</code><br> scp -r zookeeper-3.4.10/ hsiehchou124:<code>$PWD</code></p><h4 id="7）注意需要修改每台机器的myid文件"><a href="#7）注意需要修改每台机器的myid文件" class="headerlink" title="7）注意需要修改每台机器的myid文件"></a>7）注意需要修改每台机器的myid文件</h4><p>设置为当前的机器编号即可</p><h4 id="8）启动ZooKeeper集群"><a href="#8）启动ZooKeeper集群" class="headerlink" title="8）启动ZooKeeper集群"></a>8）启动ZooKeeper集群</h4><p>bin/zkServer.sh start</p><h4 id="9）查看ZooKeeper状态"><a href="#9）查看ZooKeeper状态" class="headerlink" title="9）查看ZooKeeper状态"></a>9）查看ZooKeeper状态</h4><p>bin/zkServer.sh status</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> ZooKeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据常用基本算法</title>
      <link href="/2019/02/18/da-shu-ju-chang-yong-ji-ben-suan-fa/"/>
      <url>/2019/02/18/da-shu-ju-chang-yong-ji-ben-suan-fa/</url>
      
        <content type="html"><![CDATA[<h4 id="1、冒泡排序"><a href="#1、冒泡排序" class="headerlink" title="1、冒泡排序"></a>1、冒泡排序</h4><p>冒泡排序（Bubble Sort），是一种计算机科学领域的较简单的排序算法，它重复地走访过要排序的元素列，依次比较两个相邻的元素，如果他们的顺序（如从大到小、首字母从A到Z）错误就把他们交换过来。走访元素的工作是重复地进行直到没有<br>相邻元素需要交换，也就是说该元素已经排序完成这个算法的名字由来是因为越大的元素会经由交换慢慢“浮”到数列的顶端（升序或降序排列），就如同碳酸饮料中二氧化碳的气泡最终会上浮到顶端一样，故名“<strong>冒泡排序</strong>” </p><p>冒泡排序算法的原理如下：<br>1）比较相邻的元素。如果第一个比第二个大，就交换他们两个 </p><p>2）对每一对相邻元素做同样的工作，从开始第一对到结尾的最后一对。在这一点，最后的元素应该会是最大的数 </p><p>3）针对所有的元素重复以上的步骤，除了最后一个 </p><p>4）持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较 </p><p>列如：<br>数组元素&gt;<br>5 1 7 2 6 4 3 16 </p><p>1）由于第一个元素5比第二个元素大1，交换它们的位置。<br>1 5 7 2 6 4 3 16 </p><p>2）对比每个相邻的元素，此时到第二个元素5与第三个元素7，不交换位置<br>1 5 7 2 6 4 3 16 </p><p>3）对比每个相邻的元素，此时到第三个元素7与第四个元素2，交换位置<br>1 5 2 7 6 4 3 16 </p><p>4）对比每个相邻的元素，此时到第四个元素7与第五个元素6，交换位置<br>1 5 2 6 7 4 3 16 </p><p>5）对比每个相邻的元素，此时到第五个元素7与第六个元素4，交换位置<br>1 5 2 6 4 7 3 16 </p><p>6）对比每个相邻的元素，此时到第六个元素7与第七个元素3，交换位置<br>1 5 2 6 4 3 7 16 </p><p>7）对比每个相邻的元素，此时到第七个元素7与第八个元素16，不换位置<br>1 5 2 6 4 3 7 16</p><h4 id="2、双冒泡排序"><a href="#2、双冒泡排序" class="headerlink" title="2、双冒泡排序"></a>2、双冒泡排序</h4><p>双向冒泡算法，极大的减少了循环排序的次数<br>1）传统冒泡气泡排序的双向进行，先让气泡排序由左向右进行，再来让气泡排序由右往左进行，如此完成一次排序的动作 </p><p>2）使用left与right两个旗标来记录左右两端已排序的元素位置 </p><p>3）当往左递进left &gt;=往右递进的 right时，则排序完成<br>例子如下所示：<br>排序前：45 19 77 81 13 28 18 19 77 11<br>往右排序：19 45 77 13 28 18 19 77 11 [81]<br>向左排序：[11] 19 45 77 13 28 18 19 77 [81]<br>往右排序：[11] 19 45 13 28 18 19 [77 77 81]<br>向左排序：[11 13] 19 45 18 28 19 [77 77 81]<br>往右排序：[11 13] 19 18 28 19 [45 77 77 81]<br>向左排序：[11 13 18] 19 19 28 [45 77 77 81]<br>往右排序：[11 13 18] 19 19 [28 45 77 77 81]<br>向左排序：[11 13 18 19 19] [28 45 77 77 81]<br>此时28&gt;=19条件成立排序完成</p><h4 id="3、快速排序"><a href="#3、快速排序" class="headerlink" title="3、快速排序"></a>3、快速排序</h4><p>快速排序（Quicksort）是对冒泡排序的一种改进快速排序的基本思想：<br>首先选取一个记录作为枢(shu)轴，不失一般性，可选第一个记 录，依它的关键字为基准重排其余记录，将所有关键字比它大的记录都安置在它之后，而将所有关键字比它小的记录都安置在之前，由此完成一趟快速排序；之后，分别对由一趟排序分割成的两个子序列进行快速排序，在大数据情况下要使用快速排序 </p><p>列如：<br>数组元素&gt;<br>5 1 7 2 6 4 3 16 </p><p>思路：<br>取第一个数，把小于它的数往左移动，把大于它的数右移动<br>1）最左侧大于5的为7，最右侧小于5的为3,7与3对调<br>以5为枢轴&gt;<br>5 1 3 2 6 4 7 16 </p><p>2）全部对调完成，此时左侧小于5，右边大于5<br>5 1 3 2 | 6 4 7 16 </p><p>3）5移动到分割位置<br>1 3 2 5 6 4 7 16 </p><p>4）如果把数组元素分为三部分的话 左侧&lt;中间&lt;右侧<br>1 3 2 | 5 | 6 4 7 16<br>此时只需对两侧再重复以上操作就可以了 </p><p>5）重复以上操作<br>1 3 2 &gt;<br>1 2 3<br>此时左侧<br>6 4 7 16 &gt;<br>4 6 7 16<br>简单来说：定义基数，比它小的往左排，比它大的往右排</p><h4 id="4、归并排序"><a href="#4、归并排序" class="headerlink" title="4、归并排序"></a>4、归并排序</h4><p><strong>归并排序（MERGESORT）</strong><br>是建立在归并操作上的一种有效的排序算法,该算法是采用<br>分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为二路归并</p><p>归并操作(merge)，也叫归并算法，指的是将两个顺序序列合并成一个顺序序列的方法<br>如 设有数列1 8 2 9 3 5 6 4 10 </p><p>1）第一次归并后：{1 8},{2 9},{ 3 5},{ 4 6}，{10}此时两两元素排序完的归并 </p><p>2）第二次归并后：{1 2 8 9}，{ 3 4 5 6} ，{10}此时两两元素归并<br>1与2 寻找最小数 1<br>8与2 寻找最小数 2<br>8与9寻找最小数 8<br>{1 2 8 9} </p><p>3）第三次归并后：{1 2 3 4 5 6 8 9} , {10}此时两两元素归并<br>1与3寻找到最小数1 {1}<br>2与3寻找最小数2 {1 2}<br>8与3寻找最小数3 {1 2 3}<br>8与4寻找最小数4 {1 2 3 4}<br>8与5寻找最小数5 {1 2 3 4 5}<br>8与6寻找最小数6 {1 2 3 4 5 6}<br>8 9 落下{1 2 3 4 5 6 8 9}<br>4）第四次归并后：{1 2 3 4 5 6 8 9 10}<br>思路：循环找到最小值落下</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据基础 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据压缩、数据倾斜join操作</title>
      <link href="/2019/02/17/shu-ju-ya-suo-shu-ju-qing-xie-join-cao-zuo/"/>
      <url>/2019/02/17/shu-ju-ya-suo-shu-ju-qing-xie-join-cao-zuo/</url>
      
        <content type="html"><![CDATA[<h3 id="1、数据压缩发生阶段"><a href="#1、数据压缩发生阶段" class="headerlink" title="1、数据压缩发生阶段"></a>1、数据压缩发生阶段</h3><table><thead><tr><th align="center">端</th><th align="center">操作</th><th align="center">Col3</th></tr></thead><tbody><tr><td align="center">数据源</td><td align="center">》数据传输</td><td align="center">数据压缩</td></tr><tr><td align="center">mapper</td><td align="center">map端输出压缩</td><td align="center"></td></tr><tr><td align="center"></td><td align="center">》数据传输</td><td align="center">数据压缩</td></tr><tr><td align="center">reducer</td><td align="center">reduce端输出压缩</td><td align="center"></td></tr><tr><td align="center"></td><td align="center">》数据传输</td><td align="center">数据压缩</td></tr><tr><td align="center">结果数据</td><td align="center"></td><td align="center"></td></tr></tbody></table><p><strong>设置map端输出压缩</strong><br>1）开启压缩<br>conf.setBoolean<br> //开启map端输出压缩<br> conf.setBoolean(“mapreduce.map.output.compress”,true);</p><p>2）设置具体压缩编码<br>conf.setClass<br> //设置压缩方式<br> //conf.setClass(“mapreduce.map.output.compress.codec”, BZip2Codec.class, CompressionCodec.class);</p><p> conf.setClass(“mapreduce.map.output.compress.codec”, DefaultCodec.class, CompressionCodec.class);</p><p><strong>设置reduce端输出压缩</strong><br>1）设置reduce输出压缩<br>FileOutputFormat.setCompressOutput</p><p>//设置reduce端输出压缩<br>FileOutputFormat.setCompressOutput(job,true);</p><p>2）设置具体压缩编码<br>FileOutputFormat.setOutputCompressorClass</p><p>//设置压缩方式<br>//FileOutputFormat.setOutputCompressorClass(job,BZip2Codec.class);</p><p>//FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);</p><p>FileOutputFormat.setOutputCompressorClass(job,DefaultCodec.class);<br>hive数据仓库：mapreduce 用hsql处理大数据</p><h3 id="2、压缩编码使用场景"><a href="#2、压缩编码使用场景" class="headerlink" title="2、压缩编码使用场景"></a>2、压缩编码使用场景</h3><h4 id="1-gt-Gzip压缩方式"><a href="#1-gt-Gzip压缩方式" class="headerlink" title="1-&gt; Gzip压缩方式"></a>1-&gt; Gzip压缩方式</h4><p>压缩率比较高，并且压缩解压缩速度很快<br>hadoop自身支持的压缩方式，用gzip格式处理数据就像直接处理文本数据是完全一样<br>的；<br>在linux系统自带gzip命令，使用很方便简洁<br>不支持split<br>使用每个文件压缩之后大小需要在128M以下（块大小）<br>200M-》设置块大小</p><h4 id="2-gt-LZO压缩方式"><a href="#2-gt-LZO压缩方式" class="headerlink" title="2-&gt;LZO压缩方式"></a>2-&gt;LZO压缩方式</h4><p>压缩解压速度比较快并且，压缩率比较合理<br>支持split<br>在linux系统不可以直接使用，但是可以进行安装<br>压缩率比gzip和bzip2要弱，hadoop本身不支持<br>需要安装</p><h4 id="3-gt-Bzip2压缩方式"><a href="#3-gt-Bzip2压缩方式" class="headerlink" title="3-&gt;Bzip2压缩方式"></a>3-&gt;Bzip2压缩方式</h4><p>支持压缩，具有很强的压缩率。hadoop本身支持<br>linux中可以安装<br>压缩解压缩速度很慢</p><h4 id="4-gt-Snappy压缩方式"><a href="#4-gt-Snappy压缩方式" class="headerlink" title="4-&gt;Snappy压缩方式"></a>4-&gt;Snappy压缩方式</h4><p>压缩解压缩速度很快，而且有合理的压缩率<br>不支持split</p><h3 id="3、数据倾斜"><a href="#3、数据倾斜" class="headerlink" title="3、数据倾斜"></a>3、数据倾斜</h3><p>reduce join<br>数据倾斜就是我们在计算数据的时候，数据的分散度不够，导致大量的数据集中到了一台或者几台机器上计算，这些数据的计算速度远远低于平均计算速度，导致整个计算过程过慢</p><h3 id="4、Hadoop中有哪些组件"><a href="#4、Hadoop中有哪些组件" class="headerlink" title="4、Hadoop中有哪些组件"></a>4、Hadoop中有哪些组件</h3><p>HDFS：数据的分布式存储<br>MapReduce:数据的分布式计算<br>Yarn:资源调度(cpu/内存…)<br>Yarn节点：resourceManager、nodeManager</p><h3 id="5、优化"><a href="#5、优化" class="headerlink" title="5、优化"></a>5、优化</h3><p>MapReduce程序的编写过程中考虑的问题<br>优化目的：提高程序运行的效率<br>优化方案：<br>存储和处理海量数据，如何优化MR<br>影响MR程序的因素<br>1）硬件<br>压缩<br>CPU/磁盘(固态、机械)/内存/网络… </p><p>2）I/O优化<br>传输<br>-》maptask与reducetask合理设置个数<br>-》数据倾斜（reducetask-》merge）<br>避免出现数据倾斜<br>-》大量小文件情况 （combineTextInputFormat）<br>-》combiner优化（不影响业务逻辑）</p><p>具体优化方式：<br>MR（数据接入、Map、Reduce、IO传输、处理倾斜、参数优化）<br>数据接入：小文件的话 进行合并 ，namenode存储元数据信息，sn<br>解决方式：CombineTextInputFormat</p><p>Map:会发生溢写，如果减少溢写次数也能达到优化<br>溢写内存增加这样就减少了溢写次数<br>解决方式：mapred-site.xml<br>属性：<br>mapreduce.task.io.sort.mb<br>100<br>调大</p><p>mapreduce.map.sort.spill.percent<br>0.8<br>调大</p><p>combiner:map后优化</p><p>Reduce:reduceTask设置合理的个数<br>写mr程序可以合理避免写reduce阶段<br>设置map/reduce共存<br>属性：<br>mapred-site.xml<br>mapreduce.job.reduce.slowstart.completedmaps<br>0.05<br>减少</p><p><strong>IO传输：压缩</strong><br>数据倾斜：避免出现数据倾斜，map端合并。手动的对数据进行分段处理，合理的<br>分区</p><p><strong>JVM重用</strong><br>不关JVM<br>一个map运行一个jvm,开启重用，在运行完这个map后JVM继续运行其它map。<br>线程池<br>属性：mapreduce.job.jvm.numtasks<br>20<br>启动40%运行时间</p><h3 id="6、进行两个表的拼接"><a href="#6、进行两个表的拼接" class="headerlink" title="6、进行两个表的拼接"></a>6、进行两个表的拼接</h3><p><strong>DistributedCacheMapper类</strong></p><pre><code>package com.hsiehchou.mapjoin;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.BufferedReader;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.net.URI;import java.util.HashMap;/** * mapjoin * 完成两张表数据的关联操作 */public class DistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; {    HashMap&lt;String, String&gt; pdMap = new HashMap&lt;String, String&gt;();    @Override    protected void setup(Context context) throws IOException, InterruptedException {        //1.加载缓存文件        URI[] cacheFiles = context.getCacheFiles();        BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(cacheFiles[0].getPath()), &quot;UTF-8&quot;));        //这里可以将文件放在当前项目文件下，如果不放就用上面的那两句        //BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(&quot;pd.txt&quot;), &quot;UTF-8&quot;));        String line;        //2.判断缓存文件不为空        while(StringUtils.isNotEmpty(line = br.readLine())){            //切割数据            String[] fields = line.split(&quot;\t&quot;);            //缓冲 到 集合; 商品ID  商品名            pdMap.put(fields[0],fields[1]);        }        br.close();    }    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        //1.获取数据        String line = value.toString();        //2.切分数据        String[] fields = line.split(&quot;\t&quot;);        //3.获取商品的pid,商品名称        String pid = fields[1];        String pName = pdMap.get(pid);        //4.拼接        line = line + &quot;\t&quot; + pName;        //5.输出        context.write(new Text(line),NullWritable.get());    }}</code></pre><p><strong>DistributedCacheDriver类</strong></p><pre><code>package com.hsiehchou.mapjoin;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.net.URI;import java.net.URISyntaxException;public class DistributedCacheDriver {    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException, URISyntaxException {      //创建job任务      Configuration conf = new Configuration();      Job job = Job.getInstance(conf);      //指定jar包位置      job.setJarByClass(DistributedCacheDriver.class);      //关联使用的Mapper      job.setMapperClass(DistributedCacheMapper.class);      //设置最终的输出的数据类型      job.setOutputKeyClass(Text.class);      job.setOutputValueClass(NullWritable.class);      //设置数据输入的路径      FileInputFormat.setInputPaths(job,new Path(&quot;e://test//table//in&quot;));      //设置数据输出的路径      FileOutputFormat.setOutputPath(job,new Path(&quot;e://test//table//out&quot;));      //加载缓存数据      job.addCacheFile(new URI(&quot;file:///e:/test/inputcache/pd.txt&quot;));      //注意：没有跑reducer  需要指定reduceTask为0      job.setNumReduceTasks(0);      //提交任务      boolean rs = job.waitForCompletion(true);      System.exit(rs? 0:1);    }}</code></pre><p><strong>本地模式测试</strong><br>URI[] cacheFiles = context.getCacheFiles();<br>BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(cacheFiles[0].getPath()), “UTF-8”));    </p><p><strong>集群模式时</strong><br>conf.set(“mapreduce.framework.name”, “yarn”);yarn模式<br>job.addCacheFile(new URI(“hdfs:///test2/pd.txt”));//添加hdfs文件做缓存</p>]]></content>
      
      
      <categories>
          
          <category> 大数据实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据基础 </tag>
            
            <tag> combiner </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据之排序、combiner、压缩</title>
      <link href="/2019/02/16/da-shu-ju-zhi-pai-xu-combiner-ya-suo/"/>
      <url>/2019/02/16/da-shu-ju-zhi-pai-xu-combiner-ya-suo/</url>
      
        <content type="html"><![CDATA[<h4 id="1、自定义分区"><a href="#1、自定义分区" class="headerlink" title="1、自定义分区"></a>1、自定义分区</h4><p>需求：统计结果进行分区，根据手机号前三位来进行分区<br>总结：<br>1）自定义类继承partitioner&lt;key,value&gt;<br>2）重写方法getPartition()<br>3）业务逻辑<br>4）在driver类中加入<br>setPartitionerClass<br>5）注意：需要指定setNumReduceTasks(个数=分区数+1) </p><p><strong>新增PhonenumPartitioner类</strong></p><pre><code>package com.hsiehchou.logs1;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;/** * 自定义分区，根据手机号前三位 * 默认分区方式，hash */public class PhonenumPartitioner extends Partitioner&lt;Text, FlowBean&gt; {    @Override    public int getPartition(Text key, FlowBean value, int numPartitions) {        //1.获取手机号的前三位        String phoneNum = key.toString().substring(0, 3);        //2.分区        int partitioner = 4;        if (&quot;135&quot;.equals(phoneNum)){            return 0;        }else if (&quot;137&quot;.equals(phoneNum)){            return 1;        }else if (&quot;138&quot;.equals(phoneNum)){            return 2;        }else if(&quot;139&quot;.equals(phoneNum)){            return 3;        }        return partitioner;    }}</code></pre><p><strong>FlowCountDriver类</strong>中增加</p><pre><code>//加入自定义分区job.setPartitionerClass(PhonenumPartitioner.class);//注意，结果文件几个？job.setNumReduceTasks(5);//7.设置数据输入的路径FileInputFormat.setInputPaths(job, new Path(&quot;E:/test/flow/in&quot;));//8.设置数据输出的路径FileOutputFormat.setOutputPath(job, new Path(&quot;E:/test/flow/out2&quot;));</code></pre><h4 id="2、排序"><a href="#2、排序" class="headerlink" title="2、排序"></a>2、排序</h4><p>需求：每个分区内进行排序？<br>总结：<br>1）实现WritableComparable接口<br>2）重写compareTo方法</p><p>combineTextInputFormat设置切片的大小 maptask</p><p>实现 </p><p><strong>FlowBean类</strong></p><pre><code>package com.hsiehchou.logs2;import org.apache.hadoop.io.WritableComparable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;public class FlowBean implements WritableComparable&lt;FlowBean&gt; {    //定义属性：上行流量 下行流量 总流量总和    private long upFlow;    private long dfFlow;    private long flowsum;    public FlowBean(){}    public FlowBean(long upFlow,long dfFlow){        this.upFlow = upFlow;        this.dfFlow = dfFlow;        this.flowsum = upFlow + dfFlow;    }    public long getUpFlow(){        return upFlow;    }    public void setUpFlow(long upFlow){        this.upFlow = upFlow;    }    public long getDfFlow(){        return dfFlow;    }    public void setDfFlow(long dfFlow){        this.dfFlow = dfFlow;    }    public long getFlowsum(){        return flowsum;    }    public void setFlowsum(long flowsum){        this.flowsum = flowsum;    }    //序列化    public void write(DataOutput out) throws IOException {        out.writeLong(upFlow);        out.writeLong(dfFlow);        out.writeLong(flowsum);    }    //反序列化    public void readFields(DataInput in) throws IOException {        upFlow = in.readLong();        dfFlow = in.readLong();        flowsum = in.readLong();    }    @Override    public String toString() {        return upFlow + &quot;\t&quot; + dfFlow + &quot;\t&quot; + flowsum;    }    public int compareTo(FlowBean o) {        //倒序        return this.flowsum &gt; o.getFlowsum() ? -1:1;    }}</code></pre><p><strong>FlowSortMapper类</strong></p><pre><code>package com.hsiehchou.logs2;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;public class FlowSortMapper extends Mapper&lt;LongWritable,Text,FlowBean,Text&gt; {    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        //1.接入数据        String line = value.toString();        //2.切割 \t        String[] fields = line.split(&quot;\t&quot;);        //3.拿到关键字段:手机号 上行流量 下行流量        String phoneNr = fields[0];        long upFlow = Long.parseLong(fields[1]);        long dfFlow = Long.parseLong(fields[2]);        //4.写出到reducer        context.write(new FlowBean(upFlow,dfFlow),new Text(phoneNr));    }}</code></pre><p><strong>FlowSortReducer类</strong></p><pre><code>package com.hsiehchou.logs2;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;import java.io.IOException;public class FlowSortReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; {    @Override    protected void reduce(FlowBean key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException {        //手机号 流量        context.write(values.iterator().next(),key);    }}</code></pre><p><strong>FlowSortPartitioner类</strong></p><pre><code>package com.hsiehchou.logs2;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;public class FlowSortPartitioner extends Partitioner&lt;FlowBean, Text&gt; {    @Override    public int getPartition(FlowBean key, Text value, int numPartitions) {        //1.获取手机号的前三位        String phoneNum = value.toString().substring(0, 3);        //2.分区        int partitioner = 4;        if (&quot;135&quot;.equals(phoneNum)){            return 0;        }else if (&quot;137&quot;.equals(phoneNum)){            return 1;        }else if (&quot;138&quot;.equals(phoneNum)){            return 2;        }else if(&quot;139&quot;.equals(phoneNum)){            return 3;        }        return partitioner;    }}</code></pre><p><strong>FlowSortDriver类</strong></p><pre><code>package com.hsiehchou.logs2;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;public class FlowSortDriver {    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {        //1.创建job任务        Configuration conf = new Configuration();        Job job = Job.getInstance(conf);        //2.指定kjar包位置        job.setJarByClass(FlowSortDriver.class);        //3.关联使用的Mapper        job.setMapperClass(FlowSortMapper.class);        //4.关联使用的Reducer类        job.setReducerClass(FlowSortReducer.class);        //5.设置mapper阶段输出的数据类型        job.setMapOutputKeyClass(FlowBean.class);        job.setMapOutputValueClass(Text.class);        //6.设置reducer阶段输出的数据类型        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(FlowBean.class);        //加入自定义分区        job.setPartitionerClass(FlowSortPartitioner.class);        //注意，结果文件几个        job.setNumReduceTasks(5);        //7.设置数据输入的路径        FileInputFormat.setInputPaths(job, new Path(&quot;E:/test/flow/out&quot;));        //8.设置数据输出的路径        FileOutputFormat.setOutputPath(job, new Path(&quot;E:/test/flow/out4&quot;));        //9.提交任务        boolean  rs = job.waitForCompletion(true);        System.exit(rs? 0:1);    }}</code></pre><h4 id="3、combiner-合并"><a href="#3、combiner-合并" class="headerlink" title="3、combiner 合并"></a>3、combiner 合并</h4><p>1）combiner是一个组件<br>注意：是Mapper和Reducer之外的一种组件<br>但是这个组件的父类是Reduer</p><p>2）如果想使用combiner继承Reduer即可</p><p>3）通过编写combiner发现与Reducer代码相同<br>只需在Driver端指定<br>setCombinerClass(WordCountReduer.class)<br>注意：前提是不能影响业务逻辑&lt;a,1&gt;&lt;c,1&gt; &lt;a,2&gt;&lt;a,1&gt; = &lt;a,3&gt;<br>数学运算：<br>(3 + 5 + 7)/3 = 5<br>(2 + 6)/2 = 4<br>不进行局部累加：（3 + 5 + 7 + 2 + 6）/5 = 23/5<br>进行了局部累加：（5+4）/2 = 9/2=4.5 不等于 23/5=4.6</p><h4 id="4、数据压缩"><a href="#4、数据压缩" class="headerlink" title="4、数据压缩"></a>4、数据压缩</h4><p>为什么对数据进行压缩？<br>MapReduce操作需要对大量数据进行传输<br>压缩技术有效的减少底层存储系统读写字节数，HDFS<br>压缩提高网络带宽和磁盘空间效率<br>数据压缩节省资源，减少网络I/O</p><p>通过压缩可以影响到MapReduce性能。(小文件优化，combiner)代码角度进行优化</p><p>注意：利用好压缩提高性能，运用不好会降低性能<br>压缩 -》 解压缩 </p><p><strong>mapreduce常用的压缩编码</strong></p><table><thead><tr><th align="center">压缩格式</th><th align="center">是否需要安装</th><th align="center">文件拓展名</th><th align="center">是否可以切分</th></tr></thead><tbody><tr><td align="center">DEFAULT</td><td align="center">直接使用</td><td align="center">.deflate</td><td align="center">否</td></tr><tr><td align="center">bzip2</td><td align="center">直接使用</td><td align="center">.bz2</td><td align="center">是</td></tr><tr><td align="center">Gzip</td><td align="center">直接使用</td><td align="center">.gz</td><td align="center">否</td></tr><tr><td align="center">LZO</td><td align="center">需要安装</td><td align="center">.lzo</td><td align="center">是</td></tr><tr><td align="center">Snappy</td><td align="center">需要安装</td><td align="center">.snappy</td><td align="center">否</td></tr></tbody></table><p><strong>性能测试</strong></p><table><thead><tr><th align="center">压缩格式</th><th align="center">原文件大小</th><th align="center">压缩后大小</th><th align="center">压缩速度</th><th align="center">解压速度</th></tr></thead><tbody><tr><td align="center">gzip</td><td align="center">8.3GB</td><td align="center">1.8GB</td><td align="center">20MB/s</td><td align="center">60MB/s</td></tr><tr><td align="center">LZO</td><td align="center">8.3GB</td><td align="center">3GB</td><td align="center">50MB/s</td><td align="center">70MB/s</td></tr><tr><td align="center">bzip2</td><td align="center">8.3GB</td><td align="center">1.1GB</td><td align="center">3MB/s</td><td align="center">10MB/s</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 大数据实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据基础 </tag>
            
            <tag> HDFS </tag>
            
            <tag> combiner </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据之MapReduce小实战</title>
      <link href="/2019/02/14/da-shu-ju-zhi-mapreduce-xiao-shi-zhan/"/>
      <url>/2019/02/14/da-shu-ju-zhi-mapreduce-xiao-shi-zhan/</url>
      
        <content type="html"><![CDATA[<h3 id="手写wordcount的程序"><a href="#手写wordcount的程序" class="headerlink" title="手写wordcount的程序"></a>手写wordcount的程序</h3><h4 id="1、pom-xml"><a href="#1、pom-xml" class="headerlink" title="1、pom.xml"></a>1、pom.xml</h4><pre><code>  &lt;dependencies&gt;    &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs-client --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;            &lt;version&gt;2.7.3&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;</code></pre><h4 id="2、新建Mapper类"><a href="#2、新建Mapper类" class="headerlink" title="2、新建Mapper类"></a>2、新建Mapper类</h4><pre><code>package com.hsiehchou.wordcount;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;/** * 海量数据 * * hello hsiehchou * nihao * * 数据的输入与输出以Key value进行传输 * keyIN:LongWritable(Long) 数据的起始偏移量 * valuewIN:具体数据 * * mapper需要把数据传递到reducer阶段（&lt;hello,1&gt;） * keyOut:单词 Text * valueOut:出现的次数IntWritable * */public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {    //对数据进行打散 ctrl+o    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        //1、接入数据 hello nihao        String line = value.toString();        //2、对数据进行切分        String[] words = line.split(&quot; &quot;);        //3、写出以&lt;hello,1&gt;        for (String w:words){            //写出reducer端            context.write(new Text(w), new IntWritable(1));        }    }}</code></pre><p><strong>mapper端原理</strong></p><p><img src="../../mapper%E7%AB%AF%E5%8E%9F%E7%90%86.PNG" alt="mapper端原理"></p><h4 id="3、新建Reducer类"><a href="#3、新建Reducer类" class="headerlink" title="3、新建Reducer类"></a>3、新建Reducer类</h4><pre><code>package com.hsiehchou.wordcount;import org.apache.curator.framework.recipes.locks.InterProcessReadWriteLock;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;import java.io.IOException;/** * reducer阶段接收的是Mapper输出的数据 * mapper的输出是reducer输入 * * keyIn:mapper输出的key的类型 * valueIn:mapper输出的value的类型 * * reducer端输出的数据类型，想要一个什么样的结果&lt;hello,1888&gt; * keyOut:Text * valueOut:IntWritalble * */public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {    //key--&gt;单词  value--&gt;次数    @Override    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {        //1、记录出现的次数        int sum = 0;        for (IntWritable v:values){            sum += v.get();        }        //2、l累加求和输出        context.write(key, new IntWritable(sum));    }}</code></pre><h4 id="4、新建驱动类"><a href="#4、新建驱动类" class="headerlink" title="4、新建驱动类"></a>4、新建驱动类</h4><pre><code>package com.hsiehchou.wordcount;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;public class WordCountDriver {    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {        //1、创建job任务        Configuration conf = new Configuration();        Job job = Job.getInstance(conf);        //2、指定jar包位置        job.setJarByClass(WordCountDriver.class);        //3、关联使用的Mapper类        job.setMapperClass(WordCountMapper.class);        //4、关联使用的Reducer类        job.setReducerClass(WordCountReducer.class);        //5、设置mapper阶段输出的数据类型        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(IntWritable.class);        //6、设置reducer阶段输出的数据类型        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(IntWritable.class);        //7、设置数据输入的路径        FileInputFormat.setInputPaths(job, new Path(args[0]));        //8设置数据输出的路径        FileOutputFormat.setOutputPath(job, new Path(args[1]));        //9、提交任务        boolean rs = job.waitForCompletion(true);        System.exit(rs ? 0:1);    }}</code></pre><p>运行结果<br>[root@hsiehchou121 ~]# hadoop jar mapreduce-1.0-SNAPSHOT.jar com.hsiehchou.wordcount.WordCountDriver /wc/in /wc/out<br>[root@hsiehchou121 ~]# hdfs dfs -cat /wc/out/part-r-00000<br>fd  1<br>fdgs    1<br>fdsbv   1<br>gd  1<br>hello   3</p><h4 id="5、IDEA的相关使用"><a href="#5、IDEA的相关使用" class="headerlink" title="5、IDEA的相关使用"></a>5、IDEA的相关使用</h4><p>Ctrl+O导入相关未实现的方法<br>Maven中的Lifecycle的package可以直接打包成jar</p><p>案例分析<br>需求：运营商流量日志<br>10086<br>计算每个用户当前使用的总流量<br>思路？总流量 = 上行流量+下行流量<br>三个字段：手机号 上行流量 下行流量<br>技术选型：PB+<br>数据分析：海量数据(存储hdfs)<br>海量数据计算(分布式计算框架MapReduce)</p><h4 id="4、实现"><a href="#4、实现" class="headerlink" title="4、实现"></a>4、实现</h4><p><strong>FlowBean类</strong></p><pre><code>package com.hsiehchou.logs;import org.apache.hadoop.io.Writable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;/** * 封装数据类型需要怎么做 * hadoop数据类型实现了序列化接口 * 如果自定义需要实现这个序列化接口 */public class FlowBean implements Writable {    //定义属性：上行流量 下行流量 总流量总和    private long upFlow;    private long dfFlow;    private long flowsum;    public FlowBean(){}    public FlowBean(long upFlow, long dfFlow){        this.upFlow = upFlow;        this.dfFlow = dfFlow;        this.flowsum = upFlow + dfFlow;    }    public long getUpFlow(){        return upFlow;    }    public void setUpFlow(long upFlow){        this.upFlow = upFlow;    }    public long getDfFlow(){        return dfFlow;    }    public void setDfFlow(long dfFlow){        this.dfFlow = dfFlow;    }    public long getFlowsum(){        return flowsum;    }    public void setFlowsum(long flowsum){        this.flowsum = flowsum;    }    //序列化    public void write(DataOutput out) throws IOException {        out.writeLong(upFlow);        out.writeLong(dfFlow);        out.writeLong(flowsum);    }    //反序列化    public void readFields(DataInput in) throws IOException {        upFlow = in.readLong();        dfFlow = in.readLong();        flowsum = in.readLong();    }    @Override    public String toString() {        return upFlow + &quot;\t&quot; + dfFlow + &quot;\t&quot; + flowsum;    }}</code></pre><p><strong>FlowCountMapper类</strong></p><pre><code>package com.hsiehchou.logs;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;/** * keyIN: * valueIN: * * 思路：根据想要的结果的kv类型  手机号  流量总和（上行+下行）自定义类 * keyOut: * valueOut: */public class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt; {    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        //1、接入数据源        String line = value.toString();        //2、切割   \t        String[] fields = line.split(&quot;\t&quot;);        //3、拿到关键字段        String phoneNr = fields[1];        long upFlow = Long.parseLong(fields[fields.length - 3]);        long dfFlow = Long.parseLong(fields[fields.length - 2]);        //4、写出到reducer        context.write(new Text(phoneNr), new FlowBean(upFlow,dfFlow));    }}</code></pre><p><strong>FlowCountReducer类</strong></p><pre><code>package com.hsiehchou.logs;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;import java.io.IOException;public class FlowCountReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; {    @Override    protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException {        long upFlow_sum = 0;        long dfFlow_sum = 0;        for (FlowBean v:values){            upFlow_sum += v.getUpFlow();            dfFlow_sum += v.getDfFlow();        }        FlowBean rsSum = new FlowBean(upFlow_sum, dfFlow_sum);        //输出结果        context.write(key, rsSum);    }}</code></pre><p><strong>FlowCountDriver类</strong></p><pre><code>package com.hsiehchou.logs;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;public class FlowCountDriver {    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {        //1.创建job任务        Configuration conf = new Configuration();        Job job = Job.getInstance(conf);        //2.指定kjar包位置        job.setJarByClass(FlowCountDriver.class);        //3.关联使用的Mapper        job.setMapperClass(FlowCountMapper.class);        //4.关联使用的Reducer类        job.setReducerClass(FlowCountReducer.class);        //5.设置mapper阶段输出的数据类型        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(FlowBean.class);        //6.设置reducer阶段输出的数据类型        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(FlowBean.class);        //优化含有大量小文件的数据        //设置读取数据切片的类        job.setInputFormatClass(CombineTextInputFormat.class);        //最大切片大小8M        CombineTextInputFormat.setMaxInputSplitSize(job, 8388608);        //最小切片大小6M        CombineTextInputFormat.setMinInputSplitSize(job, 6291456);        //7.设置数据输入的路径        FileInputFormat.setInputPaths(job, new Path(args[0]));        //8.设置数据输出的路径        FileOutputFormat.setOutputPath(job, new Path(args[1]));        //9.提交任务        boolean  rs = job.waitForCompletion(true);        System.exit(rs? 0:1);    }}</code></pre><p>运行结果<br>[root@hsiehchou121 ~]# hdfs dfs -mkdir -p /flow/in<br>[root@hsiehchou121 ~]# hdfs dfs -put HTTP_20180313143750.dat /flow/in<br>[root@hsiehchou121 ~]# hadoop jar mapreduce-1.0-SNAPSHOT.jar com.hsiehchou.logs.FlowCountDriver /flow/in /flow/out<br>[root@hsiehchou121 ~]# hdfs dfs -cat /flow/out/part-r-00000<br>13480253104    120       1320      1440<br>13502468823    735       11349     12084<br>13510439658    1116      954       2070<br>13560436326    1136      94        1230<br>13560436666    1136      94        1230<br>13560439658    918       4938      5856<br>13602846565    198       910       1108<br>13660577991    660       690       1350<br>13719199419    240       0         240<br>13726130503    299       681       980<br>13726238888    2481      24681     27162<br>13760778710    120       120       240<br>13822544101    264       0         264<br>13884138413    4116      1432      5548<br>13922314466    3008      3720      6728<br>13925057413    11058     4243      15301<br>13926251106    240       0         240<br>13926435656    132       1512      1644<br>15013685858    369       338       707<br>15889002119    938       380       1318<br>15920133257    316       296       612<br>18212575961    1527      2106      3633<br>18320173382    9531      212       9743</p><p><strong>小文件优化</strong></p><p>如果企业中存在海量的小文件数据<br>TextInputFormat按照文件规划切片，文件不管多小都是一个单独的切片，启动mapt<br>ask任务去执行，这样会产生大量的maptask，浪费资源</p><p><strong>优化手段</strong></p><p>小文件合并大文件，如果不动这个小文件内容</p>]]></content>
      
      
      <categories>
          
          <category> 大数据实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据基础 </tag>
            
            <tag> HDFS </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据基础之HDFS3</title>
      <link href="/2019/02/12/da-shu-ju-ji-chu-zhi-hdfs3/"/>
      <url>/2019/02/12/da-shu-ju-ji-chu-zhi-hdfs3/</url>
      
        <content type="html"><![CDATA[<h4 id="1、hdfs的副本的配置"><a href="#1、hdfs的副本的配置" class="headerlink" title="1、hdfs的副本的配置"></a>1、hdfs的副本的配置</h4><p>修改hdfs-site.xml文件</p><pre><code>&lt;!-- 注释配置数据块的冗余度，默认是3 --&gt;&lt;property&gt;    &lt;name&gt;dfs.replication&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt;&lt;/property&gt;&lt;!--注释配置HDFS的权限检查，默认是true--&gt;&lt;property&gt;    &lt;name&gt;dfs.permissions&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; &lt;property&gt;    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;    &lt;value&gt;hsiehchou122:50090&lt;/value&gt;&lt;/property&gt;</code></pre><p>需要同步到其它机器：<br>scp hdfs-site.xml hsiehchou122:<code>$PWD</code><br>scp hdfs-site.xml hsiehchou123:<code>$PWD</code><br>scp hdfs-site.xml hsiehchou124:<code>$PWD</code></p><p>这里我划重点(亲自经历)<br>如果原来的分布式hadoop集群的主节点有Secondary NameNode，需要配置到其他节点，因为如果主节点挂了，其也是挂了，它的作用是在HDFS中提供一个检查点，相当于NameNode的助手节点<br>职责是：合并NameNode的edit logs到fsimage文件中</p><h4 id="2、hadoop启动方式"><a href="#2、hadoop启动方式" class="headerlink" title="2、hadoop启动方式"></a>2、hadoop启动方式</h4><p>1）启动hdfs集群<br>start-dfs.sh </p><p>2）启动yarn集群<br>start-yarn.sh </p><p>3）启动hadoop集群<br>start-all.sh</p><h4 id="3、大数据干什么的"><a href="#3、大数据干什么的" class="headerlink" title="3、大数据干什么的"></a>3、大数据干什么的</h4><p>1）海量数据的存储(mysql/oracle)<br>分布式文件系统hdfs<br>dfs-&gt;Hdfs<br>mapreduce-&gt;mapreduce<br>bigtable-&gt;hbase<br>分而治之！</p><p>2）海量数据的计算<br>分布式计算框架mapreduce<br>配置checkpoint时间</p><pre><code>&lt;property&gt;&lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt;&lt;value&gt;7200&lt;/value&gt;&lt;/property&gt;</code></pre><p>systemctl set-default graphical.target由命令行模式更改为图形界面模式<br>systemctl set-default multi-user.target由图形界面模式更改为命令行模式</p><h4 id="4、hdfs-namenode工作机制"><a href="#4、hdfs-namenode工作机制" class="headerlink" title="4、hdfs-namenode工作机制"></a>4、hdfs-namenode工作机制</h4><p>1）加载编辑日志与镜像文件到内存（NameNode）<br>edits_0001<br>edits_0002<br>fsimage fsimage fsimage </p><p>2）户端发起命令（client）<br>hdfs dfs -ls / </p><p>3）动正在写的edits（NameNode） </p><p>4）录操作日志更新 滚动日志（NameNode） </p><p>5）贝到Secondary NameNode<br>NameNode请求是否需要checkpoint</p><p>Secondary NameNode 触发checkpoint条件：<br>1）定时的时间<br>Secondary NameNode询问NameNode是否需要checkpoint<br>直接带回NameNode是否检查结果</p><p>2）edits中数据已满<br>Secondary NameNode请求执行checkpoint </p><p>3）NameNode滚动正在写的edits日志</p><p>4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode</p><p>5）Secondary NameNode加载编辑日志和镜像文件到内存，并行合并 </p><p>6）生成新的镜像文件fsimage.checkpoint </p><p>7）拷贝fsimage.chkpoint到NameNode </p><p>8）NameNode对fsimage.checkpoint重命名成fsimage</p><h4 id="5、hadoop2-8-4安装部署"><a href="#5、hadoop2-8-4安装部署" class="headerlink" title="5、hadoop2.8.4安装部署"></a>5、hadoop2.8.4安装部署</h4><p>1）准备工作<br>设置主机名：vi /etc/hostname<br>注意：需要重启 reboot<br>设置映射：vi /etc/hosts<br>设置免密登录：ssh-keygen<br>ssh-copy-id hsiehchou121 </p><p>2）安装jdk<br>上传安装包<br>CRT:alt+p </p><p>解压<br>tar -zxvf .tar.gz</p><p>配置环境变量<br>export JAVA_HOME=/root/hd/jdk1.8.0_192<br>export PATH=<code>$JAVA_HOME/bin:$PATH</code></p><p>注意：需要source /etc/profile<br>分发jdk<br>scp jdk hsiehchou122:/root/hd<br>scp /etc/profile hsiehchou122:/etc/<br>source /etc/profile</p><p>3）安装hadoop<br>上传安装包<br>alt + p<br>解压<br>tar -zxvf .tar.gz<br>修改配置文件<br>core-site.xml</p><pre><code>&lt;property&gt;    &lt;name&gt;fs.defaultFS&lt;/name&gt;    &lt;value&gt;hdfs://hsiehchou121:9000&lt;/value&gt;&lt;/property&gt;hdfs-site.xml&lt;property&gt;    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;    &lt;value&gt;/root/hd/dfs/name&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;    &lt;value&gt;/root/hd/dfs/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;    &lt;value&gt;hsiehchou122:50090&lt;/value&gt;&lt;/property&gt;mapred-site.xml&lt;property&gt;    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;    &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;yarn-site.xml&lt;property&gt;    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;    &lt;value&gt;hsiehchou121&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt;        &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;        &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt;        &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;        &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;</code></pre><h4 id="6、配置环境变量"><a href="#6、配置环境变量" class="headerlink" title="6、配置环境变量"></a>6、配置环境变量</h4><p>export HADOOP_HOME=/root/hd/hadoop-2.8.4<br>export PATH=<code>$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH</code><br>修改slaves文件加入从节点<br>格式化namenode<br>hadoop namenode -format<br>启动:start-all.sh</p><h4 id="7、hadoopMapReduce"><a href="#7、hadoopMapReduce" class="headerlink" title="7、hadoopMapReduce"></a>7、hadoopMapReduce</h4><p>官方：Apache™Hadoop®项目开发了用于可靠，可扩展的分布式计算的开源软件<br>Apache Hadoop软件库是一个框架，允许使用简单的编程模型跨计算机集群分布式处理 大型数据集。它旨在从单个服务器扩展到数千台计算机，每台计算机都提供本地计算和 存储。该库本身不是依靠硬件来提供高可用性，而是设计用于检测和处理应用层的故 障，从而在计算机集群之上提供高可用性服务，每个计算机都可能容易出现故障<br>阿里的Flink（9000万欧元） Blink</p><p>MapReduce分布式计算程序的编程框架。基于hadoop的数据分析的应用<br>MR优点：<br>1)框架易于编程<br>2)可靠容错（集群）<br>3)可以处理海量数据（1T+ PB+） 1PB = 1024TB<br>4)拓展性，可以通过动态的增减节点来拓展计算能力</p><h4 id="8、MapReduce的思想"><a href="#8、MapReduce的思想" class="headerlink" title="8、MapReduce的思想"></a>8、MapReduce的思想</h4><p>数据:海量单词<br>hello reba<br>hello mimi<br>hello liya<br>mimi big<br>需求：对每个单词出现的次数统计出来<br>思想：分而治之！<br>解决方式：<br>1）每个单词记录一次(map阶段)<br>&lt;hello,1&gt; &lt;reba,1&gt; &lt;hello,1&gt; &lt;mimi,1&gt; </p><p>2）相同单词的key不变，value累加求和即可（reduce阶段）<br>&lt;hello,1+1+1&gt;<br>对数据进行计算</p><h4 id="9、对wordcount例子程序分析"><a href="#9、对wordcount例子程序分析" class="headerlink" title="9、对wordcount例子程序分析"></a>9、对wordcount例子程序分析</h4><p>1）整个wordcount分为几个阶段？<br>三个 </p><p>2）有哪几个阶段？<br>mapper<br>reducer<br>driver </p><p>3）每个阶段有什么作用<br>mapper:对数据进行打散&lt;hello,1&gt;&lt;mimi,1&gt;<br>reducer:对数据进行聚合&lt;hello,1+1+1&gt;<br>driver:提交任务 </p><p>4）详解</p><p><strong>Mapper阶段</strong></p><p>将数据转换为String<br>对数据进行切分处理<br>把每个单词后加1<br>输出到reducer阶段</p><p>Reducer阶段<br>根据key进行聚合<br>输出key出现总的次数</p><p>Driver阶段<br>创建任务<br>关联使用的Mapper/Reducer类<br>指定mapper输出数据的kv类型<br>指定reducer输出的数据的kv类型<br>指定数据的输入路径与输出路径<br>提交</p><h4 id="10、hadoop数据类型"><a href="#10、hadoop数据类型" class="headerlink" title="10、hadoop数据类型"></a>10、hadoop数据类型</h4><p>我们看到的wordcount程序中的泛型中的数据类型其实是hadoop的序列化的数据类<br>型<br>为什么要进行序列化？用java的类型行不行？（可以）<br>Java的序列化:Serliazable太重<br>hadoop自己开发了一套序列化机制。Writable，精简高效。海量数据<br>hadoop序列化类型与Java数据类型</p><table><thead><tr><th align="center">Java数据类型</th><th align="center">Hadoop序列化类型</th></tr></thead><tbody><tr><td align="center">int</td><td align="center">IntWritable</td></tr><tr><td align="center">long</td><td align="center">LongWritable</td></tr><tr><td align="center">boolean</td><td align="center">BooleanWritable</td></tr><tr><td align="center">byte</td><td align="center">ByteWritable</td></tr><tr><td align="center">float</td><td align="center">FloatWritable</td></tr><tr><td align="center">double</td><td align="center">DoubleWritable</td></tr><tr><td align="center">String</td><td align="center">Text</td></tr></tbody></table><h4 id="11、wordcount测试"><a href="#11、wordcount测试" class="headerlink" title="11、wordcount测试"></a>11、wordcount测试</h4><p>1）本地模式<br>2）集群模式<br>hadoop jar .jar wordcount /wc/in /wc/out</p><p>hadoop jar mapreduce-1.0-SNAPSHOT.jar 全类名 /wc/in /wc/out</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据基础 </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据基础之HDFS2</title>
      <link href="/2019/02/09/da-shu-ju-ji-chu-zhi-hdfs2/"/>
      <url>/2019/02/09/da-shu-ju-ji-chu-zhi-hdfs2/</url>
      
        <content type="html"><![CDATA[<h4 id="1、HDFS下载文件原理"><a href="#1、HDFS下载文件原理" class="headerlink" title="1、HDFS下载文件原理"></a>1、HDFS下载文件原理</h4><p>1、请求<br>2、创建client<br>DFS –&gt;DFSClient<br>3、建立RPC通信<br>4、得到代理对象proxy，通过代理对象请求得到文件元信息<br>5、查找元信息<br>6、返回元信息<br>7、创建输入流<br>8、下载数据块<br>FSDataInputStream<br>9、整合下载文件</p><p>注意：HDFS维护失败列表</p><h4 id="2、安全模式-safe-mode"><a href="#2、安全模式-safe-mode" class="headerlink" title="2、安全模式 safe mode"></a>2、安全模式 safe mode</h4><p>检查副本率是否满足配置要求。副本率不够的时候，会水平复制，当下次那个挂掉的节点如果又活过来的话，副本数就会超过N了，就超了，系统会自动选一个多余的副本删掉<br>（1）冗余度：dfs.replication 3.有几个冗余的副本<br>hdfs-site.xml</p><pre><code>  &lt;!--注释配置数据块的冗余度，默认是3--&gt;  &lt;property&gt;         &lt;name&gt;dfs.replication&lt;/name&gt;         &lt;value&gt;1&lt;/value&gt;  &lt;/property&gt;</code></pre><p>（2）副本率：数据块实际冗余度（M），HDFS配置的数据块应该具有的冗余度（N）<br>M/N*100%;<br>例如：知否知否.avi M=2；HDFS配置的 N=3；<br>2/3=0.667。要求的副本率为 0.99，系统会水平复制数据块到其他节点<br>如果是副本率过高，M=6，N=3，副本率=2；大于0.99.系统会删除多余的数据块<br>在安全模式下 无法操作HDFS，因为正在进行副本率的检查工作<br>进入或查看安全模式的命令：<br>hdfs dfsadmin -safemode get/enter/leave/wait</p><h4 id="3、快照：是一种备份，默认：HDFS快照是关闭"><a href="#3、快照：是一种备份，默认：HDFS快照是关闭" class="headerlink" title="3、快照：是一种备份，默认：HDFS快照是关闭"></a>3、快照：是一种备份，默认：HDFS快照是关闭</h4><p>一般不建议使用<br>快照的本质：将需要备份的数据放到一个隐藏目录下<br>（1）开启和关闭快照<br>hdfs dfsadmin -allowSnapshot <code>&lt;snapshotDir&gt;</code><br>hdfs dfsadmin -disallowSnapshot<br>hdfs lsSnapshottableDir //查看开启快照的所有文件夹  </p><p>（2）创建快照<br>需要创建快照的目录 快照目录的名字<br>hdfs dfs -createSnapshot /test1 backup_test1_20190216<br>快照打出的日志：<br>Created snapshot /test1/.snapshot/backup_test1_20190216 </p><p>（3）删除快照<br>hdfs dfs -deleteSnapshot /test1 backupt1_test1_20190216 </p><p>（4）恢复快照<br>hdfs dfs -cp /test1/.snapshot/backup_test1_20190216/a.txt /test1</p><h4 id="4、回收站：默认HDFS的回收站禁用"><a href="#4、回收站：默认HDFS的回收站禁用" class="headerlink" title="4、回收站：默认HDFS的回收站禁用"></a>4、回收站：默认HDFS的回收站禁用</h4><p>（1）回收站的配置：<br>core-site.xml fs.trash.interval(时间间隔 分钟)<br>关闭集群后才能起作用</p><pre><code>&lt;!--配置回收站，单位是分钟，默认是0--&gt;&lt;property&gt;    &lt;name&gt;fs.trash.interval&lt;/name&gt;    &lt;value&gt;1440&lt;/value&gt;&lt;/property&gt;</code></pre><p>（2）本质是剪切：回收站开启之后，会把删除的文件放到一个/user/root/.Trash/Current<br>（3）回收站恢复也就是粘贴的过程<br>hdfs dfs -cp /user/root/.Trash/Current /</p><h4 id="5、配额：Quota"><a href="#5、配额：Quota" class="headerlink" title="5、配额：Quota"></a>5、配额：Quota</h4><p>（1）名称配额<br>限定HDFS目录下，存放文件（目录）的个数&gt;1，最多存放N-1个<br>setQuota–指定名称配额<br>clrQuota–清除名称配额<br>例如：<br>hdfs dfs -mkdir /myquota1<br>hdfs dfsadmin -setQuota 3 /myquota1<br>hdfs dfs -put ~/a.txt /myquota1—-第1个<br>hdfs dfs -put ~/student01.txt /myquota1–第2个<br>hdfs dfs -put ~/students01.txt /myquota1—第3个 无法放<br>错误：put: The NameSpace quota (directories and files) of directory /myquota1 is exceeded: quota=3 file count=4 </p><p>（2）空间配额 –必须要大于 默认数据块大小<br>setSpaceQuota<br>clrSpaceQuota</p><h4 id="6、HDFS底层原理-RPC"><a href="#6、HDFS底层原理-RPC" class="headerlink" title="6、HDFS底层原理-RPC"></a>6、HDFS底层原理-RPC</h4><p>Remote Procedure Call：远程过程调用，调用代码不在本地执行，实现调用者与被调用者之间的连接和通信 </p><p>基于Client server，相当于 DFSClient 相当于客户端。NameNode集群相当于Server</p><h4 id="7、HDFS底层原理-代理对象Proxy"><a href="#7、HDFS底层原理-代理对象Proxy" class="headerlink" title="7、HDFS底层原理-代理对象Proxy"></a>7、HDFS底层原理-代理对象Proxy</h4><p>（1）代理—明星的经纪人<br>是一种设计模式，提供了对目标对象的另一种访问方式。通过代理对象访问目标对象<br>（2）代理分为静态代理和动态代理<br>a、静态代理：接口的定义 实现接口，被代理对象与对象实现相同的接口<br>b、动态代理：接口的定义 不需要实现接口（匿名内部类+反射 invoke）</p><h4 id="8、RPC与Proxy程序示例"><a href="#8、RPC与Proxy程序示例" class="headerlink" title="8、RPC与Proxy程序示例"></a>8、RPC与Proxy程序示例</h4><p>针对log4j warn<br>log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).<br>log4j:WARN Please initialize the log4j system properly.<br>log4j:WARN See <a href="http://logging.apache.org/log4j/1.2/faq.html#noconfig" target="_blank" rel="noopener">http://logging.apache.org/log4j/1.2/faq.html#noconfig</a> for more info.<br>可以在src/resource/通过 增加 log4j.properties解决</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据基础 </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据基础之HDFS1</title>
      <link href="/2019/02/07/da-shu-ju-ji-chu-zhi-hdfs1/"/>
      <url>/2019/02/07/da-shu-ju-ji-chu-zhi-hdfs1/</url>
      
        <content type="html"><![CDATA[<h4 id="1、免密码登录的原理和配置"><a href="#1、免密码登录的原理和配置" class="headerlink" title="1、免密码登录的原理和配置"></a>1、免密码登录的原理和配置</h4><p>ssh不对称加密算法（加密和解密是两个文件）（对称加密： 加密和解密文件是同一个）<br>（1）公钥–锁：给出去 给其他机器<br>（2）私钥–钥匙：自己留着，解密<br>step1:ssh-keygen -t rsa(3次回车)<br>step2:ssh-copy-id -i ~/.ssh/id_rsa.pub root@hsiehchou121(自己也要拷贝给自己)</p><h4 id="2、Hadoop安装—全分布模式-（重点）"><a href="#2、Hadoop安装—全分布模式-（重点）" class="headerlink" title="2、Hadoop安装—全分布模式 （重点）"></a>2、Hadoop安装—全分布模式 （重点）</h4><p>（1）规划：<br>192.168.116.121 hsiehchou121 ：主节点<br>192.168.116.122 hsiehchou122 ：从节点<br>192.168.116.123 hsiehchou123 ：从节点<br>192.168.116.124 hsiehchou124 ：从节点 </p><p>（2）准备工作:<br>step 1: jdk、防火墙、ssh免密码登录（3次拷贝）、在etc/hosts 添加主机名<br>对于同时操作多台机器可通过 工具-》发送键输入到所有会话 在选项卡排列 实现 水平排列 </p><p>step 2:时间同步（如果能够上网） 使用网络时间（GUI设置）默认的都是一致的<br>不能上网： date -s 2019-01-10(同时操作多台机器) 集群紊乱<br>ntp：在机器里面指定一个服务器 作为时钟服务器 </p><p>step 3: 修改配置文件</p><p>主要在hsiehchou 121操作，其他机器通过scp拷贝</p><h4 id="3、slaves-和自己的从节点机器名字一致"><a href="#3、slaves-和自己的从节点机器名字一致" class="headerlink" title="3、slaves(和自己的从节点机器名字一致)"></a>3、slaves(和自己的从节点机器名字一致)</h4><p>hsiehchou122<br>hsiehchou123<br>hsiehchou124</p><h4 id="4、通过hdfs-namenode-格式化"><a href="#4、通过hdfs-namenode-格式化" class="headerlink" title="4、通过hdfs namenode 格式化"></a>4、通过hdfs namenode 格式化</h4><p>  hdfs namenode -format<br>  成功的标志： Storage directory /opt/module/hadoop-2.7.3/tmp/dfs/name has been successfully formatted</p><h4 id="5、通过scp拷贝"><a href="#5、通过scp拷贝" class="headerlink" title="5、通过scp拷贝"></a>5、通过scp拷贝</h4><p>scp -r /opt/module/hadoop-2.7.3/ root@hsiehchou122:/opt/module/<br>scp -r /opt/module/hadoop-2.7.3/ root@hsiehchou123:/opt/module/<br>scp -r /opt/module/hadoop-2.7.3/ root@hsiehchou124:/opt/module/<br>学会看 vi /opt/module/hadoop-2.7.3/logs/hadoop-root-datanode-hsiehchou123.log<br>Shift+G 看启动日志<br>hdfs体系架构（Yarn资源放在后面）</p><h4 id="6、HDFS-NameNode：名称节点"><a href="#6、HDFS-NameNode：名称节点" class="headerlink" title="6、HDFS-NameNode：名称节点"></a>6、HDFS-NameNode：名称节点</h4><p>（1）职责：对HDFS的节点进行管理，管理员<br>接收客户端（命令行、Java）的请求：创建目录、上传数据、下载数据和删除数据<br>管理和维护hdfs的日志和元信息 </p><p>（2）dfs/name:<br>a、current：主要存放日志和元信息 存贮路径：/opt/module/hadoop-2.7.3/tmp/dfs/name/current<br>edits文件：二进制文件，体现了hdfs的最新状态</p><p>hdfs oev -i edits_inprogress_0000000000000000003 -o ~/a.xml<br>o:表示 offline<br>inprogress:表示最新的</p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;EDITS&gt;  &lt;EDITS_VERSION&gt;-63&lt;/EDITS_VERSION&gt;  &lt;RECORD&gt;    &lt;OPCODE&gt;OP_START_LOG_SEGMENT&lt;/OPCODE&gt;    &lt;DATA&gt;      &lt;TXID&gt;4&lt;/TXID&gt;    &lt;/DATA&gt;  &lt;/RECORD&gt;  &lt;RECORD&gt;    &lt;OPCODE&gt;OP_MKDIR&lt;/OPCODE&gt;    &lt;DATA&gt;      &lt;TXID&gt;5&lt;/TXID&gt;      &lt;LENGTH&gt;0&lt;/LENGTH&gt;      &lt;INODEID&gt;16386&lt;/INODEID&gt;      &lt;PATH&gt;/input&lt;/PATH&gt;      &lt;TIMESTAMP&gt;1550209288319&lt;/TIMESTAMP&gt;      &lt;PERMISSION_STATUS&gt;        &lt;USERNAME&gt;root&lt;/USERNAME&gt;        &lt;GROUPNAME&gt;supergroup&lt;/GROUPNAME&gt;        &lt;MODE&gt;493&lt;/MODE&gt;      &lt;/PERMISSION_STATUS&gt;    &lt;/DATA&gt;  &lt;/RECORD&gt;&lt;/EDITS&gt;</code></pre><p>b、元信息文件 fsimage：记录的数据块的位置信息和数据块冗余信息，没有体现hdfs的最新状态，二进制文件</p><p> hdfs oiv -i fsimage_0000000000000000002 -o ~/b.xml -p XML</p><p>（3）in_use.lock 避免同一文件被多使用，只能启动一个namenode</p><h4 id="7、hdfs-DataNode：数据节点"><a href="#7、hdfs-DataNode：数据节点" class="headerlink" title="7、hdfs-DataNode：数据节点"></a>7、hdfs-DataNode：数据节点</h4><p>（1）主要用来进行数据的存储<br>1.x 64M<br>2.x 128M( hdfs-site.xml 可以修改 blocksize) </p><p>（2）数据块的表现形式就是一个个的blk文件<br>位置：/opt/module/hadoop-2.7.3/tmp/dfs/data/current/BP-298124919-192.168.116.121-1550208140930 ###/current/finalized/subdir0/subdir0<br>尝试上传一个 大于128M的文件（128<em>1024</em>1024）<br>Hadoop 3.x 有 纠删码技术，节约存储空间</p><h4 id="8、上传文件"><a href="#8、上传文件" class="headerlink" title="8、上传文件"></a>8、上传文件</h4><p>首先创建文件夹<br>hdfs dfs -mkdir /software/input<br>上传我本地文件到hdfs上<br>hdfs dfs -put hdfs dfs -put /opt/software/hadoop-2.7.3.tar.gz /software/input<br>就OK了<br>之后可以使用上面的命令查看</p><h4 id="9、hdfs-SecondaryNameNode：第二名称节点"><a href="#9、hdfs-SecondaryNameNode：第二名称节点" class="headerlink" title="9、hdfs-SecondaryNameNode：第二名称节点"></a>9、hdfs-SecondaryNameNode：第二名称节点</h4><p>（1）进行日志信息的合并，根据checkpoint或者时间间隔（3600s）或者edits文件达到64M </p><p>（2）edits文件合并到fsimage里面 edits文件可以清空<br>看日志<br>/opt/moudle/hadoop-2.7.3/logs vi shift+G</p><h4 id="10、hdfs-Web-Console"><a href="#10、hdfs-Web-Console" class="headerlink" title="10、hdfs-Web Console"></a>10、hdfs-Web Console</h4><p>hdfs dfsadmin -report<br><a href="http://192.168.116.125:50070/dfshealth.html#tab-overview" target="_blank" rel="noopener">http://192.168.116.125:50070/dfshealth.html#tab-overview</a> </p><p>（1） Overview–展示hdfs的基本信息<br>Safemode is off.—高级特性 </p><p>（2）DataNodes-数据节点信息<br>增加和删除数据节点（Decomissioning–&gt;Dead） </p><p>（3）Datanode Volume Failures–数据节点 硬件错误 </p><p>（4）Snapshot（快照）—高级特性<br>快照实现数据的备份，防止数据的误操作和丢失。默认是关闭的</p><p>（5）Startup Progress–启动过程 </p><p>（6）Uitlities:<br>Browse 文件 —hdfs -dfs -ls /<br>logs—查看日志</p><h4 id="11、hdfs-普通操作命令–hdfs-dfs-hadoop-dfs"><a href="#11、hdfs-普通操作命令–hdfs-dfs-hadoop-dfs" class="headerlink" title="11、hdfs 普通操作命令–hdfs dfs(hadoop dfs)"></a>11、hdfs 普通操作命令–hdfs dfs(hadoop dfs)</h4><p>（1）创建目录–mkdir<br>hdfs dfs -mkdir / </p><p>（2）查看–ls<br>查看目录和子目录 hdfs dfs -ls -R /<br>hdfs dfs -lsr / </p><p>（3）上传数据<br>hdfs dfs -put hadoop-root-namenode-hsiehchou125.log /test1<br>-put ：<br>-copyFromLocal： 本地路径 hdfs路径<br>hdfs dfs -copyFromLocal ~/temp/a.txt /test0113/<br>-moveFromLocal: 会删除本地文件 剪切 </p><p>（4）下载数据<br>-get:<br>-copyToLocal:从hdfs下载到本地 </p><p>（5）删除数据<br>-rm<br>-rmr: 删除hdfs的目录和子目录<br>删除日志： Deleted /test1<br>回收站—高级特性 默认是关闭</p><p>（6）合并数据–（为hive表数据操作做准备）<br>-getmerge :hdfs 把某个hdfs的目录下的文件进行先合并后下载<br>*：通配符 ？<br>hdfs dfs -getmerge /students /root/students.txt </p><p>（7）计数和文件大小<br>-count 显示 文件夹、文件个数 文件总的大小<br>-du 显示每个文件夹和文件的大小<br>[root@hsiehchou125 ~]# hdfs dfs -count /students<br>1 4 38/students<br>hdfs[root@hsiehchou125 ~]# hdfs dfs -du /students<br>25 /students/students01.txt<br>13 /students/students02.txt </p><p>（8）负载均衡 balancer<br>实现DataNode 数据存储均衡 </p><p><code>##hdfs balancer ##</code></p><h4 id="12、hdfs-管理员命令"><a href="#12、hdfs-管理员命令" class="headerlink" title="12、hdfs 管理员命令"></a>12、hdfs 管理员命令</h4><p>（1）hdfs dfsadmin -report 打印报告 </p><p>（2） -safemode &lt;enter | leave | get | wait&gt;<br>enter:手动进入安全模式<br>leave:手动离开安全模式<br>get:获得当前安全模式的状态<br>hdfs dfsadmin -safemode get<br>[root@hsiehchou125 ~]# hdfs dfsadmin -safemode enter<br>Safe mode is ON </p><p>（3）快照命令<br>[-allowSnapshot <code>&lt;snapshotDir&gt;</code>]<br>[-disallowSnapshot <code>&lt;snapshotDir&gt;</code>]</p><p>（4）Quota 配额<br>a、名称配额–数量<br>[-setQuota <code>&lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;</code>]<br>[-clrQuota <code>&lt;dirname&gt;...&lt;dirname&gt;</code>]</p><p>b、空间配额–空间大小</p><p>[-setSpaceQuota <code>&lt;quota&gt;</code> [-storageType <code>&lt;storagetype&gt;</code>] <code>&lt;dirname&gt;...&lt;dirname&gt;</code>]<br>[-clrSpaceQuota [-storageType <code>&lt;storagetype&gt;</code>] <code>&lt;dirname&gt;...&lt;dirname&gt;</code>]</p><h4 id="13、IDEA-Maven工程简介"><a href="#13、IDEA-Maven工程简介" class="headerlink" title="13、IDEA Maven工程简介"></a>13、IDEA Maven工程简介</h4><p>（1）IDEA 下载地址：<br><a href="https://www.jetbrains.com/idea/download/" target="_blank" rel="noopener">https://www.jetbrains.com/idea/download/</a><br>破解方法自行查找 </p><p>（2）File-new Project-&gt;Maven<br>GroupID: 公司名字<br>artifactId：工程名字<br>java程序在：src-》main-&gt;java 右键 新建 java class文件<br>target: 是运行程序生成的class文件 </p><p>（3）管理包<br>/opt/moudle/hadoop-2.7.3/share/hadoop/common/<em>.jar<br>/opt/moudle/hadoop-2.7.3/share/hadoop/common/lib/</em>.jar<br>/opt/moudle/hadoop-2.7.3/share/hadoop/hdfs/<em>.jar<br>/opt/moudle/hadoop-2.7.3/share/hadoop/hdfs/lib/</em>.jar<br>通过maven只需要配置POM文件<br>a、 下载一个maven版本<br><a href="http://maven.apache.org/index.html" target="_blank" rel="noopener">http://maven.apache.org/index.html</a> </p><p>b、通过 File-settings-Maven<br>修改： E:\apache-maven-3.6.0\conf\settings.xml<br>55行：<br><code>&lt;localRepository&gt;</code>E:\Maven\m2\Repository<code>&lt;/localRepository&gt;</code><br> MaveHome：E:\apache-maven-3.6.0<br> User settings:E:\apache-maven-3.6.0\conf\settings.xml</p><p>c、POM中写入包的依赖<br>参考：<a href="https://mvnrepository.com/search?q=hadoop" target="_blank" rel="noopener">https://mvnrepository.com/search?q=hadoop</a></p><pre><code>  &lt;dependencies&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;            &lt;version&gt;2.7.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;            &lt;version&gt;2.7.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;            &lt;version&gt;2.7.3&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;</code></pre><h4 id="14、文件夹的创建"><a href="#14、文件夹的创建" class="headerlink" title="14、文件夹的创建"></a>14、文件夹的创建</h4><pre><code>import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import java.io.IOException;public class hdfsMkDir {public static void main(String[] args) throws IOException {System.setProperty(&quot;HADOOP_USER_NAME&quot;,&quot;root&quot;);//step1 配置参数，指定namenode地址Configuration conf = new Configuration();conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://192.168.116.125:9000&quot;);//step2 创建客户端FileSystem client = FileSystem.get(conf);//step3 创建目录client.mkdirs(new Path(&quot;/test2&quot;));client.close();System.out.println(&quot;Successful&quot;);   }}</code></pre><h4 id="15、hdfs权限问题"><a href="#15、hdfs权限问题" class="headerlink" title="15、hdfs权限问题"></a>15、hdfs权限问题</h4><p>针对用户操作没有权限 permission denied：<br>（1）修改 hdfs-site.xml 去掉权限检查（关闭hdfs服务 stop-all.sh;修改后 重新 Start-all.sh）</p><pre><code>&lt;property&gt;    &lt;name&gt;dfs.permissions&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;</code></pre><p>（2）通过设定用户名字 rootSystem.setProperty(“HADOOP_USER_NAME”,”root”);</p><p>（3）通过java的-D参数传递。 HADOOP_USER_NAME=root （命令行的方式）<br>public static void main(String[] args)</p><p>Java -D命令对应的代码中获取-D后面的参数 和 多个参数时-D命令的使用</p><p>Java代码：</p><pre><code>public class DP {    public static void main(String[] args) {      String fg = System.getProperty(&quot;P&quot;);      System.err.println(fg);    }}</code></pre><p>cmd命令：<br>java -DP=hdfshdfs DP</p><p>执行命令后输出：hdfshdfs<br>注意：-D和Para之间不能有空格</p><p>使用多个参数，如P、P1</p><pre><code>public class DP {    public static void main(String[] args) {        String fg = System.getProperty(&quot;P&quot;);        System.out.println(fg);        String fg1 = System.getProperty(&quot;P1&quot;);        System.out.println(fg1);    }}</code></pre><p>java -DP=hdfshdfs -DP1=1212 DP<br>执行命令后输出：<br>hdfshdfs<br>1212</p><p>（4）hdfs dfs -chmod 777 /input 让所有用户访问</p><p>（5）针对hdfs权限问题，有kerberos认证<br>Kerberos: The Network Authentication Protocol<br><a href="https://www.cnblogs.com/wukenaihe/p/3732141.html" target="_blank" rel="noopener">https://www.cnblogs.com/wukenaihe/p/3732141.html</a></p><h4 id="16、IDEA-Maven工程实现hdfs的文件上传与下载"><a href="#16、IDEA-Maven工程实现hdfs的文件上传与下载" class="headerlink" title="16、IDEA Maven工程实现hdfs的文件上传与下载"></a>16、IDEA Maven工程实现hdfs的文件上传与下载</h4><p>Maven环境中 只有当 POM文件中所有的依赖包全部变成白色<br>pom.xml</p><pre><code>&lt;dependencies&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;            &lt;version&gt;2.7.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;            &lt;version&gt;2.7.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;            &lt;version&gt;2.7.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-core --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt;            &lt;version&gt;1.2.1&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;</code></pre><p>（1）hdfs文件上传<br>查看源码：crtl+鼠标左键<br><code>## Failed to locate the winutils binary in the hadoop binary path java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries. ##</code><br>step1:<br>下载：hadoop2.7.3 winutils binary<br><a href="https://github.com/rucyang/hadoop.dll-and-winutils.exe-for-hadoop2.7.3-on-windows_X64" target="_blank" rel="noopener">https://github.com/rucyang/hadoop.dll-and-winutils.exe-for-hadoop2.7.3-on-windows_X64</a> </p><p>step2: 配置环境变量 拷贝进入 D:\hadoop-2.7.3\bin文件下<br>hadoop.home.dir —bin/winutils.exe<br>HADOOP_HOME:D:\hadoop-2.7.3,然后再path里面增加 %HADOOP_HOME%\bin<br>或者：System.setProperty(“hadoop.home.dir”, “D:\hadoop-2.7.3”);</p><pre><code>import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import java.io.*;public class hdfsUpload {    public static void main(String[] args) throws IOException {        System.setProperty(&quot;HADOOP_USER_NAME&quot;,&quot;root&quot;);        //System.setProperty(&quot;hadoop.home.dir&quot;,&quot;E:\\hadoop-2.7.3&quot;);        //step1 建立客户端        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://192.168.116.125:9000&quot;);        //使用IP地址  因为没有指定hsiehchou125对应的IP        FileSystem client = FileSystem.get(conf);        //step2 创建本地数据 hdfs dfs -put copyFromLocal        File file1 = new File(&quot;C:\\Users\\hsiehchou\\Desktop\\hadooplibs\\test.txt&quot;);        InputStream input = new FileInputStream(file1);//多态        //step3 创建本地输出流 指向hdfs        OutputStream output = client.create(new Path(&quot;/test8/a.txt&quot;),true);        //step4 开始写入hdfs        /**方法1**///        byte[] buffer = new byte[1024];//        int len = 0;//        //因为read 当读到文件末尾的时候 会返回-1//        while((len=input.read(buffer)) != -1){//            output.write(buffer, 0, len);//        }//循环写入数据//        output.flush();//        input.close();//        output.close();        /**方法2 IOUtils**/        IOUtils.copyBytes(input,output,1024);    }}</code></pre><p>（2）hdfs文件下载<br><code>### 使用IOUtils 输入路径 输出路径###</code><br>IOUtils.copyBytes(input,output,1024);</p><pre><code>import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import java.io.*;public class hdfsDownload {    public static void main(String[] args) throws IOException {        System.setProperty(&quot;HADOOP_USER_NAME&quot;,&quot;root&quot;);        //step1 建立客户端        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://192.168.116.125:9000&quot;);        //使用IP地址  因为没有指定hsiehchou125对应的IP        FileSystem client = FileSystem.get(conf);        //step2 创建数据输入 指向hdfs  从hdfs读取数据  hdfs dfs -get copyToLocal        InputStream input = client.open(new Path(&quot;/test8/a.txt&quot;));        //step3 创建本地输出流 指向hdfs        OutputStream output = new FileOutputStream(&quot;E:\\test\\b.txt&quot;);        //step4 开始写入hdfs        /**IOUtils**/        IOUtils.copyBytes(input,output,1024);    }}</code></pre><p>文件元信息 </p><pre><code>{     文件名: *.txt     路径: /text     大小: 100KB     冗余度: 3     数据块1: DNS1,DNS2,DNS3     (如果文件大切分) }</code></pre><h4 id="17、hdfs上传文件原理"><a href="#17、hdfs上传文件原理" class="headerlink" title="17、hdfs上传文件原理"></a>17、hdfs上传文件原理</h4><p>1、请求上传数据<br>2、创建客户端<br>3、建立RPC通信<br>4、NameNode对象<br>代理对象NameNodeProxies<br>5、请求创建文件元信息<br>6、创建文件元信息<br>7、缓存文件元信息(1000M)<br>8、返回元信息<br>9、根据元信息创建输出流<br>10、上传第一个数据块<br>11、数据块自动复制<br>12、循环上传</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据基础 </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据基础2</title>
      <link href="/2019/02/05/da-shu-ju-ji-chu-2/"/>
      <url>/2019/02/05/da-shu-ju-ji-chu-2/</url>
      
        <content type="html"><![CDATA[<h4 id="1、什么是大数据？"><a href="#1、什么是大数据？" class="headerlink" title="1、什么是大数据？"></a>1、什么是大数据？</h4><p>2002 大数据提出 美国引入。—麦肯锡报告<br>维克托·迈尔-舍恩伯格—大数据之父<br>4V特征：<br>即<br>Volume（数据量大）：PB级<br>Variety（数据多样性）：文本、图像、视频、音频等<br>Velocity（输入和处理速度快）：流式数据<br>Value（价值密度低）： 积累很多的数据才能发掘大数据隐含的意义</p><p>只要能发挥和挖掘数据隐藏的价值，不用纠结与数据量大小<br>大数据核心问题存储、计算和分析—-通过组件（计算框架）解决了</p><h4 id="2、数据仓库和大数据"><a href="#2、数据仓库和大数据" class="headerlink" title="2、数据仓库和大数据"></a>2、数据仓库和大数据</h4><p>（1）传统方式：DW（Data Warehouse），基于传统的关系数据库（Oracle、MySQL等），一般只做 查询分析，TD（Teradata 天睿）–数据仓库一体机</p><p>（2）大数据的方式–分布式<br>GP：greenplum</p><h4 id="3、OLTP和OLAP"><a href="#3、OLTP和OLAP" class="headerlink" title="3、OLTP和OLAP"></a>3、OLTP和OLAP</h4><p>（1）OLTP：Online Transaction Processing 联机事务处理：（insert update、delete）<br>ACID：所有的数据可追溯。——-传统关系型数据库（Oracle Mysql Postgresql等） </p><p>（2）OLAP：Online Analytic Processing 联机分析处理<br>真正生产中是二者的结合：OLTP（后台操作 前台展示 数据设计等）+OLAP（Hive Hbase Spark等）</p><h4 id="4、Google的基本思想：三篇论文重点"><a href="#4、Google的基本思想：三篇论文重点" class="headerlink" title="4、Google的基本思想：三篇论文重点"></a>4、Google的基本思想：三篇论文重点</h4><p>（1）GFS: Google File System—-HDFS —解决存储<br>a、数据库太贵。主要是为了解决 google搜索内容的存储问题。–造价低 易扩展</p><p>b、倒排索引（Reverted Index）：<br>int arry[ ] = {1,2,3,4}<br>索引不一定提高查询速度。—key value </p><p>c、没有公布源码，—-Hadoop之父 Doug Cutting<br>HDFS 默认文件块大小 128M（Hadoop 2.X） 64M（Hadoop 1.x），<br>默认3副本</p><p>（2）MapReduce:分布计算模型<br>PageRank </p><p>（3）BigTable：大表<br>对HDFS进行封装和二次开发，提高查询效率。把所有数据存入一张表中，通过牺牲空间，换取时间</p><h4 id="5、Hadoop的简介"><a href="#5、Hadoop的简介" class="headerlink" title="5、Hadoop的简介"></a>5、Hadoop的简介</h4><p><a href="http://hadoop.apache.org/" target="_blank" rel="noopener">http://hadoop.apache.org/</a><br>Hadoop YARN: A framework for job scheduling and cluster resource management.<br>Apache：HDFS+MapReduce+<br>Yarn<br><a href="https://hbase.apache.org/" target="_blank" rel="noopener">https://hbase.apache.org/</a></p><h4 id="6、HDFS的体系架构"><a href="#6、HDFS的体系架构" class="headerlink" title="6、HDFS的体系架构"></a>6、HDFS的体系架构</h4><p>HDFS 副本数可以再 hdfs-site.xml中修改。不超过机器个数 建议不超过3<br>/opt/module/hadoop-2.7.3/etc/hadoop<br>HDFS=NameNode（主节点 名称节点）+SecondaryNameNode（第二名称节点）+DataNode（数据节点）</p><h4 id="7、MR编程模型"><a href="#7、MR编程模型" class="headerlink" title="7、MR编程模型"></a>7、MR编程模型</h4><p>包含两个阶段 key value 的设计是关键</p><h4 id="8、大数据典型应用场景"><a href="#8、大数据典型应用场景" class="headerlink" title="8、大数据典型应用场景"></a>8、大数据典型应用场景</h4><p>（1）商品推荐–协同过滤<br>（2）画像<br>（3）套牌车</p><h4 id="9、Hadoop的安装准备工作"><a href="#9、Hadoop的安装准备工作" class="headerlink" title="9、Hadoop的安装准备工作"></a>9、Hadoop的安装准备工作</h4><p>Hadoop名字来源–Doug Cutting<br>（1）安装好linux操作系统（IP配置）<br>（2）关闭防火墙<br>systemctl stop（disable） firewalld.service<br>（3）安装Jdk–winscp 上传 opt/software 解压到 opt/module<br>（4）Hadoop安装包—虚拟机的克隆 scp（拷贝）</p><p>a、提前准备好 mkdir /opt/module<br>tar -zxvf hadoop-2.7.3.tar.gz -C /opt/module/ </p><p>b、vi ~/.bash_profile （用于当前用户）或者/etc/profile（所有用户都可以用）增加下面内</p><p>export JAVA_HOME=<code>/opt/module/jdk1.8.0_192</code><br>export PATH=<code>$JAVA_HOME/bin:$PATH</code></p><p>HADOOP_HOME=<code>/opt/module/hadoop-2.7.3</code><br>export HADOOP_HOME</p><p>PATH=<code>$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH</code><br>export PATH</p><p>c、 环境变量生效<br>source ~/.bash_profile<br>通过 输入 start 按两下tab 看是否有内容</p><p>虚拟机克隆<br>（1）保证虚拟机处于关闭状态<br>（2）右键-&gt;管理-&gt;克隆 当前状态 完整克隆–&gt;<br>（3）hostname–修改<br>ip修改 – reboot</p><p>Hadoop（HDFS+Yarn） 本地 伪分布 全分布</p><h4 id="10、Hadoop安装—本地安装"><a href="#10、Hadoop安装—本地安装" class="headerlink" title="10、Hadoop安装—本地安装"></a>10、Hadoop安装—本地安装</h4><p>（1）特点：没有HDFS和Yarn 只能够测试MR程序是否成功， 作为一个普通的java程序。<br>（2）修改文件：<br>vi hadoop-env.sh<br>set number<br>修改25行（行数不一 hadoop版本不一致）</p><p>JAVA_HOME=/opt/module/jdk1.8.0_181<br>cd /root/<br>mkdir temp<br>touch a.txt<br>vi a.txt<br>mapred-site.xml 默认没有，我克隆的文件里面有 这个文件没有被覆盖指定了yarn资源</p><h4 id="11、Hadoop安装—本地安装伪分布模式"><a href="#11、Hadoop安装—本地安装伪分布模式" class="headerlink" title="11、Hadoop安装—本地安装伪分布模式"></a>11、Hadoop安装—本地安装伪分布模式</h4><p>（1）特点：在一台机器上模拟一个分布式环境具备hadoop的所有功能。<br>HDFS：NameNode+DataNode+SecondaryNameNode<br>Yarn：ResourceManager+NodeManager </p><p>（2）修改的文件：<br>step1:hadoop-env.sh<br>vi —– :set number  修改25行<br>JAVA_HOME=<code>/opt/module/jdk1.8.0_192</code><br><code>&lt;!--测试hadoop是否成功--&gt;</code><br>hadoop jar hadoop-mapreduce-examples-2.7.3.jar wordcount ~/temp/a.txt ~/temp/output/wc0107</p><p>step2:hdfs-site.xml</p><pre><code>&lt;!--注释配置数据块的冗余度，默认是3--&gt; &lt;property&gt;    &lt;name&gt;dfs.replication&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;!--注释配置HDFS的权限检查，默认是true--&gt; &lt;!-- &lt;property&gt;    &lt;name&gt;dfs.permissions&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; --&gt;</code></pre><p>step3:core-site.xml</p><pre><code>&lt;!--配置HDFS主节点，namenode的地址,9000是RPC通信端口--&gt;  &lt;property&gt;     &lt;name&gt;fs.defaultFS&lt;/name&gt;     &lt;value&gt;hdfs://hsiehchou121:9000&lt;/value&gt;  &lt;/property&gt; &lt;!--配置HDFS数据块和元数据保存的目录,一定要修改--&gt;   &lt;property&gt;     &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;     &lt;value&gt;/opt/moudle/hadoop-2.7.3/tmp&lt;/value&gt;  &lt;/property&gt;</code></pre><p>step4：mapred-site.xml(默认没有)</p><pre><code>cp mapred-site.xml.template  mapred-site.xml&lt;!--配置MR程序运行的框架--&gt;&lt;property&gt;      &lt;name&gt;mapreduce.framework.name&lt;/name&gt;    &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; </code></pre><p>step5：yarn-site.xml</p><pre><code>&lt;!--配置Yarn的节点--&gt;&lt;property&gt;     &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;   &lt;value&gt;hsiehchou121&lt;/value&gt;&lt;/property&gt; &lt;!--NodeManager执行MR任务的方式是Shuffle洗牌--&gt;&lt;property&gt;     &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;   &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt; </code></pre><p>step 6：通过HDFS namenode 格式化<br>在第4步中，hadoop.tmp.dir–格式化<br> cd /opt/module/hadoop-2.7.3/tmp/<br> [root@hsiehchou121 hadoop]# cd /opt/module/hadoop-2.7.3/tmp/<br> [root@hsiehchou121 tmp]# hdfs namenode -format </p><p>重复格式化:hadoop.tmp.dir   先停止集群，需要删除原来的tmp文件。 rm -rf 重新格式化 启动集群</p><p>命令：hdfs namenode -format<br>验证：是否格式化成功：<br>Storage directory /opt/moudle/hadoop-2.7.3/tmp/dfs/name has been successfully formatted. </p><p>最后启动，通过start-all.sh启动<br>验证：<br>[root@hsiehchou121 tmp]# jps<br>        2336 NameNode<br>        2867 NodeManager<br>        3972 Jps<br>        2629 SecondaryNameNode<br>        2774 ResourceManager<br>        2441 DataNode<br>web访问:<br><a href="http://192.168.116.121:8088" target="_blank" rel="noopener">http://192.168.116.121:8088</a> yarn<br><a href="http://192.168.116.121:50070" target="_blank" rel="noopener">http://192.168.116.121:50070</a> HDFS</p><h4 id="12、免密码登录的原理和配置"><a href="#12、免密码登录的原理和配置" class="headerlink" title="12、免密码登录的原理和配置"></a>12、免密码登录的原理和配置</h4><p>SSH无密码登录<br>1）配置ssh<br>（1）基本语法<br>ssh 另一台电脑的ip地址<br>（2）ssh连接时出现Host key verification failed的解决方法<br>[root@hsiehchou121 opt]# ssh 192.168.116.103<br>The authenticity of host ‘192.168.116.103 (192.168.116.103)’ can’t be established.<br>RSA key fingerprint is cf:1e:de:d7:d0:4c:2d:98:60:b4:fd:ae:b1:2d:ad:06.<br>Are you sure you want to continue connecting (yes/no)?<br>Host key verification failed.<br>（3）解决方案如下：直接输入yes </p><p>2）无密钥配置<br>（1）进入到我的home目录<br>[root@hsiehchou121 opt]$ cd ~/.ssh</p><p>（2）生成公钥和私钥：<br>[root@hsiehchou121 .ssh]$ ssh-keygen -t rsa<br>然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） </p><p>（3）将公钥拷贝到要免密登录的目标机器上<br>[root@hsiehchou121 .ssh]$ <code>ssh-copy-id hsiehchou121</code><br>[root@hsiehchou121 .ssh]$ <code>ssh-copy-id hsiehchou122</code><br>[root@hsiehchou121 .ssh]$ <code>ssh-copy-id hsiehchou123</code><br>[root@hsiehchou121 .ssh]$ <code>ssh-copy-id hsiehchou124</code></p><p>（4）在hsiehchou122、hsiehchou123、hsiehchou124上分别执行所有操作</p><h4 id="13、Hadoop安装—全分布模式"><a href="#13、Hadoop安装—全分布模式" class="headerlink" title="13、Hadoop安装—全分布模式"></a>13、Hadoop安装—全分布模式</h4><p>作业：准备3台机器。完成1 的准备工作<br>加入到 etc/hosts<br>192.168.116.121 hsiehchou121<br>192.168.116.122 hsiehchou122<br>192.168.116.123 hsiehchou123<br>192.168.116.124 hsiehchou124</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git快速上手</title>
      <link href="/2019/02/03/git-kuai-su-shang-shou/"/>
      <url>/2019/02/03/git-kuai-su-shang-shou/</url>
      
        <content type="html"><![CDATA[<h3 id="一、Linux-平台上安装"><a href="#一、Linux-平台上安装" class="headerlink" title="一、Linux 平台上安装"></a>一、Linux 平台上安装</h3><p>Git 的工作需要调用 curl，zlib，openssl，expat，libiconv 等库的代码，所以需要先安装这些依赖工具<br>在有 yum 的系统上（比如 Fedora）或者有 apt-get 的系统上（比如 Debian 体系），可以用下面的命令安装：<br>各 Linux 系统可以很简单多使用其安装包管理工具进行安装：</p><h4 id="1、Debian-Ubuntu"><a href="#1、Debian-Ubuntu" class="headerlink" title="1、Debian/Ubuntu"></a>1、Debian/Ubuntu</h4><p>Debian/Ubuntu Git 安装命令为：</p><pre><code>apt-get install libcurl4-gnutls-dev libexpat1-dev gettext  libz-dev libssl-devapt-get install git-coregit --version</code></pre><h4 id="2、Centos-RedHat"><a href="#2、Centos-RedHat" class="headerlink" title="2、Centos/RedHat"></a>2、Centos/RedHat</h4><p>如果你使用的系统是 Centos/RedHat 安装命令为：</p><pre><code>yum install curl-devel expat-devel gettext-devel openssl-devel zlib-develyum -y install git-coregit --version</code></pre><h4 id="3、Git-配置"><a href="#3、Git-配置" class="headerlink" title="3、Git 配置"></a>3、Git 配置</h4><p><code>--system</code> 针对所有用户<br><code>--global</code> 针对当前用户<br>什么都不加参数，当前项目<br>优先级别：当前项目&gt;global&gt;system</p><h4 id="4、用户信息"><a href="#4、用户信息" class="headerlink" title="4、用户信息"></a>4、用户信息</h4><p>配置个人的用户名称和电子邮件地址：</p><pre><code>git config --global user.name &quot;&quot;git config --global user.email @qq.com</code></pre><h4 id="5、查看配置信息"><a href="#5、查看配置信息" class="headerlink" title="5、查看配置信息"></a>5、查看配置信息</h4><p>要检查已有的配置信息，可以使用 git config <code>--list</code> 命令：<br>git config <code>--list</code><br>也可以直接查阅某个环境变量的设定，只要把特定的名字跟在后面即可，像这样：<br>git config user.name<br>hsiehchou</p><h4 id="6、设置SSH"><a href="#6、设置SSH" class="headerlink" title="6、设置SSH"></a>6、设置SSH</h4><p>[root@test ~]# ssh-keygen -t rsa -C “@qq.com”</p><h4 id="7、Linux环境"><a href="#7、Linux环境" class="headerlink" title="7、Linux环境"></a>7、Linux环境</h4><p>vi /etc/hosts<br>添加一行：13.229.188.59　　github.com<br>linux下</p><p>在~/下， touch创建文件 .git-credentials, 用vim编辑此文件，输入：<br>vi ~/.git-credentials<br>https://{用户名}:@{密码}@github.com<br>注意去掉{}</p><p>在终端下执行 git config –global credential.helper store</p><p>可以看到~/.gitconfig文件，会多了一项：<br>[credential]<br>helper = store</p><h3 id="二、Windows环境"><a href="#二、Windows环境" class="headerlink" title="二、Windows环境"></a>二、Windows环境</h3><p>C:\Windows\System32\drivers\etc\hosts<br>添加一行：13.229.188.59　　github.com</p><h4 id="1、Github端的操作"><a href="#1、Github端的操作" class="headerlink" title="1、Github端的操作"></a>1、Github端的操作</h4><p>在Github的setting里面的SSH and GPG key的SSH keys添加公钥<br>公钥的获取方法是：<br>cat id_rsa_github.pub<br>连接成功<br>[root@test .ssh]# ssh -T <a href="mailto:git@github.com">git@github.com</a></p><p>-i ~/.ssh/id_rsa_github</p><h4 id="2、使用命令"><a href="#2、使用命令" class="headerlink" title="2、使用命令"></a>2、使用命令</h4><p>在git bash 中执行<br>设置记住密码（默认15分钟）：<br>git config <code>--global</code> credential.helper cache</p><p>如果想自己设置时间，可以这样做：<br>git config credential.helper  cache <code>--timeout</code>=7200’<br>这样就设置l两个小时之后失效 </p><p>长期存储密码：<br>git config <code>--global</code> credential.helper store</p><h4 id="3、在git-bash里边输入-git-remote-v"><a href="#3、在git-bash里边输入-git-remote-v" class="headerlink" title="3、在git bash里边输入 git remote -v"></a>3、在git bash里边输入 git remote -v</h4><p>git remote rm origin //删除http<br>git remote add origin <a href="mailto:git@github.com">git@github.com</a>:<del>/</del>.git //添加ssh<br>git push origin //执行更改</p><p>[root@test ~]# mkdir test<br>[root@test ~]# cd test/<br>[root@test test]# git init<br>Initialized empty Git repository in /root/test/.git/<br>[root@test test]# vim readme.md<br>[root@test test]# git status<br>[root@test test]# git add readme.md<br>[root@test test]# git status<br>[root@test test]# vim test1<br>[root@test test]# git add test1<br>[root@test test]# git status<br>[root@test test]# git commit -m “first commit”</p><p>简易提交显示<br>oneline 将 每个提交放在一行显示<br>[root@test test]# git log <code>--pretty</code>=oneline</p><p>图示表示版本提交<br>[root@test test]# git log <code>--graph</code></p><p>commit 8dd0b3d1a2be7064f7bb27e83a6ddc91146d38d0<br>| Author:<br>| Date:<br>|<br>| first commit<br>|</p><p>commit 5c2519af68ea0d0746894122b51a77cd11071ab8</p><p>reset<br>a-&gt;b-&gt;c-&gt;d使用git reset方法回到版本c，有3种方式：<br><code>--hard</code><br>版本库：c<br>暂存区：c，删掉版本d的暂存区<br>工作区：c，删掉版本d的工作区</p><p><code>--sort</code><br>版本库：c<br>暂存区：c，保留版本d的暂存区<br>工作区：c，保留版本d的工作区</p><p><code>--mixed</code>(默认)<br>版本库：c<br>暂存区：c，删除版本d的暂存区<br>工作区：回到版本c，同时保留版本d的工作区</p><p>git diff：暂存区 （比较前的文件）和工作区比较（比较后的文件）<br>git diff <code>--cached</code>：版本库（比较前的文件）和暂存区比较<br>git diff HEAD：版本库（比较前的文件）和工作区比较</p><p>— a/a<br>+++ b/a<br>@ -1 +1 @@<br>-1<br>+123</p><p>工作区：就是电脑上看到的目录，比如目录下test里的文件(.git隐藏目录版本库除外)。或者以后需要再新建的目录文件等等都属于工作区范畴</p><p>版本库(Repository)：工作区有一个隐藏目录.git,这个不属于工作区，这是版本库。其中版本库里面存了很多东西，其中最重要的就是stage(暂存区)，还有Git为我们自动创建了第一个分支master,以及指向master的一个指针HEAD</p><p>git add 把文件添加进去，实际上就是把文件添加到暂存区<br>git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支上</p><p>diff<br>[root@test diff]# git diff<br>diff –git a/a b/a (a（编辑前的版本，暂存区）/a ，b（编辑后的版本，工作区）/a<br>index 14cf074..ac80211 100644（两个文件的哈希值比较）<br>— a/a（—文件变动前的版本，暂存区）<br>+++ b/a（+++文件变动后的版本，工作区）<br>@ -1,2 +1,2 @@（-代表变动前，+代表变动后，1代表第一行，1,2代表连续两行）<br>-123:代表原版本<br>+123 4:代表变动后的版本在前版本上面的增加后的<br>fd</p><p>log<br>git reflog ：查询所有的提交历史<br>git log：看不到commit id的删除记录<br>例如：<br>[root@test log]# git reset <code>--hard</code> 7677a3235b46c<br>HEAD is now at 7677a32 b<br>[root@test log]# git log</p><p>ls ：显示不隐藏的文件与文件夹<br>ls -a：显示当前目录下的所有文件及文件夹包括隐藏的.和..等<br>ls -l ：显示不隐藏的文件与文件夹的详细信息<br>ls -al ：显示当前目录下的所有文件及文件夹包括隐藏的.和..等的详细信息</p><p>mkdir head<br>……<br>项目在创建的时候，git init在.git目录下有个HEAD文件，里面的内容指向了/refs/heads/master，但是没有master文件，说明没有任何提交</p><p>master<br>创建分支：git branch dev<br>切换到分支：git checkout dev</p><p>HEAD告诉我们当前在哪个分支上面，而且是哪一次提交</p><p>合并分支merge<br>[root@test branch]# git log <code>--oneline</code><br>74a5c98 a<br>[root@test branch]# git merge feature<br>Updating 74a5c98..1636fd2<br>Fast-forward<br>b | 1 +<br>c | 1 +<br>d | 1 +<br>3 files changed, 3 insertions(+)<br>create mode 100644 b<br>create mode 100644 c<br>create mode 100644 d<br>[root@test branch]# git log –oneline<br>1636fd2 d<br>8837eb1 b<br>1ef174d c<br>74a5c98 a</p><p>创建并切换新分支<br>[root@test merge]# git checkout -b feature</p><p>在当前分支基础上创建新的分支<br>[root@test merge]# git branch hotfix feature<br>[root@test merge]# git branch -v</p><p>git merge （直接合并到主分支）</p><p>合并并再次提交（合并到主分支，再次提交一次）<br>[root@test merge]# git merge hotfix <code>--no-ff</code></p><p>分支的内容合并后放到主分支里面<br>[root@test merge]# git merge hotfix <code>--squash</code></p><p>查看本地分支<br>git branch -v</p><p>如果新建的项目没有任何提交，是不能创建分支的</p><p>切换分支：git checkout <code>&lt;branchname&gt;</code><br>删除分支：不能再当前分区上删除当前分区</p><p>删除分支：git branch -d <code>&lt;branchname&gt;</code><br>如果当前分支有提交，而没有合并，就只能使用强制删除分支</p><p>强制删除分支：git branch -D <code>&lt;branchname&gt;</code><br>重命名分支：git branch -m <code>&lt;oldbranchname&gt; &lt;newbranchname&gt;</code><br>重命名分支可以在当前分支操作</p><p>创建远程分支：git push -u origin <code>&lt;branchname&gt;</code><br>拉取远程分支：git pull origin <code>&lt;branchname&gt;</code><br>删除远程分支： git push origin <code>--delete</code> <code>&lt;branchname&gt;</code><br>重命名远程分支：<br>1、先删除本地分支<br>2、重命名本地分支<br>3、向远程增加分支</p><p>远程分支覆盖本地分支<br>git pull origin master<br>git reset <code>--hard</code> FETCH_HEAD</p><p>git merge：会有清晰的提交历史<br>git rebase：整洁的提交历史</p><p>git rebase合并中出现冲突情况的解决<br>1、git rebase<br>2、git status<br>3、vim &lt;冲突文件&gt;<br>4、git add &lt;解决完的冲突文件&gt;<br>5、git status<br>6、git rebase <code>--continue</code></p><p>情形1<br>开发新功能问题<br>解决步骤：<br>1、拉取远程仓库代码<br>git pull origin master<br>2、创建新的分支，并在这个分支上写代码，提交<br>3、将自己的代码合并到master分支<br>4、然后将这个master分支推送到远程master分支<br>例如：<br>[root@test newfeature]# git remote add origin <a href="mailto:git@github.com">git@github.com</a>:Hsiehchou/hsiehchou001.git<br>[root@test newfeature]# git pull origin master<br>[root@test newfeature]# vim README.md<br>[root@test newfeature]# git add README.md<br>[root@test newfeature]# git commit -m “new feature develop over”<br>[root@test newfeature]# git checkout master<br>[root@test newfeature]# git push origin master</p><p>情形2<br>自己正在开发新代码，而且暂存区和本地仓库都有代码，在这个时候，老板说线上有个棘手的bug需要修复，自己需要停止手上的新功能开发<br>解决步骤：<br>1、git stash 将现有的暂存区和工作区的代码保留<br>2、然后创建新分支，修复bug<br>3、重新将stash里面的内容拿取出来</p><p>例如：<br>git checkout -b session<br>vim a<br>git add a<br>vim b<br>git status</p><p>git stash<br>git checkout -b hotfix<br>git checkout session<br>git stash pop</p><p>git stash <code>--help</code></p><p>情形3<br>自己在某个分支开发代码，然后别人在 另外的分支开发代码，现在别人已经提交好了代码，然后你想要别人的某几次提交，间隔，或者某一个提交<br>解决步骤：<br>git cherry-pick 合并某一个，或者某几个提交</p><p>git cherry-pick <code>&lt;commit id&gt; &lt;commit id&gt;</code><br>git cherry-pick <code>&lt;commit id&gt;...&lt;commit id&gt;</code><br>合并的时候不包括左边的，但是包括右边的</p><p>[root@test cherry]# git init<br>Initialized empty Git repository in /root/cherry/.git/<br>[root@test cherry]# touch a<br>[root@test cherry]# git add a<br>[root@test cherry]# git commit -m “a”<br>[master (root-commit) 47f4286] a<br>[root@test cherry]# git checkout -b hotfix<br>[root@test cherry]# touch b<br>[root@test cherry]# git add b<br>[root@test cherry]# git commit -m “b”<br>[hotfix 1b01f55] b<br>[root@test cherry]# touch c<br>[root@test cherry]# git add c<br>[root@test cherry]# git commit -m “c”<br>[root@test cherry]# touch d<br>[root@test cherry]# git add d<br>[root@test cherry]# git commit -m “d”<br>[root@test cherry]# git log <code>--oneline</code><br>45e8c64 d<br>cb1f9ef c<br>1b01f55 b<br>47f4286 a<br>[root@test cherry]# git checkout master<br>[root@test cherry]# git cherry-pick cb1f9ef<br>[root@test cherry]# git log <code>--oneline</code><br>c00f63d c<br>47f4286 a</p><p>[root@test cherry]# git log <code>--oneline</code><br>9b2ac83 d<br>8accef7 f<br>ca85346 e<br>c00f63d c<br>47f4286 a<br>[root@test cherry]# git checkout master<br>[root@test cherry]# git log <code>--oneline</code><br>c00f63d c<br>47f4286 a<br>[root@test cherry]# git cherry-pick ca85346 9b2ac83<br>[root@test cherry]# git log <code>--oneline</code><br>413b51b d<br>62fa796 e<br>c00f63d c<br>47f4286 a</p><p>git format-patch生成补丁<br>master a-&gt;b<br>feature a-&gt;b-&gt;c，将bc两次提交的不同生成patch给master，然后master应用补丁</p><p>远程仓库你没有权限，然后你fork远程仓库，代码，然后你clone到本地，接着修改，这是一个很小的修改，然后你又没有权限直接push到别人的仓库，这个时候你就把修改的代码生成一个patch</p><p>git format-patch <code>&lt;commit id&gt;</code>生成patch</p><p>git apply <code>--check</code> *.patch 检查这个patch是否能用</p><p>git am *.patch应用patch，创建git.am这个应用环境</p><p>git apply <code>--reject</code> *.patch会在当前文件夹下生成这个.rej，看这个.rej文件说明，去修改相应冲突的文件，修改完之后，用git add&lt;冲突的文件&gt;</p><p>git am <code>--resolved</code><br>例如：<br>[root@test ~]# mkdir patch<br>[root@test ~]# cd patch/<br>[root@test patch]# git init<br>[root@test patch]# touch a<br>[root@test patch]# git add a<br>[root@test patch]# git commit -m “a”<br>[root@test patch]# git checkout -b feature<br>[root@test patch]# vim b<br>[root@test patch]# git add b<br>[root@test patch]# git commit -m “b”<br>[root@test patch]# vim a<br>[root@test patch]# git add a<br>[root@test patch]# git commit -m “feature a”<br>[root@test patch]# git log <code>--oneline</code><br>01fd4c5 feature a<br>0445b79 b<br>ff994db a<br>[root@test patch]# git checkout master<br>[root@test patch]# vim a<br>[root@test patch]# git add a<br>[root@test patch]# git commit -m “master a”<br>[root@test patch]# git log <code>--oneline</code><br>f8af4f3 master a<br>ff994db a<br>[root@test patch]# git checkout feature<br>[root@test patch]# git log <code>--oneline</code><br>01fd4c5 feature a<br>0445b79 b<br>ff994db a<br>[root@test patch]# git format-patch <code>--help</code><br>[root@test patch]# git format-patch ff994db<br>[root@test patch]# git checkout master<br>[root@test patch]# git apply <code>--check</code> 0001-b.patch<br>[root@test patch]# git apply <code>--check</code> 0002-feature-a.patch<br>[root@test patch]# git am <em>.patch<br>[root@test patch]# git apply 0002-feature-a.patch <code>--reject</code><br>[root@test patch]# cat a.rej<br>diff a/a b/a (rejected hunks)<br>@ -0,0 +1 @@<br>+a<br>[root@test patch]# cat b<br>b<br>[root@test patch]# cat a<br>master a<br>[root@test patch]# vim a<br>[root@test patch]# git add a<br>[root@test patch]# git status<br>[root@test patch]# git am <code>--resolved</code><br>[root@test patch]# cat a<br>master a<br>b<br>[root@test patch]# git status<br>[root@test patch]# cat a.rej<br>diff a/a b/a (rejected hunks)<br>@ -0,0 +1 @@<br>+a<br>[root@test patch]# cat a<br>master a<br>b<br>[root@test patch]# rm -rf a.rej<br>[root@test patch]# rm -rf 000</em></p><p>[root@test patch]# git checkout -b hotfix<br>[root@test patch]# vim a<br>[root@test patch]# git add a<br>[root@test patch]# git commit -m “hotfix”<br>[root@test patch]# git log <code>--oneline</code><br>1cde42d hotfix<br>6b1b69b feature a<br>8301986 b<br>f8af4f3 master a<br>ff994db a<br>[root@test patch]# git format-patch <code>--help</code><br>[root@test patch]# git format-patch 6b1b69b -o /root<br>[root@test patch]# git checkout master<br>[root@test patch]# vim a<br>[root@test patch]# git add a<br>[root@test patch]# git commit -m “master2”<br>[root@test patch]# git apply <code>--check</code> /root/0001-hotfix.patch<br>[root@test patch]# git am /root/0001-hotfix.patch<br>[root@test patch]# git status<br>nothing to commit, working directory clean<br>[root@test patch]# git apply <code>--reject</code> /root/0001-hotfix.patch<br>[root@test patch]# ll<br>[root@test patch]# cat a.rej<br>diff a/a b/a (rejected hunks)<br>@ -1,2 +1,5 @@<br>master a<br>b<br><code>+</code><br>+hotfix<br><code>+</code><br>[root@test patch]# vim a<br>[root@test patch]# git add a<br>[root@test patch]# git am <code>--resolved</code><br>[root@test patch]# git status</p>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git快速上手 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据基础1</title>
      <link href="/2019/02/03/da-shu-ju-ji-chu-1/"/>
      <url>/2019/02/03/da-shu-ju-ji-chu-1/</url>
      
        <content type="html"><![CDATA[<h4 id="1、Linux下命令行中的复制和粘贴"><a href="#1、Linux下命令行中的复制和粘贴" class="headerlink" title="1、Linux下命令行中的复制和粘贴"></a>1、Linux下命令行中的复制和粘贴</h4><p>安装gpm：yum install -y gpm*</p><p>开启gpm服务：systemctl start gpm</p><h4 id="2、打开网卡"><a href="#2、打开网卡" class="headerlink" title="2、打开网卡"></a>2、打开网卡</h4><p>vi /etc/sysconfig/network-scripts/ifcfg-ens33</p><h4 id="3、修改为静态IP"><a href="#3、修改为静态IP" class="headerlink" title="3、修改为静态IP"></a>3、修改为静态IP</h4><p>BOOTPROTO=”dhcp” 这个是动态IP<br>BOOTPROTO=”static”这个是静态IP<br>BOOTPROTO=”none”这个是无</p><h4 id="4、IP地址"><a href="#4、IP地址" class="headerlink" title="4、IP地址"></a>4、IP地址</h4><p>IPADDR=192.168.116.121</p><h4 id="5、网关"><a href="#5、网关" class="headerlink" title="5、网关"></a>5、网关</h4><p>GATEWAY=192.168.116.2</p><h4 id="6、子网掩码"><a href="#6、子网掩码" class="headerlink" title="6、子网掩码"></a>6、子网掩码</h4><p>NETMASK=255.255.255.0</p><h4 id="7、DNS服务器1、2"><a href="#7、DNS服务器1、2" class="headerlink" title="7、DNS服务器1、2"></a>7、DNS服务器1、2</h4><p>DNS1=8.8.8.8<br>DNS2=8.8.4.4</p><h4 id="8、vi-etc-resolve-conf"><a href="#8、vi-etc-resolve-conf" class="headerlink" title="8、vi /etc/resolve.conf"></a>8、vi /etc/resolve.conf</h4><p>nameserver 8.8.8.8<br>nameserver 8.8.4.4</p><h4 id="9、重启网卡"><a href="#9、重启网卡" class="headerlink" title="9、重启网卡"></a>9、重启网卡</h4><p>service network restart</p><h4 id="10、-如何测试可以ping通"><a href="#10、-如何测试可以ping通" class="headerlink" title="10、 如何测试可以ping通"></a>10、 如何测试可以ping通</h4><p>ping 192.168.116.2<br>ping <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a></p><h4 id="11、解压JDK命令"><a href="#11、解压JDK命令" class="headerlink" title="11、解压JDK命令"></a>11、解压JDK命令</h4><p>tar -zxvf jdk-8u192-linux-x64.tar.gz -C /opt/module/</p><h4 id="12、系统JDK环境变量的位置"><a href="#12、系统JDK环境变量的位置" class="headerlink" title="12、系统JDK环境变量的位置"></a>12、系统JDK环境变量的位置</h4><p>vi /etc/profile</p><h4 id="13、写入"><a href="#13、写入" class="headerlink" title="13、写入"></a>13、写入</h4><p>export JAVA_HOME=<code>/opt/module/jdk1.8.0_192</code><br>export PATH=<code>$JAVA_HOME/bin:$PATH</code></p><h4 id="14、环境变量生效"><a href="#14、环境变量生效" class="headerlink" title="14、环境变量生效"></a>14、环境变量生效</h4><p>source /etc/profile</p><h4 id="15、检验JDK是否生效"><a href="#15、检验JDK是否生效" class="headerlink" title="15、检验JDK是否生效"></a>15、检验JDK是否生效</h4><p>输入javac，回车</p><h4 id="16、解压Hadoop"><a href="#16、解压Hadoop" class="headerlink" title="16、解压Hadoop"></a>16、解压Hadoop</h4><p>使用windscp上传<br>tar -zxvf hadoop-2.7.3.tar.gz -C /opt/module/</p><h4 id="17、解压Hadoop"><a href="#17、解压Hadoop" class="headerlink" title="17、解压Hadoop"></a>17、解压Hadoop</h4><p>使用windscp上传<br>tar -zxvf hadoop-2.7.3.tar.gz -C /opt/module/</p><h4 id="18、创建日志logs和临时目录tmp"><a href="#18、创建日志logs和临时目录tmp" class="headerlink" title="18、创建日志logs和临时目录tmp"></a>18、创建日志logs和临时目录tmp</h4><p>mkdir logs tmp</p><p>[root@localhost hadoop-2.7.3]#cd etc/hadoop/</p><h4 id="19、vi-hadoop-env-sh"><a href="#19、vi-hadoop-env-sh" class="headerlink" title="19、vi hadoop-env.sh"></a>19、vi hadoop-env.sh</h4><p>export JAVA_HOME=/opt/module/jdk1.8.0_192</p><h4 id="20、vi-core-site-xml"><a href="#20、vi-core-site-xml" class="headerlink" title="20、vi core-site.xml"></a>20、vi core-site.xml</h4><pre><code>&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt;    &lt;name&gt;fs.defaultFS&lt;/name&gt;    &lt;value&gt;hdfs://hsiehchou121:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt;    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;    &lt;value&gt;/opt/module/hadoop-2.7.3/tmp&lt;/value&gt;&lt;/property&gt;</code></pre><h4 id="21、vi-hdfs-site-xml"><a href="#21、vi-hdfs-site-xml" class="headerlink" title="21、vi hdfs-site.xml"></a>21、vi hdfs-site.xml</h4><pre><code>&lt;!-- 指定HDFS副本的数量 --&gt;&lt;property&gt;    &lt;name&gt;dfs.replication&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt;&lt;/property&gt;</code></pre><h4 id="22、修改主机名"><a href="#22、修改主机名" class="headerlink" title="22、修改主机名"></a>22、修改主机名</h4><p>hostnamectl set-hostname 主机名<br>如：hostnamectl set-hostname hsiehchou121</p><h4 id="23、检查主机名"><a href="#23、检查主机名" class="headerlink" title="23、检查主机名"></a>23、检查主机名</h4><p>hostname</p><h4 id="24、格式化-hdfs"><a href="#24、格式化-hdfs" class="headerlink" title="24、格式化 hdfs"></a>24、格式化 hdfs</h4><p>[root@localhost hadoop-2.7.3]#bin/hdfs namenode -format</p><h4 id="25、启动-hdfs"><a href="#25、启动-hdfs" class="headerlink" title="25、启动 hdfs"></a>25、启动 hdfs</h4><p>[root@localhost hadoop-2.7.3]#sbin/start-dfs.sh</p><h4 id="26、关闭-hdfs"><a href="#26、关闭-hdfs" class="headerlink" title="26、关闭 hdfs"></a>26、关闭 hdfs</h4><p>[root@localhost hadoop-2.7.3]#sbin/stop-dfs.sh</p><h4 id="27、查看进程"><a href="#27、查看进程" class="headerlink" title="27、查看进程"></a>27、查看进程</h4><p>jps</p><p>[root@localhost hadoop-2.7.3]# jps<br>39668 DataNode<br>39547 NameNode<br>39932 Jps<br>39823 SecondaryNameNode</p><h4 id="28、页面"><a href="#28、页面" class="headerlink" title="28、页面"></a>28、页面</h4><p>IP地址:50070<br>如:192.168.116.121:50070<br>显示页面就对了</p><h4 id="29、临时关闭防火墙"><a href="#29、临时关闭防火墙" class="headerlink" title="29、临时关闭防火墙"></a>29、临时关闭防火墙</h4><p>systemctl stop firewalld.service</p><h4 id="30、永久禁用防火墙"><a href="#30、永久禁用防火墙" class="headerlink" title="30、永久禁用防火墙"></a>30、永久禁用防火墙</h4><p>systemctl disable firewalld.service</p><h4 id="31、查看防火墙状态"><a href="#31、查看防火墙状态" class="headerlink" title="31、查看防火墙状态"></a>31、查看防火墙状态</h4><p>systemctl status firewalld.service</p><h4 id="32、SSH无密码登录"><a href="#32、SSH无密码登录" class="headerlink" title="32、SSH无密码登录"></a>32、SSH无密码登录</h4><p>[root@localhost hadoop-2.7.3]$ ssh-keygen -t rsa<br>然后敲（三个回车）</p><p>[root@localhost hadoop-2.7.3]# ssh-copy-id hsiehchou121<br>[root@localhost hadoop-2.7.3]# ssh hsiehchou121<br>[root@hsiehchou121 ~]#<br>[root@hsiehchou121 ~]# exit<br>logout</p><h4 id="33、关闭selinux防火墙"><a href="#33、关闭selinux防火墙" class="headerlink" title="33、关闭selinux防火墙"></a>33、关闭selinux防火墙</h4><p>vi /etc/selinux/config</p><p>SELINUX=enforcing 改成<br>SELINUX=disabled</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker基本使用</title>
      <link href="/2019/02/01/docker-ji-ben-shi-yong/"/>
      <url>/2019/02/01/docker-ji-ben-shi-yong/</url>
      
        <content type="html"><![CDATA[<h2 id="Docker在Linux中的安装"><a href="#Docker在Linux中的安装" class="headerlink" title="Docker在Linux中的安装"></a>Docker在Linux中的安装</h2><h3 id="一、rpm离线安装docker17-12"><a href="#一、rpm离线安装docker17-12" class="headerlink" title="一、rpm离线安装docker17.12"></a>一、rpm离线安装docker17.12</h3><h4 id="1-下载docker安装包"><a href="#1-下载docker安装包" class="headerlink" title="1.下载docker安装包"></a>1.下载docker安装包</h4><p>在<a href="https://download.docker.com/linux/centos/7/x86_64/stable/Packages/下载docker-ce-17.12.0.ce-1.el7.centos.x86_64.rpm" target="_blank" rel="noopener">https://download.docker.com/linux/centos/7/x86_64/stable/Packages/下载docker-ce-17.12.0.ce-1.el7.centos.x86_64.rpm</a></p><h4 id="2-下载9个依赖"><a href="#2-下载9个依赖" class="headerlink" title="2.下载9个依赖"></a>2.下载9个依赖</h4><p>在<a href="http://mirrors.163.com/centos/7/os/x86_64/Packages/下载8个依赖" target="_blank" rel="noopener">http://mirrors.163.com/centos/7/os/x86_64/Packages/下载8个依赖</a><br>audit-libs-python-2.7.6-3.el7.x86_64.rpm<br>checkpolicy-2.5-4.el7.x86_64.rpm<br>libcgroup-0.41-13.el7.x86_64.rpm<br>libseccomp-2.3.1-3.el7.x86_64.rpm<br>libsemanage-python-2.5-8.el7.x86_64.rpm<br>policycoreutils-python-2.5-17.1.el7.x86_64.rpm<br>python-IPy-0.75-6.el7.noarch.rpm<br>setools-libs-3.3.8-1.1.el7.x86_64.rpm</p><p>在<a href="http://rpm.pbone.net/index.php3?stat=3&amp;limit=1&amp;srodzaj=1&amp;dl=40&amp;search=container-selinux&amp;field[]=1&amp;field[]=2下载container-selinux-2.9-4.el7.noarch.rpm" target="_blank" rel="noopener">http://rpm.pbone.net/index.php3?stat=3&amp;limit=1&amp;srodzaj=1&amp;dl=40&amp;search=container-selinux&amp;field[]=1&amp;field[]=2下载container-selinux-2.9-4.el7.noarch.rpm</a></p><p>rpm -ivh /root/docker/*.rpm –nodeps –force</p><p>curl -sSL <a href="http://acs-public-mirror.oss-cn-hangzhou.aliyuncs.com/docker-engine/internet" target="_blank" rel="noopener">http://acs-public-mirror.oss-cn-hangzhou.aliyuncs.com/docker-engine/internet</a> | sh -</p><p>###二、 启动Docker引擎<br>sudo systemctl enable docker<br>sudo systemctl start docker</p><h3 id="三、建立docker用户组"><a href="#三、建立docker用户组" class="headerlink" title="三、建立docker用户组"></a>三、建立docker用户组</h3><p>sudo groupadd docker</p><h3 id="四、将用户加入docker组"><a href="#四、将用户加入docker组" class="headerlink" title="四、将用户加入docker组"></a>四、将用户加入docker组</h3><p>sudo usermod -aG docker $USER</p><p>这里使用阿里云的容器镜像服务，目前公测，免费的。 </p><p>1) 去阿里云官网，登录控制台，在产品与服务里面找到容器镜像服务<br>2）点击开通<br>3) 点击镜像加速器，变有了加速地址（不用镜像加速器的话，镜像都是国外的，因为墙，所有下载是龟速）</p><h3 id="五、镜像加速器"><a href="#五、镜像加速器" class="headerlink" title="五、镜像加速器"></a>五、镜像加速器</h3><p>vi /etc/systemd/system/multi-user.target.wants/docker.service<br>ExecStart=/usr/bin/dockerd –registry-mirror=https://….mi<br>rror.aliyuncs.com</p><p>sudo systemctl daemon-reload<br>sudo systemctl restart docker</p><h3 id="六、检查加速器是否生效"><a href="#六、检查加速器是否生效" class="headerlink" title="六、检查加速器是否生效"></a>六、检查加速器是否生效</h3><p>sudo ps -ef | grep dockerd<br>root 5346 1 0 19:03 ? 00:00:00 /usr/bin/dockerd<br>–registry-mirror=https://*****.mirror.aliyuncs.com</p><p>docker -v</p><p>systemctl start docker</p><p>验证 docker 是否安装成功并在容器中执行一个测试的镜像<br>docker run ubuntu echo hello docker</p><p>docker run nginx</p><p>docker run -p 8080:80 -d nginx<br><a href="http://192.168.116.104:8080/" target="_blank" rel="noopener">http://192.168.116.104:8080/</a></p><p>[root@test3 share]# docker cp index.html<br>[root@test3 share]# docker commit -m ‘fun’ d0e976512485 nginx-fun</p><h3 id="七、删除镜像"><a href="#七、删除镜像" class="headerlink" title="七、删除镜像"></a>七、删除镜像</h3><p>docker rmi IMAGE ID</p><h3 id="八、查看镜像"><a href="#八、查看镜像" class="headerlink" title="八、查看镜像"></a>八、查看镜像</h3><p>docker images<br>docker ps -a</p><h3 id="九、小结"><a href="#九、小结" class="headerlink" title="九、小结"></a>九、小结</h3><table><thead><tr><th align="center">命令</th><th align="center">用途</th></tr></thead><tbody><tr><td align="center">docker pull</td><td align="center">获取image</td></tr><tr><td align="center">docker build</td><td align="center">创建image</td></tr><tr><td align="center">docker images</td><td align="center">列出image</td></tr><tr><td align="center">docker run</td><td align="center">运行container</td></tr><tr><td align="center">docker ps</td><td align="center">列出container</td></tr><tr><td align="center">docker rm</td><td align="center">删除container</td></tr><tr><td align="center">docker rmi</td><td align="center">删除image</td></tr><tr><td align="center">docker cp</td><td align="center">在host 和container之间拷贝文件</td></tr><tr><td align="center">docker commit</td><td align="center">保存改动为新的image</td></tr></tbody></table><h3 id="十、Dockerfile语法"><a href="#十、Dockerfile语法" class="headerlink" title="十、Dockerfile语法"></a>十、Dockerfile语法</h3><p>FROM alpine:latest<br>MAINTAINER hsiehchou<br>CMD echo ‘hello docker’</p><p>touch Dockerfile</p><p>++++++++++++++++++++++++++++++++++++++++<br>FROM ubuntu<br>MAINTAINER hsiehchou<br>RUN sed -i ‘s/archive.ubuntu.com/mirros.ustc.edu.cn/g’ /etc/apt/sources.list<br>RUN apt-get update<br>RUN apt-get install -y nginx<br>COPY index.html /var/www/html<br>ENTRYPOINT [“/usr/sbin/nginx”, “-g”, “daemon off;”]前台运行<br>EXPOSE 80</p><p>docker build -t hsiehchou/hello-nginx .</p><p>docker run -d -p 80:80 hsiehchou/hello-nginx</p><h3 id="十一、小结"><a href="#十一、小结" class="headerlink" title="十一、小结"></a>十一、小结</h3><table><thead><tr><th align="center">命令</th><th align="center">用途</th></tr></thead><tbody><tr><td align="center">FROM</td><td align="center">base image</td></tr><tr><td align="center">RUN</td><td align="center">执行命令</td></tr><tr><td align="center">ADD</td><td align="center">添加文件</td></tr><tr><td align="center">COPY</td><td align="center">拷贝文件</td></tr><tr><td align="center">CMD</td><td align="center">执行命令</td></tr><tr><td align="center">EXPOSE</td><td align="center">暴露端口</td></tr><tr><td align="center">WORKDIR</td><td align="center">指定路径</td></tr><tr><td align="center">MIANTAINER</td><td align="center">维护者</td></tr><tr><td align="center">ENV</td><td align="center">设定环境变量</td></tr><tr><td align="center">ENVRYPOINT</td><td align="center">容器入口</td></tr><tr><td align="center">USER</td><td align="center">指定用户</td></tr><tr><td align="center">VOLUME</td><td align="center">mount point</td></tr></tbody></table><h3 id="十二、镜像分层"><a href="#十二、镜像分层" class="headerlink" title="十二、镜像分层"></a>十二、镜像分层</h3><p>Dockerfile中的每一行都产生一个新层</p><h3 id="十三、Volume"><a href="#十三、Volume" class="headerlink" title="十三、Volume"></a>十三、Volume</h3><p>提供独立于容器之外的持久化存储</p><p>docker run -d –name nginx -v /usr/share/nginx/html nginx</p><p>docker exec -it nginx /bin/bash</p><p>docker run -v $PWD/html:/usr/share/nginx/html nginx</p><p>++++++++++++++++++++++++++++<br>docker create -v $PWD/data:/var /mydata –name data_container ubuntu</p><p>docker run -it –volumes-from data_container ubuntu /bin/bash<br>mount</p><h3 id="十四、Registry"><a href="#十四、Registry" class="headerlink" title="十四、Registry"></a>十四、Registry</h3><p>镜像仓库</p><h3 id="十五、术语"><a href="#十五、术语" class="headerlink" title="十五、术语"></a>十五、术语</h3><table><thead><tr><th align="center">English</th><th align="center">中文</th></tr></thead><tbody><tr><td align="center">host</td><td align="center">宿主机</td></tr><tr><td align="center">image</td><td align="center">镜像</td></tr><tr><td align="center">container</td><td align="center">容器</td></tr><tr><td align="center">registry</td><td align="center">仓库</td></tr><tr><td align="center">daemon</td><td align="center">守护程序</td></tr><tr><td align="center">client</td><td align="center">客户端</td></tr></tbody></table><p>docker search whalesay<br>docker pull docker/whalesay<br>docker push myname/whalesay</p><p>国内的一些仓库<br>daocloud<br>时速云<br>aliyun</p><p>[root@test3 dockerfiler2]# docker run docker/whalesay cowsay Docker你好！</p><p>docker tag docker/whalesay hch/whalesay</p><p>curl -L <a href="https://github.com/docker/compose/releases/download/1.9.0/docker-compose-" target="_blank" rel="noopener">https://github.com/docker/compose/releases/download/1.9.0/docker-compose-</a>(uname -m) &gt; /usr/local/bin/docker-compose</p><p><strong>docker-compose.yml常用命令</strong></p><table><thead><tr><th align="center">命令</th><th align="center">用途</th></tr></thead><tbody><tr><td align="center">build</td><td align="center">本地创建镜像</td></tr><tr><td align="center">command</td><td align="center">覆盖缺省命令</td></tr><tr><td align="center">depends_on</td><td align="center">连接容器</td></tr><tr><td align="center">ports</td><td align="center">暴露端口</td></tr><tr><td align="center">volumes</td><td align="center">卷</td></tr><tr><td align="center">image</td><td align="center">pull镜像</td></tr></tbody></table><p><strong>docker-compose命令</strong></p><table><thead><tr><th align="center">命令</th><th align="center">用途</th></tr></thead><tbody><tr><td align="center">up</td><td align="center">启动服务</td></tr><tr><td align="center">stop</td><td align="center">停止服务</td></tr><tr><td align="center">rm</td><td align="center">删除服务中的各个容器</td></tr><tr><td align="center">logs</td><td align="center">观察各个容器的日志</td></tr><tr><td align="center">ps</td><td align="center">列出服务相关的容器</td></tr></tbody></table><h3 id="十六、docker基本命令"><a href="#十六、docker基本命令" class="headerlink" title="十六、docker基本命令"></a>十六、docker基本命令</h3><p><strong>docker ps</strong>：查看正在运行的容器<br><strong>docker images</strong>：查看现有的镜像<br><strong>docker logs</strong>： 查看某个容器的日志<br><strong>docker run</strong>： 运行某个容器<br><strong>docker inspect</strong>：查看某个容器<br><strong>docker exec</strong>：进入某个容器<br><strong>docker start/stop</strong>：启动或者停止某个容器</p><p>[root@test3 hadoop-docker]# touch Dockerfile<br>[root@test3 hadoop-docker]# ll<br>total 0<br>-rw-r–r– 1 root root 0 Feb 27 19:18 Dockerfile<br>[root@test3 hadoop-docker]# vim Dockerfile</p><pre><code>FROM  ubuntu:14.04MAINTAINER hsiehchouWORKDIR /root# install openssh-server, openjdk and wgetRUN apt-get update &amp;&amp; apt-get install -y openssh-server openjdk-7-jdk wget# install hadoop 2.7.2RUN wget https://github.com/kiwenlau/compile-hadoop/release/download/2.7.2/hadoop-2.7.2.tar.gz &amp;&amp; \        tar -zxvf hadoop-2.7.2.tar.gz &amp;&amp; \        mv hadoop-2.7.2 /usr/local/hadoop &amp;&amp; \        rm hadoop-2.7.2.tar.gz# set environment variableENV JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd-64ENV HADOOP_HOME=/usr/local/hadoopENV PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin# ssh without keyRUN ssh-keygen -t rsa -f ~/.ssh/id_rsa -p &#39;&#39; &amp;&amp; \        cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keysRUN mkdir -p ~/hdfs/namenode &amp;&amp; \    mkdir -p ~/hdfs/datanode &amp;&amp; \    mkdir $HADOOP_HOME/logsCOPY config/* /tmp/RUN mv /tmp/ssh_config ~/.ssh/config &amp;&amp; \    mv /tmp/hadoop-env.sh /usr/local/hadoop/etc/hadoop/hadoop-env.sh &amp;&amp; \    mv /tmp/hdfs-site.xml $HADOOP_HOME/etc/hadoop/hdfs-site.xml &amp;&amp; \    mv /tmp/core-site.xml $HADOOP_HOME/etc/hadoop/core-site.xml &amp;&amp; \    mv /tmp/mapred-site.xml $HADOOP_HOME/etc/hadoop/mapred-site.xml &amp;&amp; \    mv /tmp/yarn-site.xml $HADOOP_HOME/etc/hadoop/yarn-site.xml &amp;&amp; \    mv /tmp/slaves $HADOOP_HOME/etc/hadoop/slaves &amp;&amp; \    mv /tmp/start-hadoop.sh ~/start-hadoop.ssh &amp;&amp; \    mv /tmp/run-wordcount.sh ~/run-wordcount.shRUN chmod +x ~/start-hadoop.sh &amp;&amp; \    chmod +x ~/run-wordcount.sh &amp;&amp; \    chmod +x $HADOOP_HOME/sbin/start-dfs.sh &amp;&amp; \    chmod +x $HADOOP_HOME/sbin/start-yarn.sh# format namenodeRUN /usr/local/hadoop/bin/hdfs namenode -formatCMD [&quot;sh&quot;, &quot;-c&quot;, &quot;service ssh start: bash&quot;]</code></pre>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker基本使用 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java之MySQL的使用</title>
      <link href="/2019/01/30/java-zhi-mysql-de-shi-yong/"/>
      <url>/2019/01/30/java-zhi-mysql-de-shi-yong/</url>
      
        <content type="html"><![CDATA[<h4 id="1、MySQL概要"><a href="#1、MySQL概要" class="headerlink" title="1、MySQL概要"></a>1、MySQL概要</h4><p>关系型数据库。—Access数据库 oracle数据库、Postgresql-<br>非关系型数据库。—-Hbase等<br>库：—package<br>表：–class<br>字段：–属性<br>Oracle旗下产品—-分两种 （GPL协议的 社区版和企业版）<br>CDH HDP–后面大数据给大家讲 Apache–hive hdfs hadoop</p><p>RDBMS：关系数据库管理系统。将数据存储在不同的库表里面<br>支持标准的SQL语句100%<br>体积小 速度快<br><a href="https://www.mysql.com/—》MySQL" target="_blank" rel="noopener">https://www.mysql.com/—》MySQL</a> Community Edition (GPL)-》MySQL OnWindows<br>MySQL Installer-》Windows (x86, 32-bit), MSI Installer 8.0.13 313.8M</p><p>mysql-installer-community-8.0.13.0.msi</p><h4 id="2、MySQL安装"><a href="#2、MySQL安装" class="headerlink" title="2、MySQL安装"></a>2、MySQL安装</h4><p>DBA–数据库管理员<br>黑框框+workbench<br>show databases;–展示所有库<br>show tables;–展示所有表;<br>describe city;–展示表里面的字段信息<br>select * from city limit 10;</p><h4 id="3、Navicat安装与操作MySQL"><a href="#3、Navicat安装与操作MySQL" class="headerlink" title="3、Navicat安装与操作MySQL"></a>3、Navicat安装与操作MySQL</h4><p>Navicat Premium 是一套数据库开发工具，让你从单一应用程序中同时连接 MySQL、MariaDB、MongoDB、SQL Server、Oracle、PostgreSQL 和 SQLite 数据库。它与 Amazon RDS、Amazon Aurora、Amazon Redshift、Microsoft Azure、Oracle Cloud、MongoDB Atlas、<br>阿里云、腾讯云和华为云等云数据库兼容。可以快速轻松地创建、管理和维护数据库</p><p>更新数据库的密码<br>ALTER USER ‘root’@’localhost’ IDENTIFIED WITH mysql_native_password BY ‘密码’;</p><h4 id="4、MySQL数据类型"><a href="#4、MySQL数据类型" class="headerlink" title="4、MySQL数据类型"></a>4、MySQL数据类型</h4><p>（1）数值类型<br>a、整型<br>tinyint 1个字节<br>smallint 2个字节<br>mediumint 3个字节<br>int 4<br>bigint 8<br>b、浮点型（float double）<br>float(M,D) 小数位数部分会四舍五入。M=3 D=2 3.15<br>c、定点数<br>可变长度 decimal(M,D) M:表示总的有效位数，D表示小数的位数<br>3.14<br>3.145 </p><p>（2）字符串类型<br>char:定长字符串 255个<br>varchar：变长字符串 varchar(25) 最大65535字符<br>blob:二进制字符串–文件 图片等<br>text:非二进制字符串–长文本</p><p>（3） 日期数据类型<br>datetime：2018-12-22 21:04:55<br>timestamp：时间戳 2019021600000 ms</p><h4 id="5、MySQL外键、主键、唯一键"><a href="#5、MySQL外键、主键、唯一键" class="headerlink" title="5、MySQL外键、主键、唯一键"></a>5、MySQL外键、主键、唯一键</h4><p>（1）外键 Foreign Key<br>如果换教室 302-303教室 需要对所有的数据进行 更新。30个学生 然后就得跟新30次<br>（2） 主键 Primary Key 唯一不可重复 只能有一个主键，不能为null<br>（3）唯一键 一个表可以有多个 唯一键 unique</p><h4 id="6、SQL语句–增删改查"><a href="#6、SQL语句–增删改查" class="headerlink" title="6、SQL语句–增删改查"></a>6、SQL语句–增删改查</h4><p>SQL：Structure Query Language。—HiveQL Spark SQL<br>查询：<br>select字段(*) from 表明 (limit count) (where); </p><p>插入语句：<br>insert into 表名 [字段名] values(值列表);</p><p>修改语句：<br>update 表名 set 字段=值 where 条件; </p><p>删除语句：<br>delete from 表名 [where 条件];</p><p>例如：<br>SELECT * FROM classroom;<br>insert into classroom VALUES(“001”,”9年级”,”11”,”CC”);<br>update classroom SET classroom.classroomid=”0003” where classroom.classroomid=”001”;<br>DELETE FROM classroom where classroom.classroomid=”0003”;<br>TRUNCATE student;</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java之MySQL的使用 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java反射</title>
      <link href="/2019/01/29/java-fan-she/"/>
      <url>/2019/01/29/java-fan-she/</url>
      
        <content type="html"><![CDATA[<h4 id="1、反射获取Class对象的三种方式"><a href="#1、反射获取Class对象的三种方式" class="headerlink" title="1、反射获取Class对象的三种方式"></a>1、反射获取Class对象的三种方式</h4><p>反编译<br>不是自己写的类，也不知道类里面有哪些方法 变量，让你能够使用程序上线了，修改程序但不终止程序的运行—-反射<br>（1）Object类 getClass 方法<br>getClass 返回此Object的运行时类<br>getName() 返回由 类对象表示的实体（类，接口，数组类，原始类型或空白）的名称，作为 String</p><p>（2）通过Class属性获得<br>都有一个静态的class属性</p><p>（3）通过 forName<br>static 类&lt;?&gt; forName(String className) 返回与给定字符串名称的类或接口相关联的 类对象</p><h4 id="2、反射获得构造方法"><a href="#2、反射获得构造方法" class="headerlink" title="2、反射获得构造方法"></a>2、反射获得构造方法</h4><p>（1）获得构造方法</p><pre><code>Constructor&lt;T&gt; getDeclaredConstructor(类&lt;?&gt;... parameterTypes) 返回一个 Constructor对象，该对象反映 Constructor对象表示的类或接口的指定 类函数Constructor&lt;?&gt;[] getDeclaredConstructors() 返回一个反映 Constructor对象表示的类声明的所有 Constructor对象的数组 类 Constructor&lt;T&gt; getConstructor(类&lt;?&gt;... parameterTypes) 返回一个 Constructor对象，该对象反映 Constructor对象表示的类的指定的公共类函数Constructor&lt;?&gt;[] getConstructors() 返回包含一个数组 Constructor对象反射由此表示的类的所有公共构造 类对象</code></pre><p>（2）使用构造方法<br>public T newInstance() throws InstantiationException, IllegalAccessException<br>访问私有的构造方法。必须通过Accessible设置为true。强行访问</p><p>public void setAccessible(boolean flag) throws SecurityException将此对象的accessible标志设置为指示的布尔值</p><p>true的值表示反射对象应该在使用时抑制Java语言访问检查。 false的值表示反映的对象应该强制执行Java语言访问检查</p><h4 id="3、反射获得成员变量"><a href="#3、反射获得成员变量" class="headerlink" title="3、反射获得成员变量"></a>3、反射获得成员变量</h4><p>（1）获得字段<br>Field[] getDeclaredFields()<br>返回的数组 Field对象反映此表示的类或接口声明的所有字段 类对象</p><p>getField(String name)<br>返回一个 Field对象，它反映此表示的类或接口的指定公共成员字段 类对象</p><p>Field[] getFields()<br>返回包含一个数组 Field对象反射由此表示的类或接口的所有可访问的公共字段 类对象</p><h4 id="4、反射获得成员方法"><a href="#4、反射获得成员方法" class="headerlink" title="4、反射获得成员方法"></a>4、反射获得成员方法</h4><p>Declared–所有的<br>Methods–公共的<br>使用成员方法<br>Object invoke(Object obj, Object… args)<br>在具有指定参数的 方法对象上调用此 方法对象表示的底层方法</p><h4 id="5、泛型"><a href="#5、泛型" class="headerlink" title="5、泛型"></a>5、泛型</h4><p>安全检测机制<br>例如：</p><pre><code> ArrayList&lt;T&gt; arrylist=new ArrayList&lt;T&gt;();</code></pre><p>存在类型错误 类型无法转换成功</p><h4 id="6、泛型方法"><a href="#6、泛型方法" class="headerlink" title="6、泛型方法"></a>6、泛型方法</h4><p>如何写一个方法 实现对 整数 浮点数 字符的输出。–泛型<br>基本原则：</p><p>   a、所有泛型方法的声明都有一个类型参数声明的部分（<code>&lt;T&gt;</code>）–表示所有的类型参数<br>   b、泛型方法只能是引用数据类型，（int double）<br>例如：</p><pre><code>public static&lt;T&gt; void show(){}</code></pre><h4 id="7、泛型类"><a href="#7、泛型类" class="headerlink" title="7、泛型类"></a>7、泛型类</h4><p>泛型类 增加了类型参数声明部分<br>例如：</p><pre><code>class Test&lt;T&gt;{    private T t;    Test(T t){        this.t=t;    }}</code></pre><h4 id="8、泛型擦除"><a href="#8、泛型擦除" class="headerlink" title="8、泛型擦除"></a>8、泛型擦除</h4><p>java本身不存在泛型。增加了泛型机制。—java虚拟机中都是确定的类型 泛型擦除</p><h4 id="9、类型通配符"><a href="#9、类型通配符" class="headerlink" title="9、类型通配符"></a>9、类型通配符</h4><?>–代替具体的类型参数 例如：```  public void print(List<?><p> data){<br>     data.get(0);<br>  }</p><p>```<br>  <code>&lt;T&gt;</code>–指所有的数据类型</p><h4 id="10、反射与泛型"><a href="#10、反射与泛型" class="headerlink" title="10、反射与泛型"></a>10、反射与泛型</h4><p>泛型：允许程序员在编译时检测到非法的数据类型，运行期间Object，泛型擦除<br>通过反射可以添加不同的数据类型</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java反射 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java多线程</title>
      <link href="/2019/01/28/java-duo-xian-cheng/"/>
      <url>/2019/01/28/java-duo-xian-cheng/</url>
      
        <content type="html"><![CDATA[<h4 id="1、ObjectInputStream"><a href="#1、ObjectInputStream" class="headerlink" title="1、ObjectInputStream"></a>1、ObjectInputStream</h4><p>反序列化<br>（1）构造函数<br>ObjectInputStream(InputStream in) 创建从指定的InputStream读取的ObjectInputStream</p><p>（2）主要方法<br>Object readObject() 从ObjectInputStream读取一个对象</p><h4 id="2、POI-实现对word、Excel等文件操作"><a href="#2、POI-实现对word、Excel等文件操作" class="headerlink" title="2、POI 实现对word、Excel等文件操作"></a>2、POI 实现对word、Excel等文件操作</h4><p>Apache–Download 安装包，根据操作系统<br>例如windows .zip<br>导包–build path-&gt;configure build path-&gt;add external jars</p><h4 id="3、多线程简介"><a href="#3、多线程简介" class="headerlink" title="3、多线程简介"></a>3、多线程简介</h4><p>进程：系统资源分配的单位。（cpu 磁盘 内存 网络）<br>线程：独立调度和分配的基本单位，共享进程资源<br>一个进程 包含多个线程，用来完成不同的工作，称之为多线程<br>进程是为了提高系统资源的利用率和系统吞吐量<br>线程是为了减少程序在并发执行时付出的时空开销</p><h4 id="4、线程的使用"><a href="#4、线程的使用" class="headerlink" title="4、线程的使用"></a>4、线程的使用</h4><p>（1）继承 thread 类<br>public class Thread extends Object implements Runnable<br>a、Thread类构造函数<br>Thread()<br>分配一个新的 Thread对象</p><p>Thread(String name)<br>分配一个新的 Thread对象</p><p>主要方法：<br>void run()<br>主要是运行线程所负责的主要任务</p><p>void setName(String name)<br>将此线程的名称更改为等于参数 name</p><p>void setPriority(int newPriority)<br>更改此线程的优先级</p><p>start()<br>导致此线程开始执行; Java虚拟机调用此线程的run方法</p><p>void setDaemon(boolean on)<br>将此线程标记为 daemon线程或用户线程</p><p>static Thread currentThread() 返回对当前正在执行的线程对象的引用</p><p>b、声明方式：</p><pre><code>public class 线程类名 extends Thread{     //重写run方法     public void run(){     } }</code></pre><p>c、调用和开启线程<br>线程类名 初始化<br>线程类名.start();<br>start() 导致此线程开始执行; Java虚拟机调用此线程的run方法</p><p>（2）实现Runnable接口<br>Interface Runnable<br>void run()<br>当实现接口的对象 Runnable被用来创建一个线程<br>启动线程使对象的 run在独立执行的线程中调用的方法</p><pre><code>   public class 线程名 implements Runnable{       //实现 run方法       public void run(){         }   }</code></pre><p>a、建立一个类实现runnable的接口<br>b、使用参数为Runnable对象的Thread构造方法。–Thread(Runnable target) 分配一个新的 Thread对象<br>c、 调用 start方法 开启线程</p><h4 id="5、线程的优先级"><a href="#5、线程的优先级" class="headerlink" title="5、线程的优先级"></a>5、线程的优先级</h4><p>默认的线程优先级：5<br>线程优先级最高为：10<br>最低的优先级为：1<br>优先级指的是一种概率</p><h4 id="6、守护线程"><a href="#6、守护线程" class="headerlink" title="6、守护线程"></a>6、守护线程</h4><p>用户线程：User Thread<br>守护线程：Daemon Thread ：主要提供服务的，为其他线程。比如 gc 垃圾回收线程<br>守护线程主要是在用户线程都执行完的情况下执行，如果没有用户线程执行，守护线程自动退出</p><h4 id="7、窗口卖票小案例"><a href="#7、窗口卖票小案例" class="headerlink" title="7、窗口卖票小案例"></a>7、窗口卖票小案例</h4><p>卖票 是针对同一个票额 同一个票库，两个线程同时访问<br>抢占资源，同一张票 卖给多个人<br>等待<br>保证数据在任何时刻只有一个线程访问，保证数据的完整性</p><h4 id="8、线程的同步"><a href="#8、线程的同步" class="headerlink" title="8、线程的同步"></a>8、线程的同步</h4><p>锁机制<br>（1）同步代码块<br>static{} {}<br>synchronized(){<br>//代码<br>}—-同步代码块</p><p>（2）同步方法<br>synchronized 修饰的方法</p><p>（3）互斥锁<br>lock，保证数据的完整性 一山不容二虎<br>ReentrantLock<br>构造方法–ReentrantLock() 创建一个 ReentrantLock的实例<br>主要方法：<br>void lock() 获得锁<br>void unlock() 尝试释放此锁</p><h4 id="9、线程的wait和notify"><a href="#9、线程的wait和notify" class="headerlink" title="9、线程的wait和notify"></a>9、线程的wait和notify</h4><p>wait：线程等待，直到另一个线程调用该对象的notify()方法或notifyAll()方法<br>notify:唤醒正在等待对象监视器的单个线程<br>notifyall:唤醒正在等待对象监视器的所有线程<br>sleep:long—睡眠时间 Thread类自带的方法—-自然醒<br>wait：继承于object的方法—-被叫醒</p><h4 id="10、线程池概述"><a href="#10、线程池概述" class="headerlink" title="10、线程池概述"></a>10、线程池概述</h4><p>多个线程运行时运行机制，包括排队策略 包括线程存活时间，框架策略<br>管理 创建 释放</p><h4 id="11、线程池使用"><a href="#11、线程池使用" class="headerlink" title="11、线程池使用"></a>11、线程池使用</h4><p>ThreadpoolExecutor<br>（1）构造方法<br>ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue workQueue)<br>corePoolSize：核心池<br>maximumPoolSize：线程池最大线程数<br>keepAliveTime：线程没有任务执行时，最多多长时间会终止&gt;corePoolSize 起作用<br>TimeUnit：keepAliveTime的时间单位<br>workQueue：用于存放待执行的任务<br>ArrayBlockingQueue、LInkedBlockingQueue、SynchronousQueue等<br>10个工人 一个工人同时做一个任务；<br>10个人都在干活，还有任务来了，任务排队<br>活太多，最多招5个人，<br>15个人还是赶不过来。放弃任务<br>额外找来的5个人，3个月后辞去，还是10个人干活<br>corePoolSize：10<br>maximumPoolSize：15<br>keepAliveTime:3个月</p><p>（2）主要方法<br>void execute(Runnable command) 在将来某个时候执行给定的任务<br>submit 提交任务<br>shutdown：关闭线程池<br>shutdownnow：立即关闭</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java多线程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java的IO流</title>
      <link href="/2019/01/26/java-de-io-liu/"/>
      <url>/2019/01/26/java-de-io-liu/</url>
      
        <content type="html"><![CDATA[<h4 id="1、IO概述"><a href="#1、IO概述" class="headerlink" title="1、IO概述"></a>1、IO概述</h4><p>Input ：将磁盘或硬盘、键盘等数据读入到内存的<br>Output：从内存输出到 磁盘、硬盘等<br>主要是以内存为基准</p><h4 id="2、输入输出流分类"><a href="#2、输入输出流分类" class="headerlink" title="2、输入输出流分类"></a>2、输入输出流分类</h4><p>（1）字节流<br>1Byte =8 bit<br>1KB=1024B<br>1MB=1024KB<br>1GB=1024MB<br>1TB=1024GB<br>1PB=1024TB——-商业的存储空间 都是以 1000为单位<br>字节流可以处理所有数据类型的数据，在Java中 以 Stream结尾的</p><p>（2）字符流<br>一个字符=2Byte<br>字符流对于处理文本数据有优势，在Java中 以 Reader 和 Writer结尾</p><p>（3）IO包<br>Java.IO—HDFS 离线计算</p><h4 id="3、File-类"><a href="#3、File-类" class="headerlink" title="3、File 类"></a>3、File 类</h4><p>文件和目录的抽象表示<br>（1）构造方法<br>File(String pathname)<br>pathname:<br>绝对路径：D:\TZ\Java黄埔9期\jdk+api+1.8_google<br>相对路径：day23 课程笔记.txt 相对我们的当前路径来说<br>./test/a.txt<br>File file=new File(pathname);<br>可以写文件目录 也可以写 具体文件</p><p>（2）常用方法<br>boolean exists() 测试此抽象路径名表示的文件或目录是否存在<br>String getAbsolutePath() 返回此抽象路径名的绝对路径名字符串<br>boolean isDirectory() 测试此抽象路径名表示的文件是否为目录<br>boolean isFile() 测试此抽象路径名表示的文件是否为普通文件<br>File[] listFiles() 返回一个抽象路径名数组，表示由该抽象路径名表示的目录中的文件</p><h4 id="4、FileInputStream–输入字节流"><a href="#4、FileInputStream–输入字节流" class="headerlink" title="4、FileInputStream–输入字节流"></a>4、FileInputStream–输入字节流</h4><p>FileInputStream用于读取诸如图像数据的原始字节流<br>public class FileInputStreamextends InputStream<br>（1）构造函数<br>FileInputStream(File file) 通过打开与实际文件的连接创建一个 FileInputStream ，<br>该文件由文件系统中的 File对象 file命名<br>File file=new File(“”);<br>FileInputStream fis=new FileInputStream(file);</p><p>（2）主要方法<br>int read() 从该输入流读取一个字节的数据，，如果达到文件的末尾， -1<br>int read(byte[] b) 从该输入流读取最多 b.length个字节的数据为字节数组。 -1</p><h4 id="5、FileOutputStream–输出字节流"><a href="#5、FileOutputStream–输出字节流" class="headerlink" title="5、FileOutputStream–输出字节流"></a>5、FileOutputStream–输出字节流</h4><p>把内存中的内容 输出到文件中去<br>文件输出流是用于将数据写入到输出流File<br>public class FileOutputStream extends OutputStream</p><p>（1）构造函数<br>FileOutputStream(File file)<br>创建文件输出流以写入由指定的 File对象表示的文件。–默认 append 为 false 覆盖更新文件内容<br>FileOutputStream(File file, boolean append)<br>创建文件输出流以写入由指定的 File对象表示的文件。追加模式可以设置为true</p><p>（2）主要函数<br>void write(int b)<br>将指定的字节写入此文件输出流</p><p>void write(byte[] b)<br>将 b.length个字节从指定的字节数组写入此文件输出流</p><p>void write(byte[] b, int off, int len)<br>将 len字节从位于偏移量 off的指定字节数组写入此文件输出流</p><p>void flush() 刷新此输出流并强制任何缓冲的输出字节被写出</p><h4 id="6、FileReader–输入字符流"><a href="#6、FileReader–输入字符流" class="headerlink" title="6、FileReader–输入字符流"></a>6、FileReader–输入字符流</h4><p>（1）构造函数<br>FileReader(File file) 创建一个新的 FileReader ，给出 File读取</p><p>（2）主要函数<br>public int read() 每次读取一个字符 -1表示到文件结尾；<br>public int read(char[] cbuf, int offset, int length)</p><h4 id="7、FileWriter-输出字符流"><a href="#7、FileWriter-输出字符流" class="headerlink" title="7、FileWriter-输出字符流"></a>7、FileWriter-输出字符流</h4><p>（1）构造函数<br>FileWriter(File file) 给一个File对象构造一个FileWriter对象<br>FileWriter(File file, boolean append) 给一个File对象构造一个FileWriter对象</p><p>（2）主要函数<br>public void write(char)<br>public void write(char[] cbuf,int off,int len)–写入字符数组<br>public void write(String str,int off,int len)–写入字符串</p><h4 id="8、BufferdReader"><a href="#8、BufferdReader" class="headerlink" title="8、BufferdReader"></a>8、BufferdReader</h4><p>(1) InputStreamReader–字符输入流<br>字节流 转为字符流的桥梁<br>编码问题，可以指定编码。–utf-8 GBK<br>InputStreamReader(InputStream in)<br>创建一个使用默认字符集的InputStreamReader</p><p>InputStreamReader(InputStream in, String charsetName)<br>创建一个使用命名字符集的InputStreamReader。</p><p>int read() 读一个字符<br>int read(char[] cbuf, int offset, int length) 将字符读入数组的一部分</p><p>（2）可以把字符流的效率提高，提供缓冲<br>可以使用 FIleReader、InputStreamReader等作为参数<br>实现字节流到字符流的缓冲<br>a、构造函数<br>BufferedReader(Reader in) 创建使用默认大小的输入缓冲区的缓冲字符输入流<br>BufferedReader(Reader in, int sz) 创建使用指定大小的输入缓冲区的缓冲字符输入流</p><p>b、主要函数<br>int read() 读一个字符<br>int read(char[] cbuf, int off, int len) 将字符读入数组的一部分。<br>String readLine() 读一行文字 —特色</p><h4 id="9、BufferedWriter"><a href="#9、BufferedWriter" class="headerlink" title="9、BufferedWriter"></a>9、BufferedWriter</h4><p>（1）OutputStreamWriter–指定字符集<br>将字节流转换为字符流的桥梁<br>OutputStreamWriter(OutputStream out, String charsetName)<br>创建一个使用命名字符集的OutputStreamWriter</p><p>主要方法：<br>void write(char[] cbuf, int off, int len)<br>写入字符数组的一部分</p><p>void write(int c)<br>写一个字符</p><p>void write(String str, int off, int len)<br>写一个字符串的一部分</p><p>（2）BufferedWriter<br>a、构造函数<br>BufferedWriter(Writer out)<br>创建使用默认大小的输出缓冲区的缓冲字符输出流</p><p>BufferedWriter(Writer out, int sz)<br>创建一个新的缓冲字符输出流，使用给定大小的输出缓冲区</p><p>b、主要方法<br>newLine()<br>写一行行分隔符</p><p>void write(char[] cbuf, int off, int len)<br>写入字符数组的一部分</p><p>void write(int c)<br>写一个字符</p><p>void write(String s, int off, int len)<br>写一个字符串的一部分</p><h4 id="10、序列化与反序列化"><a href="#10、序列化与反序列化" class="headerlink" title="10、序列化与反序列化"></a>10、序列化与反序列化</h4><p>序列化 就是把对象转换为字节序列的过程<br>反序列化 就是把字节恢复为对象的过程</p><h4 id="11、ObjectOutputStream–序列化"><a href="#11、ObjectOutputStream–序列化" class="headerlink" title="11、ObjectOutputStream–序列化"></a>11、ObjectOutputStream–序列化</h4><p>（1）构造函数<br>ObjectOutputStream() 为完全重新实现ObjectOutputStream的子类提供一种方法，不必分配刚刚被ObjectOutputStream实现使用的私有数据</p><p>ObjectOutputStream(OutputStream out)<br>创建一个写入指定的OutputStream的ObjectOutputStream</p><p>（2）主要方法<br>void write(byte[] buf) 写入一个字节数组<br>void write(byte[] buf, int off, int len) 写入一个子字节数组<br>void write(int val) 写一个字节<br>void writeObject(Object obj)<br>将指定的对象写入ObjectOutputStream<br>把类进行序列化 需要首先实现 Serializable接口<br>加密传输数据的一种方式</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java的IO流 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java异常</title>
      <link href="/2019/01/24/java-yi-chang/"/>
      <url>/2019/01/24/java-yi-chang/</url>
      
        <content type="html"><![CDATA[<p>Java异常</p><h4 id="1、异常概述"><a href="#1、异常概述" class="headerlink" title="1、异常概述"></a>1、异常概述</h4><p>（1）异常分为：编译时异常 运行时异常<br>（2）编译时异常：Javac IDE（，‘’），一般是指的 语法错误，比较容易修正<br>（3）运行时的异常：运行错误和逻辑错误<br>1/0;<br>（4）不正常的事件<br>异常的类，创建对象<br>NullPointException：空指针异常<br>Student stu;stu—&gt;对象<br>（5）异常处理机制<br>抛出异常—110<br>catch 异常— 依靠自己</p><h4 id="2、异常的分类"><a href="#2、异常的分类" class="headerlink" title="2、异常的分类"></a>2、异常的分类</h4><p>（1）Throwable—异常类的鼻祖。Throwable类是Java语言中所有错误和异常的Throwable类<br>（2）Error：错误<br>（3）Exception：<br>CheckedException：try catch来显示的捕获<br>例如：<br>RuntimeException<br>ArithmeticException：算术异常 例如 除数为0<br>IndexOutOfBoundsException:数组越界<br>NullPointException：空指针异常<br>IOException ：IO异常<br>FileNotFoundException：文件异常<br>ClassNotFoundException：找不到指定类<br>SQLException：SQL执行语句</p><h4 id="3、异常-gt-方法抛出异常-throw-关键字"><a href="#3、异常-gt-方法抛出异常-throw-关键字" class="headerlink" title="3、异常-&gt;方法抛出异常 throw 关键字"></a>3、异常-&gt;方法抛出异常 throw 关键字</h4><p>（1） throw 抛出异常，手动引发异常<br>例如： throw new IOException();<br>（2） throws 抛出异常，会抛出多个异常并不是处理异常 推卸责任<br>谁调用 抛给谁。 多个异常之间可以通过 ，分割</p><h4 id="4、异常-gt-异常的处理方式-try…catch…finally"><a href="#4、异常-gt-异常的处理方式-try…catch…finally" class="headerlink" title="4、异常-&gt;异常的处理方式 try…catch…finally"></a>4、异常-&gt;异常的处理方式 try…catch…finally</h4><pre><code>try{  可能出现异常的代码；}catch(异常处理的类型1 变量){    处理异常的代码}catch(异常处理类型2 变量){}...</code></pre><p>（1） catch 可以有多个<br>（2）异常的捕获必须从小类的异常 到 大类型的异常<br>（3）最多执行1个 catch语句块<br>finally ：一定会执行的代码，一般用来做资源的释放<br>例如：数据库连接的关闭<br>try catch finally 也可以直接与 try 连用<br>try finally<br>try catch finally 不能都单独存在。 catch 与 finally 必须与try 连用</p><h4 id="5、自定义异常"><a href="#5、自定义异常" class="headerlink" title="5、自定义异常"></a>5、自定义异常</h4><p>写一个子类 继承 RuntimeException。主要应对 Exception类内置异常无法解决问题时</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java异常 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java集合</title>
      <link href="/2019/01/20/java-ji-he/"/>
      <url>/2019/01/20/java-ji-he/</url>
      
        <content type="html"><![CDATA[<h4 id="1、集合概念"><a href="#1、集合概念" class="headerlink" title="1、集合概念"></a>1、集合概念</h4><p>回忆数组–数组有固定的长度<br>int[] arry=new int[10];</p><p>针对数据长度可变的情况—》集合<br>Java集合 应对动态增长数据（在编译的时候无法知道具体的数据量）<br>集合类–&gt;可变容器类</p><h4 id="2、集合和数组的区别"><a href="#2、集合和数组的区别" class="headerlink" title="2、集合和数组的区别"></a>2、集合和数组的区别</h4><p>都是容器<br>（1）数组是固定长度，集合的长度是可变的<br>（2）数组放的数据都是基本类型数据（四类8种），但是集合放的数据都是引用数据类型<br>（String、自定义的对象、Integer–int、Long）<br>（3）集合中对于基本数据会转换为引用数据类型再存储</p><h4 id="3、集合包含内容"><a href="#3、集合包含内容" class="headerlink" title="3、集合包含内容"></a>3、集合包含内容</h4><p>（1）Collection–接口 Interface</p><pre><code>Interface Collection&lt;E&gt;---add 方法public abstract class AbstractCollection&lt;E&gt; extends Object implements Collection&lt;E&gt;public abstract class AbstractList&lt;E&gt; extends AbstractCollection&lt;E&gt; implements List&lt;E&gt;</code></pre><p>a、List（接口）集合—特定顺序的元素</p><pre><code>public interface List&lt;E&gt; extends Collection&lt;E&gt;</code></pre><p>add(int index, E element) —指定索引处增加元素的位置<br>iterator() 以正确的顺序返回该列表中的元素的迭代器</p><p>b、Set（接口）集合–不能够有重复的元素</p><p>（2）Map–类似于数据库<br>主要存储”键值对” key-value MapReduce</p><p>（3）Iterable 集合的访问迭代 返回此集合中的元素的迭代器<br>没有关于元素返回顺序的保证（除非这个集合是提供保证的某个类的实例）</p><h4 id="4、集合框架-gt-集合的继承关系图"><a href="#4、集合框架-gt-集合的继承关系图" class="headerlink" title="4、集合框架-&gt;集合的继承关系图"></a>4、集合框架-&gt;集合的继承关系图</h4><p>Collection接口 Map<br>Collection 、Map 、List 、Set 等都是 Interface<br>AbstractCollection、 Abstractlist等 抽象类 实现了 Interface的部分方法<br>ArrayList 、LinkedList等 具体实现类 实现了 所有方法</p><h4 id="5、List集合介绍"><a href="#5、List集合介绍" class="headerlink" title="5、List集合介绍"></a>5、List集合介绍</h4><p>List集合是一个有序（索引有序）、可重复的集合，集合中每个元素都有对应的顺序索引<br>List允许加入重复元素是因为可以通过索引来访问指定位置的元素<br>List集合默认按照元素的添加顺序增加元素的索引</p><h4 id="6、List集合-gt-ArrayList"><a href="#6、List集合-gt-ArrayList" class="headerlink" title="6、List集合-&gt;ArrayList"></a>6、List集合-&gt;ArrayList</h4><p>（1）ArrayList简介<br>ArrayList 是基于数组实现的List类。实现所有可选列表操作，并允许所有元素，包括null </p><p>（2）初始化 ArrayList</p><pre><code>ArrayList&lt;E&gt; arrayList=new ArrayList&lt;E&gt;（）；---初始数据类型为E，容量大小为10的List</code></pre><p>（3）主要方法<br>boolean add(E e) 将指定的元素追加到此列表的末尾<br>void add(int index, E element) 在此列表中的指定位置插入指定的元素<br>boolean addAll(Collection&lt;? extends E&gt; c) 按指定集合的Iterator返回的顺序将指定集合中的所有元素追加到此列表的末尾<br>boolean addAll(int index, Collection&lt;? extends E&gt; c) 将指定集合中的所有元素插入到此列表中，从指定的位置开始<br>boolean contains(Object o) 如果此列表包含指定的元素，则返回 true<br>E get(int index) 返回此列表中指定位置的元素<br>E remove(int index) 删除该列表中指定位置的元素<br>E set(int index, E element) 用指定的元素替换此列表中指定位置的元素<br>Object[] toArray() 以正确的顺序（从第一个到最后一个元素）返回一个包含此列表中所有元素的数组</p><p>（4）List集合遍历的四种方法<br>a、通过 List.size<br>b、通过Iterator<br>boolean hasNext() 如果迭代具有更多元素，则返回 true<br>E next() 返回迭代中的下一个元素</p><h4 id="7、List集合-gt-LinkedList"><a href="#7、List集合-gt-LinkedList" class="headerlink" title="7、List集合-&gt;LinkedList"></a>7、List集合-&gt;LinkedList</h4><p>LinkedList 指的是链表类数据结构<br>与ArrayList的不同<br>（1）链表中的元素可以任意的增加和删除，效率很高，但是 查询效率不如ArrayList（有索引）<br>a-&gt;b-&gt;c….<br>（2）将对象存放在独立的空间中，而且每个空间保存了下一个连接的索引<br>（3）初始化<br>LinkedList linkedlist=new LinkedList();<br>（4）主要的方法<br>void addFirst(E e) 在该列表开头插入指定的元素<br>void addLast(E e) 将指定的元素追加到此列表的末尾<br>E peekFirst() 检索但不删除此列表的第一个元素，如果此列表为空，则返回 null<br>peekLast() 检索但不删除此列表的最后一个元素，如果此列表为空，则返回 null<br>pop() 从此列表表示的堆栈中弹出一个元素</p><h4 id="8、Set接口的介绍"><a href="#8、Set接口的介绍" class="headerlink" title="8、Set接口的介绍"></a>8、Set接口的介绍</h4><p>set集合存放无序不可重复的元素<br>list集合 存放有序可重复的元素。—索引<br>set集合不按照特定方式进行排序，只是放元素放在集合<br>set主要是由 HashSet和TreeSet具体实现类实现</p><h4 id="9、Set集合-gt-HashSet"><a href="#9、Set集合-gt-HashSet" class="headerlink" title="9、Set集合-&gt;HashSet"></a>9、Set集合-&gt;HashSet</h4><p>Hash（哈希算法）—-哈希函数定义的好坏<br>HashCode—哈希值<br>（1）equals（）方法判断两个元素的HashCode值是否相同 </p><p>（2）如果Hashcode值相同，继续与集合的元素作比较，<br>如果还相同则视为同一个对象，不保存在HashSet中<br>如果对象不相同，理论上要存储（比价麻烦）–避免发生 </p><p>（3）如果HashCode值不相同，直接把元素存放在该元素的Hashcode位置<br>public class HashSet extends AbstractSet implements Set, Cloneable, Serializable </p><p>（4）构造函数<br>HashSet hashSet=new HashSet();<br>boolean add(E e) 将指定的元素添加到此集合（如果尚未存在）<br>boolean contains(Object o) 如果此集合包含指定的元素，则返回 true</p><h4 id="10、Set集合-gt-TreeSet"><a href="#10、Set集合-gt-TreeSet" class="headerlink" title="10、Set集合-&gt;TreeSet"></a>10、Set集合-&gt;TreeSet</h4><p>TreeSet 是一个有序集合，默认将元素按照升序排列，Comparable接口<br>equals方法 判断元素是否重复<br>比较器 比较一下大小顺序</p><h4 id="11、Map-集合"><a href="#11、Map-集合" class="headerlink" title="11、Map 集合"></a>11、Map 集合</h4><p>Set 与list 都属于 Collection<br>Map每个元素的值都包含两个对象：key-value 键值对<br>key不能够重复；唯一的key 可以对应多个value<br>map中不存在索引，有key<br>循环访问的方式</p><h4 id="12、Map集合-gt-HashMap"><a href="#12、Map集合-gt-HashMap" class="headerlink" title="12、Map集合-&gt;HashMap"></a>12、Map集合-&gt;HashMap</h4><p>Hash算法<br>public class HashMap&lt;K,V&gt;extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable<br>允许null的值和null键<br>（1）初始化<br>HashMap&lt;key,value&gt; hashMap=new HashMap&lt;key,value&gt; ();</p><p>（2）主要的方法<br>put(K key, V value) 将指定的值与此映射中的指定键相关联<br>get(Object key) 返回到指定键所映射的值，或 null如果此映射包含该键的映射<br>Set keySet() 返回此地图中包含的键的Set视图<br>boolean containsKey(Object key) 如果此映射包含指定键的映射，则返回 true<br>boolean containsValue(Object value) 如果此地图将一个或多个键映射到指定值，则返回 true</p><h4 id="13、Map集合-gt-HashTable"><a href="#13、Map集合-gt-HashTable" class="headerlink" title="13、Map集合-&gt;HashTable"></a>13、Map集合-&gt;HashTable</h4><p>不接受 Null<br>为了成功的在hashtable中存储和获取对象，用作键的对象必须实现 hashcode和equals方法</p><h4 id="14、总结"><a href="#14、总结" class="headerlink" title="14、总结"></a>14、总结</h4><p>集合动态可扩展<br>Set代表无序集合不重复，（TreeSet 有序）<br>List集合有序可重复<br>Map 集合存储键值对- key value<br>自定义对象 要重写 方法（HashCode Comparator equals等）</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java集合 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java常用API</title>
      <link href="/2019/01/18/java-chang-yong-api/"/>
      <url>/2019/01/18/java-chang-yong-api/</url>
      
        <content type="html"><![CDATA[<h4 id="1、Java-API概述"><a href="#1、Java-API概述" class="headerlink" title="1、Java API概述"></a>1、Java API概述</h4><p>Java写好的包 类 方法的使用—API<br>Application Programing Interface：应用程序编程接口。Java提供的一些预定义的函数目的：基于API实现程序的快速编写。只需了解实现的作用，无需关注源代码</p><p>针对一个API首先看 概述了解 类的作用，然后看 构造函数了解如何创建类之后看方法，了解如何调用<br>Java lang–核心包 提供对Java编程语言设计至关重要的类，可以直接使用，不用import</p><h4 id="2、数值运算-Math类"><a href="#2、数值运算-Math类" class="headerlink" title="2、数值运算 Math类"></a>2、数值运算 Math类</h4><p>Math类为Java提供的支持数值运算的类<br>Math类包含执行基本数字运算的方法，如基本指数，对数，平方根和三角函数</p><p>public final class Math—-完美类<br>（1）Math类提供的基本方法：<br>static double abs(double a) 返回值为 double绝对值<br>static double acos(double a) 返回值的反余弦值; 返回的角度在0.0到pi的范围内<br>static double atan(double a)<br>向上取整：<br>static double ceil(double a) 返回大于或等于参数的最小（最接近负无穷大） double值，等于一个数学整数<br>向下取整：<br>static double floor(double a) 返回小于或等于参数的最大（最接近正无穷大） double值，等于一个数学整数<br>四舍五入：<br>static long round(double a) 返回参数中最接近的 long ，其中 long四舍五入为</p><p>static double log(double a) 返回的自然对数（以 e为底） double值<br>static double log10(double a) 返回一个 double的基数10对数值<br>static int max(int a, int b) 返回两个 int值中的较大值<br>static double random() 返回值为 double值为正号，大于等于 0.0 ，小于 1.0<br>public static double sqrt(double a)</p><h4 id="3、字符串运算-String类"><a href="#3、字符串运算-String类" class="headerlink" title="3、字符串运算 String类"></a>3、字符串运算 String类</h4><p>特殊的引用数据类型<br>public final class String—完美类<br>String a;<br>int a;</p><p>类名 对象名=new 类名();<br>String str = “abc”;<br>相当于：<br>char data[] = {‘a’, ‘b’, ‘c’};<br>String str = new String(data);—-不常见</p><p>char charAt(int index) 返回 char指定索引处的值</p><p>boolean contains(CharSequence s) 当且仅当此字符串包含指定的char值序列时才返回true</p><p>boolean equals(Object anObject) 将此字符串与指定对象进行比较</p><p>indexOf(String str) 返回指定子字符串第一次出现的字符串内的索引</p><p>length() 返回此字符串的长度。—循环的中止条件</p><p>boolean matches(String regex) 告诉这个字符串是否匹配给定的 regular expression</p><p>String replace(char oldChar, char newChar) 返回从替换所有出现的导致一个字符串 oldChar在此字符串 newChar</p><p>String[] split(String regex) 将此字符串分割为给定的 regular expression的匹配</p><p>String substring(int beginIndex) 返回一个字符串，该字符串是此字符串的子字符串</p><p>String toLowerCase() 将所有在此字符 String使用默认语言环境的规则，以小写</p><p>String toUpperCase() 将所有在此字符 String使用默认语言环境的规则大写</p><p>String trim() 返回一个字符串，其值为此字符串，并删除任何前导和尾随空格</p><p>/类型转换 将基本数据类型转换为 字符串/<br>static String valueOf(boolean b)<br>返回 boolean参数的字符串 boolean形式</p><p>static String valueOf(char c)<br>返回 char参数的字符串 char形式</p><p>static String valueOf(char[] data)<br>返回 char数组参数的字符串 char形式</p><p>static String valueOf(char[] data, int offset, int count)<br>返回 char数组参数的特定子阵列的字符串 char形式</p><p>static String valueOf(double d)<br>返回 double参数的字符串 double形式</p><p>static String valueOf(float f)<br>返回 float参数的字符串 float形式</p><p>static String valueOf(int i)<br>返回 int参数的字符串 int形式</p><p>static String valueOf(long l)<br>返回 long参数的字符串 long形式</p><p>static String valueOf(Object obj)<br>返回 Object参数的字符串 Object形式</p><p>String == 与equals的区别</p><p>如果声明String 是通过 String str=”” ，可以用 ==和equals<br>声明String 通过 new String(“”),不可以用 ==（调用 Object的equals方法） 只能用 equals</p><h4 id="4、字符串运算-大写字母-小写字母-数字出现的次数"><a href="#4、字符串运算-大写字母-小写字母-数字出现的次数" class="headerlink" title="4、字符串运算-大写字母 小写字母 数字出现的次数"></a>4、字符串运算-大写字母 小写字母 数字出现的次数</h4><p>getCount(string s) AscII码对比 length 遍历</p><h4 id="5、字符串运算-查找父字符串中某一个子字符串出现的次数"><a href="#5、字符串运算-查找父字符串中某一个子字符串出现的次数" class="headerlink" title="5、字符串运算-查找父字符串中某一个子字符串出现的次数"></a>5、字符串运算-查找父字符串中某一个子字符串出现的次数</h4><p>indexof 循环遍历子字符串出现的字数 就需要 截取 substring 把已找到的部分截取遍历后面的<br>边界条件</p><h4 id="6、字符串运算-split方法"><a href="#6、字符串运算-split方法" class="headerlink" title="6、字符串运算-split方法"></a>6、字符串运算-split方法</h4><p>public String[] split(String regex)<br>该方法的工作原理是通过使用给定表达式和限制参数为零调用双参数split方法<br>因此，尾随的空字符串不会包含在结果数组中<br>例如：String s=“1#2#3#4#5#”<br>String[] res=s.split(“#”);<br>res=[“1”,”2”,”3”,”4”,”5”]</p><p>返回的子串的次数 应该是 数组的长度-1</p><p>前后台交互或者进行数据接口会用到</p><h4 id="7、字符串运算-规则匹配"><a href="#7、字符串运算-规则匹配" class="headerlink" title="7、字符串运算-规则匹配"></a>7、字符串运算-规则匹配</h4><p>正则表达式 身份证号 电话号码 邮箱 QQ号等等<br>字符类：</p><table><thead><tr><th align="center">表示</th><th align="center">规则解释</th></tr></thead><tbody><tr><td align="center">[abc]</td><td align="center">a或b或c 都可以</td></tr><tr><td align="center">[a-zA-Z]</td><td align="center">a-z或者A到Z 两头的字母包含在内，所有字母都可以</td></tr><tr><td align="center">[0-9]</td><td align="center">0-9的数字都可以</td></tr><tr><td align="center">\d</td><td align="center">0-9的数字都可以</td></tr><tr><td align="center">\D</td><td align="center">[^0-9] 不是数字</td></tr><tr><td align="center">\w</td><td align="center">表示字母数字 下划线在内的任何字符 [a-zA-Z0-9_]</td></tr><tr><td align="center">X?</td><td align="center">X出现一次或一次也没有</td></tr><tr><td align="center">X*</td><td align="center">X零次或多次</td></tr><tr><td align="center">X+</td><td align="center">X至少出现一次</td></tr><tr><td align="center">X{n}</td><td align="center">恰好只有n次</td></tr><tr><td align="center">X{n,m}</td><td align="center">n=</td></tr></tbody></table><p>规则表达式：<br>^: 表示正则表达式的开头<br>$ : 表示正则表达式的结尾</p><p>例如：验证一个QQ号码<br>要求：<br>（1）QQ号码 5-15位<br>（2）0不能开头<br>规则表达式(字符串)：<br>regex=”[1-9]\d{4,14}”</p><p>public boolean matches(String regex)<br>告诉这个字符串是否匹配给定的regular expression<br>这种形式为str .matches( regex )方法的)产生与表达式完全相同的结果</p><p>Pattern. matches(regex, str)<br>参数<br>regex - 要匹配此字符串的正则表达式<br>结果<br>true如果，并且只有这个字符串与给定的正则表达式匹配<br>异常<br>PatternSyntaxException - 如果正则表达式的语法无效<br>//两个反斜杠 是转义的意思 (.\w{2,3})+ 表示 点出现一次或多次<br>// +号所跟的内容 如果在括号里面 表示 \w{2,3} 可以出现 多次</p><h4 id="8、Date"><a href="#8、Date" class="headerlink" title="8、Date"></a>8、Date</h4><p>（1）概述<br>包含集合框架，旧集合类，事件模型，日期和时间设施<br>国际化和其他实用程序类（字符串tokenizer，随机数生成器和位数组）<br>Java.util.* 工具包<br>在JDK 1.1之前， Date有两个附加功能，它允许将日期解释为年，月，日，小时，分钟和第二个值。 它还允许格式化和解析日期字符串。 不幸的是，这些功能的API不适合国际化。 从JDK 1.1开始， Calendar类应该用于在日期和时间字段之间进行转换，<br>并且DateFormat类应用于格式化和解析日期字符串。 在相应的方法Date被弃用<br>允许JDBC将其标识为 SQlDate值<br>格林尼治标准时间（GMT）定义的，相当于世界时间（UT）<br>（2）构造方法<br>Date()<br>分配一个 Date对象，并初始化它，以便它代表它被分配的时间，测量到最近的毫秒</p><p>Date(long date)<br>分配一个 Date对象，并将其初始化为表示自称为“时代”的标准基准时间以后的指定毫秒数，即1970年1月1日00:00:00 GMT</p><p>（3）常用方法<br>boolean after(Date when)<br>测试此日期是否在指定日期之后</p><p>boolean before(Date when)<br>测试此日期是否在指定日期之前</p><p>Object clone()<br>返回此对象的副本</p><p>int compareTo(Date anotherDate)<br>比较两个日期进行订购</p><p>boolean equals(Object obj)<br>比较两个日期来平等</p><p>static Date from(Instant instant)<br>从 Instant对象获取一个 Date的实例</p><p>getTime()<br>返回自1970年1月1日以来，由此 Date对象表示的00:00:00 GMT的毫秒数</p><h4 id="9、Calendar"><a href="#9、Calendar" class="headerlink" title="9、Calendar"></a>9、Calendar</h4><p>（1）简介<br>相对比较新的日期类，抽象类</p><p>  public abstract class Calendar extends Object<br>    implements Serializable, Cloneable, Comparable<Calendar><br>所述Calendar类是一个抽象类<br>可以为在某一特定时刻和一组之间的转换的方法calendar fields如<br>YEAR ， MONTH ， DAY_OF_MONTH ， HOUR 等等，以及用于操纵该日历字段</p><p>（2）初始化<br>Calendar提供了一种类方法getInstance 用于获取此类型的一般有用的对象<br>Calendar的getInstance方法返回一个Calendar对象，其日历字段已使用当前日期和时间进行初始化<br>Date date=new Date(); —- 对象的初始化<br>Calendar rightNow = Calendar.getInstance(); —抽象类自带方法 获得对象</p><p>（3）常用的方法<br>boolean after(Object when)<br>返回 Calendar是否 Calendar指定时间之后的时间 Object</p><p>boolean before(Object when)<br>返回此 Calendar是否 Calendar指定的时间之前指定的时间 Object</p><p>int getWeekYear()<br>返回这个 Calendar</p><p>set(int year, int month, int date)<br>—Date的时候 需要计算一下 距离 1970 ms数</p><p>public abstract void add(int field,int amount)<br>根据日历的规则，将指定的时间量添加或减去给定的日历字段</p><p>例如，要从当前日历的时间减去5天，可以通过调用以下方法来实现<br>add(Calendar.DAY_OF_MONTH, -5)<br>public abstract void roll(int field,boolean up)<br>在给定时间字段上添加或减少单个时间单位，而不改变较大的字段</p><p>字段可以直接访问 static final ，使用 get set 方法 获得或设置字段值</p><p>public static final int YEAR—直接通过类名可以访问到年<br>public static final int MONTH<br>public static final int WEEK_OF_YEAR<br>public static final int WEEK_OF_MONTH<br>public static final int DAY_OF_YEAR</p><h4 id="10、DateFormat–xxxx年xx月xx日"><a href="#10、DateFormat–xxxx年xx月xx日" class="headerlink" title="10、DateFormat–xxxx年xx月xx日"></a>10、DateFormat–xxxx年xx月xx日</h4><p>格式化日期<br>（1）简介<br>public abstract class DateFormat extends Format<br>DateFormat可帮助格式化和解析任何区域设置的日期</p><p>（2）初始化<br>public static final DateFormat getDateInstance()<br>// 抽象类<br>DateFormat df=DateFormat.getDateInstance;</p><p>（3）主要方法<br>String format(Date date) 将日期格式化成日期/时间字符串<br>Date parse(String source) 从给定字符串的开始解析文本以生成日期</p><h4 id="11、simpleDateFormat–DateFormat子类"><a href="#11、simpleDateFormat–DateFormat子类" class="headerlink" title="11、simpleDateFormat–DateFormat子类"></a>11、simpleDateFormat–DateFormat子类</h4><p>（1）实现了 DateFormat 不是抽象类–优秀的实现类<br>public class SimpleDateFormat extends DateFormat</p><p>例如：转换成 2018/12/5 2018年12月5日<br>SimpleDateFormat允许从选择日期时间格式化的任何用户定义的模式开始。<br>yyyy年MM月dd日</p><p>（2）构造方法<br>SimpleDateFormat() 构造一个 SimpleDateFormat使用默认模式和日期格式符号为默认的 FORMAT区域设置<br>SimpleDateFormat(String pattern) 使用给定模式 SimpleDateFormat并使用默认的 FORMAT语言环境的默认日期格式符号<br>（不需要再使用后面的 applyPattern方法 可以直接赋值）</p><p>（3）主要方法<br>applyPattern(String pattern) 给定的模式字符串应用于此日期格式<br>String format(Date date) 将日期格式化成日期/时间字符串。—进行了重写<br>Date parse(String source) 从给定字符串的开始解析文本以生成日期。–进行了重写</p><p>小案例：我活了多久</p><h4 id="12、StringBuffer"><a href="#12、StringBuffer" class="headerlink" title="12、StringBuffer"></a>12、StringBuffer</h4><p>（1）简介<br>和String一样 final –完美类 可以任意调节数据字符串的长度和内容<br>public final class StringBuffer<br>extends Object<br>implements Serializable, CharSequence</p><p>字符串缓冲区就像一个String ，但可以修改。 在任何时间点，它包含一些特定的字符序列。但可以通过某些方法调用来更改序列的长度和内容。 —-可以变化<br>例如： string s =”abc“;<br>s+=”1”;–abc1<br>Stringbuffer sbuffer =new StringBuffer();</p><p>（2）构造函数<br>StringBuffer() 构造一个没有字符的字符串缓冲区，初始容量为16个字符</p><p>（3）主要方法<br>append(String str) 将指定的字符串附加到此字符序列<br>—相当于 String+insert(int offset, String str) 将字符串插入到此字符序列中<br>toString() 将字符串转为string型</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java常用API </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java面向对象3</title>
      <link href="/2019/01/16/java-mian-xiang-dui-xiang-3/"/>
      <url>/2019/01/16/java-mian-xiang-dui-xiang-3/</url>
      
        <content type="html"><![CDATA[<h4 id="1、访问控制权限（public，private，protected，default）"><a href="#1、访问控制权限（public，private，protected，default）" class="headerlink" title="1、访问控制权限（public，private，protected，default）"></a>1、访问控制权限（public，private，protected，default）</h4><p>public&gt;protected&gt;default&gt;private<br>Java中用来控制类及类的方法和变量访问权限<br>（1）public ：公共的 表示包（package）内及包外的任何类（包括子类和普通类）都可以访问。—最开放<br>（2）protected：受保护的 表示包内的任何类及包外继承了该类的子类才能访问，突出继承<br>（3）default：默认的 表示包内的任何类都可以访问，但是包外的任何类都不能访问<br>（4）private：私有的 只有本类可以访问，包内外的任何类均不能访问。—封装</p><table><thead><tr><th align="center">访问控制修饰符</th><th align="center">同类</th><th align="center">同包</th><th align="center">子类</th><th align="center">不同的包</th></tr></thead><tbody><tr><td align="center">public</td><td align="center">1</td><td align="center">1</td><td align="center">1</td><td align="center">1</td></tr><tr><td align="center">protected</td><td align="center">1</td><td align="center">1</td><td align="center">1</td><td align="center">0</td></tr><tr><td align="center">default</td><td align="center">1</td><td align="center">1</td><td align="center">0</td><td align="center">0</td></tr><tr><td align="center">private</td><td align="center">1</td><td align="center">0</td><td align="center">0</td><td align="center">0</td></tr></tbody></table><h4 id="2、多态定义"><a href="#2、多态定义" class="headerlink" title="2、多态定义"></a>2、多态定义</h4><p>多态分为编译时的多态和运行时多态。其中编译时多态 也可称为静态多态<br>运行时的多态为动态多态，主要通过动态绑定来实现，常说默认的多态<br>多态 为了应对不同的变现形式</p><h4 id="3、静态多态"><a href="#3、静态多态" class="headerlink" title="3、静态多态"></a>3、静态多态</h4><p>其实就是 方法的重载，主要根据参数列表的不同来区分不同的函数<br>静态多态 不需要继承</p><h4 id="4、动态多态"><a href="#4、动态多态" class="headerlink" title="4、动态多态"></a>4、动态多态</h4><p>例如：品酒大师<br>三个杯子 倒了 3杯酒<br>酒 a= 五粮液；<br>酒 b= 茅台酒；<br>酒 c= 二锅头。<br>声明一个 酒的类，三种不同的酒 相当于不同的子类<br>只有在运行时 才能知道 喝的什么酒<br>所谓动态多态就是指 引用在不同的情况下所表现的实际对象<br>（1）继承（实现接口）。在多态中必须存在 父类与子类的关系<br>（2）重写。子类必须对父类的某些方法进行重新定义，在调用这些方法时 就会调用子类的方法<br>（3）向上转型：父类引用指向子类的对象</p><h4 id="5、向上转型"><a href="#5、向上转型" class="headerlink" title="5、向上转型"></a>5、向上转型</h4><p>向上转型：子类的对象转换为父类类型<br>例如：<br>Wine wine=new WLY();—向上转型<br>Wine wine=new Wine();–正常实例化对象<br>子类的单独定义的方法会丢失，能访问子类重写父类的方法</p><h4 id="6、动态多态小案例–动物喂食"><a href="#6、动态多态小案例–动物喂食" class="headerlink" title="6、动态多态小案例–动物喂食"></a>6、动态多态小案例–动物喂食</h4><p>养了一堆宠物 有狗 有猫。宠物喜欢吃什么 也要根据宠物的类型 来选择喂食<br>狗–骨头<br>猫–鱼 </p><pre><code>if(animal is dog ){     food=bone；     eat food; }else if(animal is cat){     food= fish;     eat fish. } </code></pre><p>可否写一个方法 来实现所有宠物的喂食</p><h4 id="7、向下转型"><a href="#7、向下转型" class="headerlink" title="7、向下转型"></a>7、向下转型</h4><p>向下转型是把父类对象转换为子类对象<br>Animal animal=new Animal();<br>Cat cat = （Cat）animal —-不对的<br>把一个动物强制转换为 猫，如果这个动物是只狗，狗是变不成猫的<br>向下转型必须得有向上转型作为前提。因为只有子类相对应的才可以转换<br>代表这个动物是 猫，之后 把动物再变回为猫。—打回原形</p><h4 id="8、-内部类定义"><a href="#8、-内部类定义" class="headerlink" title="8、.内部类定义"></a>8、.内部类定义</h4><p>在Java当中的一个类中在声明一个类 就叫 内部类<br>例如：</p><pre><code> class Outter{    成员变量；    class Inner{    }    成员方法； } </code></pre><h4 id="9、内部类分类"><a href="#9、内部类分类" class="headerlink" title="9、内部类分类"></a>9、内部类分类</h4><p>（1）（普通）成员内部类：与成员level一样，内部类中不能存在 static 关键字，不能够声明静态的方法、属性、静态代码块；<br>最普通的内部类<br>（2）静态（成员）内部类：使用static修饰的成员内部类<br>（3）（普通）局部内部类：局部范围内有效的内部类（例如：方法里面）<br>（4）匿名（局部）内部类：没有名字的局部内部类 </p><p><strong>成员内部类定义</strong><br>（1）定义：与我们的成员变量一样，可以声明类名，在成员内部类中可以声明属性和方法<br>（2）作用：<br>a、成员内部类可以无限制访问外部类的变量和方法（包括private修饰的）<br>b、内部类可以有多个<br>c、成员内部类与外部类如果存在同名的成员变量或方法，优先是内部的。如果访问外部类的<br>需要 Outter.this.(变量或方法名)</p><h4 id="10、成员内部类与外部类的访问"><a href="#10、成员内部类与外部类的访问" class="headerlink" title="10、成员内部类与外部类的访问"></a>10、成员内部类与外部类的访问</h4><p>（1）成员内部类访问外部类 无限制<br>（2）外部类访问内部类的成员，不是无限制的<br>首先要传建一个内部类的对象，然后通过对象来访问</p><h4 id="11、成员内部类的初始化"><a href="#11、成员内部类的初始化" class="headerlink" title="11、成员内部类的初始化"></a>11、成员内部类的初始化</h4><p>不是在类里面操作，如果是其他类要访问时，要访问内部类，首先实现外部类的实例化之后再实例化内部类<br>（1）在外部类对象初始化基础之上初始化内部类，调用内部类的构造函数<br>Outter.Inner inner=outter.new Inner();<br>（2）通过外部类的成员方法获得成员内部类的对象，然后访问其变量和方法</p><h4 id="12、静态内部类"><a href="#12、静态内部类" class="headerlink" title="12、静态内部类"></a>12、静态内部类</h4><p>使用 static修饰的成员内部类叫做静态内部类<br>定义格式如下：</p><pre><code>class Outter{    static  class inner{    }}</code></pre><p>外部类不是静态也可以声明静态内部类<br>静态内部类 要类比 静态成员变量<br>静态内部类可以通过外类直接调用 new Outter.Inner();<br>静态内部类内部可以直接访问外部类中所有的静态变量和方法（包含private）</p><h4 id="13、局部内部类"><a href="#13、局部内部类" class="headerlink" title="13、局部内部类"></a>13、局部内部类</h4><p>定义在代码块、方法体等的类叫局部内部类<br>—局部变量 类比<br>不能够有 public protected private 以及 static 修饰</p><pre><code>class Outter{    public void func(){         class inner{         }     }}</code></pre><p>局部内部类只是在一个方法或区域里起作用</p><h4 id="14、匿名内部类"><a href="#14、匿名内部类" class="headerlink" title="14、匿名内部类"></a>14、匿名内部类</h4><p>没有名字的局部内部类<br>必须要继承一个父类或者实现一个接口<br>定义形式：<br>正常初始化对象：</p><pre><code>类名 对象名=new 类名（）；匿名内部类：new 父类构造方法（）{     //重写一个函数     修饰符 返回参数类型 方法名（参数列表）{      } }；</code></pre><p>局部内部类的区别 局部的位置不同<br>匿名内部类当中不能够有静态属性和静态方法<br>匿名内部类 不需要新建一个类 而是通过匿名的形式吧 实现方法的重写<br>匿名内部类尤其针对 Android开发 例如 监听 鼠标事件 键盘 触屏输入</p><pre><code> Lisenter（）{     @override     MouseMoniter（）{     } }；</code></pre><h4 id="15、总结内部类"><a href="#15、总结内部类" class="headerlink" title="15、总结内部类"></a>15、总结内部类</h4><p>（1）成员内部类<br>（2）静态内部类<br>（3）局部内部类<br>（4）匿名内部类<br>a、每个内部类都可以独立的继承或实现一个接口，而外部类也可以继承一个直接父类。 —多继承的一种表现<br>b、通过内部类可以实现对外界的隐藏。–封装<br>c、内部类可以无限制的使用外部类的成员变量（包括私有），不用生成外部类的对象<br>d、匿名内部类可以简化代码的编写，方便编写事件驱动的程序、线程程序<br>e、成员内部类 静态内部类 可以对比 成员变量和静态变量<br>局部内部类 匿名内部类 可以对比局部变量</p><h4 id="16、面向对象总结"><a href="#16、面向对象总结" class="headerlink" title="16、面向对象总结"></a>16、面向对象总结</h4><p>封装 继承 多态<br>面向对象的思路去设计程序</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java面向对象 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java面向对象2</title>
      <link href="/2019/01/14/java-mian-xiang-dui-xiang-2/"/>
      <url>/2019/01/14/java-mian-xiang-dui-xiang-2/</url>
      
        <content type="html"><![CDATA[<h4 id="1、static关键字"><a href="#1、static关键字" class="headerlink" title="1、static关键字"></a>1、static关键字</h4><p>（1）主要用来修饰类的成员（成员变量、方法）<br>例如：main函数 static 修饰<br>（2）static 特点<br>a、static 修饰的成员在类加载的时候直接运行，优先级要高<br>b、通过类直接访问 类名.成员<br>c、static是针对所有对象的属性值相同时才使用 static 修饰<br>d、被static修饰的方法 无法是有非静态变量；非静态方法 不受限制</p><h4 id="2、静态构造代码块"><a href="#2、静态构造代码块" class="headerlink" title="2、静态构造代码块"></a>2、静态构造代码块</h4><p>形如： </p><pre><code>class 类名{     static{         变量;     } } </code></pre><p>主要是为了 方便变量的统一初始化 执行且只执行一次</p><h4 id="3、构造代码块"><a href="#3、构造代码块" class="headerlink" title="3、构造代码块"></a>3、构造代码块</h4><p>直接在类中定义没有被 static修饰的代码块<br>形如： </p><pre><code>class 类名{     {         变量;     }–构造代码块     func（）{         {        }–普通代码块    } } </code></pre><p>构造代码块可以执行多次，在创建对象的时候使用<br>优先级：先是 静态构造代码块&gt;构造代码块&gt;构造函数</p><h4 id="4、继承的介绍与使用"><a href="#4、继承的介绍与使用" class="headerlink" title="4、继承的介绍与使用"></a>4、继承的介绍与使用</h4><p>（1） extends 多个类中存在相同属性和行为时，将这些内容抽象到单独的一个类中，那么多个类<br>无序再定义这些属性和行为，只需要继承即可<br>父类：又叫基类，超类<br>子类：派生类<br>（2）子类可以访问父类中的非私有的属性和行为<br>（3）子类不能够继承父类的构造方法<br>（4）父类可以被多个子类继承，但是子类只有一个直接父类<br>（5）继承多以存在多级</p><h4 id="5、方法重写"><a href="#5、方法重写" class="headerlink" title="5、方法重写"></a>5、方法重写</h4><p>重载：在同一类中 方法名一样 参数列表不同<br>重写：在继承中出现的，是子类与父类具有相同的方法，子类的这一个方法 叫做重写<br>方法名、返回值、参数列表相同（不同的是函数体） 覆盖</p><h4 id="6、super关键字"><a href="#6、super关键字" class="headerlink" title="6、super关键字"></a>6、super关键字</h4><p>super 作用<br>（1）在子类的构造方法中直接通过super关键字 调用父类的构造方法<br>如果父类有多个构造函数 根据 参数列表来区分 必须放在第一行<br>（2）如果父类与子类中有同名成员变量，此时要访问父类成员变量可以通过super<br>（3）如果子类重写了父类的方法 ，可以通过 super调用父类的方法<br>this–当前对象 子类的方法、属性<br>super–父类对象 父类的方法、属性<br>（4）子类而言 是不是继承了我们父类的所有，自然我们继承了 父类的父类的成员变量和方法<br>所以可以直接通过super调用<br>super.super 多余了<br>（5）破坏了 Java的封装性 只有一个直接父类</p><h4 id="7、final关键字的使用"><a href="#7、final关键字的使用" class="headerlink" title="7、final关键字的使用"></a>7、final关键字的使用</h4><p>final关键字 是一个修饰符，用来修饰 类 方法 变量<br>（1）final修饰一个类，则不能够被继承<br>final类 不想被重新进行重写方法、扩展属性—-直接用 不想被人改变 完美<br>例如：String<br>（2）final 修饰方法，则方法不能够被重写<br>（3）final修饰变量，如果这个值一旦被指定 则 无法改变</p><h4 id="8、static-与-final-关键字"><a href="#8、static-与-final-关键字" class="headerlink" title="8、static 与 final 关键字"></a>8、static 与 final 关键字</h4><p>static：静态变量 只保留一个副本<br>final：用来表示变量不可变<br>被static 修饰以后 只有一个值<br>final 有多个值 因为每次都会赋予一个值 只是保证赋予的这个值不变</p><h4 id="9、Object-类"><a href="#9、Object-类" class="headerlink" title="9、Object 类"></a>9、Object 类</h4><p>顶级父类，是任何类的父类，可以显式的继承 也可以隐式的继承<br>需要重写的方法<br>toString 方法：需要重写 来满足业务需求<br>equals 方法。比较的是地址<br>应用比较广泛</p><h4 id="10、抽象方法"><a href="#10、抽象方法" class="headerlink" title="10、抽象方法"></a>10、抽象方法</h4><p>（1）抽象方法是一种特殊的方法，只有声明没有方法体<br>（2）声明的格式为：<br>abstract 返回值类型 func(参数列表)–抽象方法<br>(public static void main(){方法体})<br>（3）抽象方法存在的意义在于 父类不想或者无法提供方法的方法体（具体实现）<br>只知道有这个方法（针对不同的类 实现方法 不一样）</p><h4 id="11、抽象类"><a href="#11、抽象类" class="headerlink" title="11、抽象类"></a>11、抽象类</h4><p>（1）如果一个类中含有抽象方法，则该类必须被定义为抽象类<br>反过来 抽象类中不一定含有抽象方法<br>（2）声明的格式：<br>abstract class 类名{}—抽象类<br>（3）抽象类特点：<br>a、抽象方法与抽象类均不可以被 final修饰<br>b、如果一个类继承抽象类，则必须完全实现其抽象方法，否则声明为抽象类<br>c、抽象方法必须为 public 或 protected 修饰，不能够用 private 或 static</p><h4 id="12、接口-interface"><a href="#12、接口-interface" class="headerlink" title="12、接口(interface)"></a>12、接口(interface)</h4><p>//数据接口–数据接口协议<br>（1）抽象类的延伸–在一个类中如果所有的方法都是抽象的，则可定义成接口<br>接口比抽象类 更加纯粹。全抽象的<br>（2）接口实现 implements —-对比 继承 extends<br>（3）Java中 子类只能够有一个直接父类，要多继承，必须使用接口<br>接口可以实现多次<br>例如：<br>class A implements I1,I2{</p><p>}<br>//可以组合写 接口可以实现无限个<br>class A extends B implements I1,I2{</p><p>}<br>（4）接口的声明：<br>interface 接口名{}<br>（5）接口不可以实例化，只能够用于实现。<br>（6）接口当中可以含有成员变量和方法，方法都是抽象的<br>变量–public static final<br>一般情况下不要在接口中定义变量<br>（7） 接口中可以定义默认的方法 default 和静态方法</p><h4 id="13、接口与抽象类的区别"><a href="#13、接口与抽象类的区别" class="headerlink" title="13、接口与抽象类的区别"></a>13、接口与抽象类的区别</h4><p>（1）抽象类可以实现接口，接口可以继承接口<br>（2）接口中定义的方法都是抽象的，而抽象类中可以含有普通方法<br>（3）接口中的成员变量 都是public static final的，而抽象类中可以有普通变量<br>（4）接口中一定不含有构造方法，但抽象类中可以有构造方法<br>（5）接口不可以实例化，抽象类可以在子类创建对象的时候自动创建抽象类的对象<br>（6）接口可以多实现，但是抽象类只能够单继承</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java面向对象 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java面向对象1</title>
      <link href="/2019/01/12/java-mian-xiang-dui-xiang-1/"/>
      <url>/2019/01/12/java-mian-xiang-dui-xiang-1/</url>
      
        <content type="html"><![CDATA[<h4 id="1、Eclipse-使用"><a href="#1、Eclipse-使用" class="headerlink" title="1、Eclipse 使用"></a>1、Eclipse 使用</h4><p>IDE：<br>idea myeclipse eclipse NetBeans （visual studio）<br>idea 目前比较流行 有兴趣的可以了解下<br>Git：版本管理工具 从GIt上下载工程<br>JSP：页面 web应用 开发jsp应用<br>点击右上角–》Java<br>（1）选择一个工作空间 —workspace<br>就是电脑上的一个路径，默认的工作空间<br>eclipse-workspace 可以有中文路径<br>（2）project 项目<br>file-&gt;new Java project-&gt;输入工程名-&gt;finish<br>（3）package 包<br>file-&gt;new package-&gt;包的名字（com.ali.entity…）<br>（4）class 类<br>file-&gt;new class-&gt;类的名字（符合规范 字母 数字 下划线 $）<br>（5） run 运行<br>点击 绿色的小三角 run<br>结果在 consonle 去查看<br>WorkSpace-&gt;project-&gt;package-&gt;class run(Javac Java)<br>（6）Eclipse 设置字体大小<br>preference——&gt;font-&gt; Java –设置字体的大小<br>（7）Eclipse 常用快捷键<br>// /<strong>/<br>ctrl+/ 单行注释 //<br>ctrl+shift+/ 多行注释 /</strong>/<br>ctrl+shift+\ 取消多行注释<br>ctrl+s 保存 没事 多按按<br>ctrl+shit+s 工程保存<br>alt+/ 自动补齐<br>ctrl+d 删除<br>ctrl+z 撤销<br>ctrl+shift+f：代码格式化（注意跟 输入法的冲突）<br>ctrl+shift+o：实现包的组织。去除无用的包 实现未导入包的导入</p><h4 id="2、面向对象概述"><a href="#2、面向对象概述" class="headerlink" title="2、面向对象概述"></a>2、面向对象概述</h4><p>Java语言最大的特点<br>面向对象是对现实世界理解和抽象的一种方法<br>核心思想：<br>大象放冰箱里<br>大象：（定义一个类 规定一些属性 身高 体重）<br>冰箱：（定义成一个类 品牌 功率 大小 ）<br>猴子对象<br>冰箱.OpenDoor();<br>冰箱.Save(大象)；<br>冰箱.Close();</p><h4 id="3、面向对象与面向过程"><a href="#3、面向对象与面向过程" class="headerlink" title="3、面向对象与面向过程"></a>3、面向对象与面向过程</h4><p>面向过程：传统程序设计的设计思路。将一个问题看成是一系列函数或者模块的集合<br>自顶向下<br>例如：<br>方法1： 开冰箱门<br>方法2： 放大象<br>方法3：关冰箱门<br>关猴子 重新写方法2<br>最大的区别：面向对象的程序设计具有更高的灵活性，便于程序的扩展和升级<br>面向过程主要是针对特定需求满足某业务条件下的设计<br>面向对象的三大特征：封装 继承 多态</p><h4 id="4、对象"><a href="#4、对象" class="headerlink" title="4、对象"></a>4、对象</h4><p>对象指的是一个具体实例，包含属性和方法<br>例如：<br>夏天属性：身高 体重 年龄 姓名<br>夏天方法：能吃 能睡 工作</p><h4 id="5、类"><a href="#5、类" class="headerlink" title="5、类"></a>5、类</h4><p>具有相同属性和方法的一组对象的集合</p><h4 id="6、类和对象的关系"><a href="#6、类和对象的关系" class="headerlink" title="6、类和对象的关系"></a>6、类和对象的关系</h4><p>对象指的是一个具体的实例<br>类：例如 同学<br>没有指名道姓就不是对象<br>类下面可以有子类 例如： 老鼠是个类 田鼠也是一个类 是老鼠的子类<br>老师、 数学老师、物理老师等都是类</p><h4 id="7、类的创建"><a href="#7、类的创建" class="headerlink" title="7、类的创建"></a>7、类的创建</h4><p>（1）4类8种 基本数据类型<br>（2）引用数据类型：String 数组 接口等<br>自定义的数据类型–用户自己创建的类<br>（3） 修饰符（public等） </p><pre><code>class 类名{     属性：成员变量；     方法：成员函数； } 例如:手机类 public class Phone {}</code></pre><h4 id="8、类和对象的创建与使用"><a href="#8、类和对象的创建与使用" class="headerlink" title="8、类和对象的创建与使用"></a>8、类和对象的创建与使用</h4><p>类名 对象名=new 类名（）;<br>（1）类名 对象名=new 类名（）；–基本形式。<br>可以调用不同参数类型的构造函数 –带参数的形式<br>（2）对象里面的属性（成员变量）、方法<br>通过 对象名.属性<br>对象名.函数<br>实现访问<br>（3）不同的对象 的属性值是不同的 ，而且不交叉<br>相当于一个独立的个体<br>具有独立的地址和存储空间<br>（4）实现对象之间的交互</p><h4 id="9、成员变量与局部变量"><a href="#9、成员变量与局部变量" class="headerlink" title="9、成员变量与局部变量"></a>9、成员变量与局部变量</h4><p>（1）成员变量：对象的属性，放在对象之内<br>（2）局部变量：是在 方法里面 或者 for(int i)<br>成员变量：堆中<br>局部变量 栈中<br>Heap：堆 是临时的 由创建对象时所开辟的一块空间，对象销毁之后，系统回收<br>栈：是方法生成的时候，压栈生成。整个程序结束后才结束。<br>封装 继承 多态 三大特征—-面向对象</p><h4 id="10、封装"><a href="#10、封装" class="headerlink" title="10、封装"></a>10、封装</h4><p>封装：在生活中 包裹。隐私性比较好<br>程序：通过封装成接口，通过方法来调用<br>（1）实现数据的访问权限控制，不是所有人都可以访问<br>（2）实现数据赋值的规范化、标准化的管控<br>例如： person中的性别<br>（3）实现封装的方法是<br>成员变量 加修饰符 private 私有的 无法直接访问 需要生成方法</p><h4 id="11、自动生成-getters-和-setters"><a href="#11、自动生成-getters-和-setters" class="headerlink" title="11、自动生成 getters 和 setters"></a>11、自动生成 getters 和 setters</h4><p>右键-&gt;source-&gt;generate getters and setters-&gt;选中对象的私有属性-&gt;直接生成方法</p><h4 id="12、构造函数"><a href="#12、构造函数" class="headerlink" title="12、构造函数"></a>12、构造函数</h4><p>new 对象的时候 直接初始化 用到构造函数。–&gt;直接赋值<br>例如： int[] arr=new int[]{1,2,3};<br>Person p=new Person(“张三”，20，’男’);<br>构造函数是一种特殊的方法，<br>主要是用来对对象初始化。总是与new 放在一起使用<br>构造函数的函数名是与类名一致<br>构造函数的重载。参数列表不一致的，但是函数名一致的方法<br>按住 ctrl+ 鼠标左键 Open declaration 进入到具体的函数或变量定义的地方</p><h4 id="13、构造函数注意事项"><a href="#13、构造函数注意事项" class="headerlink" title="13、构造函数注意事项"></a>13、构造函数注意事项</h4><p>（1）构造函数 没有返回值<br>（2）构造函数默认存在一个无参的， 自己写一个无参构造函数后 会把默认的冲掉<br>（3）对象在生成的时候调用且只调用一次构造函数<br>（4）如果构造函数调用失败 则无法创建对象<br>（5）对象实例化时 由虚拟机自动调用的</p><h4 id="14、this关键字"><a href="#14、this关键字" class="headerlink" title="14、this关键字"></a>14、this关键字</h4><p>this 表示当前类的对象，哪个对象调用了this所属的方法，this表示哪个对象<br>通过this 可以调用当前对象的 成员变量和方法<br>this(); 表示调用当前对象的无参构造函数</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java面向对象 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础5</title>
      <link href="/2019/01/11/java-ji-chu-5/"/>
      <url>/2019/01/11/java-ji-chu-5/</url>
      
        <content type="html"><![CDATA[<h4 id="1、数组概述"><a href="#1、数组概述" class="headerlink" title="1、数组概述"></a>1、数组概述</h4><p>数组是相同数据类型的一组数据集合。 4类8种基本数据类型<br>数组有索引–代表不同的数值<br>football[7]–&gt;C罗<br>Basketball[23]–&gt;乔丹<br>不同的球队 可以看成不同的数组<br>同一个球队里面 每个球员的编号 唯一<br>数组的长度固定<br>数组的索引从0开始<br>length 数组大小</p><h4 id="2、一维数组"><a href="#2、一维数组" class="headerlink" title="2、一维数组"></a>2、一维数组</h4><p>（1）定义数组<br>dataType 数组名[]<br>dataType[] 数组名—-》<br>例如： int[] array;<br>（2）初始化数组<br>a、首先要确定数组的大小<br>定义时候直接确定：dataType[] array=new dataType[size];<br>dataType[] array;<br>array=new dataType[size];<br>(3) 数组的赋值<br>给数组的元素进行赋值<br>a、 动态赋值<br>b、静态赋值</p><pre><code>dataType[] array=new dataType[]{}; dataType[] array={};</code></pre><h4 id="3、二维数组"><a href="#3、二维数组" class="headerlink" title="3、二维数组"></a>3、二维数组</h4><p>矩阵。 m[i][j] 第i行 j列<br>表示一个 由行列组成的数据，例如 表格<br>10个班级 每个班级有 20 学生 成绩 记录下来<br>用行数 表示 班级<br>用列数表示 每个班级的学生<br>a[10][20]=成绩<br>比如： 小明 第2 班级的底1号学生<br>a[1][0]=90;<br>String[] s;<br>(1)二维数组的声明<br>dataType[][] d_arr=new dataType[row][col];<br>(2)二维数据的初始化<br>a、动态赋值<br>嵌套for循环 遍历二维数组的每个元素<br>b、静态赋值 </p><pre><code>dataType[][] d_arr=new dataType[][]{{},,…,{}}; dataTyep[][] d_arr={{},{},…,{}}; </code></pre><p>二维数组实现 矩阵相乘</p><h4 id="4、方法的概述"><a href="#4、方法的概述" class="headerlink" title="4、方法的概述"></a>4、方法的概述</h4><p>解决某件事情的办法；函数 main<br>计算一个结果<br>处理一段业务逻辑<br>有助于程序的模块化开发和处理<br>方法=函数<br>main函数里面 String[] args 表示的 main函数接受的参数</p><h4 id="5、方法的定义格式"><a href="#5、方法的定义格式" class="headerlink" title="5、方法的定义格式"></a>5、方法的定义格式</h4><p>修饰符 返回值类型 方法的名字（参数列表…）{<br>方法的功能主体<br>return ；// 也可以没有<br>}</p><h4 id="6、方法定义的注意事项"><a href="#6、方法定义的注意事项" class="headerlink" title="6、方法定义的注意事项"></a>6、方法定义的注意事项</h4><p>（1）方法不能定义在其他方法之中 独一性<br>（2）方法如果有返回值类型 一定要返回相应类型的数据<br>例如： double func1（） { return double；不能为 int}<br>（3）调用方法的时候 参数列表一定要对应好<br>例如 func1（int a,b,c）{ (a+b)*c}<br>（4）方法不能重复定义 如果一个方法名字 已经用过了 如果还要用 就需要重载<br>（5） 参数类型与返回值类型无关</p><h4 id="7、方法的重载特性"><a href="#7、方法的重载特性" class="headerlink" title="7、方法的重载特性"></a>7、方法的重载特性</h4><p>同一个类中 允许出现同名的方法，只是方法的参数列表不同，这样的方法称为重载<br>参数列表不同：表示 参数的个数不同 参数数据类型不同<br>（1）重载与参数变量名无关<br>（2）重载与返回值类型无关<br>（3）重载与修饰符无关</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础4</title>
      <link href="/2019/01/09/java-ji-chu-4/"/>
      <url>/2019/01/09/java-ji-chu-4/</url>
      
        <content type="html"><![CDATA[<h4 id="1、位运算符"><a href="#1、位运算符" class="headerlink" title="1、位运算符"></a>1、位运算符</h4><p>主要针对二进制数。 只有 0 1 两种形态。加快运行速度<br>&amp;：位与 两个数同时为1 则为1 否则为0<br>|：位或 两个数中有一个为1 则为1 否则为0<br>^: 异或 相同为0 不同为1</p><p>: 右移运算符 代表位数向右移动<br>&lt;&lt;:左移运算符 代表位数向左移动</p><p>: 无符号右移<br>移动位数很多时，其实按数据的实际有效位数例如 32位，移动位数100%最大位数32 肯定是在32位之内</p><h4 id="2、三元运算符"><a href="#2、三元运算符" class="headerlink" title="2、三元运算符"></a>2、三元运算符</h4><p>布尔表达式？结果1：结果2<br>如果布尔表达式的结果为 true ，进行结果1<br>如果布尔表达式的结果为 false ，进行结果2</p><h4 id="3、转义运算符"><a href="#3、转义运算符" class="headerlink" title="3、转义运算符"></a>3、转义运算符</h4><p>字符并不是你看起来的那个样子，转义了<br>a、八进制转义<br>+用1-3位的8进制数字，范围‘000’-‘377’<br>例如： \0;<br>b、unicode 转义字符<br>\u+ 4位十六进制数字：0-65535<br>\u0000<br>c、特殊字符<br>\”：表示双引号<br>\’:单引号<br>:反斜线<br>d、控制字符<br>\r :回车<br>\n: 换行<br>\t: tab<br>\b:退格</p><p>程序控制语句（顺序 条件 循环）</p><h4 id="4、-if-条件语句"><a href="#4、-if-条件语句" class="headerlink" title="4、 if 条件语句"></a>4、 if 条件语句</h4><p>只要满足某种条件就处理，不完全是 顺序结构，可以跳着执行<br>（1） if （条件语句）{<br>—建议将{ 起始位置写在 if条件之后 便于知道 if语句的范围<br>执行语句；<br>}<br>if else 如果满足条件，我将如何做，否则我该如何做<br>（2） if(条件语句){ </p><pre><code>if(条件语句){     执行语句1； }else{     执行语句2； } </code></pre><p>（3） if..else if（多个）.. else </p><pre><code>if(1){     学习； }else if(2){     运动； }else if(3){     看电视剧;}else{     睡觉}</code></pre><h4 id="5、-switch-条件语句"><a href="#5、-switch-条件语句" class="headerlink" title="5、 switch 条件语句"></a>5、 switch 条件语句</h4><p>形式如下：与 if else if else 很类似 </p><pre><code>switch （条件表达式）{     case 值1：     语句1；     break ；     case 值2：     语句2；     break ；     ….     default :     语句n；     break ； }</code></pre><h4 id="6、-for-循环语句—使用非常广泛"><a href="#6、-for-循环语句—使用非常广泛" class="headerlink" title="6、 for 循环语句—使用非常广泛"></a>6、 for 循环语句—使用非常广泛</h4><p>（1）单层 for 循环语句<br>for(表达式1；表达式2；表达式3){<br>循环体。//就是表示此部分语句需要执行多次。 回旋 跑圈<br>}<br>表达式1：主要是赋一个初始化值， 循环变量的最开始值；<br>表达式2：用来判断 循环变量的值 是否达到 临界值<br>表达式3：主要用来实现 循环变量的增加或减少<br>执行顺序：表达式1 表达式2 循环体 表达式3 表达式2 循环体 表达式3 表达式2 循环体<br>{}–注意 循环体的花括号 可以省略 但是是针对循环体内只有一条语句的情况。<br>(2)嵌套for循环–》在for循环体里面又至少写了一层for循环 </p><pre><code>for(;;){     for(;;){     ….     } }</code></pre><h4 id="7、-while-循环语句"><a href="#7、-while-循环语句" class="headerlink" title="7、 while 循环语句"></a>7、 while 循环语句</h4><p>while(条件表达式){<br>    循环体；<br>}<br>注意 ：条件表达式 一定要注意终止和结束 出现死循环 </p><h4 id="8、-do-while-循环语句"><a href="#8、-do-while-循环语句" class="headerlink" title="8、 do while 循环语句"></a>8、 do while 循环语句</h4><p>do{</p><p>}while(条件表达式)<br>区别： do while 是先执行后判断，至少执行一次<br>while 循环 是先判断后执行</p><h4 id="9、-break-中止语句"><a href="#9、-break-中止语句" class="headerlink" title="9、 break 中止语句"></a>9、 break 中止语句</h4><p>应用：循环体 + 条件语句 switch case<br>（1）针对单层循环结构，表示退出循环<br>（2）针对嵌套循环，表示退出当前的循环<br>（3）switch 条件语句 表示中止 条件语句</p><h4 id="10、-continue-语句"><a href="#10、-continue-语句" class="headerlink" title="10、 continue 语句"></a>10、 continue 语句</h4><p>继续。循环语句里面 使用 continue，并不是中止循环体</p><h4 id="11、-return-语句"><a href="#11、-return-语句" class="headerlink" title="11、 return 语句"></a>11、 return 语句</h4><p>return 的作用主要是<br>（1）用来返回方法的指定类型值<br>（2）结束方法的执行<br>都能中止方法的运行</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础3</title>
      <link href="/2019/01/07/java-ji-chu-3/"/>
      <url>/2019/01/07/java-ji-chu-3/</url>
      
        <content type="html"><![CDATA[<h4 id="1、数据类型的转换"><a href="#1、数据类型的转换" class="headerlink" title="1、数据类型的转换"></a>1、数据类型的转换</h4><p>主要是指的 不同的数据类型之间进行转换<br>（1）自动类型转换<br>范围小的数据类型值，转换为范围大的数据类型的值<br>例如 byte int 自动 byte 转换为 int<br>byte-&gt;short-&gt;int-&gt;long-&gt;float-&gt;double<br>（2）强制数据类型转换<br>数据范围大的转换为数据类型小的<br>强制类型转换不会报错，只是损失了精度<br>例如：喝多了：：： 记不住 精度 就丢失了<br>double 2.134 –&gt; int 2 0.134 没了<br>数据类型之间进行强制转换。比如：<br>int 转换为 String 或者 String转换为 int<br>String与日期类型 转换<br>“2018-11-6 20:37:66:002”–&gt;Date 先记住 后面会在API<br>//Integer.Valueof() ParseInt()</p><h4 id="2、算术运算符"><a href="#2、算术运算符" class="headerlink" title="2、算术运算符"></a>2、算术运算符</h4><p>加减 乘除 求余运算。 + - * / %<br>运算后赋值。赋值运算。<br>+= 相当于 +完之后 赋值 例如 int a=0; a+=10; a=a+10;<br>-=<br>/=<br>关于/，一定要记得 0不能作为除数。异常</p><h4 id="3、自增自减运算符"><a href="#3、自增自减运算符" class="headerlink" title="3、自增自减运算符"></a>3、自增自减运算符</h4><p>++ – int a； 都代表 1次<br>a++:表示自己增加1 表示 先使用变量a 再进行自加运算<br>++a:表示自己增加1 表示 先自加运算 再使用变量a<br>a–:表示自己减少1 表示 先使用变量a 再进行自加运算<br>–a:表示自己减少1 表示 先自减运算 再使用变量a</p><p>一般是在 循环的时候使用–后面讲流程控制时 会详细讲</p><h5 id="4、比较运算符"><a href="#4、比较运算符" class="headerlink" title="4、比较运算符"></a>4、比较运算符</h5><p>&lt; &lt;= &gt;= == !=<br>进行数据的比较，最后的结果为一个 boolean类型的结果<br>条件语句。（if else case while）</p><h4 id="5、逻辑运算符"><a href="#5、逻辑运算符" class="headerlink" title="5、逻辑运算符"></a>5、逻辑运算符</h4><p>逻辑与：<br>&amp;：表示只有表达式两边都是 true 结果才为 true<br>&amp;&amp;：表示只有表达式两边都是 true 结果才为 true<br>区别：短路，提前结束这个判断过程<br>&amp;&amp; 如果第一个条件为 false 则 后面的语句不再运行。 可以加快速度<br>&amp;： 不具有短路功能，从左到右 依次执行<br>逻辑或<br>||：有一个为 true 就为 true<br>|：有一个为 true 就为 true<br>区别： 短路，提前结束这个判断过程<br>||：如果第一个条件为 true 那么后面不再判断，直接输出为 true；<br>|：不具有短路功能，从左到右 依次执行<br>逻辑非<br>！非真即假 非假即真</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础2</title>
      <link href="/2019/01/06/java-ji-chu-2/"/>
      <url>/2019/01/06/java-ji-chu-2/</url>
      
        <content type="html"><![CDATA[<h4 id="1、Java注释"><a href="#1、Java注释" class="headerlink" title="1、Java注释"></a>1、Java注释</h4><p>(1)单行注释 //<br>只能注释一行，而且注释使用实在 // 之后。不会运行<br>例如：<br>(2)多行注释 /<em>*/<br>可以注释多行内容。 主要用来说明一段代码或者一个函数的作用<br>(3)文档注释 /</em>/<br>主要用来说明类的功能，包含的函数、字段以及主要的作者、版本 相关的参数 异常等<br>author：作者<br>version：版本<br>see：参考 一个url链接<br>param：参数<br>return 返回值<br>代码注释很重要 因为不止是对别人看你或理解你的代码，而且对自己也有好处</p><h4 id="2、Java标识符"><a href="#2、Java标识符" class="headerlink" title="2、Java标识符"></a>2、Java标识符</h4><p>所谓标识符就是对Java当中的 变量、类名、对象名、函数等自己的名字，<br>名字必须得符合规范<br>字母、数字、下划线和美元符号组成<br>注意：a、不能以数字开头<br>例如： 1a 错误 但是可以写成 a1<br>b、一般情况下不以美元开头<br>c、见名知意：<br>例如：zhidao licheng juli xingming—不可取<br>Distance Name—推荐用英语 并不是完全的英语照搬<br>xuehao –StudentNumber–&gt;StuNumber、 StdNum StdId<br>d、Java严格区分大小写<br>例如： a 与 A 就是两个变量<br>e、不要使用关键字：<br>例如： public static void if else switch<br>f、驼峰命名法<br>变量： 头一个字母小写 stdName<br>类名：首字母大写 Student StudentInfo<br>具体与单位的要求有关</p><h4 id="3、Java关键字"><a href="#3、Java关键字" class="headerlink" title="3、Java关键字"></a>3、Java关键字</h4><p>（1）所有Java里面被赋予了特殊含义的单词，就叫做关键字<br>（2）Java关键字都是小写<br>a、用于定义数据类型： int class 等<br>b、数据类型值： true false null 等<br>c、流程控制的： for if 等<br>d、访问控制权限的： public 等<br>e、变量、函数等修饰的： static 等<br>f、异常处理的： try catch 等</p><h4 id="4、Java基础数据类型"><a href="#4、Java基础数据类型" class="headerlink" title="4、Java基础数据类型"></a>4、Java基础数据类型</h4><p>计算机里面存储设备的最小单元：位（bit）<br>最小存储单元：1 B=8位 Bit<br>1KB=1024B<br>1MB=1024KB<br>1GB=1024MB<br>1TB=1024GB<br>PB级 的数据量 大数据<br>1PB=1024TB</p><h4 id="5、基本数据类型-4类8种"><a href="#5、基本数据类型-4类8种" class="headerlink" title="5、基本数据类型 4类8种"></a>5、基本数据类型 4类8种</h4><p>（1）整数类型： byte ：1个字节（8位） 范围比较小：-128~127<br>short ：2<br>int:4<br>long :8<br>（2）浮点型（小数）：<br>float ：4 单精度<br>double ：8 双精度 精度高<br>（3）字符型：<br>char ：2 一个字符<br>例如：’笑’、’A’<br>（4）布尔型： boolean ：1个字节 true false</p><h4 id="6、引用数据类型（包含自定义）"><a href="#6、引用数据类型（包含自定义）" class="headerlink" title="6、引用数据类型（包含自定义）"></a>6、引用数据类型（包含自定义）</h4><p>String 型<br>“大家好，欢迎来TZ学习。”<br>数组<br>类等</p><h4 id="7、常量"><a href="#7、常量" class="headerlink" title="7、常量"></a>7、常量</h4><p>常量是一种特殊的变量，只不过是值被设定后，不能改变<br>例如： final(关键字) PI=3.1415926;</p><h4 id="8、变量"><a href="#8、变量" class="headerlink" title="8、变量"></a>8、变量</h4><p>变量是可以随着程序的变化而改变赋值<br>int a=10;<br>a=11;</p><h4 id="9、定义基本数据类型变量"><a href="#9、定义基本数据类型变量" class="headerlink" title="9、定义基本数据类型变量"></a>9、定义基本数据类型变量</h4><p>三要素： 指明类型（整数型、浮点型等）、变量命名、变量赋值（可以没有）</p><h4 id="10、字符串变量的定义"><a href="#10、字符串变量的定义" class="headerlink" title="10、字符串变量的定义"></a>10、字符串变量的定义</h4><p>字符串变量 引用数据类型。应用比较广泛。<br>字符串变量赋值变化时，相当于重新指向了一个对象</p><h4 id="11、数据类型的转换"><a href="#11、数据类型的转换" class="headerlink" title="11、数据类型的转换"></a>11、数据类型的转换</h4><p>主要是指的 不同的数据类型之间进行转换<br>（1）自动类型转换<br>范围小的数据类型值，转换为范围大的数据类型的值<br>例如 byte int 自动 byte 转换为 int<br>（2）强制数据类型转换<br>数据范围大的转换为数据类型小的<br>数据类型之间进行强制转换。比如：<br>int 转换为 String 或者 String转换为 int<br>String与日期类型 转换</p><h4 id="12、算术运算符"><a href="#12、算术运算符" class="headerlink" title="12、算术运算符"></a>12、算术运算符</h4><p>加减 乘除 求余运算<br>+=<br>-=<br>/=<br>自增自减运算符</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础1</title>
      <link href="/2019/01/03/java-ji-chu-1/"/>
      <url>/2019/01/03/java-ji-chu-1/</url>
      
        <content type="html"><![CDATA[<h4 id="1、计算机语言发展史"><a href="#1、计算机语言发展史" class="headerlink" title="1、计算机语言发展史"></a>1、计算机语言发展史</h4><p>Java语言是计算机语言的一种<br>（1）语言：汉语 英语 阿拉伯语 日语—&gt;人与人进行沟通的 一种方式<br>语义。—-》自然语言处理 人工智能中 文本分析 NLP<br>（2）机器语言：人与计算机沟通的语言。—Java 就是其中一种<br>类似于英语在自然语言中的地位 很流行 很主流<br>a、机器语言–初级形态：用二进制编码来表示计算机能够识别和执行的一种机器指令集合<br>例如：0 1 二进制编码 10进制<br>101010110—》启动声卡<br>b、机器语言—中级形态：汇编语言，用一种助记符来机器指令，成为符号语言。<br>例如：mov–表示数据的移动<br>rm-删除<br>add<br>c、机器语言—-高级形态：高级语言。一种接近人们使用习惯高级程序语言<br>例如：c=a+b; 实现数据的加和<br>常见的高级程序语言：Java、C、C++、C#、R、Python、Scala、VB、PHP等等</p><h4 id="2、Java语言概述"><a href="#2、Java语言概述" class="headerlink" title="2、Java语言概述"></a>2、Java语言概述</h4><p>Java语言是一门非常年轻的语言 90后。最早是SUN —–Jamse Gosling（Java之父）<br>Oak–橡树。—Java 看到一个人 拿着爪哇杯 喝咖啡<br>Java语言随着互联网的发展，跨系统、跨平台 能够运行<br>Java语言获得了飞速的发展<br>Java 也形成了自己的一套方法 体系。封装了很多成熟可用的方法可以直接调用<br>API文档 —葵花宝典 Java 字典</p><h4 id="3、Java语言的特性和优点"><a href="#3、Java语言的特性和优点" class="headerlink" title="3、Java语言的特性和优点"></a>3、Java语言的特性和优点</h4><p>（1）跨平台—一次编写 到处运行<br>（2）面向对象—万事万物，皆为对象。 类<br>（3）相对简单—有C语言基础或者其他语言基础，语言之间是有相同性<br>要知道 Java语言的基本语法、基本数据类型、基本程序控制</p><h4 id="4、Java的开发环境"><a href="#4、Java的开发环境" class="headerlink" title="4、Java的开发环境"></a>4、Java的开发环境</h4><p>（1）JDK：Java development Kit： 开发者工具包<br>（2）JRE：Java Runtime Environment：Java 运行环境—只做运行 不做开发时<br>（3）JVM：Java Virtual Machine：Java 虚拟机<br>所有的Java程序都运行在 jvm上<br>JDK或JRE具备后，程序会调用生成 JVM<br>JDk包含JRE</p><h4 id="5、JDK的安装与配置"><a href="#5、JDK的安装与配置" class="headerlink" title="5、JDK的安装与配置"></a>5、JDK的安装与配置</h4><p>jdk 下载官网地址：<a href="https://www.oracle.com/technetwork/Java/Javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/Java/Javase/downloads/jdk8-downloads-2133151.html</a><br>（1）针对从官网上下载的 .exe的安装包<br>a、 选择适合你电脑操作系统和位数的 JDK版本<br>例如Mac linux 还有 32位或64位<br>b、要增加以下<br>1）Java_HOME:jdk 安装的目录。C:\Program Files\Java\jdk1.8.0_171<br>2）在 Path里面 增加 ：<br>英文的字符<br>;%Java_HOME%\bin;%Java_HOME%\jre\bin<br>一定要记得点确定。还要把cmd给关掉，重新打开 cmd 输入Java、Javac等命令验证<br>（2）考一个jdk安装包，之后再Java_home里面进行更改<br>（3）classpath可以不添加</p><h4 id="6、Java程序的概述"><a href="#6、Java程序的概述" class="headerlink" title="6、Java程序的概述"></a>6、Java程序的概述</h4><p>Java程序需要首先完成：<br>（1）Java源文件， .Java 结尾的文件<br>（2）编译生成字节码文件，.class 结尾的文件 很多编码 二进制（16进制）组成的文件<br>（3）将字节码文件 编译器（compiler） JVM能够识别和运行的文件<br>首先编写源文件–》其次通过编译成.class文件–》最后JVM运行</p><h4 id="7、DOS常见的命令"><a href="#7、DOS常见的命令" class="headerlink" title="7、DOS常见的命令"></a>7、DOS常见的命令</h4><p>dir:列出当前目录下的文件及文件夹<br>换盘：直接输入盘符：，例如 切换到D盘 D:<br>cd:换目录 tab键 可以自动补齐。把目录的名字进行补齐<br>md:新建文件目录<br>del:删除文件目录<br>cls:清屏<br>exit：退出<br>上下箭头：可以调用之前输出的命令</p><h4 id="8、第一个Helloworld-Java程序"><a href="#8、第一个Helloworld-Java程序" class="headerlink" title="8、第一个Helloworld Java程序"></a>8、第一个Helloworld Java程序</h4><p>（1）helloworld 的文件名字一定要与 类名（class 后面紧跟的 名字）保持一致<br>（2）设置一下 .Java 源文件编码方式 UTF-8<br>（3）如果设置为 UTF-8 会发现中文输出为乱码，原因是 Java源文件的编码方式与<br>Java编译时的编码方式不一样，造成了乱码</p><h4 id="9、Java注释"><a href="#9、Java注释" class="headerlink" title="9、Java注释"></a>9、Java注释</h4><p>单行注释 //<br>多行注释 /<em>*/<br>文档注释 /</em>/<br>代码注释很重要 因为不止是对别人看你或理解你的代码，而且对你自己也有好处。</p><h4 id="10、Java标识符"><a href="#10、Java标识符" class="headerlink" title="10、Java标识符"></a>10、Java标识符</h4><p>所谓标识符就是对Java当中的 变量、类名、对象名、函数等自己的名字，名字必须得符合规范<br>字母、数字、下划线和美元符号组成</p><h4 id="11、Java关键字"><a href="#11、Java关键字" class="headerlink" title="11、Java关键字"></a>11、Java关键字</h4><p>以Java来编码的 文件 ，蓝色的字符 关键字</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java基础 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
