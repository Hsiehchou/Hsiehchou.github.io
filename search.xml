<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>第一章 Python概述与开发环境安装</title>
      <link href="/2020/04/30/di-yi-zhang-python-gai-shu-yu-kai-fa-huan-jing-an-zhuang/"/>
      <url>/2020/04/30/di-yi-zhang-python-gai-shu-yu-kai-fa-huan-jing-an-zhuang/</url>
      
        <content type="html"><![CDATA[<h2 id="第一章-Python概述与开发环境安装"><a href="#第一章-Python概述与开发环境安装" class="headerlink" title="第一章 Python概述与开发环境安装"></a>第一章 Python概述与开发环境安装</h2><h3 id="1、Python开发环境安装"><a href="#1、Python开发环境安装" class="headerlink" title="1、Python开发环境安装"></a>1、Python开发环境安装</h3><h3 id="2、Anaconda安装"><a href="#2、Anaconda安装" class="headerlink" title="2、Anaconda安装"></a>2、Anaconda安装</h3><p>选择 just me<br>不用勾选添加本地环境变量</p><ol><li>查看Anaconda环境是否安装成功（查看Anaconda版本号）：conda –version</li><li>查看目前安装了哪些环境变量：conda info –envs</li><li>查看 Anaconda 当前版本以及安装了哪些包：conda list</li></ol><h3 id="3、Spyder"><a href="#3、Spyder" class="headerlink" title="3、Spyder"></a>3、Spyder</h3><h4 id="3-1-读取文件里面的行数"><a href="#3-1-读取文件里面的行数" class="headerlink" title="3.1 读取文件里面的行数"></a>3.1 读取文件里面的行数</h4><pre><code>import sys import os.path# 文件目录dir = os.path.dirname(sys.executable)# 打开文件进行操作with open(dir+&#39;\\num.txt&#39;, encoding = &#39;utf-8&#39;) as fp:    content = fp.readlines()# 打印文件内容的类型print(type(content))# 打印文件内容print(content)# 打印文件所在的目录print(dir)# 打印文件里面内容的行数print(len(content))</code></pre><p><strong>结果：</strong><br>&lt;class ‘list’&gt;<br>[‘12\n’, ‘6\n’, ‘2\n’, ‘35\n’, ‘11\n’, ‘22\n’, ‘23\n’, ‘11\n’, ‘254\n’, ‘12’]<br>F:\Anaconda<br>10</p><h3 id="4、Jupyter-Notebook"><a href="#4、Jupyter-Notebook" class="headerlink" title="4、Jupyter Notebook"></a>4、Jupyter Notebook</h3><p>默认地址：<a href="http://localhost:8888" target="_blank" rel="noopener">http://localhost:8888</a></p><pre><code># 使用递归def fib(n):    if n==1 or n==2:        return 1    elif n==0:        return 0    return fib(n-1)+fib(n-2)# 输出第10个斐波那契数列print(fib(10))print(fib(0))</code></pre><p><strong>结果：</strong><br>55<br>0</p><h3 id="5、Python环境管理"><a href="#5、Python环境管理" class="headerlink" title="5、Python环境管理"></a>5、Python环境管理</h3><h4 id="5-1-打开管理终端"><a href="#5-1-打开管理终端" class="headerlink" title="5.1 打开管理终端"></a>5.1 打开管理终端</h4><p>Windows用户打开“Anaconda Prompt”</p><p>macOS和Linux用户打开”Terminal”（终端）</p><h4 id="5-2-创建新环境"><a href="#5-2-创建新环境" class="headerlink" title="5.2 创建新环境"></a>5.2 创建新环境</h4><pre><code>conda create --name &lt;env_name&gt; &lt;package_name&gt;</code></pre><p><strong>注：</strong></p><ol><li><code>env_name</code>–创建的环境名，建议英文命名，且不加空格，名称两边不加尖括号”&lt;&gt;”</li><li><code>package_name</code>–安装环境中的包名，名称两边不加尖括号”&lt;&gt;”</li><li>如果要安装指定的版本号， 则只需要在包名后面以=和版本号的形式执行。如：<code>conda create name python2  python=2.7</code>，即创建一个名为“python2”的环境，环境中安装版本为2.7的python。</li><li>如果要在新创建的环境中创建多个包，则直接在<code>&lt;package_names&gt;</code>后以空格隔开，添加多个即可。如：<code>conda create -n python3 python=3.7 numpy pandas</code>，即创建一个名为“python”的环境，环境中安装版本为3.7的python，同时也安装了<code>numpy</code>和<code>pandas</code>。</li><li>默认情况下，新创建的环境会被保存在<code>/User/&lt;username&gt;/anaconda3/env</code>目录下，其中<code>&lt;user_name&gt;</code>为系统当前用户的用户名。</li></ol><h4 id="5-3-激活-退出环境"><a href="#5-3-激活-退出环境" class="headerlink" title="5.3 激活/退出环境"></a>5.3 激活/退出环境</h4><p>激活：<code>conda activate python3</code></p><p>退出：<code>conda deactivate</code></p><h4 id="5-4-删除环境"><a href="#5-4-删除环境" class="headerlink" title="5.4 删除环境"></a>5.4 删除环境</h4><p> <code>conda  remove --name python3 --all</code></p><h3 id="6、Python扩展库安装"><a href="#6、Python扩展库安装" class="headerlink" title="6、Python扩展库安装"></a>6、Python扩展库安装</h3><h4 id="6-1-添加清华大学的Anaconda镜像"><a href="#6-1-添加清华大学的Anaconda镜像" class="headerlink" title="6.1 添加清华大学的Anaconda镜像"></a>6.1 添加清华大学的Anaconda镜像</h4><pre><code>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/# 设置搜索时显示通道地址conda config --set show_channel_urls yes</code></pre><p><code>conda install numpy</code> 测试</p><p><strong>查询可供安装的扩展库版本</strong><br><code>conda search --full-name pandas</code></p><p><strong>获取当前环境中已安装的扩展库信息</strong><br><code>conda list</code></p><h4 id="6-2-在指定环境中安装包"><a href="#6-2-在指定环境中安装包" class="headerlink" title="6.2 在指定环境中安装包"></a>6.2 在指定环境中安装包</h4><pre><code>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yesconda activate python3conda install numpy</code></pre><h4 id="6-3-在当前环境中卸载包"><a href="#6-3-在当前环境中卸载包" class="headerlink" title="6.3 在当前环境中卸载包"></a>6.3 在当前环境中卸载包</h4><p><code>conda remove &lt;package_name&gt;</code></p><h4 id="6-4-在指定环境中卸载包"><a href="#6-4-在指定环境中卸载包" class="headerlink" title="6.4 在指定环境中卸载包"></a>6.4 在指定环境中卸载包</h4><p><code>conda remove --name &lt;env_name&gt; &lt;package_name&gt;</code></p><h3 id="7、-Python扩展库导入"><a href="#7、-Python扩展库导入" class="headerlink" title="7、 Python扩展库导入"></a>7、 Python扩展库导入</h3><p><strong>建议先导入标准库再导入扩展库对象，只导入确实需要使用的标准库和扩展库对象，提高加载速度，减少打包体积</strong></p><h4 id="7-1-import-模块名-as-别名"><a href="#7-1-import-模块名-as-别名" class="headerlink" title="7.1 import 模块名[as 别名]"></a>7.1 import 模块名[as 别名]</h4><p><strong>使用时需要在对象之前加上模块名作为前缀，即“模块名.对象名”</strong></p><pre><code>import math import randomimport posixpath as pathprint(math.sqrt(16))                # 计算并输出16的平方根print(math.cos(math.pi/4))            # 计算余弦值print(random.choices(&#39;abcd&#39;, k=8))    # 从字符串&#39;abcd&#39;随机选择8个字符                                    # 允许重复print(path.isfile(r&#39;C:Windows\notepad.exe&#39;))    #测试指定路径是否为文件</code></pre><p><strong>结果：</strong><br>4.0<br>0.7071067811865476<br>[‘b’, ‘b’, ‘d’, ‘b’, ‘a’, ‘d’, ‘a’, ‘c’]<br>False</p><h4 id="7-2-from-模块名-import-对象名-as-别名"><a href="#7-2-from-模块名-import-对象名-as-别名" class="headerlink" title="7.2 from  模块名  import  对象名 [as 别名]"></a>7.2 from  模块名  import  对象名 [as 别名]</h4><p>不需要模块名作为前缀，导入方式可以减少查询次数，提高访问速度</p><pre><code>from math import pi as PIfrom os.path import getsizefrom random import choicer = 3print(round(PI*r*r, 2))                     # 计算半径为3的圆面积print(getsize(r&#39;C:Windows\notepad.exe&#39;))    # 计算文件大小，单位为字节print(choice(&#39;Python&#39;))                        # 从字符串中随机选择一个字符</code></pre><p><strong>结果：</strong><br>28.27<br>254464<br>o</p><h4 id="7-3-from-模块名-import"><a href="#7-3-from-模块名-import" class="headerlink" title="7.3 from  模块名  import  *"></a>7.3 from  模块名  import  *</h4><p><strong>不推荐使用</strong></p><pre><code>from itertools import *characters = &#39;1234&#39;for item in combinations(characters, 3):    # 从4个字符中任选3个组合    print(item, end=&#39; &#39;)                    # end=&#39; &#39; 表示输出后不换行print(&#39;\n&#39;+&#39;=&#39;*20)                          # 行号后输出20个等于号for item in permutations(characters, 3):    # 从4个字符中任选3个的排列    print(item, end=&#39; &#39;)                    </code></pre><h3 id="8、Python常用标准库"><a href="#8、Python常用标准库" class="headerlink" title="8、Python常用标准库"></a>8、Python常用标准库</h3><h4 id="8-1-字符串"><a href="#8-1-字符串" class="headerlink" title="8.1 字符串"></a>8.1 字符串</h4><p><code>re</code>：正则表达式。用来判断是否是你指定的特定字符串。<br><code>StringIO</code>：提供以文件形式来读写字符串。<br><code>struct</code>：以二进制字节序列来解释字符串。可以通过格式化参数，指定类型、长度、字节序（大小端）、内存对齐等。</p><pre><code>import re print(re.findall(r&#39;f[a-z]*&#39;,  &#39;which foot or hand fell fastest&#39;))</code></pre><p><strong>结果：</strong><br>[‘foot’, ‘fell’, ‘fastest’]</p><p>如果只需要简单的功能，应该首先考虑字符串，因为简单，易于阅读和调试，如：</p><pre><code>print(&#39;tea for too&#39;.replace(&#39;&#39;too,&#39;&#39;two&#39;))</code></pre><p>结果：<br>‘tea for two’</p><h4 id="8-2-数据类型"><a href="#8-2-数据类型" class="headerlink" title="8.2  数据类型"></a>8.2  数据类型</h4><p><code>datetime</code>：提供操作日期和时间的类。<br><code>collections</code>：高性能容器数据类型。实现了Python的通用内置容器、字典、列表、集合，和元组专门的数据类型。<br><code>pprint</code>：提供“整洁打印”功能，具有打印任意Python数据结构的能力。</p><h4 id="8-3-数学运算"><a href="#8-3-数学运算" class="headerlink" title="8.3 数学运算"></a>8.3 数学运算</h4><p><code>random</code>：各种分布的伪随机数的生成器。<br><code>math</code>：数学函数。提供了由C标准的数学函数访问。该库函数不适用于复数。<br><code>cmath</code>：为复数提供的数学函数。<br><code>operator</code>： 重载运算符。</p><p><strong>math 模块为浮点运算提供了对底层C函数库的访问</strong></p><pre><code>import math print(math.cos(math.pi/4))print(math.log(1024, 2))</code></pre><p><strong>结果：</strong><br>0.7071067811865476<br>10.0</p><p><strong>random 提供了生成随机数的工具</strong></p><pre><code>import randomfruits = random.choice([&#39;apple&#39;, &#39;pear&#39;, &#39;banana&#39;])x = random.sample(range(100), 10)     # 0-100选择不能重复的10个数y = random.random()                    # 随机浮点数z = random.randrange(6)                # 从范围0-6中选择随机整数print(fruits)print(x)print(y)print(z)</code></pre><p><strong>结果：</strong><br>apple<br>[64, 97, 91, 21, 40, 55, 63, 79, 77, 1]<br>0.8885638928051524<br>0</p><h4 id="8-4-文件和目录"><a href="#8-4-文件和目录" class="headerlink" title="8.4 文件和目录"></a>8.4 文件和目录</h4><p><code>os.path</code>：常用路径名操作。<br><code>filecmp</code>：文件和目录的比较。<br><code>shutil</code>：高级的文件操作：支持文件复制和删除。</p><h4 id="8-5-操作系统"><a href="#8-5-操作系统" class="headerlink" title="8.5 操作系统"></a>8.5 操作系统</h4><p><code>time</code>：时间获取和转换，各种与时间相关的函数。<br><code>argparse</code>：命令行选项、参数和子命令的解析器。<br><code>io</code>：提供接口处理的IO流。<br><code>logging</code>： Python的日志工具，提供日志记录的API。<br><code>logging.config</code>：Python日志配置，用于配置日志模块的API。<br><code>os</code>：提供丰富的与MAC，NT，Posix等操作系统进行交互的能力。<br><code>sys</code>：提供访问和维护python解释器的能力。这包括了提示信息，版本，整数的最大值，可用模块，路径钩子，标准错误，标准输入输出的定位和解释器调用的命令参数。</p><p><strong>os模块提供了不少与操作系统相关联的函数</strong></p><pre><code>import os print(os.getcwd())            # 返回当前的工作目录os.chdir(r&#39;C:Users\winner\Python3Learn\Lesson1Code&#39;) # 修改当前的工作目录os.system(&#39;mkdir today&#39;)     # 执行系统命令 mkdirprint(os.getcwd())            # 返回当前的工作目录</code></pre><p>建议使用<code>import os</code> 风格而非<code>from os import *</code>，这样可以保证随操作系统不同而有所变化的os.open()不会覆盖内置函数open()。</p><p>在使用os这样的大型模块时，内置的dir()和help()函数非常有用。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对抗击新冠肺炎疫情斗争牺牲烈士和逝世同胞的深切哀悼</title>
      <link href="/2020/04/04/dui-kang-ji-xin-guan-fei-yan-yi-qing-dou-zheng-xi-sheng-lie-shi-he-shi-shi-tong-bao-de-shen-qie-ai-dao/"/>
      <url>/2020/04/04/dui-kang-ji-xin-guan-fei-yan-yi-qing-dou-zheng-xi-sheng-lie-shi-he-shi-shi-tong-bao-de-shen-qie-ai-dao/</url>
      
        <content type="html"><![CDATA[<h3 id="对抗击新冠肺炎疫情斗争牺牲烈士和逝世同胞的深切哀悼"><a href="#对抗击新冠肺炎疫情斗争牺牲烈士和逝世同胞的深切哀悼" class="headerlink" title="对抗击新冠肺炎疫情斗争牺牲烈士和逝世同胞的深切哀悼"></a>对抗击新冠肺炎疫情斗争牺牲烈士和逝世同胞的深切哀悼</h3><p>牺牲的烈士们，逝世同胞们，一路走好，愿你们天堂安息！！！<br>致敬英雄，缅怀逝者！！！</p>]]></content>
      
      
      <categories>
          
          <category> 全国性哀悼活动 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 哀悼 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL将Windows和Linux时区同步的时区表</title>
      <link href="/2020/04/03/wang-zhan-zai-ai-dao-jie-ri-shi-shi-zheng-ge-ye-mian-hui-se-de-dai-ma/"/>
      <url>/2020/04/03/wang-zhan-zai-ai-dao-jie-ri-shi-shi-zheng-ge-ye-mian-hui-se-de-dai-ma/</url>
      
        <content type="html"><![CDATA[<h3 id="网站在哀悼节日时使整个页面灰色的代码"><a href="#网站在哀悼节日时使整个页面灰色的代码" class="headerlink" title="网站在哀悼节日时使整个页面灰色的代码"></a>网站在哀悼节日时使整个页面灰色的代码</h3><p>html {<br>    -webkit-filter: grayscale(100%);<br>    -moz-filter: grayscale(100%);<br>    -ms-filter: grayscale(100%);<br>    -o-filter: grayscale(100%);<br>    filter: grayscale(100%);<br>    filter: progid:DXImageTransform.Microsoft.BasicImage(grayscale=1);<br>}</p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 哀悼 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>npm换成淘宝的源</title>
      <link href="/2020/02/26/npm-huan-cheng-tao-bao-de-yuan/"/>
      <url>/2020/02/26/npm-huan-cheng-tao-bao-de-yuan/</url>
      
        <content type="html"><![CDATA[<p><strong>安装npm install时，长时间停留在fetchMetadata: sill fetchPackageMetaData error for …</strong></p><p><strong>方法如下</strong></p><h3 id="更换成淘宝的源"><a href="#更换成淘宝的源" class="headerlink" title="更换成淘宝的源"></a>更换成淘宝的源</h3><p>npm config set registry <a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a> </p><h3 id="验证是否成功"><a href="#验证是否成功" class="headerlink" title="验证是否成功"></a>验证是否成功</h3><p>npm config get registry </p>]]></content>
      
      
      <categories>
          
          <category> npm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> npm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL将Windows和Linux时区同步的时区表</title>
      <link href="/2020/02/26/mysql-jiang-windows-he-linux-shi-qu-tong-bu-de-shi-qu-biao/"/>
      <url>/2020/02/26/mysql-jiang-windows-he-linux-shi-qu-tong-bu-de-shi-qu-biao/</url>
      
        <content type="html"><![CDATA[<p><strong>时区表</strong> mysql_tzinfo_to_sql /usr/share/zoneinfo | mysql -u root mysql -p</p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Github Hexo 建站</title>
      <link href="/2020/02/24/github-hexo-jian-zhan/"/>
      <url>/2020/02/24/github-hexo-jian-zhan/</url>
      
        <content type="html"><![CDATA[<h2 id="Github-Hexo-建站"><a href="#Github-Hexo-建站" class="headerlink" title="Github Hexo 建站"></a>Github Hexo 建站</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><pre><code> hexo init git init npm install --save hexo-deployer-git</code></pre><h3 id="代码高亮"><a href="#代码高亮" class="headerlink" title="代码高亮"></a>代码高亮</h3><pre><code> npm i -S hexo-prism-plugin</code></pre><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><pre><code>npm install hexo-generator-search --save</code></pre><h3 id="中文链接转拼音"><a href="#中文链接转拼音" class="headerlink" title="中文链接转拼音"></a>中文链接转拼音</h3><pre><code>npm i hexo-permalink-pinyin --save</code></pre><h3 id="文章字数统计插件"><a href="#文章字数统计插件" class="headerlink" title="文章字数统计插件"></a>文章字数统计插件</h3><pre><code>npm i --save hexo-wordcount</code></pre><h3 id="添加-RSS-订阅支持"><a href="#添加-RSS-订阅支持" class="headerlink" title="添加 RSS 订阅支持"></a>添加 RSS 订阅支持</h3><pre><code>npm install hexo-generator-feed --save</code></pre><h3 id="添加百度sitemap-xml"><a href="#添加百度sitemap-xml" class="headerlink" title="添加百度sitemap.xml"></a>添加百度sitemap.xml</h3><pre><code> npm install hexo-generator-sitemap --save-dev npm install hexo-generator-baidu-sitemap --save-dev</code></pre><h3 id="生成静态文件和部署到github"><a href="#生成静态文件和部署到github" class="headerlink" title="生成静态文件和部署到github"></a>生成静态文件和部署到github</h3><pre><code>hexo ghexo d</code></pre>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Django阿里云部署详解之服务器安装安装Nginx</title>
      <link href="/2020/02/22/django-a-li-yun-bu-shu-xiang-jie-zhi-fu-wu-qi-an-zhuang-an-zhuang-nginx/"/>
      <url>/2020/02/22/django-a-li-yun-bu-shu-xiang-jie-zhi-fu-wu-qi-an-zhuang-an-zhuang-nginx/</url>
      
        <content type="html"><![CDATA[<h3 id="Django阿里云部署详解之服务器安装安装Nginx"><a href="#Django阿里云部署详解之服务器安装安装Nginx" class="headerlink" title="Django阿里云部署详解之服务器安装安装Nginx"></a>Django阿里云部署详解之服务器安装安装Nginx</h3><h4 id="（本系统就是阿里云部署的）"><a href="#（本系统就是阿里云部署的）" class="headerlink" title="（本系统就是阿里云部署的）"></a>（本系统就是阿里云部署的）</h4><h4 id="大家照做就行，这我试了很多次才总结出来的"><a href="#大家照做就行，这我试了很多次才总结出来的" class="headerlink" title="大家照做就行，这我试了很多次才总结出来的"></a>大家照做就行，这我试了很多次才总结出来的</h4><p>这个在开始准备环境时已经安装了</p><pre><code>yum install epel-releaseyum install -y nginx</code></pre><p>在/etc/nginx/nginx.conf里面修改sever{}</p><pre><code>    listen 443 ssl;    server_name 你的网站名字;    charset utf-8;    ssl_certificate cert/?.pem;    ssl_certificate_key cert/?.key;    ssl_session_timeout 5m;    ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;    ssl_prefer_server_ciphers on;    client_max_body_size 75M;    location /static {        alias /home/tests/static_collected;    }    location /media {        alias /home/tests/media;    }    location / {        uwsgi_pass 127.0.0.1:8001;        include /etc/nginx/uwsgi_params;    }</code></pre><p><strong>查看运行nginx</strong></p><pre><code>nginx -tservice nginx restartservice nginx reloadservice nginx startservice nginx statusservice nginx stop</code></pre><p><strong>找到运行的端口号</strong></p><pre><code>netstat -antp</code></pre><p><strong>杀死进程</strong></p><pre><code>kill -9  编号</code></pre>]]></content>
      
      
      <categories>
          
          <category> Django </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>github ssh 连接失败解决方法</title>
      <link href="/2020/02/21/github-ssh-lian-jie-shi-bai-jie-jue-fang-fa/"/>
      <url>/2020/02/21/github-ssh-lian-jie-shi-bai-jie-jue-fang-fa/</url>
      
        <content type="html"><![CDATA[<h3 id="github-ssh-连接失败"><a href="#github-ssh-连接失败" class="headerlink" title="github ssh 连接失败"></a>github ssh 连接失败</h3><p>$ ssh -T <a href="mailto:git@github.com">git@github.com</a><br>ssh: connect to host github.com port 22: Connection timed out</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>在.ssh下面新加config文件，不要后缀，内容如下：</p><pre><code>Host github.comUser 417952939@qq.comHostname ssh.github.comPreferredAuthentications publickeyIdentityFile ~/.ssh/id_rsaPort 22</code></pre>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Django阿里云部署详解之服务器安装安装uwsgi</title>
      <link href="/2020/02/16/django-a-li-yun-bu-shu-xiang-jie-zhi-fu-wu-qi-an-zhuang-an-zhuang-uwsgi/"/>
      <url>/2020/02/16/django-a-li-yun-bu-shu-xiang-jie-zhi-fu-wu-qi-an-zhuang-an-zhuang-uwsgi/</url>
      
        <content type="html"><![CDATA[<h3 id="Django阿里云部署详解之服务器安装安装uwsgi"><a href="#Django阿里云部署详解之服务器安装安装uwsgi" class="headerlink" title="Django阿里云部署详解之服务器安装安装uwsgi"></a>Django阿里云部署详解之服务器安装安装uwsgi</h3><h4 id="（本系统就是阿里云部署的）"><a href="#（本系统就是阿里云部署的）" class="headerlink" title="（本系统就是阿里云部署的）"></a>（本系统就是阿里云部署的）</h4><h4 id="大家照做就行，这我试了很多次才总结出来的"><a href="#大家照做就行，这我试了很多次才总结出来的" class="headerlink" title="大家照做就行，这我试了很多次才总结出来的"></a>大家照做就行，这我试了很多次才总结出来的</h4><h4 id="1、安装uwsgi"><a href="#1、安装uwsgi" class="headerlink" title="1、安装uwsgi"></a>1、安装uwsgi</h4><p>注意：<br>    1）在系统环境安装，非虚拟环境<br>    2）使用对应python版本安装<br>    3）要先安装python开发包<br><strong>pip install uwsgi</strong></p><h4 id="2、测试-uwsgi-是否正常"><a href="#2、测试-uwsgi-是否正常" class="headerlink" title="2、测试 uwsgi 是否正常"></a>2、测试 uwsgi 是否正常</h4><p>vim test.py<br>新建 test.py 文件，内容如下：</p><pre><code>def application(env, start_response):    start_response(&#39;200 OK&#39;, [(&#39;Content-Type&#39;,&#39;text/html&#39;)])    return &quot;Hello World&quot;</code></pre><p>然后在终端运行：</p><pre><code>uwsgi --wsgi-file test.py  --http :8001</code></pre><p>注意：需要开启端口才可以正常访问<br><strong>杀死uwsgi</strong></p><pre><code>ps -aux | grep uwsgi + awk &#39;{print $2}&#39; | xargs kill -9</code></pre><h4 id="3、可以用uwsgi的http协议访问django写的网站"><a href="#3、可以用uwsgi的http协议访问django写的网站" class="headerlink" title="3、可以用uwsgi的http协议访问django写的网站"></a>3、可以用uwsgi的http协议访问django写的网站</h4><p>执行如下命令可以测试自己的项目</p><pre><code>uwsgi --http :8001 --chdir /home/tests --home /home/test_env --module tests.wsgi:application</code></pre><p><strong>mkdir tests_uwsgi</strong><br><strong>tests.ini</strong></p><pre><code>[uwsgi]chdir=/home/tests    #项目地质home=/home/test_env  #环境地质module=tests.wsgi:applicationmaster=Trueprocesses=4          #工作进程数harakiri=60          #60秒重启max-requests=5000      #服务5000个请求后重新启动进程socket=127.0.0.1:8001uid=nginxgid=nginxpidfile=/home/tests_uwsgi/master.piddaemonize=/home/tests_uwsgi/tests.logvacuum=True   #清理</code></pre><p><strong>初始化ini</strong><br>uwsgi –ini /home/tests_uwsgi/tests.ini</p><p><strong>重新运行uwsgi</strong><br>uwsgi –reload /home/tests_uwsgi/master.pid</p><p><strong>查看uwsgi是否运行</strong><br>ps aux | grep uwsgi</p>]]></content>
      
      
      <categories>
          
          <category> Django </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Django阿里云部署详解之服务器安装MySQL8.0系列的版本（服务器是Linux的系统）</title>
      <link href="/2020/02/10/django-a-li-yun-bu-shu-xiang-jie-zhi-fu-wu-qi-an-zhuang-mysql8.0-xi-lie-de-ban-ben-fu-wu-qi-shi-linux-de-xi-tong/"/>
      <url>/2020/02/10/django-a-li-yun-bu-shu-xiang-jie-zhi-fu-wu-qi-an-zhuang-mysql8.0-xi-lie-de-ban-ben-fu-wu-qi-shi-linux-de-xi-tong/</url>
      
        <content type="html"><![CDATA[<h3 id="Django阿里云部署详解之服务器安装MySQL8-0系列的版本（服务器是Linux的系统）"><a href="#Django阿里云部署详解之服务器安装MySQL8-0系列的版本（服务器是Linux的系统）" class="headerlink" title="Django阿里云部署详解之服务器安装MySQL8.0系列的版本（服务器是Linux的系统）"></a>Django阿里云部署详解之服务器安装MySQL8.0系列的版本（服务器是Linux的系统）</h3><h4 id="（本系统就是阿里云部署的）"><a href="#（本系统就是阿里云部署的）" class="headerlink" title="（本系统就是阿里云部署的）"></a>（本系统就是阿里云部署的）</h4><h4 id="大家照做就行，这我试了很多次才总结出来的"><a href="#大家照做就行，这我试了很多次才总结出来的" class="headerlink" title="大家照做就行，这我试了很多次才总结出来的"></a>大家照做就行，这我试了很多次才总结出来的</h4><pre><code>wget https://dev.mysql.com/get/mysql80-community-release-el7-3.noarch.rpmrpm -ivh mysql80-community-release-el7-3.noarch.rpmyum -y install mysql-community-server</code></pre><h4 id="修改-etc-my-cnf"><a href="#修改-etc-my-cnf" class="headerlink" title="修改/etc/my.cnf"></a>修改/etc/my.cnf</h4><pre><code>[mysqld]# 设置mysql的安装目录basedir=/usr/local/mysql# 设置mysql数据库的数据的存放目录datadir=/usr/local/mysql/data# 设置默认使用的端口port=3306# 允许最大连接数max_connections=200# 允许连接失败的次数。这是为了防止有人试图攻击数据库max_connect_errors=10# 服务端使用的字符集character-set-server=utf8mb4# 数据库字符集对应一些排序等规则使用的字符集collation-server=utf8mb4_general_ci# 创建新表时将使用的默认存储引擎default-storage-engine=INNODB# 默认使用“mysql_native_password”插件作为认证加密方式# MySQL8.0默认认证加密方式为caching_sha2_passworddefault_authentication_plugin=mysql_native_password#server_id=socket=/var/lib/mysql/mysql.sock#这里可以加也可以不加，如果有lc_messages_dir警告就加上，，lc_messages_dir=/usr/local/mysql/share lc_messages=en_USlog-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pid</code></pre><h4 id="创建目录（一定要，不然初始化不成功，因为是你自己设定好的文件夹）"><a href="#创建目录（一定要，不然初始化不成功，因为是你自己设定好的文件夹）" class="headerlink" title="创建目录（一定要，不然初始化不成功，因为是你自己设定好的文件夹）"></a>创建目录（一定要，不然初始化不成功，因为是你自己设定好的文件夹）</h4><p>这里要自己创建/usr/local/mysql和/usr/local/mysql/share<br>报错的时候这里加（看自己报什么错，可选）</p><pre><code>copy /usr/share/mysql-8.0/bulgarian/errmsg.sys /usr/local/mysql/share </code></pre><pre><code>systemctl start mysqldsystemctl status mysqldsystemctl stop mysqld</code></pre><p>初始化MySQL(有时候不一定要，在/var/log/mysqld.log里面可能有，基本上systemctl start mysqld是可以找到临时密码的)</p><pre><code>mysqld --initialize --user=mysql</code></pre><h4 id="重置root密码"><a href="#重置root密码" class="headerlink" title="重置root密码"></a>重置root密码</h4><pre><code>alter user  &#39;root&#39;@&#39;localhost&#39; identified by &#39;your passwoed&#39;;</code></pre><h4 id="新建新账户针对一个数据库（为了安全起见的）"><a href="#新建新账户针对一个数据库（为了安全起见的）" class="headerlink" title="新建新账户针对一个数据库（为了安全起见的）"></a>新建新账户针对一个数据库（为了安全起见的）</h4><pre><code>show databases;create database ?_db default charset=utf8 default collate utf8_unicode_ci;create user &#39;&#39;@&#39;localhost&#39; identified by &#39;&#39;;grant all privileges on ?_db.* to &#39;xiezhouHCH&#39;@&#39;localhost&#39;;flush privileges;</code></pre><p>新账户只能访问指定的数据库</p>]]></content>
      
      
      <categories>
          
          <category> Django </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Django阿里云部署详解之服务器安装虚拟环境、Python</title>
      <link href="/2020/01/31/django-a-li-yun-bu-shu-xiang-jie-zhi-fu-wu-qi-an-zhuang-xu-ni-huan-jing-python/"/>
      <url>/2020/01/31/django-a-li-yun-bu-shu-xiang-jie-zhi-fu-wu-qi-an-zhuang-xu-ni-huan-jing-python/</url>
      
        <content type="html"><![CDATA[<h3 id="Django阿里云部署详解之服务器安装虚拟环境、Python"><a href="#Django阿里云部署详解之服务器安装虚拟环境、Python" class="headerlink" title="Django阿里云部署详解之服务器安装虚拟环境、Python"></a>Django阿里云部署详解之服务器安装虚拟环境、Python</h3><h4 id="（本系统就是阿里云部署的）"><a href="#（本系统就是阿里云部署的）" class="headerlink" title="（本系统就是阿里云部署的）"></a>（本系统就是阿里云部署的）</h4><h4 id="大家照做就行，这我试了很多次才总结出来的"><a href="#大家照做就行，这我试了很多次才总结出来的" class="headerlink" title="大家照做就行，这我试了很多次才总结出来的"></a>大家照做就行，这我试了很多次才总结出来的</h4><pre><code>yum update -yyum -y install gcc gcc-c++yum -y groupinstall &quot;Development tools&quot;yum -y install zlib zlib-devel openssl openssl-devel ncurses-devel sqlite sqlite-devel bzip2-deve readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-develyum install epel-releaseyum -y install nginxyum install libffi-devel -y</code></pre><pre><code>wget https://www.python.org/ftp/python/3.8.1/Python-3.8.1.tgz</code></pre><pre><code>mkdir -p /usr/local/python3tar -zxvf Python-3.8.1.tgz</code></pre><pre><code>cd Python-3.8.1./configure --prefix=/usr/local/python3make &amp;&amp; make install</code></pre><pre><code>ln -s /usr/local/python3/bin/python3 /usr/bin/python3</code></pre><p>原来的pip自己备份下：<br>mv /usr/bin/pip /usr/bin/pip.bak<br>这里使用新的pip</p><pre><code>ln -s /usr/local/python3/bin/pip3 /usr/bin/pippip install --upgrade pip</code></pre><h4 id="修改系统默认的python为自己装的版本"><a href="#修改系统默认的python为自己装的版本" class="headerlink" title="修改系统默认的python为自己装的版本"></a>修改系统默认的python为自己装的版本</h4><p>先找到新版本python安装位置，<br>然后</p><pre><code>vi /etc/profile.d/python.sh</code></pre><p>创建新文件，然后输入</p><pre><code>alias python=&#39;/usr/bin/python3&#39;  # 此处的路径为新版本python的路径，通过我上一篇文章来</code></pre><p>查找此路径<br>重启会话使配置生效</p><pre><code>source /etc/profile.d/python.sh</code></pre><pre><code>pip install --upgrade pippip install --upgrade setuptoolspip install virtualenv</code></pre><pre><code>ln -s /usr/local/python3/bin/virtualenv /usr/bin/virtualenvcd /home/virtualenv --python=/usr/bin/python test_envvirtualenv test_env(也一样是python3，之前已经修改了默认Python版本)source test_env/bin/activate</code></pre><h4 id="进入虚拟环境安装各个软件"><a href="#进入虚拟环境安装各个软件" class="headerlink" title="进入虚拟环境安装各个软件"></a>进入虚拟环境安装各个软件</h4><pre><code>pip install Django==3.0.2 pip install uwsgiln -s /usr/local/python3/bin/uwsgi /usr/bin/uwsgipip install pillowpip install django-mdeditorpip install Markdown</code></pre><h4 id="特别的，安装mysqlclient需要系统安装过mysql-devel，不然报错"><a href="#特别的，安装mysqlclient需要系统安装过mysql-devel，不然报错" class="headerlink" title="特别的，安装mysqlclient需要系统安装过mysql-devel，不然报错"></a>特别的，安装mysqlclient需要系统安装过mysql-devel，不然报错</h4><pre><code>yum -y install mysql-devel</code></pre><pre><code>pip install mysqlclient</code></pre>]]></content>
      
      
      <categories>
          
          <category> Django </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Django开启虚拟环境（Windows中）</title>
      <link href="/2020/01/10/django-kai-qi-xu-ni-huan-jing-windows-zhong/"/>
      <url>/2020/01/10/django-kai-qi-xu-ni-huan-jing-windows-zhong/</url>
      
        <content type="html"><![CDATA[<h3 id="Django开启虚拟环境（Windows中）"><a href="#Django开启虚拟环境（Windows中）" class="headerlink" title="Django开启虚拟环境（Windows中）"></a>Django开启虚拟环境（Windows中）</h3><p>开启本地虚拟环境<br>1)避免多个项目之间python库的冲突<br>2)完整便捷导出python库的列表</p><pre><code>pip install virtualenv</code></pre><p>创建：virtualenv &lt;虚拟环境名称&gt;</p><pre><code>virtualenv blog_env</code></pre><p>启动：Scripts\activate</p><pre><code>pip install Django</code></pre><p>退出：deactivate</p><p>建立目录</p><pre><code>django-admin startproject django_introduction</code></pre><p>运行</p><pre><code>python manage.py runserver 80</code></pre><p>创建应用</p><pre><code>python manage.py startapp blog</code></pre><p>制作数据迁移</p><pre><code>python manage.py makemigrations (app)</code></pre><p>迁移动作</p><pre><code>python manage.py migrate</code></pre><pre><code>python manage.py createsuperuser</code></pre>]]></content>
      
      
      <categories>
          
          <category> Django </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pip 一键导出和安装</title>
      <link href="/2020/01/10/pip-yi-jian-dao-chu-he-an-zhuang/"/>
      <url>/2020/01/10/pip-yi-jian-dao-chu-he-an-zhuang/</url>
      
        <content type="html"><![CDATA[<h3 id="pip-一键导出和安装"><a href="#pip-一键导出和安装" class="headerlink" title="pip 一键导出和安装"></a>pip 一键导出和安装</h3><p>将本地使用的虚拟环境pip install 的所有软件导出<br>pip freeze &gt; requirements.txt</p><p>另一个新的环境安装<br>pip install -r requirements.txt</p>]]></content>
      
      
      <categories>
          
          <category> pip </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pip </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>中华人民共和国70周年生日快乐，我爱你中国！！！！！！！</title>
      <link href="/2019/10/01/zhong-hua-ren-min-gong-he-guo-70-zhou-nian-qing/"/>
      <url>/2019/10/01/zhong-hua-ren-min-gong-he-guo-70-zhou-nian-qing/</url>
      
        <content type="html"><![CDATA[<h2 id="热烈祝贺中华人民共和国70周年生日快乐，我爱你中国！！！！！！！"><a href="#热烈祝贺中华人民共和国70周年生日快乐，我爱你中国！！！！！！！" class="headerlink" title="热烈祝贺中华人民共和国70周年生日快乐，我爱你中国！！！！！！！"></a>热烈祝贺中华人民共和国70周年生日快乐，我爱你中国！！！！！！！</h2><h3 id="我爱你祖国！！！！！！！"><a href="#我爱你祖国！！！！！！！" class="headerlink" title="我爱你祖国！！！！！！！"></a>我爱你祖国！！！！！！！</h3><h3 id="让我们一起祝贺祖国母亲生日快乐！！！！！！！"><a href="#让我们一起祝贺祖国母亲生日快乐！！！！！！！" class="headerlink" title="让我们一起祝贺祖国母亲生日快乐！！！！！！！"></a>让我们一起祝贺祖国母亲生日快乐！！！！！！！</h3><h3 id="祝祖国母亲繁荣昌盛！！！！！！！"><a href="#祝祖国母亲繁荣昌盛！！！！！！！" class="headerlink" title="祝祖国母亲繁荣昌盛！！！！！！！"></a>祝祖国母亲繁荣昌盛！！！！！！！</h3><p><img src="/medias/%E5%BA%86%E7%A5%9D%E4%B8%AD%E5%8D%8E%E4%BA%BA%E6%B0%91%E5%85%B1%E5%92%8C%E5%9B%BD%E6%88%90%E7%AB%8B70%E5%91%A8%E5%B9%B4%E6%B4%BB%E5%8A%A8%E6%A0%87%E8%AF%86.PNG" alt="庆祝中华人民共和国成立70周年活动标识"></p><p><img src="/medias/70%E5%91%A8%E5%B9%B4%E5%8D%8E%E8%AF%9E.jpg" alt="70周年华诞"></p><h3 id="今天是你的生日"><a href="#今天是你的生日" class="headerlink" title="今天是你的生日"></a>今天是你的生日</h3><p>今天是你的生日我的中国<br>清晨我放飞一群白鸽<br>为你衔来一枚橄榄叶<br>鸽子在崇山峻岭飞过<br>我们祝福你的生日我的中国<br>愿你永远没有忧患永远宁静<br>我们祝福你的生日我的中国<br>这是儿女们心中期望的歌<br>今天是你的生日我的中国<br>清晨我放飞一群白鸽<br>为你带回远方儿女的思念<br>鸽子在茫茫海天飞过<br>我们祝福你的生日我的中国<br>愿你月儿常圆儿女永远欢乐<br>我们祝福你的生日我的中国<br>这是儿女在远方爱的诉说<br>今天是你的生日我的中国<br>清晨我放飞一群白鸽<br>为你衔来一棵金色麦穗<br>鸽子在风风雨雨中飞过<br>我们祝福你的生日我的中国<br>愿你逆风起飞雨中获得收获<br>我们祝福你的生日我的中国<br>这是儿女们心中期望的歌</p>]]></content>
      
      
      <categories>
          
          <category> 中华人民共和国70周年 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 中华人民共和国70周年 </tag>
            
            <tag> 祖国生日 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>解决IDEA创建Maven项目速度慢问题</title>
      <link href="/2019/08/02/jie-jue-idea-chuang-jian-maven-xiang-mu-su-du-man-wen-ti/"/>
      <url>/2019/08/02/jie-jue-idea-chuang-jian-maven-xiang-mu-su-du-man-wen-ti/</url>
      
        <content type="html"><![CDATA[<p>add Maven Property<br>Name:archetypeCatalog<br>Value:internal</p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IDEA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark-submit提交集群执行</title>
      <link href="/2019/07/29/spark-submit-ti-jiao-ji-qun-zhi-xing/"/>
      <url>/2019/07/29/spark-submit-ti-jiao-ji-qun-zhi-xing/</url>
      
        <content type="html"><![CDATA[<p>spark-submit<br><code>--</code>master yarn-cluster    //集群启动<br><code>--</code>num-executors 1        //分配多少个进程<br><code>--</code>driver-memory 500m  //driver内存<br><code>--</code>executor-memory 1g //进程内存<br><code>--</code>executor-cores 1       //开多少个核，线程<br><code>--</code>jars $(echo /usr/chl/spark8/jars/*.jar | tr ‘ ‘ ‘,’) //加载jar<br><code>--</code>class com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> SparkStreaming </tag>
            
            <tag> spark-sumbit </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IDEA中jar冲突查找快捷键快速定位</title>
      <link href="/2019/07/27/idea-zhong-jar-chong-tu-cha-zhao-kuai-jie-jian-kuai-su-ding-wei/"/>
      <url>/2019/07/27/idea-zhong-jar-chong-tu-cha-zhao-kuai-jie-jian-kuai-su-ding-wei/</url>
      
        <content type="html"><![CDATA[<p><strong>Ctrl+Alt+Shift+N</strong></p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IDEA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>企业网络日志分析</title>
      <link href="/2019/07/27/qi-ye-wang-luo-ri-zhi-fen-xi/"/>
      <url>/2019/07/27/qi-ye-wang-luo-ri-zhi-fen-xi/</url>
      
        <content type="html"><![CDATA[<h3 id="一、背景数据介绍"><a href="#一、背景数据介绍" class="headerlink" title="一、背景数据介绍"></a>一、背景数据介绍</h3><p><strong>1.    WiFi有哪些数据？</strong><br>手机号<br>机构<br>机构<br>机构<br>网页快照<br>论坛帖子<br>微博<br>邮件<br>IM聊天<br>表单数据<br>APP使用</p><p><strong>2.    WiFi价值</strong><br>客户体验：方便客户、基础设施<br>客户数据：精准营销、获取客户上网行为、获取客户信息、客户接触渠道</p><p><strong>3.    WiFi数据获取</strong><br>Wi-Fi 网络可以捕获附近智能手机的 IMSI 号码，无线跟踪并监控用户的根源在于智能手机（包括 Android 和 iOS 设备）连接 Wi-Fi 网络的方式。</p><p>在大多数现代移动操作系统中有两种广泛实现的协议：<br>可扩展认证协议（EAP）<br>认证和密钥协商（AKA）协议</p><p>这些协议允许智能手机通过自身设备的 IMSI 号码切换登录到已知的 Wi-Fi 网络，实现 WiFi 网络自动连接而无需所有者交互。</p><p><strong>4.    wifi数据应用</strong><br><img src="/medias/wifi%E6%95%B0%E6%8D%AE%E5%BA%94%E7%94%A8.PNG" alt="wifi数据应用"></p><p>画像系统<br><img src="/medias/%E7%94%BB%E5%83%8F%E7%B3%BB%E7%BB%9F.PNG" alt="画像系统"></p><p><strong>5.    数据架构</strong><br><img src="/medias/%E7%BD%91%E7%BB%9C%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E6%8D%AE%E6%9E%B6%E6%9E%84%E5%9B%BE.PNG" alt="网络用户行为数据架构图"></p><p><strong>6.    数据结构</strong><br>（1）    <strong>文件命名</strong><br>数据类型_来源_UUID.txt<br>如BASE_SOURCE_UUID.txt</p><p>定一套字段标准 ，类型标准<br>（2）    <strong>字段</strong><br>（3）  <strong>通用字段</strong></p><table><thead><tr><th align="center">参数1</th><th align="center">参数2</th><th align="center">参数3</th><th align="center">参数4</th></tr></thead><tbody><tr><td align="center">imei</td><td align="center">imei号，手机唯一识别码</td><td align="center"></td><td align="center">手机IMEI码由15-17位数字组成</td></tr><tr><td align="center">imsi</td><td align="center">IMSI，SIM卡唯一识别码</td><td align="center">460011418603055</td><td align="center">14-15位数字</td></tr><tr><td align="center">longitude</td><td align="center">经度</td><td align="center"></td><td align="center">精确到小数点6位</td></tr><tr><td align="center">latitude</td><td align="center">纬度</td><td align="center"></td><td align="center">精确到小数点6位</td></tr><tr><td align="center">phone_mac</td><td align="center">手机MAC</td><td align="center"></td><td align="center">格式需要统一（清洗）aa-aa-aa-aa-aa-aa（范围1-9，a-f）</td></tr><tr><td align="center">device_mac</td><td align="center">采集设备MAC</td><td align="center"></td><td align="center">格式需要统一（清洗）aa-aa-aa-aa-aa-aa（范围任意数字加字母）</td></tr><tr><td align="center">device_number</td><td align="center">采集设备号</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">collect_time</td><td align="center">collect_time</td><td align="center"></td><td align="center"></td></tr></tbody></table><p><strong>微信数据(wechat)</strong></p><table><thead><tr><th align="center">参数1</th><th align="center">参数2</th><th align="center">参数3</th><th align="center">参数4</th></tr></thead><tbody><tr><td align="center">username</td><td align="center">微信昵称</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">phone</td><td align="center">手机号</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">object_username</td><td align="center">对方微信号</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">send_message</td><td align="center">发送内容（不能破解）</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">accept_message</td><td align="center">接收内容（不能破解）</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">message_time</td><td align="center">通信时间</td><td align="center"></td><td align="center"></td></tr></tbody></table><p><strong>邮箱数据(Mail)</strong></p><table><thead><tr><th align="center">参数1</th><th align="center">参数2</th><th align="center">参数3</th><th align="center">参数4</th></tr></thead><tbody><tr><td align="center">send_mail</td><td align="center">发送邮箱</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">send_time</td><td align="center">发送时间</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">accept_mail</td><td align="center">接收邮箱</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">accept_time</td><td align="center">接收时间</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">mail_content</td><td align="center">发送内容</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">mail_type</td><td align="center">发送还是接收</td><td align="center"></td><td align="center">send  accept</td></tr></tbody></table><p><strong>搜索数据(Search)</strong></p><table><thead><tr><th align="center">参数1</th><th align="center">参数2</th><th align="center">参数3</th><th align="center">参数4</th></tr></thead><tbody><tr><td align="center">search_content</td><td align="center">搜索内容</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">search_url</td><td align="center">搜索URL</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">search_type</td><td align="center">搜索引擎</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">search_time</td><td align="center">搜索时间</td><td align="center"></td><td align="center"></td></tr></tbody></table><p><strong>基础数据(Base)</strong></p><table><thead><tr><th align="center">参数1</th><th align="center">参数2</th><th align="center">参数3</th><th align="center">参数4</th></tr></thead><tbody><tr><td align="center">name</td><td align="center">姓名</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">is_marry</td><td align="center">是否已婚</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">phone</td><td align="center">手机号</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">address</td><td align="center">户籍所在地</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">address_new</td><td align="center">现在居住地址</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">birthday</td><td align="center">出生日期</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">car_number</td><td align="center">车牌号</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">idcard</td><td align="center">身份证</td><td align="center"></td><td align="center"></td></tr></tbody></table><p>问题：数据结构，数据字段如何确定？<br>根据实际的需求自己确定。</p><h3 id="二．基础架构搭建"><a href="#二．基础架构搭建" class="headerlink" title="二．基础架构搭建"></a>二．基础架构搭建</h3><h4 id="1、创建Maven父项"><a href="#1、创建Maven父项" class="headerlink" title="1、创建Maven父项"></a>1、创建Maven父项</h4><p><strong>总的pom.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;  &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;  &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;  &lt;packaging&gt;pom&lt;/packaging&gt;  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;  &lt;modules&gt;    &lt;module&gt;xz_bigdata_common&lt;/module&gt;    &lt;module&gt;xz_bigdata_es&lt;/module&gt;    &lt;module&gt;xz_bigdata_flume&lt;/module&gt;    &lt;module&gt;xz_bigdata_hbase&lt;/module&gt;    &lt;module&gt;xz_bigdata_kafka&lt;/module&gt;    &lt;module&gt;xz_bigdata_redis&lt;/module&gt;    &lt;module&gt;xz_bigdata_resources&lt;/module&gt;    &lt;module&gt;xz_bigdata_spark&lt;/module&gt;  &lt;/modules&gt;  &lt;name&gt;xz_bigdata2&lt;/name&gt;  &lt;properties&gt;    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;    &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;    &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;    &lt;cdh.version&gt;cdh5.14.0&lt;/cdh.version&gt;    &lt;junit.version&gt;4.12&lt;/junit.version&gt;    &lt;org.slf4j.version&gt;1.7.5&lt;/org.slf4j.version&gt;    &lt;zookeeper.version&gt;3.4.5&lt;/zookeeper.version&gt;    &lt;scala.version&gt;2.10.5&lt;/scala.version&gt;  &lt;/properties&gt;  &lt;repositories&gt;    &lt;repository&gt;      &lt;id&gt;Akka repository&lt;/id&gt;      &lt;url&gt;https://repo.akka.io/releases&lt;/url&gt;    &lt;/repository&gt;    &lt;!--cloudera依赖--&gt;    &lt;repository&gt;      &lt;id&gt;cloudera&lt;/id&gt;      &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;    &lt;/repository&gt;  &lt;/repositories&gt;  &lt;!--日志依赖--&gt;  &lt;dependencies&gt;    &lt;dependency&gt;      &lt;groupId&gt;org.slf4j&lt;/groupId&gt;      &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;      &lt;version&gt;${org.slf4j.version}&lt;/version&gt;    &lt;/dependency&gt;  &lt;/dependencies&gt;  &lt;build&gt;    &lt;plugins&gt;      &lt;plugin&gt;        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;        &lt;version&gt;3.1&lt;/version&gt;        &lt;configuration&gt;          &lt;source&gt;1.8&lt;/source&gt;          &lt;target&gt;1.8&lt;/target&gt;          &lt;encoding&gt;UTF-8&lt;/encoding&gt;        &lt;/configuration&gt;      &lt;/plugin&gt;    &lt;/plugins&gt;  &lt;/build&gt;&lt;/project&gt;</code></pre><h4 id="2、项目整体结构"><a href="#2、项目整体结构" class="headerlink" title="2、项目整体结构"></a>2、项目整体结构</h4><p><img src="/medias/xz_bigdata2%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84.PNG" alt="xz_bigdata2整体结构"></p><h4 id="3、创建子模块"><a href="#3、创建子模块" class="headerlink" title="3、创建子模块"></a>3、创建子模块</h4><p>选中xz_bigdata2，右键选择Module，新建maven子模块，上面图中的那些模块都是这样创建的。<br>注意：开发时使用jdk1.8以上版本，里面使用了jdk1.8特有的内容，低版本开发是报错的，使用jdk1.8方便开发。</p><p>ctrl+shift+alt+s：打开Project Structure里面可以进行操作。</p><p>ctrl+alt+s：打开Settings，可以配置本地Maven（在Build,Execution,Deployment下面的Build Tools下面的Maven配置自己的本地Maven仓库路径）。</p><p>Settings里面还可以看见之前说的Plugins，安装插件，Maven Helper以及后面的Scala插件都可以这里安装。</p><h3 id="三、Common开发"><a href="#三、Common开发" class="headerlink" title="三、Common开发"></a>三、Common开发</h3><p><strong>pom.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_common&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;        &lt;ant.version&gt;1.9.1&lt;/ant.version&gt;        &lt;jaxen.version&gt;1.1.6&lt;/jaxen.version&gt;        &lt;guava.version&gt;12.0.1&lt;/guava.version&gt;        &lt;dom4j.version&gt;1.6.1&lt;/dom4j.version&gt;        &lt;fastjson.version&gt;1.2.5&lt;/fastjson.version&gt;        &lt;disruptor.version&gt;3.3.6&lt;/disruptor.version&gt;        &lt;org.slf4j.version&gt;1.7.5&lt;/org.slf4j.version&gt;        &lt;commons.io.version&gt;2.4&lt;/commons.io.version&gt;        &lt;httpclient.version&gt;4.2.5&lt;/httpclient.version&gt;        &lt;commons.exec.version&gt;1.3&lt;/commons.exec.version&gt;        &lt;commons.lang.version&gt;2.4&lt;/commons.lang.version&gt;        &lt;commons-vfs2.version&gt;2.1&lt;/commons-vfs2.version&gt;        &lt;commons.math3.version&gt;3.4.1&lt;/commons.math3.version&gt;        &lt;commons.logging.version&gt;1.2&lt;/commons.logging.version&gt;        &lt;commons-httpclient.version&gt;3.1&lt;/commons-httpclient.version&gt;        &lt;commons.collections4.version&gt;4.1&lt;/commons.collections4.version&gt;        &lt;commons.configuration.version&gt;1.6&lt;/commons.configuration.version&gt;        &lt;mysql.connector.version&gt;5.1.46&lt;/mysql.connector.version&gt;        &lt;commons-dbutils.version&gt;1.6&lt;/commons-dbutils.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;commons-dbutils&lt;/groupId&gt;            &lt;artifactId&gt;commons-dbutils&lt;/artifactId&gt;            &lt;version&gt;${commons-dbutils.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;mysql&lt;/groupId&gt;            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;            &lt;version&gt;5.1.46&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;            &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;            &lt;version&gt;${org.slf4j.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;            &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;            &lt;version&gt;${org.slf4j.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;commons-io&lt;/groupId&gt;            &lt;artifactId&gt;commons-io&lt;/artifactId&gt;            &lt;version&gt;${commons.io.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;commons-lang&lt;/groupId&gt;            &lt;artifactId&gt;commons-lang&lt;/artifactId&gt;            &lt;version&gt;${commons.lang.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;commons-configuration&lt;/groupId&gt;            &lt;artifactId&gt;commons-configuration&lt;/artifactId&gt;            &lt;version&gt;${commons.configuration.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;dom4j&lt;/groupId&gt;            &lt;artifactId&gt;dom4j&lt;/artifactId&gt;            &lt;version&gt;${dom4j.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;            &lt;version&gt;${fastjson.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- &lt;dependency&gt;             &lt;groupId&gt;log4j&lt;/groupId&gt;             &lt;artifactId&gt;log4j&lt;/artifactId&gt;             &lt;version&gt;1.2.17&lt;/version&gt;         &lt;/dependency&gt;--&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><h4 id="1、config-ConfigUtil-java—配置文件读取"><a href="#1、config-ConfigUtil-java—配置文件读取" class="headerlink" title="1、config/ConfigUtil.java—配置文件读取"></a>1、config/ConfigUtil.java—配置文件读取</h4><pre><code>package com.hsiehchou.common.config;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.io.InputStream;import java.util.Properties;public class ConfigUtil {    private static Logger LOG = LoggerFactory.getLogger(ConfigUtil.class);    private static ConfigUtil configUtil;    public static ConfigUtil getInstance(){        if(configUtil == null){            configUtil = new ConfigUtil();        }        return configUtil;    }    public Properties getProperties(String path){        Properties properties = new Properties();        try {            LOG.info(&quot;开始加载配置文件&quot; + path);            //流式读取配置文件            InputStream insss = this.getClass().getClassLoader().getResourceAsStream(path);            properties = new Properties();            properties.load(insss);        } catch (IOException e) {            LOG.info(&quot;加载配置文件&quot; + path + &quot;失败&quot;);            LOG.error(null,e);        }        LOG.info(&quot;加载配置文件&quot; + path + &quot;成功&quot;);        System.out.println(&quot;文件内容：&quot;+properties);        return properties;    }    public static void main(String[] args) {        ConfigUtil instance = ConfigUtil.getInstance();        Properties properties = instance.getProperties(&quot;common/datatype.properties&quot;);        //Properties properties = instance.getProperties(&quot;spark/relation.properties&quot;);       // properties.get(&quot;relationfield&quot;);        System.out.println(properties);    }}</code></pre><h4 id="2、config-JsonReader-java"><a href="#2、config-JsonReader-java" class="headerlink" title="2、config/JsonReader.java"></a>2、config/JsonReader.java</h4><pre><code>package com.hsiehchou.common.config;import org.apache.commons.io.FileUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.File;public class JsonReader {    private static Logger LOG = LoggerFactory.getLogger(JsonReader.class);    public static String readJson(String json_path){        JsonReader jsonReader = new JsonReader();        return jsonReader.getJson(json_path);    }    private String getJson(String json_path){        String jsonStr = &quot;&quot;;        try {            String path = getClass().getClassLoader().getResource(json_path).toString();            path = path.replace(&quot;\\&quot;, &quot;/&quot;);            if (path.contains(&quot;:&quot;)) {                path = path.replace(&quot;file:/&quot;,&quot;&quot;);            }            jsonStr = FileUtils.readFileToString(new File(path), &quot;UTF-8&quot;);            LOG.error(&quot;读取json文件{}成功&quot;,path);        } catch (Exception e) {            LOG.error(&quot;读取json文件失败&quot;,e);        }        return jsonStr;    }}</code></pre><h4 id="3、adjuster-Adjuster-java—数据调整接口"><a href="#3、adjuster-Adjuster-java—数据调整接口" class="headerlink" title="3、adjuster/Adjuster.java—数据调整接口"></a>3、adjuster/Adjuster.java—数据调整接口</h4><pre><code>package com.hsiehchou.common.adjuster;/** * 数据调整接口 */public interface Adjuster&lt;T, E&gt; {    E doAdjust(T data);}</code></pre><h4 id="4、adjuster-StringAdjuster-java"><a href="#4、adjuster-StringAdjuster-java" class="headerlink" title="4、adjuster/StringAdjuster.java"></a>4、adjuster/StringAdjuster.java</h4><pre><code>package com.hsiehchou.common.adjuster;public abstract class StringAdjuster&lt;E&gt; implements Adjuster&lt;String, E&gt; {}</code></pre><h4 id="5、file-FileCommon-java"><a href="#5、file-FileCommon-java" class="headerlink" title="5、file/FileCommon.java"></a>5、file/FileCommon.java</h4><pre><code>package com.hsiehchou.common.file;import org.apache.commons.io.FileUtils;import org.apache.commons.io.IOUtils;import java.io.File;import java.io.FileInputStream;import java.io.IOException;import java.net.URL;import java.util.List;public class FileCommon {    private FileCommon(){}    /**     * 判断文件是否存在     * @param name     * @return     */    public static boolean exist(String name){        return exist(new File(name));    }    public static boolean exist(File file){        return file.exists();    }    /**     * 创建文件     * @param file     * @return     * @throws IOException     */    public static boolean createFile(String file) throws IOException {        return createFile(new File(file));    }    public static boolean createFile(File file) throws IOException {        if(!file.exists()){            if(file.isDirectory()){                return file.mkdirs();            }else{                File parentDir = file.getParentFile();                if(!parentDir.exists()) {                    if (parentDir.mkdirs()) {                        return file.createNewFile();                    }                }else{                    return file.createNewFile();                }            }        }        return true;    }    /**     * 读取文件内容 按行     * @param file     * @return     * @throws IOException     */    public static List&lt;String&gt; readLines(String file) throws IOException{        return readLines(new File(file), &quot;UTF-8&quot;);    }    public static List&lt;String&gt; readLines(String file, String encording) throws IOException{        return readLines(new File(file), encording);    }    public static List&lt;String&gt; readLines(File file, String encording) throws IOException {        List&lt;String&gt; lines = null;        if(FileCommon.exist(file)) {            FileInputStream fileInputStream = new FileInputStream(file);            lines = IOUtils.readLines(fileInputStream, encording);            fileInputStream.close();        }        return lines;    }    /**     * 获取文件前缀     * @param fileName     * @return     */    public static String getPrefix(String fileName){        String prefix = fileName;        int pos = fileName.lastIndexOf(&quot;.&quot;);        if (pos != -1){            prefix = fileName.substring(0,pos);        }        return prefix;    }    /**     * 获取文件名后缀     * @param fileName     * @return     */    public static String getFilePostfix(String fileName){        String filePostfix = fileName.substring(fileName.lastIndexOf(&quot;.&quot;) + 1);        return filePostfix.toLowerCase();    }    /**     * 删除文件     * @param filePath     * @return     */    public static boolean delFile(String filePath) {        boolean flag = false;        File file = new File(filePath);        if (file.isFile() &amp;&amp; file.exists()) {            flag = file.delete();        }        return flag;    }    /**     * 移动文件     * @param oldPath     * @param newPath     * @return     */    public static boolean mvFile(String oldPath,String newPath){        boolean flag = false;        File oldfile = new File(oldPath);        File newfile = new File(newPath);        if(oldfile.isFile() &amp;&amp; oldfile.exists()){            if(newfile.exists()){                delFile(newfile.getAbsolutePath());            }            flag = oldfile.renameTo(newfile);        }        return flag;    }    /**     * 删除目录     * @param dir     * @return     */    public static boolean deleteDir(File dir){        if (dir.isDirectory()) {            String[] children = dir.list();            //递归删除目录中的子目录下            if(children!=null){                for (int i=0; i&lt;children.length; i++) {                    boolean success = deleteDir(new File(dir, children[i]));                    if (!success) {                        return false;                    }                }            }        }        // 目录此时为空，可以删除        return dir.delete();    }    //递归建立目录，解压缩相关类中使用    public static void mkdirs(File file) {        File parent = file.getParentFile();        if (parent != null &amp;&amp; (!parent.exists())) {            parent.mkdirs();        }    }    public static String getJarFilePathByClass(String clazz) throws ClassNotFoundException {        return getJarFilePathByClass(Class.forName(clazz));    }    public static String getJarFileDirByClass(String clazz) throws ClassNotFoundException {        return getJarFileDirByClass(Class.forName(clazz));    }    public static String getJarFilePathByClass(Class&lt;?&gt; clazz){        return new File(clazz.getProtectionDomain().getCodeSource().getLocation().getFile()).getAbsolutePath();    }    public static String getJarFileDirByClass(Class&lt;?&gt; clazz){        return new File(getJarFilePathByClass(clazz)).getParent();    }    public static String getAbstractPath(String abstractPath) throws Exception{        URL url = FileCommon.class.getClassLoader().getResource(abstractPath);        System.out.println(&quot;配置文件路径为&quot; + url);        File file = new File(url.getFile());        String content= FileUtils.readFileToString(file,&quot;UTF-8&quot;);        return content;    }    public static String getAbstractPath111(String abstractPath) throws Exception{        File file = new File(abstractPath);        String content= FileUtils.readFileToString(file,&quot;UTF-8&quot;);        return content;    }}</code></pre><h4 id="6、filter—数据过滤顶层接口"><a href="#6、filter—数据过滤顶层接口" class="headerlink" title="6、filter—数据过滤顶层接口"></a>6、filter—数据过滤顶层接口</h4><pre><code>package com.hsiehchou.common.filter;/** * 数据过滤顶层接口 */public interface Filter&lt;T&gt; {    boolean filter(T obj);}</code></pre><h4 id="7、net-HttpRequest-java"><a href="#7、net-HttpRequest-java" class="headerlink" title="7、net/HttpRequest.java"></a>7、net/HttpRequest.java</h4><pre><code>package com.hsiehchou.common.net;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.*;import java.net.HttpURLConnection;import java.net.URL;import java.net.URLConnection;import java.net.URLEncoder;import java.util.Map;public class HttpRequest {    private static final Logger LOG = LoggerFactory.getLogger(HttpRequest.class);    /**     * 向指定URL发送GET方法的请求     * @param url  发送请求的URL     * @param param  请求参数，请求参数应该是 name1=value1&amp;name2=value2 的形式。     * @return URL  所代表远程资源的响应结果     */    public static String sendGet(String url, String param) {        String result = &quot;&quot;;        BufferedReader in = null;        try {            String urlNameString = url + &quot;?&quot; + param;            URL realUrl = new URL(urlNameString);            // 打开和URL之间的连接            URLConnection connection = realUrl.openConnection();            // 设置通用的请求属性            connection.setRequestProperty(&quot;accept&quot;, &quot;*/*&quot;);            connection.setRequestProperty(&quot;connection&quot;, &quot;Keep-Alive&quot;);            connection.setRequestProperty(&quot;user-agent&quot;,                    &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)&quot;);            // 建立实际的连接            connection.connect();            // 获取所有响应头字段            //Map&lt;String, List&lt;String&gt;&gt; map = connection.getHeaderFields();            // 遍历所有的响应头字段            // 定义 BufferedReader输入流来读取URL的响应            in = new BufferedReader(new InputStreamReader(connection.getInputStream(),&quot;UTF-8&quot;));            String line;            while ((line = in.readLine()) != null) {                result += line;            }        } catch (Exception e) {            LOG.info(&quot;发送GET请求出现异常！&quot; + (url+param));            System.out.println(&quot;发送GET请求出现异常！&quot; + e);            e.printStackTrace();        }        // 使用finally块来关闭输入流        finally {            try {                if (in != null) {                    in.close();                }            } catch (Exception e2) {                e2.printStackTrace();            }        }        return result;    }    /**     * 向指定URL发送GET方法的请求     * @param url  发送请求的URL     * @param param  请求参数，请求参数应该是 name1=value1&amp;name2=value2 的形式。     * @return URL 所代表远程资源的响应结果     */    public static String sendGet(String url, String param,String authorization) {        String result = &quot;&quot;;        BufferedReader in = null;        try {            String urlNameString = url + &quot;?&quot; + param;            URL realUrl = new URL(urlNameString);            // 打开和URL之间的连接            URLConnection connection = realUrl.openConnection();            // 设置通用的请求属性            connection.setRequestProperty(&quot;accept&quot;, &quot;*/*&quot;);            connection.setRequestProperty(&quot;connection&quot;, &quot;Keep-Alive&quot;);            connection.setRequestProperty(&quot;Authorization&quot;, authorization);            connection.setRequestProperty(&quot;user-agent&quot;,                    &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)&quot;);            // 建立实际的连接            connection.connect();            // 获取所有响应头字段            connection.getHeaderFields();            // 遍历所有的响应头字段/*            for (String key : map.keySet()) {                System.out.println(key + &quot;---&gt;&quot; + map.get(key));            }*/            // 定义 BufferedReader输入流来读取URL的响应            in = new BufferedReader(new InputStreamReader(                    connection.getInputStream(),&quot;UTF-8&quot;));            String line;            while ((line = in.readLine()) != null) {                result += line;            }        } catch (Exception e) {            LOG.info(&quot;发送POST请求出现异常！&quot; + (url+param));            System.out.println(&quot;发送POST请求出现异常！&quot; + e);            e.printStackTrace();        }        // 使用finally块来关闭输入流        finally {            try {                if (in != null) {                    in.close();                }            } catch (Exception e2) {                e2.printStackTrace();            }        }        return result;    }    public static void main(String[] args) throws Exception{    }    /**     * 向指定 URL 发送POST方法的请求     * @param url  发送请求的 URL     * @param param  请求参数，请求参数应该是 name1=value1&amp;name2=value2 的形式。     * @return  所代表远程资源的响应结果     */    public static String sendPost(String url, String param) {        PrintWriter out = null;        BufferedReader in = null;        String result = &quot;&quot;;        try {            URL realUrl = new URL(url);            // 打开和URL之间的连接            URLConnection conn = realUrl.openConnection();            // 设置通用的请求属性            conn.setRequestProperty(&quot;Content-Type&quot;,&quot;application/json&quot;);            //conn.setInstanceFollowRedirects(false);            // conn.setRequestProperty(&quot;Content-Type&quot;,&quot;application/x-www-form-urlencoded&quot;);            conn.setRequestProperty(&quot;accept&quot;, &quot;*/*&quot;);            conn.setRequestProperty(&quot;connection&quot;, &quot;Keep-Alive&quot;);            conn.setRequestProperty(&quot;user-agent&quot;,                    &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)&quot;);            // 发送POST请求必须设置如下两行            conn.setReadTimeout(30000);            conn.setDoOutput(true);            conn.setDoInput(true);            // 获取URLConnection对象对应的输出流            out = new PrintWriter(conn.getOutputStream());            // 发送请求参数            out.print(param);            // flush输出流的缓冲            out.flush();            // 定义BufferedReader输入流来读取URL的响应            InputStream inputStream = conn.getInputStream();            in = new BufferedReader(new InputStreamReader(inputStream,&quot;UTF-8&quot;));            String line;            while ((line = in.readLine()) != null) {                result += line;            }        }        catch (IOException e) {            LOG.info(&quot;发送POST请求出现异常！&quot; + (url+param),e);        }        //使用finally块来关闭输出流、输入流        finally{            try{                if(out!=null){                    out.close();                }                if(in!=null){                    in.close();                }            }            catch(IOException ex){                ex.printStackTrace();            }        }        return result;    }    /*     * params 填写的URL的参数 encode 字节编码     */    public static String sendPostMessage(String url1,Map&lt;String,Object&gt; params){        String response = null;        Reader in = null;        try {            //访问准备            URL url = new URL(url1);            //开始访问            StringBuilder postData = new StringBuilder();            for (Map.Entry&lt;String,Object&gt; param : params.entrySet()) {                if (postData.length() != 0) postData.append(&#39;&amp;&#39;);                postData.append(URLEncoder.encode(param.getKey(), &quot;UTF-8&quot;));                postData.append(&#39;=&#39;);                postData.append(URLEncoder.encode(String.valueOf(param.getValue()), &quot;UTF-8&quot;));            }            byte[] postDataBytes = postData.toString().getBytes(&quot;UTF-8&quot;);            URLConnection conn = url.openConnection();            //URLConnection conn = url.openConnection();            //conn.setRequestMethod(&quot;POST&quot;);            //conn.setInstanceFollowRedirects(false);            //conn.setRequestProperty(&quot;Content-Type&quot;, &quot;application/x-www-form-urlencoded&quot;);            conn.setRequestProperty(&quot;Content-Type&quot;, &quot;application/json&quot;);            conn.setRequestProperty(&quot;Content-Length&quot;, String.valueOf(postDataBytes.length));            conn.setDoOutput(true);            conn.getOutputStream().write(postDataBytes);            in = new BufferedReader(new InputStreamReader(conn.getInputStream(), &quot;UTF-8&quot;));            StringBuilder sb = new StringBuilder();            for (int c; (c = in.read()) &gt;= 0;)                sb.append((char)c);            response = sb.toString();           //System.out.println(response);        } catch (IOException e) {            LOG.error(null,e);        }finally {            if(in != null){                try {                    in.close();                } catch (IOException e) {                    e.printStackTrace();                }            }        }        return response;    }    /**     * 向指定 URL 发送POST方法的请求     * @param url  发送请求的 URL     * @param param  请求参数，请求参数应该是 name1=value1&amp;name2=value2 的形式。     * @return  所代表远程资源的响应结果     */    public static void sendPostWithoutReturn(String url, String param) {        PrintWriter out = null;        BufferedReader in = null;        String result = &quot;&quot;;        try {            URL realUrl = new URL(url);            // 打开和URL之间的连接            HttpURLConnection conn = (HttpURLConnection )realUrl.openConnection();            // 设置通用的请求属性            conn.setRequestProperty(&quot;Content-Type&quot;,&quot;application/json&quot;);            conn.setRequestProperty(&quot;accept&quot;, &quot;*/*&quot;);            conn.setRequestProperty(&quot;connection&quot;, &quot;Keep-Alive&quot;);            conn.setRequestProperty(&quot;user-agent&quot;,                    &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)&quot;);            //根据需求设置读超时的时间            conn.setReadTimeout(1000);            // 发送POST请求必须设置如下两行            conn.setDoOutput(true);            conn.setDoInput(true);            // 获取URLConnection对象对应的输出流            out = new PrintWriter(conn.getOutputStream());            // 发送请求参数            out.print(param);            // flush输出流的缓冲            out.flush();            // 定义BufferedReader输入流来读取URL的响应            if (conn.getResponseCode() == 200) {                System.out.println(&quot;连接成功,传送数据...&quot;);            } else {                System.out.println(&quot;连接失败,错误代码:&quot;+conn.getResponseCode());            }        }        catch (IOException e) {            LOG.info(&quot;发送POST请求出现异常！&quot; + (url+param),e);        }        //使用finally块来关闭输出流、输入流        finally{            try{                if(out!=null){                    out.close();                }                in.close();            }            catch(Exception ex){                ex.printStackTrace();            }        }    }}</code></pre><h4 id="8、netb-db-DBCommon—mysql的连接、关闭基础类"><a href="#8、netb-db-DBCommon—mysql的连接、关闭基础类" class="headerlink" title="8、netb/db/DBCommon—mysql的连接、关闭基础类"></a>8、netb/db/DBCommon—mysql的连接、关闭基础类</h4><pre><code>package com.hsiehchou.common.netb.db;import com.hsiehchou.common.config.ConfigUtil;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.sql.*;import java.util.Properties;public class DBCommon {    private static Logger LOG = LoggerFactory.getLogger(DBCommon.class);    private static String MYSQL_PATH = &quot;common/mysql.properties&quot;;    private static Properties properties = ConfigUtil.getInstance().getProperties(MYSQL_PATH);    private static Connection conn ;    private DBCommon(){}    public static void main(String[] args) {        System.out.println(properties);        Connection xz_bigdata = DBCommon.getConn(&quot;test&quot;);        System.out.println(xz_bigdata);    }    //TODO  配置文件    private static final String JDBC_DRIVER = &quot;com.mysql.jdbc.Driver&quot;;    private static final String USER_NAME = properties.getProperty(&quot;user&quot;);    private static final String PASSWORD = properties.getProperty(&quot;password&quot;);    private static final String IP = properties.getProperty(&quot;db_ip&quot;);    private static final String PORT = properties.getProperty(&quot;db_port&quot;);    private static final String DB_CONFIG = &quot;?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull&amp;autoReconnect=true&amp;failOverReadOnly=false&quot;;    static {        try {            Class.forName(JDBC_DRIVER);        } catch (ClassNotFoundException e) {            LOG.error(null, e);        }    }    /**     * 获取数据库连接     * @param dbName     * @return     */    public static Connection getConn(String dbName) {        Connection conn = null;        String  connstring = &quot;jdbc:mysql://&quot;+IP+&quot;:&quot;+PORT+&quot;/&quot;+dbName+DB_CONFIG;        try {            conn = DriverManager.getConnection(connstring, USER_NAME, PASSWORD);        } catch (SQLException e) {            e.printStackTrace();            LOG.error(null, e);        }        return conn;    }    /**     * @param url eg:&quot;jdbc:oracle:thin:@172.16.1.111:1521:d406&quot;     * @param driver eg:&quot;oracle.jdbc.driver.OracleDriver&quot;     * @param user eg:&quot;ucase&quot;     * @param password eg:&quot;ucase123&quot;     * @return     * @throws ClassNotFoundException     * @throws SQLException     */    public static Connection getConn(String url, String driver, String user,                                     String password) throws ClassNotFoundException, SQLException{        Class.forName(driver);        conn = DriverManager.getConnection(url, user, password);        return  conn;    }    public static void close(Connection conn){        try {            if( conn != null ){                conn.close();            }        } catch (SQLException e) {            LOG.error(null,e);        }    }    public static void close(Statement statement){        try {            if( statement != null ){                statement.close();            }        } catch (SQLException e) {            LOG.error(null,e);        }    }    public static void close(Connection conn,PreparedStatement statement){        try {            if( conn != null ){                conn.close();            }            if( statement != null ){                statement.close();            }        } catch (SQLException e) {            LOG.error(null,e);        }    }    public static void close(Connection conn,Statement statement,ResultSet resultSet) throws SQLException{        if( resultSet != null ){            resultSet.close();        }        if( statement != null ){            statement.close();        }        if( conn != null ){            conn.close();        }    }}</code></pre><h4 id="9、project-datatype-DataTypeProperties-java"><a href="#9、project-datatype-DataTypeProperties-java" class="headerlink" title="9、project/datatype/DataTypeProperties.java"></a>9、project/datatype/DataTypeProperties.java</h4><pre><code>package com.hsiehchou.common.project.datatype;import com.hsiehchou.common.config.ConfigUtil;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.*;public class DataTypeProperties {    private static final Logger logger = LoggerFactory.getLogger(DataTypeProperties.class);    private static final String DATA_PATH = &quot;common/datatype.properties&quot;;    public static Map&lt;String,ArrayList&lt;String&gt;&gt; dataTypeMap = null;    static {        Properties properties = ConfigUtil.getInstance().getProperties(DATA_PATH);        dataTypeMap = new HashMap&lt;&gt;();        Set&lt;Object&gt; keys = properties.keySet();        keys.forEach(key-&gt;{            String[] split = properties.getProperty(key.toString()).split(&quot;,&quot;);            dataTypeMap.put(key.toString(),new ArrayList&lt;&gt;(Arrays.asList(split)));        });    }    public static void main(String[] args) {        Map&lt;String, ArrayList&lt;String&gt;&gt; dataTypeMap = DataTypeProperties.dataTypeMap;        System.out.println(dataTypeMap.toString());    }}</code></pre><h4 id="10、regex-Validation-java—验证工具类"><a href="#10、regex-Validation-java—验证工具类" class="headerlink" title="10、regex/Validation.java—验证工具类"></a>10、regex/Validation.java—验证工具类</h4><pre><code>package com.hsiehchou.common.regex;import java.util.regex.Matcher;import java.util.regex.Pattern;/** * 验证工具类 */public class Validation {    // ------------------常量定义    /**     * Email正则表达式=     * &quot;^([a-z0-9A-Z]+[-|\\.]?)+[a-z0-9A-Z]@([a-z0-9A-Z]+(-[a-z0-9A-Z]+)?\\.)+[a-zA-Z]{2,}$&quot;     * ;     */    // public static final String EMAIL =    // &quot;^([a-z0-9A-Z]+[-|\\.]?)+[a-z0-9A-Z]@([a-z0-9A-Z]+(-[a-z0-9A-Z]+)?\\.)+[a-zA-Z]{2,}$&quot;;;    public static final String EMAIL = &quot;\\w+(\\.\\w+)*@\\w+(\\.\\w+)+&quot;;    /**     * 电话号码正则表达式=     * (^(\d{2,4}[-_－—]?)?\d{3,8}([-_－—]?\d{3,8})?([-_－—]?\d{1,7})?$)|     * (^0?1[35]\d{9}$)     */    public static final String PHONE = &quot;(^(\\d{2,4}[-_－—]?)?\\d{3,8}([-_－—]?\\d{3,8})?([-_－—]?\\d{1,7})?$)|(^0?1[35]\\d{9}$)&quot;;    /**     * 手机号码正则表达式=^(13[0-9]|15[0-9]|18[0-9])\d{8}$     */    public static final String MOBILE = &quot;^((13[0-9])|(14[5-7])|(15[^4])|(17[0-8])|(18[0-9]))\\d{8}$&quot;;    /**     * Integer正则表达式 ^-?(([1-9]\d*$)|0)     */    public static final String INTEGER = &quot;^-?(([1-9]\\d*$)|0)&quot;;    /**     * 正整数正则表达式 &gt;=0 ^[1-9]\d*|0$     */    public static final String INTEGER_NEGATIVE = &quot;^[1-9]\\d*|0$&quot;;    /**     * 负整数正则表达式 &lt;=0 ^-[1-9]\d*|0$     */    public static final String INTEGER_POSITIVE = &quot;^-[1-9]\\d*|0$&quot;;    /**     * Double正则表达式 ^-?([1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0)$     */    public static final String DOUBLE = &quot;^-?([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0)$&quot;;    /**     * 正Double正则表达式 &gt;=0 ^[1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0$　     */    public static final String DOUBLE_NEGATIVE = &quot;^[1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0$&quot;;    /**     * 负Double正则表达式 &lt;= 0 ^(-([1-9]\d*\.\d*|0\.\d*[1-9]\d*))|0?\.0+|0$     */    public static final String DOUBLE_POSITIVE = &quot;^(-([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*))|0?\\.0+|0$&quot;;    /**     * 年龄正则表达式 ^(?:[1-9][0-9]?|1[01][0-9]|120)$ 匹配0-120岁     */    public static final String AGE = &quot;^(?:[1-9][0-9]?|1[01][0-9]|120)$&quot;;    /**     * 邮编正则表达式 [0-9]\d{5}(?!\d) 国内6位邮编     */    public static final String CODE = &quot;[0-9]\\d{5}(?!\\d)&quot;;    /**     * 匹配由数字、26个英文字母或者下划线组成的字符串 ^\w+$     */    public static final String STR_ENG_NUM_ = &quot;^\\w+$&quot;;    /**     * 匹配由数字和26个英文字母组成的字符串 ^[A-Za-z0-9]+$     */    public static final String STR_ENG_NUM = &quot;^[A-Za-z0-9]+&quot;;    /**     * 匹配由26个英文字母组成的字符串 ^[A-Za-z]+$     */    public static final String STR_ENG = &quot;^[A-Za-z]+$&quot;;    /**     * 过滤特殊字符串正则 regEx=     * &quot;[`~!@#$%^&amp;*()+=|{}&#39;:;&#39;,\\[\\].&lt;&gt;/?~！@#￥%……&amp;*（）——+|{}【】‘；：”“’。，、？]&quot;;     */    public static final String STR_SPECIAL = &quot;[`~!@#$%^&amp;*()+=|{}&#39;:;&#39;,\\[\\].&lt;&gt;/?~！@#￥%……&amp;*（）——+|{}【】‘；：”“’。，、？]&quot;;    /***     * 日期正则 支持： YYYY-MM-DD YYYY/MM/DD YYYY_MM_DD YYYYMMDD YYYY.MM.DD的形式     */    public static final String DATE_ALL = &quot;((^((1[8-9]\\d{2})|([2-9]\\d{3}))([-\\/\\._]?)(10|12|0?[13578])([-\\/\\._]?)(3[01]|[12][0-9]|0?[1-9])$)&quot;            + &quot;|(^((1[8-9]\\d{2})|([2-9]\\d{3}))([-\\/\\._]?)(11|0?[469])([-\\/\\._]?)(30|[12][0-9]|0?[1-9])$)&quot;            + &quot;|(^((1[8-9]\\d{2})|([2-9]\\d{3}))([-\\/\\._]?)(0?2)([-\\/\\._]?)(2[0-8]|1[0-9]|0?[1-9])$)|(^([2468][048]00)([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|(^([3579][26]00)&quot;            + &quot;([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)&quot;            + &quot;|(^([1][89][0][48])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|(^([2-9][0-9][0][48])([-\\/\\._]?)&quot;            + &quot;(0?2)([-\\/\\._]?)(29)$)&quot;            + &quot;|(^([1][89][2468][048])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|(^([2-9][0-9][2468][048])([-\\/\\._]?)(0?2)&quot;            + &quot;([-\\/\\._]?)(29)$)|(^([1][89][13579][26])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|&quot;            + &quot;(^([2-9][0-9][13579][26])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$))&quot;;    /***     * 日期正则 支持： YYYY-MM-DD     */    public static final String DATE_FORMAT1 = &quot;(([0-9]{3}[1-9]|[0-9]{2}[1-9][0-9]{1}|[0-9]{1}[1-9][0-9]{2}|[1-9][0-9]{3})-(((0[13578]|1[02])-(0[1-9]|[12][0-9]|3[01]))|((0[469]|11)-(0[1-9]|[12][0-9]|30))|(02-(0[1-9]|[1][0-9]|2[0-8]))))|((([0-9]{2})(0[48]|[2468][048]|[13579][26])|((0[48]|[2468][048]|[3579][26])00))-02-29)&quot;;    /**     * URL正则表达式 匹配 http www ftp     */    public static final String URL = &quot;^(http|www|ftp|)?(://)?(\\w+(-\\w+)*)(\\.(\\w+(-\\w+)*))*((:\\d+)?)(/(\\w+(-\\w+)*))*(\\.?(\\w)*)(\\?)?&quot;            + &quot;(((\\w*%)*(\\w*\\?)*(\\w*:)*(\\w*\\+)*(\\w*\\.)*(\\w*&amp;)*(\\w*-)*(\\w*=)*(\\w*%)*(\\w*\\?)*&quot;            + &quot;(\\w*:)*(\\w*\\+)*(\\w*\\.)*&quot;            + &quot;(\\w*&amp;)*(\\w*-)*(\\w*=)*)*(\\w*)*)$&quot;;    /**     * 身份证正则表达式     */    public static final String IDCARD = &quot;((11|12|13|14|15|21|22|23|31|32|33|34|35|36|37|41|42|43|44|45|46|50|51|52|53|54|61|62|63|64|65)[0-9]{4})&quot;            + &quot;(([1|2][0-9]{3}[0|1][0-9][0-3][0-9][0-9]{3}&quot;            + &quot;[Xx0-9])|([0-9]{2}[0|1][0-9][0-3][0-9][0-9]{3}))&quot;;    /**     * 机构代码     */    public static final String JIGOU_CODE = &quot;^[A-Z0-9]{8}-[A-Z0-9]$&quot;;    /**     * 匹配数字组成的字符串 ^[0-9]+$     */    public static final String STR_NUM = &quot;^[0-9]+$&quot;;    // //------------------验证方法    /**     * 判断字段是否为空 符合返回ture     * @param str     * @return boolean     */    public static synchronized boolean StrisNull(String str) {        return null == str || str.trim().length() &lt;= 0 ? true : false;    }    /**     * 判断字段是非空 符合返回ture     * @param str     * @return boolean     */    public static boolean StrNotNull(String str) {        return !StrisNull(str);    }    /**     * 字符串null转空     * @param str     * @return boolean     */    public static String nulltoStr(String str) {        return StrisNull(str) ? &quot;&quot; : str;    }    /**     * 字符串null赋值默认值     * @param str  目标字符串     * @param defaut  默认值     * @return  String     */    public static String nulltoStr(String str, String defaut) {        return StrisNull(str) ? defaut : str;    }    /**     * 判断字段是否为Email 符合返回ture     * @param str     * @return boolean     */    public static boolean isEmail(String str) {        return Regular(str, EMAIL);    }    /**     * 判断是否为电话号码 符合返回ture     * @param str     * @return boolean     */    public static boolean isPhone(String str) {        return Regular(str, PHONE);    }    /**     * 判断是否为手机号码 符合返回ture     * @param str     * @return boolean     */    public static boolean isMobile(String str) {        return RegularSJHM(str, MOBILE);    }    /**     * 判断是否为Url 符合返回ture     * @param str     * @return boolean     */    public static boolean isUrl(String str) {        return Regular(str, URL);    }    /**     * 判断字段是否为数字 正负整数 正负浮点数 符合返回ture     * @param str     * @return boolean     */    public static boolean isNumber(String str) {        return Regular(str, DOUBLE);    }    /**     * 判断字段是否为INTEGER 符合返回ture     * @param str     * @return boolean     */    public static boolean isInteger(String str) {        return Regular(str, INTEGER);    }    /**     * 判断字段是否为正整数正则表达式 &gt;=0 符合返回ture     * @param str     * @return boolean     */    public static boolean isINTEGER_NEGATIVE(String str) {        return Regular(str, INTEGER_NEGATIVE);    }    /**     * 判断字段是否为负整数正则表达式 &lt;=0 符合返回ture     * @param str     * @return boolean     */    public static boolean isINTEGER_POSITIVE(String str) {        return Regular(str, INTEGER_POSITIVE);    }    /**     * 判断字段是否为DOUBLE 符合返回ture     * @param str     * @return boolean     */    public static boolean isDouble(String str) {        return Regular(str, DOUBLE);    }    /**     * 判断字段是否为正浮点数正则表达式 &gt;=0 符合返回ture     * @param str     * @return boolean     */    public static boolean isDOUBLE_NEGATIVE(String str) {        return Regular(str, DOUBLE_NEGATIVE);    }    /**     * 判断字段是否为负浮点数正则表达式 &lt;=0 符合返回ture     * @param str     * @return boolean     */    public static boolean isDOUBLE_POSITIVE(String str) {        return Regular(str, DOUBLE_POSITIVE);    }    /**     * 判断字段是否为日期 符合返回ture     * @param str     * @return boolean     */    public static boolean isDate(String str) {        return Regular(str, DATE_ALL);    }    /**     * 验证2010-12-10     * @param str     * @return     */    public static boolean isDate1(String str) {        return Regular(str, DATE_FORMAT1);    }    /**     * 判断字段是否为年龄 符合返回ture     * @param str     * @return boolean     */    public static boolean isAge(String str) {        return Regular(str, AGE);    }    /**     * 判断字段是否超长 字串为空返回fasle, 超过长度{leng}返回ture 反之返回false     * @param str     * @param leng     * @return boolean     */    public static boolean isLengOut(String str, int leng) {        return StrisNull(str) ? false : str.trim().length() &gt; leng;    }    /**     * 判断字段是否为身份证 符合返回ture     * @param str     * @return boolean     */    public static boolean isIdCard(String str) {        if (StrisNull(str))            return false;        if (str.trim().length() == 15 || str.trim().length() == 18) {            return Regular(str, IDCARD);        } else {            return false;        }    }    /**     * 判断字段是否为邮编 符合返回ture     * @param str     * @return boolean     */    public static boolean isCode(String str) {        return Regular(str, CODE);    }    /**     * 判断字符串是不是全部是英文字母     * @param str     * @return boolean     */    public static boolean isEnglish(String str) {        return Regular(str, STR_ENG);    }    /**     * 判断字符串是不是全部是英文字母+数字     * @param str     * @return boolean     */    public static boolean isENG_NUM(String str) {        return Regular(str, STR_ENG_NUM);    }    /**     * 判断字符串是不是全部是英文字母+数字+下划线     * @param str     * @return boolean     */    public static boolean isENG_NUM_(String str) {        return Regular(str, STR_ENG_NUM_);    }    /**     * 过滤特殊字符串 返回过滤后的字符串     * @param str     * @return boolean     */    public static String filterStr(String str) {        Pattern p = Pattern.compile(STR_SPECIAL);        Matcher m = p.matcher(str);        return m.replaceAll(&quot;&quot;).trim();    }    /**     * 校验机构代码格式     * @return     */    public static boolean isJigouCode(String str) {        return Regular(str, JIGOU_CODE);    }    /**     * 判断字符串是不是数字组成     * @param str     * @return boolean     */    public static boolean isSTR_NUM(String str) {        return Regular(str, STR_NUM);    }    /**     * 匹配是否符合正则表达式pattern 匹配返回true     * @param str 匹配的字符串     * @param pattern 匹配模式     * @return boolean     */    private static boolean Regular(String str, String pattern) {        if (null == str || str.trim().length() &lt;= 0)            return false;        Pattern p = Pattern.compile(pattern);        Matcher m = p.matcher(str);        return m.matches();    }    /**     * 匹配是否符合正则表达式pattern 匹配返回true     * @param str 匹配的字符串     * @param pattern 匹配模式     * @return boolean     */    private static boolean RegularSJHM(String str, String pattern) {        if (null == str || str.trim().length() &lt;= 0){            return false;        }        if(str.contains(&quot;+86&quot;)){            str=str.replace(&quot;+86&quot;,&quot;&quot;);        }        Pattern p = Pattern.compile(pattern);        Matcher m = p.matcher(str);        return m.matches();    }    /**     * description:匹配yyyyMMddHHmmss格式时间     * @param time     * @return boolean 2016-7-19 下午5:13:25 by      */    public static final String yyyyMMddHHmmss = &quot;[0-9]{14}&quot;;    public static boolean isyyyyMMddHHmmss(String time) {        if (time == null) {            return false;        }        boolean bool = time.matches(yyyyMMddHHmmss);        return bool;    }    /**     * description:匹配yyyyMMddHHmmss格式时间     * @param time     * @return boolean 2016-7-19 下午5:13:25 by      */    public static final String isMac = &quot;^[A-F0-9]{2}(-[A-F0-9]{2}){5}$&quot;;    public static boolean isMac(String mac) {        if (mac == null) {            return false;        }        boolean bool = mac.matches(isMac);        return bool;    }    /**     * description:匹配yyyyMMddHHmmss格式时间     * @param time     * @return boolean 2016-7-19 下午5:13:25 by      */    public static final String longtime = &quot;[0-9]{10}&quot;;    public static boolean isTimestamp(String timestamp) {        if (timestamp == null) {            return false;        }        boolean bool = timestamp.matches(longtime);        return bool;    }    /**     * 判断字段是否为datatype 符合返回ture     * @param str     * @return boolean     */    public static final String DATATYPE = &quot;^\\d{7}$&quot;;    public static boolean isDATATYPE(String str) {        return Regular(str, DATATYPE);    }    /**     * 判断字段是否为QQ 符合返回ture     * @param str     * @return boolean     */    public static final String QQ = &quot;^\\d{5,15}$&quot;;    public static boolean isQQ(String str) {        return Regular(str, QQ);    }    /**     * 判断字段是否为IMSI 符合返回ture     * @param str     * @return boolean     */    //public static final String IMSI = &quot;^4600[0,1,2,3,4,5,6,7,9]\\d{10}|(46011|46020)\\d{10}$&quot;;    public static final String IMSI = &quot;^[1-9][0-9][0-9]0[0,1,2,3,4,5,6,7,9]\\d{10}|[1-9][0-9][0-9](11|20)\\d{10}$&quot;;    public static boolean isIMSI(String str) {        return Regular(str, IMSI);    }    /**     * 判断字段是否为IMEI 符合返回ture     * @param str     * @return boolean     */    public static final String IMEI = &quot;^\\d{8}$|^[a-fA-F0-9]{14}$|^\\d{15}$&quot;;    public static boolean isIMEI(String str) {return Regular(str, IMEI);}    /**     * 判断字段是否为CAPTURETIME 符合返回ture     * @param str     * @return boolean     */    public static final String CAPTURETIME = &quot;^\\d{10}|(20[0-9][0-9])\\d{10}$&quot;;    public static boolean isCAPTURETIME(String str) {return Regular(str, CAPTURETIME);}    /**     * description:检测认证类型     * @param auth     * @return boolean     */    public static final String AUTH_TYPE = &quot;^\\d{7}$&quot;;    public static boolean isAUTH_TYPE(String str) {return Regular(str, CAPTURETIME);}    /**     * description:检测FIRM_CODE     * @param auth     * @return boolean     */    public static final String FIRM_CODE = &quot;^\\d{9}$&quot;;    public static boolean isFIRM_CODE(String str) {return Regular(str, FIRM_CODE);}    /**     * description:检测经度     * @param auth     * @return boolean     */    public static final String LONGITUDE = &quot;^-?(([1-9]\\d?)|(1[0-7]\\d)|180)(\\.\\d{1,8})?$&quot;;    //public static final String LONGITUDE =&quot;^([-]?(\\d|([1-9]\\d)|(1[0-7]\\d)|(180))(\\.\\d*)\\,[-]?(\\d|([1-8]\\d)|(90))(\\.\\d*))$&quot;;    public static boolean isLONGITUDE(String str) {return Regular(str, LONGITUDE);}    /**     * description:检测纬度     * @param auth     * @return boolean     */    public static final String LATITUDE = &quot;^-?(([1-8]\\d?)|([1-8]\\d)|90)(\\.\\d{1,8})?$&quot;;    public static boolean isLATITUDE(String str) {return Regular(str, LATITUDE);}    public static void main(String[] args) {        boolean bool = isLATITUDE(&quot;26.0615854&quot;);        System.out.println(bool);    }}</code></pre><h4 id="11、thread-ThreadPoolManager-java—线程池管理器单例"><a href="#11、thread-ThreadPoolManager-java—线程池管理器单例" class="headerlink" title="11、thread/ThreadPoolManager.java—线程池管理器单例"></a>11、thread/ThreadPoolManager.java—线程池管理器单例</h4><pre><code>package com.hsiehchou.common.thread;import java.io.Serializable;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;/** *     线程池管理器单例 *  默认创建   ewCachedThreadPool ：创建一个可缓存的线程池 *  可通过指定线程的数量来创建：newFixedThreadPool  ： 创建固定大小的线程池 */public class ThreadPoolManager implements Serializable {    private static final long serialVersionUID = 1465361469484903956L;    public static final ThreadPoolManager threadPoolManager =  new ThreadPoolManager();    private static ThreadPoolManager tpm;    private transient ExecutorService newCachedThreadPool;    private transient ExecutorService newFixedThreadPool;    private int poolCapacity;    private ThreadPoolManager(){        if( newCachedThreadPool == null )            newCachedThreadPool = Executors.newCachedThreadPool();    }    @Deprecated    public static ThreadPoolManager getInstance(){        if( tpm == null ){            synchronized(ThreadPoolManager.class){            if( tpm == null )                tpm =  new ThreadPoolManager();            }        }        return tpm;    }    /**      * 返回 newCachedThreadPool     */    public ExecutorService getExecutorService(){        if( newCachedThreadPool == null ){            synchronized(ThreadPoolManager.class){                if( newCachedThreadPool == null )                    newCachedThreadPool = Executors.newCachedThreadPool();            }        }        return newCachedThreadPool;    }    /**       * 返回 newFixedThreadPool     */    public ExecutorService getExecutorService(int poolCapacity){        return getExecutorService(poolCapacity, false);    }    /**      * 返回 newFixedThreadPool     */    public synchronized ExecutorService getExecutorService(int poolCapacity, boolean closeOld){        if(newFixedThreadPool == null || (this.poolCapacity != poolCapacity)){            if(newFixedThreadPool != null &amp;&amp; closeOld){                newFixedThreadPool.shutdown();            }            newFixedThreadPool = Executors.newFixedThreadPool(poolCapacity);            this.poolCapacity = poolCapacity;        }        return newFixedThreadPool;    }}</code></pre><h4 id="12、time-TimeTranstationUtils-java—时间转换工具类"><a href="#12、time-TimeTranstationUtils-java—时间转换工具类" class="headerlink" title="12、time/TimeTranstationUtils.java—时间转换工具类"></a>12、time/TimeTranstationUtils.java—时间转换工具类</h4><pre><code>package com.hsiehchou.common.time;import org.apache.commons.lang.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;import java.util.HashMap;import java.util.Map;/** * Description: 时间转换工具类 */public class TimeTranstationUtils {    private static final Logger logger = LoggerFactory.getLogger(TimeTranstationUtils.class);/*    private static SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);    private static SimpleDateFormat sdFormatternew = new SimpleDateFormat(&quot;yyyyMMddHH&quot;);    private static SimpleDateFormat sdFormatter1 = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);    private static SimpleDateFormat sdFormatter2 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);    private static SimpleDateFormat sdFormatter3 = new SimpleDateFormat(&quot;yyyyMMdd&quot;);*/    private static Date nowTime;    public static String Date2yyyyMMddHHmmss() {        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);        nowTime = new Date(System.currentTimeMillis());        String time = sdFormatter.format(nowTime);        return time;    }    public static String Date2yyyyMMddHHmmss(long timestamp) {        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);        nowTime = new Date(timestamp);        String time = sdFormatter.format(nowTime);        return time;    }    public static String Date2yyyyMMdd(long timestamp) {        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMdd&quot;);        nowTime = new Date(timestamp);        String time = sdFormatter.format(nowTime);        return time;    }    public static String Date2yyyyMMddHH(String str) {        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);        SimpleDateFormat sdFormatternew = new SimpleDateFormat(&quot;yyyyMMddHH&quot;);        try {            nowTime = sdFormatter.parse(str);        } catch (ParseException e) {            e.printStackTrace();        }        String time = sdFormatternew.format(nowTime);        return time;    }    public static String Date2yyyy_MM_dd() {        SimpleDateFormat sdFormatter1 = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);        nowTime = new Date(System.currentTimeMillis());        String time = sdFormatter1.format(nowTime);        return time;    }    public static String Date2yyyy_MM_dd_HH_mm_ss() {        SimpleDateFormat sdFormatter2 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);        nowTime = new Date(System.currentTimeMillis());        String time = sdFormatter2.format(nowTime);        return time;    }    public static String Date2yyyyMMdd() {        SimpleDateFormat sdFormatter3 = new SimpleDateFormat(&quot;yyyyMMdd&quot;);        nowTime = new Date(System.currentTimeMillis());        String time = sdFormatter3.format(nowTime);        return time;    }    public static String Date2yyyyMMdd(String str) {        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);        SimpleDateFormat sdFormatter3 = new SimpleDateFormat(&quot;yyyyMMdd&quot;);        try {            nowTime = sdFormatter.parse(str);        } catch (ParseException e) {            e.printStackTrace();        }        String time = sdFormatter3.format(nowTime);        return time;    }    public static Long Date2yyyyMMddHHmmssToLong() {        return System.currentTimeMillis() / 1000;    }    public static String long2date(String capturetime){        SimpleDateFormat sdf= new SimpleDateFormat(&quot;yyyyMMdd&quot;);        //前面的lSysTime是秒数，先乘1000得到毫秒数，再转为java.util.Date类型        Date dt = new Date(Long.valueOf(capturetime) * 1000);        String sDateTime = sdf.format(dt);  //得到精确到秒的表示：08/31/2006 21:08:00        return sDateTime;    }    public static Long yyyyMMddHHmmssToLong(String time) {        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);        if (StringUtils.isBlank(time)) {            return 0L;        } else {            boolean isNum = time.matches(&quot;[0-9]+&quot;);            if (isNum) {                long long1 = 0;                try {                    long1 = sdFormatter.parse(time).getTime();                } catch (ParseException e) {                    logger.error(time + &quot;时间转换为long错误&quot; + isNum);                    return 0L;                }                return long1 / 1000;            }        }        return 0L;    }    public static Date yyyyMMddHHmmssToDate(String time) {        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);        if (StringUtils.isBlank(time)) {            return new Date();        } else {            boolean isNum = time.matches(&quot;[0-9]+&quot;);            if (isNum) {                Date date = null;                try {                    date = sdFormatter.parse(time);                } catch (ParseException e) {                    logger.error(time + &quot;时间转换为date错误&quot; + isNum, e);                    System.out.println(time);                    System.out.println(isNum);                    e.printStackTrace();                }                return date;            }        }        return new Date();    }    public static Date yyyyMMddHHmmssToDate() {        Date date = null;        SimpleDateFormat sdFormatter2 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);        try {            date = sdFormatter2.parse(Date2yyyy_MM_dd_HH_mm_ss());        } catch (ParseException e) {            // TODO Auto-generated catch block            e.printStackTrace();        }        return date;    }    public static java.sql.Date strToDate(String strDate) {        String str = strDate;        SimpleDateFormat format = new SimpleDateFormat(&quot;yyyy-mm-dd&quot;);        Date d = null;        try {            d = format.parse(str);        } catch (Exception e) {            e.printStackTrace();        }        java.sql.Date date = new java.sql.Date(d.getTime());        return date;    }    public static Long str2Long(String str){        if(!StringUtils.isBlank(str)){            return Long.valueOf(str);        }else{            return 0L;        }    }    public static Double str2Double(String str){        if(!StringUtils.isBlank(str)){            return Double.valueOf(str);        }else{            return 0.0;        }    }    public static HashMap&lt;String,Object&gt; mapString2Long(Map&lt;String,String&gt; map, String key, HashMap&lt;String,Object&gt; objectMap) {        String logouttime = map.get(key);        if (!StringUtils.isBlank(logouttime)) {            objectMap.put(key, Long.valueOf(logouttime));        } else {            objectMap.put(key, 0L);        }        return objectMap;    }    public static void main(String[] args) throws InterruptedException {        System.out.println(long2date(&quot;1463487992&quot;));    }}</code></pre><h3 id="四、Resources开发"><a href="#四、Resources开发" class="headerlink" title="四、Resources开发"></a>四、Resources开发</h3><h4 id="xz-bigdata-resources结构"><a href="#xz-bigdata-resources结构" class="headerlink" title="xz_bigdata_resources结构"></a>xz_bigdata_resources结构</h4><p><img src="/medias/xz_bigdata_resources%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84.PNG" alt="xz_bigdata_resources整体结构"></p><p>注意：这里的resources要选中右键，选择Make Directory as，选择下级的Resources Root，变成Resources配置源文件，项目可以任意调用。</p><h4 id="1、resources下面"><a href="#1、resources下面" class="headerlink" title="1、resources下面"></a>1、resources下面</h4><p><strong>log4j2.properties</strong></p><pre><code>log4j.rootLogger = error,stdout,D,Elog4j.appender.stdout = org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.Target = System.outlog4j.appender.stdout.layout = org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern = [%-5p] %d{yyyy-MM-dd HH:mm:ss,SSS} method:%l%n%m%nlog4j.appender.D = org.apache.log4j.DailyRollingFileAppenderlog4j.appender.D.File = F://logs/log.loglog4j.appender.D.Append = truelog4j.appender.D.Threshold = DEBUG log4j.appender.D.layout = org.apache.log4j.PatternLayoutlog4j.appender.D.layout.ConversionPattern = %-d{yyyy-MM-dd HH:mm:ss}  [ %t:%r ] - [ %p ]  %m%nlog4j.appender.E = org.apache.log4j.DailyRollingFileAppenderlog4j.appender.E.File =F://logs/error.log log4j.appender.E.Append = truelog4j.appender.E.Threshold = ERROR log4j.appender.E.layout = org.apache.log4j.PatternLayoutlog4j.appender.E.layout.ConversionPattern = %-d{yyyy-MM-dd HH:mm:ss}  [ %t:%r ] - [ %p ]  %m%n</code></pre><h4 id="2、common"><a href="#2、common" class="headerlink" title="2、common"></a>2、common</h4><p><strong>datatype.properties</strong></p><pre><code># base = datatype,idcard,name,age,collecttime,imei# wechat = datatype,wechat,phone,collecttime,imeiwechat = imei,imsi,longitude,latitude,phone_mac,device_mac,device_number,collect_time,username,phone,object_username,send_message,accept_message,message_timemail = imei,imsi,longitude,latitude,phone_mac,device_mac,device_number,collect_time,send_mail,send_time,accept_mail,accept_time,mail_content,mail_typeqq = imei,imsi,longitude,latitude,phone_mac,device_mac,device_number,collect_time,username,phone,object_username,send_message,accept_message,message_time</code></pre><p><strong>mysql.properties</strong></p><pre><code>db_ip = 192.168.116.201db_port = 3306user = rootpassword = root</code></pre><h4 id="3、es"><a href="#3、es" class="headerlink" title="3、es"></a>3、es</h4><p><strong>es_cluster.properties</strong></p><pre><code>es.cluster.name=xz_eses.cluster.nodes = hadoop1,hadoop2,hadoop3es.cluster.nodes1 = hadoop1es.cluster.nodes2 = hadoop2es.cluster.nodes3 = hadoop3es.cluster.tcp.port = 9300es.cluster.http.port = 9200</code></pre><p><strong>mapping/base.json</strong></p><pre><code>{  &quot;_source&quot;: {    &quot;enabled&quot;: true  },  &quot;properties&quot;: {    &quot;datatype&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;idcard&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;name&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;age&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;collecttime&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;}  }}</code></pre><p><strong>mapping/fieldmapping.properties</strong></p><pre><code>tables = wechat,mail,qqwechat.imei = stringwechat.imsi = stringwechat.longitude = doublewechat.latitude = doublewechat.phone_mac = stringwechat.device_mac = stringwechat.device_number = stringwechat.collect_time = longwechat.username = stringwechat.phone = stringwechat.object_username = stringwechat.send_message = stringwechat.accept_message = stringwechat.message_time = longwechat.id = stringwechat.table = stringwechat.filename = stringwechat.absolute_filename  = stringmail.imei = stringmail.imsi = stringmail.longitude = doublemail.latitude = doublemail.phone_mac = stringmail.device_mac = stringmail.device_number = stringmail.collect_time = longmail.send_mail = stringmail.send_time = longmail.accept_mail = stringmail.accept_time = longmail.mail_content = stringmail.mail_type = stringmail.id = stringmail.table = stringmail.filename = stringmail.absolute_filename  = stringqq.imei = stringqq.imsi = stringqq.longitude = doubleqq.latitude = doubleqq.phone_mac = stringqq.device_mac = stringqq.device_number = stringqq.collect_time = longqq.username = stringqq.phone = stringqq.object_username = stringqq.send_message = stringqq.accept_message = stringqq.message_time = longqq.id = stringqq.table = stringqq.filename = stringqq.absolute_filename  = string</code></pre><p><strong>mapping/mail.json</strong></p><pre><code>{  &quot;_source&quot;: {    &quot;enabled&quot;: true  },  &quot;properties&quot;: {    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;imsi&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;longitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;latitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;phone_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_number&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;collect_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;send_mail&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;send_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;accept_mail&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;accept_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;mail_content&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;mail_type&quot;:{&quot;type&quot;: &quot;keyword&quot;},     &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;table&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;absolute_filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}  }}</code></pre><p><strong>mapping/qq.json</strong></p><pre><code>{  &quot;_source&quot;: {    &quot;enabled&quot;: true  },  &quot;properties&quot;: {    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;imsi&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;longitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;latitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;phone_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_number&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;collect_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;phone&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;object_username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;send_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;accept_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;message_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;table&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;absolute_filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}  }}</code></pre><p><strong>mapping/test.json</strong></p><pre><code>{  &quot;_source&quot;: {    &quot;enabled&quot;: true  },  &quot;properties&quot;: {    &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;source&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;target&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;library_id&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;source_sign&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;target_sign&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;create_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;create_user_id&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;is_audit&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;is_del&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;last_modify_user_id&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;last_modify_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;init_version&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;version&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;score&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;level&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;example&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;conflict&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;srcLangId&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;srcLangCN&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;tarLangId&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;tarLangCN&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;docId&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;source_simhash&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;sentence_id&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;section_id&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;type&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;industry&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;industry_name&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;querycount&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;reviser&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;comment&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}  }}</code></pre><p><strong>mapping/wechat.json</strong></p><pre><code>{  &quot;_source&quot;: {    &quot;enabled&quot;: true  },  &quot;properties&quot;: {    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;imsi&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;longitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;latitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;phone_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_number&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;collect_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;phone&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;object_username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;send_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;accept_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;message_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;table&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;absolute_filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}  }}</code></pre><h4 id="4、flume"><a href="#4、flume" class="headerlink" title="4、flume"></a>4、flume</h4><p><strong>datatype.properties</strong></p><p><strong>flume-config.properties</strong></p><pre><code>#kafka topickafkatopic=test100</code></pre><p><strong>validation.properties</strong></p><pre><code># 文件名验证开关FILENAME_VALIDATION=1# DATATYPE转换开关DATATYPE_TRANSACTION=1# 经纬度验证开关LONGLAIT_VALIDATION=1# 是否入错误数据到ESERROR_ES=1</code></pre><h4 id="5、hadoop"><a href="#5、hadoop" class="headerlink" title="5、hadoop"></a>5、hadoop</h4><p><strong>core-site.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--Autogenerated by Cloudera Manager--&gt;&lt;configuration&gt;  &lt;property&gt;    &lt;name&gt;fs.defaultFS&lt;/name&gt;    &lt;value&gt;hdfs://hadoop1:8020&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;fs.trash.interval&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;io.compression.codecs&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.authentication&lt;/name&gt;    &lt;value&gt;simple&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.authorization&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.rpc.protection&lt;/name&gt;    &lt;value&gt;authentication&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;    &lt;value&gt;DEFAULT&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.oozie.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.oozie.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.mapred.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.mapred.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.flume.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.flume.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.HTTP.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.HTTP.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hive.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hive.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.httpfs.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.httpfs.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hdfs.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hdfs.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.yarn.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.yarn.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.group.mapping&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.security.ShellBasedUnixGroupsMapping&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.instrumentation.requires.admin&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;net.topology.script.file.name&lt;/name&gt;    &lt;value&gt;/etc/hadoop/conf.cloudera.yarn/topology.py&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;io.file.buffer.size&lt;/name&gt;    &lt;value&gt;65536&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.ssl.enabled&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.ssl.require.client.cert&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;    &lt;final&gt;true&lt;/final&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.ssl.keystores.factory.class&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory&lt;/value&gt;    &lt;final&gt;true&lt;/final&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;    &lt;value&gt;ssl-server.xml&lt;/value&gt;    &lt;final&gt;true&lt;/final&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.ssl.client.conf&lt;/name&gt;    &lt;value&gt;ssl-client.xml&lt;/value&gt;    &lt;final&gt;true&lt;/final&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre><p><strong>hdfs-site.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--Autogenerated by Cloudera Manager--&gt;&lt;configuration&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;    &lt;value&gt;file:///dfs/nn&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.servicerpc-address&lt;/name&gt;    &lt;value&gt;hadoop1:8022&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.https.address&lt;/name&gt;    &lt;value&gt;hadoop1:50470&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.https.port&lt;/name&gt;    &lt;value&gt;50470&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;    &lt;value&gt;hadoop1:50070&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.replication&lt;/name&gt;    &lt;value&gt;3&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.blocksize&lt;/name&gt;    &lt;value&gt;134217728&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.use.datanode.hostname&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;fs.permissions.umask-mode&lt;/name&gt;    &lt;value&gt;022&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.acls.enabled&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.use.legacy.blockreader&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;    &lt;value&gt;/var/run/hdfs-sockets/dn&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.read.shortcircuit.skip.checksum&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.domain.socket.data.traffic&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.datanode.hdfs-blocks-metadata.enabled&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="6、hbase"><a href="#6、hbase" class="headerlink" title="6、hbase"></a>6、hbase</h4><p><strong>core-site.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--Autogenerated by Cloudera Manager--&gt;&lt;configuration&gt;  &lt;property&gt;    &lt;name&gt;fs.defaultFS&lt;/name&gt;    &lt;value&gt;hdfs://hadoop1:8020&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;fs.trash.interval&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;io.compression.codecs&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.authentication&lt;/name&gt;    &lt;value&gt;simple&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.authorization&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.rpc.protection&lt;/name&gt;    &lt;value&gt;authentication&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;    &lt;value&gt;DEFAULT&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.oozie.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.oozie.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.mapred.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.mapred.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.flume.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.flume.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.HTTP.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.HTTP.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hive.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hive.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.httpfs.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.httpfs.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hdfs.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hdfs.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.yarn.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.yarn.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.group.mapping&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.security.ShellBasedUnixGroupsMapping&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.security.instrumentation.requires.admin&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.ssl.require.client.cert&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;    &lt;final&gt;true&lt;/final&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.ssl.keystores.factory.class&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory&lt;/value&gt;    &lt;final&gt;true&lt;/final&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;    &lt;value&gt;ssl-server.xml&lt;/value&gt;    &lt;final&gt;true&lt;/final&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.ssl.client.conf&lt;/name&gt;    &lt;value&gt;ssl-client.xml&lt;/value&gt;    &lt;final&gt;true&lt;/final&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre><p><strong>hbase-server-config.properties</strong></p><pre><code>#hbase  开发环境need.init.hbase=true# hbase.zookeeper.quorum=hadoop1.ultiwill.com,hadoop2.ultiwill.com,hadoop3.ultiwill.comhbase.zookeeper.quorum=hadoop1,hadoop2,hadoop3hbase.zookeeper.property.clientPort=2181hbase.rpc.timeout=120000hbase.client.scanner.timeout.period=120000</code></pre><p><strong>hbase-site.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--Autogenerated by Cloudera Manager--&gt;&lt;configuration&gt;  &lt;property&gt;    &lt;name&gt;hbase.rootdir&lt;/name&gt;    &lt;value&gt;hdfs://hadoop1:8020/hbase&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.replication&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.client.write.buffer&lt;/name&gt;    &lt;value&gt;2097152&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.client.pause&lt;/name&gt;    &lt;value&gt;100&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.client.retries.number&lt;/name&gt;    &lt;value&gt;35&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.client.scanner.caching&lt;/name&gt;    &lt;value&gt;100&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.client.keyvalue.maxsize&lt;/name&gt;    &lt;value&gt;10485760&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.ipc.client.allowsInterrupt&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.client.primaryCallTimeout.get&lt;/name&gt;    &lt;value&gt;10&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.client.primaryCallTimeout.multiget&lt;/name&gt;    &lt;value&gt;10&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.fs.tmp.dir&lt;/name&gt;    &lt;value&gt;/user/${user.name}/hbase-staging&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.client.scanner.timeout.period&lt;/name&gt;    &lt;value&gt;60000&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.regionserver.thrift.http&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.thrift.support.proxyuser&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.rpc.timeout&lt;/name&gt;    &lt;value&gt;60000&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.snapshot.enabled&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.snapshot.master.timeoutMillis&lt;/name&gt;    &lt;value&gt;60000&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.snapshot.region.timeout&lt;/name&gt;    &lt;value&gt;60000&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.snapshot.master.timeout.millis&lt;/name&gt;    &lt;value&gt;60000&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.security.authentication&lt;/name&gt;    &lt;value&gt;simple&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.rpc.protection&lt;/name&gt;    &lt;value&gt;authentication&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;zookeeper.session.timeout&lt;/name&gt;    &lt;value&gt;60000&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;zookeeper.znode.parent&lt;/name&gt;    &lt;value&gt;/hbase&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;zookeeper.znode.rootserver&lt;/name&gt;    &lt;value&gt;root-region-server&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;    &lt;value&gt;hadoop1,hadoop3,hadoop2&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;    &lt;value&gt;2181&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.rest.ssl.enabled&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre><p><strong>hdfs-site.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--Autogenerated by Cloudera Manager--&gt;&lt;configuration&gt;  &lt;property&gt;    &lt;name&gt;dfs.permissions&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;    &lt;value&gt;file:///dfs/nn&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.servicerpc-address&lt;/name&gt;    &lt;value&gt;hadoop1:8022&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.https.address&lt;/name&gt;    &lt;value&gt;hadoop1:50470&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.https.port&lt;/name&gt;    &lt;value&gt;50470&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;    &lt;value&gt;hadoop1:50070&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.replication&lt;/name&gt;    &lt;value&gt;3&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.blocksize&lt;/name&gt;    &lt;value&gt;134217728&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.use.datanode.hostname&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;fs.permissions.umask-mode&lt;/name&gt;    &lt;value&gt;022&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.namenode.acls.enabled&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.use.legacy.blockreader&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;    &lt;value&gt;/var/run/hdfs-sockets/dn&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.read.shortcircuit.skip.checksum&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.client.domain.socket.data.traffic&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;dfs.datanode.hdfs-blocks-metadata.enabled&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="7、kafka"><a href="#7、kafka" class="headerlink" title="7、kafka"></a>7、kafka</h4><p><strong>kafka-data-push-info</strong></p><pre><code>--config                            kafka自动推送数据配置目录--timeOut                           推送超时时间    默认 15 min  单位为分钟kafka自动推送数据配置：data.sources                        数据源列表。  （例如：data.sources =bhdb1,dpxx）  {source}.source.type                某个数据源的类型。 （数据源分为数据库和文件两大类， 若为数据库 则使用 数据的名称 例如 oracle,mysql,sqlserver等， 否则使用 file）                                                                                                            例如：bhdb1.source.type=oracle 或者  dpxx.source.type=file数据源为数据库：{source}.db.name                    数据库的名称{source}.db.host                    数据库的ip或者主机名{source}.db.port                    数据库的访问端口， 若不填写则使用该种数据库的默认端口{source}.db.user                    用户名{source}.db.pwd                     密码                                                                 {source}.push.topic                 推送到topic的全局配置，即该数据库下配置的表没有配置topic的时候，其数据会推送到该topic。   {source}.push.tables                需要推送数的表列表 {source}.{table}.push.sql           只推送使用该sql查询到的数据    。       不填则表示推送全部。{source}.{table}.push.adjusterfactory 对推送的数据进行调整  ， 必须为com.bh.d406.bigdata.kafka.producer.DataAdjuster的子类   ，  需要进行调整数据的时候填写{source}.{table}.push.topic         该表的数据推送到topic名称  ， 若不填则使用全局的topic配置数据源为文件：{source}.file.dir                   文件目录    （注意：只支持本地目录 ）    {source}.file.encoding              文件编码      （默认UTF-8）{source}.file.extensions            需要过滤的文件格式列表{source}.file.data.loaderfactory    文件加载器工厂类   {source}.file.data.fields           记录的字段列表      与顺序有关{source}.file.data.spliter          数据的分割符         默认 \t{source}.file.skip.firstline        是否跳过第一行数据                       false  or true{source}.file.data.adjusterfactory  数据矫正工厂类{source}.push.thread.num            读取文件的线程数{source}.push.batch.size            分批推送数据 ， 每批数据大小{source}.push.topic                 数据推送的目标topic名称{source}.store.table                存储的表名</code></pre><p><strong>kafka-server-config.properties</strong></p><pre><code>#################Kafka 全局配置 ######################## 格式为host1:port1,host2:port2，# 这是一个broker列表，用于获得元数据(topics，partitions和replicas)，建立起来的socket连接用于发送实际数据，# 这个列表可以是broker的一个子集，或者一个VIP，指向broker的一个子集# metadata.broker.list=hadoop1:9092,slaver01:9092,slaver02:9092metadata.broker.list=hadoop1:9092# zookeeper列表zk.connect=hadoop1:2181,hadoop2:2181,hadoop3:2181# 字消息的序列化类，默认是的encoder处理一个byte[]，返回一个byte[]# 默认值为 kafka.serializer.DefaultEncoderserializer.class=kafka.serializer.StringEncoder# 用来控制一个produce请求怎样才能算完成，准确的说，是有多少broker必须已经提交数据到log文件，并向leader发送ack，可以设置如下的值：# 0，意味着producer永远不会等待一个来自broker的ack，这就是0.7版本的行为。这个选项提供了最低的延迟，但是持久化的保证是最弱的，当server挂掉的时候会丢失一些数据。# 1，意味着在leader replica已经接收到数据后，producer会得到一个ack。这个选项提供了更好的持久性，因为在server确认请求成功处理后，client才会返回。如果刚写到leader上，还没来得及复制leader就挂了，那么消息才可能会丢失。# -1，意味着在所有的ISR都接收到数据后，producer才得到一个ack。这个选项提供了最好的持久性，只要还有一个replica存活，那么数据就不会丢失。# 默认值  为 0request.required.acks=1# 请求超时时间     默认为 10000request.timeout.ms=60000#决定消息是否应在一个后台线程异步发送。#合法的值为sync，表示异步发送；sync表示同步发送。#设置为async则允许批量发送请求，这回带来更高的吞吐量，但是client的机器挂了的话会丢失还没有发送的数据。#默认值为 syncproducer.type=sync</code></pre><h4 id="8、redis"><a href="#8、redis" class="headerlink" title="8、redis"></a>8、redis</h4><p><strong>redis.properties</strong></p><pre><code>redis.hostname = 192.168.116.202redis.port  = 6379</code></pre><h4 id="9、spark"><a href="#9、spark" class="headerlink" title="9、spark"></a>9、spark</h4><p><strong>hive_fields_mapping.properties</strong></p><pre><code>datatype= base,wechat#base = datatype,idcard,name,age,collecttime,imei#wechat = datatype,wechat,phone,collecttime,imei#============================================================basebase.datatype = stringbase.idcard = stringbase.name = stringbase.age = longbase.collecttime = stringbase.imei = string#============================================================wechatwechat.datatype = stringwechat.wechat = stringwechat.phone = stringwechat.collecttime = stringwechat.imei = string</code></pre><p><strong>relation.properties</strong></p><pre><code>#需要关联的字段relationfield = phone_mac,phone,username,send_mail,imei,imsicomplex_relationfield = card,phone_mac,phone,username,send_mail,imei,imsi</code></pre><p><strong>spark-batch-config.properties</strong></p><pre><code># spark 常规 配置   不包括 流式处理的 配置#################### 全局  ############################## 在用户没有指定时，用于分布式随机操作(groupByKey,reduceByKey等等)的默认的任务数（ shuffle过程中 task的个数 ）# 默认为 8spark.default.parallelism=16# Spark用于缓存的内存大小所占用的Java堆的比率。这个不应该大于JVM中老年代所分配的内存大小# 默认情况下老年代大小是堆大小的2/3，但是你可以通过配置你的老年代的大小，然后再去增加这个比率# 默认为 0.66# spark 1.6 后 过期# spark.storage.memoryFraction=0.66# 在spark1.6.0版本默认大小为： (“Java Heap” – 300MB) * 0.75# 例如：如果堆内存大小有4G，将有2847MB的Spark Memory,Spark Memory=(4*1024MB-300)*0.75=2847MB# 这部分内存会被分成两部分：Storage Memory和Execution Memory# 而且这两部分的边界由spark.memory.storageFraction参数设定，默认是0.5即50%# 新的内存管理模型中的优点是，这个边界不是固定的，在内存压力下这个边界是可以移动的# 如一个区域内存不够用时可以从另一区域借用内存spark.memory.fraction=0.75spark.memory.storageFraction=0.5# 是否要压缩序列化的RDD分区（比如，StorageLevel.MEMORY_ONLY_SER）# 在消耗一点额外的CPU时间的代价下，可以极大的提高减少空间的使用# 默认为 falsespark.rdd.compress=true# The codec used to compress internal data such as RDD partitions,# broadcast variables and shuffle outputs. By default,# Spark provides three codecs: lz4, lzf, and snappy. You can also use fully qualified class names to specify the codec,# e.g.# 1. org.apache.spark.io.LZ4CompressionCodec, # 2. org.apache.spark.io.LZFCompressionCodec, # 3. org.apache.spark.io.SnappyCompressionCodec.   defaultspark.io.compression.codec=org.apache.spark.io.SnappyCompressionCodec# Block size (in bytes) used in Snappy compression,# in the case when Snappy compression codec is used.# Lowering this block size will also lower shuffle memory usage when Snappy is used.# default : 32Kspark.io.compression.snappy.blockSize=32768# 同时获取每一个分解任务的时候，映射输出文件的最大的尺寸（以兆为单位）。# 由于对每个输出都需要我们去创建一个缓冲区去接受它，这个属性值代表了对每个分解任务所使用的内存的一个上限值，# 因此除非你机器内存很大，最好还是配置一下这个值。# 默认48spark.reducer.maxSizeInFlight=48# 这个配置参数仅适用于HashShuffleMananger的实现，同样是为了解决生成过多文件的问题，# 采用的方式是在不同批次运行的Map任务之间重用Shuffle输出文件，也就是说合并的是不同批次的Map任务的输出数据，# 但是每个Map任务所需要的文件还是取决于Reduce分区的数量，因此，它并不减少同时打开的输出文件的数量，# 因此对内存使用量的减少并没有帮助。只是HashShuffleManager里的一个折中的解决方案。# 默认为false#spark.shuffle.consolidateFiles=false#java.io.Externalizable. Java serialization is flexible but often quite slow, and leads to large serialized formats for many classes.#default java.io.Serializable#spark.serializer=org.apache.spark.serializer.KryoSerializer# Speculation是在任务调度的时候，如果没有适合当前本地性要求的任务可供运行，# 将跑得慢的任务在空闲计算资源上再度调度的行为，这些参数调整这些行为的频率和判断指标，默认是不使用Speculation的# 默认为false# 慎用   可能导致数据重复的现象#spark.speculation=true# task失败重试次数# 默认为4spark.task.maxFailures=8# Spark 是有任务的黑名单机制的，但是这个配置在官方文档里面并没有写，可以设置下面的参数，# 比如设置成一分钟之内不要再把任务发到这个 Executor 上了，单位是毫秒。# spark.scheduler.executorTaskBlacklistTime=60000# 超过这个时间，可以执行 NODE_LOCAL 的任务# 默认为 3000spark.locality.wait.process=1# 超过这个时间，可以执行 RACK_LOCAL 的任务# 默认为 3000spark.locality.wait.node=3 # 超过这个时间，可以执行 ANY 的任务# 默认为 3000spark.locality.wait.rack=1000#################### yarn  ############################ 提交的jar文件  的副本数# 默认为 3spark.yarn.submit.file.replication=1# container中的线程数# 默认为 25spark.yarn.containerLauncherMaxThreads=25# 解决yarn-cluster模式下 对处理  permGen space oom异常很有用# spark.yarn.am.extraJavaOptions=# spark.driver.extraJavaOptions=-XX:PermSize=512M -XX:MaxPermSize=1024M# 对象指针压缩 和 gc日志收集打印# spark.executor.extraJavaOptions=-XX:PermSize=512M -XX:MaxPermSize=1024M -XX:MaxDirectMemorySize=1536M -XX:+UseCompressedOops -XX:+PrintGCDetails -XX:+PrintGCTimeStamps# -XX:-UseGCOverheadLimit# GC默认情况下有一个限制，默认是GC时间不能超过2%的CPU时间，但是如果大量对象创建（在Spark里很容易出现，代码模式就是一个RDD转下一个RDD），# 就会导致大量的GC时间，从而出现OutOfMemoryError: GC overhead limit exceeded，可以通过设置-XX:-UseGCOverheadLimit关掉它。# -XX:+UseCompressedOops  可以压缩指针（8字节变成4字节）spark.executor.extraJavaOptions=-XX:PermSize=512M -XX:MaxPermSize=1024m -XX:+CMSClassUnloadingEnabled -Xmn512m -XX:MaxTenuringThreshold=15 -XX:-UseGCOverheadLimit -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=80 -XX:+UseCompressedOops -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:gc.log -XX:+HeapDumpOnOutOfMemoryError# 当shuffle缓存的数据超过此值  强制刷磁盘  单位为 byte# spark.shuffle.spill.initialMemoryThreshold=671088640################### AKKA 相关 ########################### 在控制面板通信（序列化任务和任务结果）的时候消息尺寸的最大值，单位是MB。# 如果你需要给驱动器发回大尺寸的结果（比如使用在一个大的数据集上面使用collect()方法），那么你就该增加这个值了。# 默认为 10spark.akka.frameSize=1024# 用于通信的actor线程数量。如果驱动器有很多CPU核心，那么在大集群上可以增大这个值。# 默认为 4spark.akka.threads=8# Spark节点之间通信的超时时间，以秒为单位# 默认为20sspark.akka.timeout=120# exector的堆外内存（不会占用 分配给executor的jvm内存）# spark.yarn.executor.memoryOverhead=2560</code></pre><p><strong>spark-start-config.properties</strong></p><pre><code># Spark 任务 使用java -cp 方式启动的参数配置#spark.executor.extraLibraryPath=/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.23/lib/hadoop/lib/nativespark.yarn.jar=local:/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.23/lib/spark/lib/spark-assembly.jarspark.authenticate=falsespark.driver.extraLibraryPath=/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.23/lib/hadoop/lib/nativespark.yarn.historyServer.address=http://BH-LAN-Virtual-hadoop-9:18088spark.yarn.am.extraLibraryPath=/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.23/lib/hadoop/lib/nativespark.eventLog.enabled=truespark.dynamicAllocation.schedulerBacklogTimeout=1SPARK_SUBMIT=truespark.yarn.config.gatewayPath=/opt/cloudera/parcelsspark.ui.killEnabled=truespark.serializer=org.apache.spark.serializer.KryoSerializerspark.shuffle.service.enabled=truespark.dynamicAllocation.minExecutors=0spark.dynamicAllocation.executorIdleTimeout=60spark.yarn.config.replacementPath={{HADOOP_COMMON_HOME}}/../../..spark.shuffle.service.port=7337spark.eventLog.dir=hdfs://nameservice1/user/spark/applicationHistoryspark.dynamicAllocation.enabled=true#/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.23/lib/spark/lib/*#/etc/spark/conf.cloudera.spark_on_yarn/#/etc/hadoop/conf.cloudera.yarn/spark.submit.deployMode=clientspark.app.name=defaultspark.master=yarn-clientspark.driver.memory=1gspark.executor.instances=1spark.executor.memory=4gspark.executor.cores=2spark.jars=</code></pre><p><strong>spark-streaming-config.properties</strong></p><pre><code># spark  流式处理的 配置# job的并行度# 默认为 1spark.streaming.concurrentJobs=1# Spark记忆任何元数据(stages生成，任务生成等等)的时间(秒)。周期性清除保证在这个时间之前的元数据会被遗忘。#当长时间几小时，几天的运行Spark的时候设置这个是很有用的。注意：任何内存中的RDD只要过了这个时间就会被清除掉。# 默认 disablespark.cleaner.ttl=3600# 将不再使用的缓存数据清除# 默认为falsespark.streaming.unpersist=true# 从网络中批量接受对象时的持续时间 , 单位  ms。# 默认为200msspark.streaming.blockInterval=200# 控制Receiver速度  单位 s# 因为当streaming程序的数据源的数据量突然变大巨大，可能会导致streaming被撑住导致吞吐不过来，所以可以考虑对于最大吞吐做一下限制。# 默认为 100000spark.streaming.receiver.maxRate=10000# kafka每个分区最大的读取速度   单位 s# 控制kafka读取的量spark.streaming.kafka.maxRatePerPartition=50# 读取kafka的分区最新offset的最大尝试次数# 默认为1spark.streaming.kafka.maxRetries=5# 1、为什么引入Backpressure# 默认情况下，Spark Streaming通过Receiver以生产者生产数据的速率接收数据，计算过程中会出现batch processing time &gt; batch interval的情况，# 其中batch processing time 为实际计算一个批次花费时间， batch interval为Streaming应用设置的批处理间隔。# 这意味着Spark Streaming的数据接收速率高于Spark从队列中移除数据的速率，也就是数据处理能力低，在设置间隔内不能完全处理当前接收速率接收的数据。# 如果这种情况持续过长的时间，会造成数据在内存中堆积，导致Receiver所在Executor内存溢出等问题（如果设置StorageLevel包含disk, 则内存存放不下的数据会溢写至disk, 加大延迟）。# Spark 1.5以前版本，用户如果要限制Receiver的数据接收速率，可以通过设置静态配制参数“spark.streaming.receiver.maxRate”的值来实现，# 此举虽然可以通过限制接收速率，来适配当前的处理能力，防止内存溢出，但也会引入其它问题。比如：producer数据生产高于maxRate，当前集群处理能力也高于maxRate，这就会造成资源利用率下降等问题。# 为了更好的协调数据接收速率与资源处理能力，Spark Streaming 从v1.5开始引入反压机制（back-pressure）,通过动态控制数据接收速率来适配集群数据处理能力。# 2、Backpressure# Spark Streaming Backpressure:  根据JobScheduler反馈作业的执行信息来动态调整Receiver数据接收率。# 通过属性“spark.streaming.backpressure.enabled”来控制是否启用backpressure机制，默认值false，即不启用spark.streaming.backpressure.enabled=truespark.streaming.backpressure.initialRate=200</code></pre><p><strong>datatype/fieldtype.properties</strong></p><p><strong>hive/hive-server-config.properties</strong></p><pre><code># hbase  开发环境</code></pre><p><strong>hive/hive-site.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--Autogenerated by Cloudera Manager--&gt;&lt;configuration&gt;  &lt;property&gt;    &lt;name&gt;hive.metastore.uris&lt;/name&gt;    &lt;value&gt;thrift://hadoop1:9083&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.metastore.client.socket.timeout&lt;/name&gt;    &lt;value&gt;300&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;    &lt;value&gt;/user/hive/warehouse&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.warehouse.subdir.inherit.perms&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.auto.convert.join&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.auto.convert.join.noconditionaltask.size&lt;/name&gt;    &lt;value&gt;20971520&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.optimize.bucketmapjoin.sortedmerge&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.smbjoin.cache.rows&lt;/name&gt;    &lt;value&gt;10000&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.server2.logging.operation.enabled&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;    &lt;value&gt;/hadoop_log/log/hive/operation_logs&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;mapred.reduce.tasks&lt;/name&gt;    &lt;value&gt;-1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.exec.reducers.bytes.per.reducer&lt;/name&gt;    &lt;value&gt;67108864&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.exec.copyfile.maxsize&lt;/name&gt;    &lt;value&gt;33554432&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.exec.reducers.max&lt;/name&gt;    &lt;value&gt;1099&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.vectorized.groupby.checkinterval&lt;/name&gt;    &lt;value&gt;4096&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.vectorized.groupby.flush.percent&lt;/name&gt;    &lt;value&gt;0.1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.compute.query.using.stats&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.vectorized.execution.enabled&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.vectorized.execution.reduce.enabled&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.merge.mapfiles&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.merge.mapredfiles&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.cbo.enable&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt;    &lt;value&gt;minimal&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.fetch.task.conversion.threshold&lt;/name&gt;    &lt;value&gt;268435456&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.limit.pushdown.memory.usage&lt;/name&gt;    &lt;value&gt;0.1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.merge.sparkfiles&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.merge.smallfiles.avgsize&lt;/name&gt;    &lt;value&gt;16777216&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.merge.size.per.task&lt;/name&gt;    &lt;value&gt;268435456&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.optimize.reducededuplication&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.optimize.reducededuplication.min.reducer&lt;/name&gt;    &lt;value&gt;4&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.map.aggr&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.map.aggr.hash.percentmemory&lt;/name&gt;    &lt;value&gt;0.5&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.optimize.sort.dynamic.partition&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.execution.engine&lt;/name&gt;    &lt;value&gt;mr&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.executor.memory&lt;/name&gt;    &lt;value&gt;1369020825&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.driver.memory&lt;/name&gt;    &lt;value&gt;966367641&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.executor.cores&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.yarn.driver.memoryOverhead&lt;/name&gt;    &lt;value&gt;102&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.yarn.executor.memoryOverhead&lt;/name&gt;    &lt;value&gt;230&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.dynamicAllocation.enabled&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.dynamicAllocation.initialExecutors&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.dynamicAllocation.minExecutors&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.dynamicAllocation.maxExecutors&lt;/name&gt;    &lt;value&gt;2147483647&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.metastore.execute.setugi&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.support.concurrency&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt;    &lt;value&gt;hadoop1,hadoop3,hadoop2&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.zookeeper.client.port&lt;/name&gt;    &lt;value&gt;2181&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.zookeeper.namespace&lt;/name&gt;    &lt;value&gt;hive_zookeeper_namespace_hive&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.cluster.delegation.token.store.class&lt;/name&gt;    &lt;value&gt;org.apache.hadoop.hive.thrift.MemoryTokenStore&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hive.server2.use.SSL&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;spark.shuffle.service.enabled&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;</code></pre><h3 id="五．Flume开发"><a href="#五．Flume开发" class="headerlink" title="五．Flume开发"></a>五．Flume开发</h3><p><strong>xz_bigdata_flume</strong></p><p><strong>FTP–&gt;FlumeSource–&gt;拦截器–&gt;FlumeChannel–&gt;FlumeSink–&gt;Kafka</strong></p><p><strong>自定义的内容有：FlumeSource、拦截器、FlumeSink</strong></p><h4 id="1、maven冲突解决和pom-xml"><a href="#1、maven冲突解决和pom-xml" class="headerlink" title="1、maven冲突解决和pom.xml"></a>1、maven冲突解决和pom.xml</h4><p>1.1 安装Maven Helper插件，在Settings里面的Plugins里面搜索Maven Helper，点击Install，安装完毕。</p><p>1.2 ETL包括数据的抽取、转换、加载<br>①数据抽取：从源数据源系统抽取目的数据源系统需要的数据：<br>②数据转换：将从源数据源获取的数据按照业务需求，转换成目的数据源要求的形式，并对错误、不一致的数据进行清洗和加工；<br>③数据加载：将转换后的数据装载到目的数据源。</p><p><img src="/medias/Flume%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.PNG" alt="Flume数据处理流程"></p><p>1.3 pom.xml</p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_flume&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_flume&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;        &lt;flume-ng.version&gt;1.6.0&lt;/flume-ng.version&gt;        &lt;hadoop.version&gt;2.6.0&lt;/hadoop.version&gt;        &lt;jdom.version&gt;1.0&lt;/jdom.version&gt;        &lt;c3p0.version&gt;0.9.5&lt;/c3p0.version&gt;        &lt;hadoop.version&gt;2.6.0&lt;/hadoop.version&gt;        &lt;mybatis.version&gt;3.1.1&lt;/mybatis.version&gt;        &lt;zookeeper.version&gt;3.4.6&lt;/zookeeper.version&gt;        &lt;net.sf.json.version&gt;2.2.3&lt;/net.sf.json.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;fastjson&lt;/artifactId&gt;                    &lt;groupId&gt;com.alibaba&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;commons-configuration&lt;/artifactId&gt;                    &lt;groupId&gt;commons-configuration&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;commons-io&lt;/artifactId&gt;                    &lt;groupId&gt;commons-io&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;commons-lang&lt;/artifactId&gt;                    &lt;groupId&gt;commons-lang&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_kafka&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;snappy-java&lt;/artifactId&gt;                    &lt;groupId&gt;org.xerial.snappy&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;scala-library&lt;/artifactId&gt;                    &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;                    &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;                    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;                    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;log4j&lt;/artifactId&gt;                    &lt;groupId&gt;log4j&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;!--flume核心依赖--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flume&lt;/groupId&gt;            &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt;            &lt;version&gt;${flume-ng.version}-${cdh.version}&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;                    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;guava&lt;/artifactId&gt;                    &lt;groupId&gt;com.google.guava&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;commons-codec&lt;/artifactId&gt;                    &lt;groupId&gt;commons-codec&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;commons-logging&lt;/artifactId&gt;                    &lt;groupId&gt;commons-logging&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;jetty&lt;/artifactId&gt;                    &lt;groupId&gt;org.mortbay.jetty&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;jetty-util&lt;/artifactId&gt;                    &lt;groupId&gt;org.mortbay.jetty&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;servlet-api&lt;/artifactId&gt;                    &lt;groupId&gt;org.mortbay.jetty&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;commons-io&lt;/artifactId&gt;                    &lt;groupId&gt;commons-io&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;commons-lang&lt;/artifactId&gt;                    &lt;groupId&gt;commons-lang&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flume&lt;/groupId&gt;            &lt;artifactId&gt;flume-ng-sdk&lt;/artifactId&gt;            &lt;version&gt;${flume-ng.version}-${cdh.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;!--flume配置依赖--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flume&lt;/groupId&gt;            &lt;artifactId&gt;flume-ng-configuration&lt;/artifactId&gt;            &lt;version&gt;${flume-ng.version}-${cdh.version}&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;guava&lt;/artifactId&gt;                    &lt;groupId&gt;com.google.guava&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;jdom&lt;/groupId&gt;            &lt;artifactId&gt;jdom&lt;/artifactId&gt;            &lt;version&gt;${jdom.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;            &lt;artifactId&gt;log4j-api&lt;/artifactId&gt;            &lt;version&gt;RELEASE&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;log4j&lt;/groupId&gt;            &lt;artifactId&gt;log4j&lt;/artifactId&gt;            &lt;version&gt;RELEASE&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;commons-io&lt;/groupId&gt;            &lt;artifactId&gt;commons-io&lt;/artifactId&gt;            &lt;version&gt;RELEASE&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;commons-lang&lt;/groupId&gt;            &lt;artifactId&gt;commons-lang&lt;/artifactId&gt;            &lt;version&gt;RELEASE&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;commons-configuration&lt;/groupId&gt;            &lt;artifactId&gt;commons-configuration&lt;/artifactId&gt;            &lt;version&gt;RELEASE&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;            &lt;version&gt;RELEASE&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.google.guava&lt;/groupId&gt;            &lt;artifactId&gt;guava&lt;/artifactId&gt;            &lt;version&gt;RELEASE&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;defaultGoal&gt;compile&lt;/defaultGoal&gt;        &lt;sourceDirectory&gt;src/main/java/&lt;/sourceDirectory&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;archive&gt;                        &lt;manifest&gt;                            &lt;addClasspath&gt;true&lt;/addClasspath&gt;                            &lt;classpathPrefix&gt;jars/&lt;/classpathPrefix&gt;                            &lt;mainClass&gt;&lt;/mainClass&gt;                        &lt;/manifest&gt;                    &lt;/archive&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;copy&lt;/id&gt;                        &lt;phase&gt;install&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;outputDirectory&gt;                                ${project.build.directory}/jars                            &lt;/outputDirectory&gt;                            &lt;excludeArtifactIds&gt;javaee-api&lt;/excludeArtifactIds&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt;                &lt;version&gt;2.7&lt;/version&gt;                &lt;configuration&gt;                    &lt;encoding&gt;UTF-8&lt;/encoding&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><h4 id="2、自定义source"><a href="#2、自定义source" class="headerlink" title="2、自定义source"></a>2、自定义source</h4><p><strong>2.1 继承AbstractSource 实现 Configurable, PollableSource接口</strong></p><pre><code>package com.hsiehchou.flume.source;import com.hsiehchou.flume.constant.FlumeConfConstant;import com.hsiehchou.flume.fields.MapFields;import com.hsiehchou.flume.utils.FileUtilsStronger;import org.apache.commons.io.FileUtils;import org.apache.flume.Context;import org.apache.flume.Event;import org.apache.flume.PollableSource;import org.apache.flume.channel.ChannelProcessor;import org.apache.flume.conf.Configurable;import org.apache.flume.event.SimpleEvent;import org.apache.flume.source.AbstractSource;import org.apache.log4j.Logger;import java.io.File;import java.util.*;/** * 固定写法，自定义Source 直接继承 AbstractSource 和 实现 Configurable, PollableSource 接口 * 可参照官网 http://flume.apache.org/releases/content/1.9.0/FlumeDeveloperGuide.html#source */public class FolderSource extends AbstractSource implements Configurable, PollableSource {    private final Logger logger = Logger.getLogger(FolderSource.class);    //tier1.sources.source1.sleeptime=5    //tier1.sources.source1.filenum=3000    //tier1.sources.source1.dirs =/usr/chl/data/filedir/    //tier1.sources.source1.successfile=/usr/chl/data/filedir_successful/    //以下为配置在flume.conf文件中    //读取的文件目录    private String dirStr;    //读取的文件目录，如果多个，以&quot;,&quot;分割，在flume.conf里面配置    private String[] dirs;    //处理成功的文件写入的目录    private String successfile;    //睡眠时间    private long sleeptime = 5;    //每批文件数量    private int filenum = 500;    //以下为配置在txtparse.properties文件中    //读取的所有文件集合    private Collection&lt;File&gt; allFiles;    //一批处理的文件大小    private List&lt;File&gt; listFiles;    private ArrayList&lt;Event&gt; eventList = new ArrayList&lt;Event&gt;();    /**     * @param context 拿到flume配置里面的所有参数     */    @Override    public void configure(Context context) {        logger.info(&quot;开始初始化flume参数&quot;);        initFlumeParams(context);        logger.info(&quot;初始化flume参数成功&quot;);    }    @Override    public Status process() {        //定义处理逻辑        try {            Thread.currentThread().sleep(sleeptime * 1000);        } catch (InterruptedException e) {            logger.error(null, e);        }        Status status = null;        try {            // for (String dir : dirs) {            logger.info(&quot;dirStr===========&quot; + dirStr);            //TODO 1.监控目录下面的所有文件            //读取目录下的文件，获取目录下所有以 &quot;txt&quot;, &quot;bcp&quot; 结尾的文件            allFiles = FileUtils.listFiles(new File(dirStr), new String[]{&quot;txt&quot;, &quot;bcp&quot;}, true);            //如果目录下文件总数大于阈值，则只取 filenum 个文件进行处理            if (allFiles.size() &gt;= filenum) {                //文件数量大于3000 只取3000条                listFiles = ((List&lt;File&gt;) allFiles).subList(0, filenum);            } else {                //文件数量小于3000，取所有文件进行处理                listFiles = ((List&lt;File&gt;) allFiles);            }            //TODO 2.遍历所有的文件进行解析            if (listFiles.size() &gt; 0) {                for (File file : listFiles) {                    //文件名是需要传到channel中的                    String fileName = file.getName();                    //解析文件  获取文件名及文件内容 文件绝对路径  文件内容                    Map&lt;String, Object&gt; stringObjectMap = FileUtilsStronger.parseFile(file, successfile);                    //返回的内容2个参数  一个是文件绝对路径  另一个是lines文件的所有内容                    //获取文件绝对路径                    String absoluteFilename = (String) stringObjectMap.get(MapFields.ABSOLUTE_FILENAME);                    //获取文件内容                    List&lt;String&gt; lines = (List&lt;String&gt;) stringObjectMap.get(MapFields.VALUE);                    //TODO 解析出来之后，需要把解析出来的数据封装为Event                    if (lines != null &amp;&amp; lines.size() &gt; 0) {                        //遍历读取的内容                        for (String line : lines) {                            //封装event Header 将文件名及文件绝对路径通过header传送到channel中                            //构建event头                            Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();                            //文件名                            map.put(MapFields.FILENAME, fileName);                            //文件绝对路径                            map.put(MapFields.ABSOLUTE_FILENAME, absoluteFilename);                            //构建event                            SimpleEvent event = new SimpleEvent();                            //把读取的一行数据转成字节                            byte[] bytes = line.getBytes();                            event.setBody(bytes);                            event.setHeaders(map);                            eventList.add(event);                        }                    }                    try {                        if (eventList.size() &gt; 0) {                            //获取channelProcessor                            ChannelProcessor channelProcessor = getChannelProcessor();                            //通过channelProcessor把eventList发送出去，可以通过拦截器进行拦截                            channelProcessor.processEventBatch(eventList);                            logger.info(&quot;批量推送到 拦截器 数据大小为&quot; + eventList.size());                        }                        eventList.clear();                    } catch (Exception e) {                        eventList.clear();                        logger.error(&quot;发送数据到channel失败&quot;, e);                    } finally {                        eventList.clear();                    }                }            }            // 处理成功，返回成功状态            status = Status.READY;            return status;        } catch (Exception e) {            status = Status.BACKOFF;            logger.error(&quot;异常&quot;, e);            return status;        }    }    /**     * 初始化flume參數     * @param context     */    public void initFlumeParams(Context context) {        //读取flume，conf配置文件，初始化参数        try {            //文件处理目录            //监控的文件目录            dirStr = context.getString(FlumeConfConstant.DIRS);            //监控多个目录            dirs = dirStr.split(&quot;,&quot;);            //成功处理的文件存放目录            successfile = context.getString(FlumeConfConstant.SUCCESSFILE);            //每批处理文件个数            filenum = context.getInteger(FlumeConfConstant.FILENUM);            //睡眠时间            sleeptime = context.getLong(FlumeConfConstant.SLEEPTIME);            logger.info(&quot;dirStr============&quot; + dirStr);            logger.info(&quot;dirs==============&quot; + dirs);            logger.info(&quot;successfile=======&quot; + successfile);            logger.info(&quot;filenum===========&quot; + filenum);            logger.info(&quot;sleeptime=========&quot; + sleeptime);        } catch (Exception e) {            logger.error(&quot;初始化flume参数失败&quot;, e);        }    }    @Override    public long getBackOffSleepIncrement() {        return 0;    }    @Override    public long getMaxBackOffSleepInterval() {        return 0;    }}</code></pre><p><strong>2.2 实现process()方法</strong><br>此处代码已经在2.1里面，不用再写了</p><pre><code> public Status process() {        //定义处理逻辑        try {            Thread.currentThread().sleep(sleeptime * 1000);        } catch (InterruptedException e) {            logger.error(null, e);        }        Status status = null;        try {            // for (String dir : dirs) {            logger.info(&quot;dirStr===========&quot; + dirStr);            //TODO 1.监控目录下面的所有文件            //读取目录下的文件，获取目录下所有以 &quot;txt&quot;, &quot;bcp&quot; 结尾的文件            allFiles = FileUtils.listFiles(new File(dirStr), new String[]{&quot;txt&quot;, &quot;bcp&quot;}, true);            //如果目录下文件总数大于阈值，则只取 filenum 个文件进行处理            if (allFiles.size() &gt;= filenum) {                //文件数量大于3000 只取3000条                listFiles = ((List&lt;File&gt;) allFiles).subList(0, filenum);            } else {                //文件数量小于3000，取所有文件进行处理                listFiles = ((List&lt;File&gt;) allFiles);            }            //TODO 2.遍历所有的文件进行解析            if (listFiles.size() &gt; 0) {                for (File file : listFiles) {                    //文件名是需要传到channel中的                    String fileName = file.getName();                    //解析文件  获取文件名及文件内容 文件绝对路径  文件内容                    Map&lt;String, Object&gt; stringObjectMap = FileUtilsStronger.parseFile(file, successfile);                    //返回的内容2个参数  一个是文件绝对路径  另一个是lines文件的所有内容                    //获取文件绝对路径                    String absoluteFilename = (String) stringObjectMap.get(MapFields.ABSOLUTE_FILENAME);                    //获取文件内容                    List&lt;String&gt; lines = (List&lt;String&gt;) stringObjectMap.get(MapFields.VALUE);                    //TODO 解析出来之后，需要把解析出来的数据封装为Event                    if (lines != null &amp;&amp; lines.size() &gt; 0) {                        //遍历读取的内容                        for (String line : lines) {                            //封装event Header 将文件名及文件绝对路径通过header传送到channel中                            //构建event头                            Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();                            //文件名                            map.put(MapFields.FILENAME, fileName);                            //文件绝对路径                            map.put(MapFields.ABSOLUTE_FILENAME, absoluteFilename);                            //构建event                            SimpleEvent event = new SimpleEvent();                            //把读取的一行数据转成字节                            byte[] bytes = line.getBytes();                            event.setBody(bytes);                            event.setHeaders(map);                            eventList.add(event);                        }                    }                    try {                        if (eventList.size() &gt; 0) {                            //获取channelProcessor                            ChannelProcessor channelProcessor = getChannelProcessor();                            //通过channelProcessor把eventList发送出去，可以通过拦截器进行拦截                            channelProcessor.processEventBatch(eventList);                            logger.info(&quot;批量推送到 拦截器 数据大小为&quot; + eventList.size());                        }                        eventList.clear();                    } catch (Exception e) {                        eventList.clear();                        logger.error(&quot;发送数据到channel失败&quot;, e);                    } finally {                        eventList.clear();                    }                }            }            // 处理成功，返回成功状态            status = Status.READY;            return status;        } catch (Exception e) {            status = Status.BACKOFF;            logger.error(&quot;异常&quot;, e);            return status;        }    }</code></pre><p><strong>source/MySource.java—Flume官网上的案例</strong></p><pre><code>package com.hsiehchou.flume.source;import org.apache.flume.Context;import org.apache.flume.Event;import org.apache.flume.EventDeliveryException;import org.apache.flume.PollableSource;import org.apache.flume.conf.Configurable;import org.apache.flume.event.SimpleEvent;import org.apache.flume.source.AbstractSource;public class MySource extends AbstractSource implements Configurable, PollableSource {    private String myProp;    /**     * 配置读取     * @param context     */    @Override    public void configure(Context context) {        String myProp = context.getString(&quot;myProp&quot;, &quot;defaultValue&quot;);        // Process the myProp value (e.g. validation, convert to another type, ...)        // Store myProp for later retrieval by process() method        this.myProp = myProp;    }    /**     * 定义自己的业务逻辑     * @return     * @throws EventDeliveryException     */    @Override    public Status process() throws EventDeliveryException {        Status status = null;        try {            // This try clause includes whatever Channel/Event operations you want to do            // Receive new data            //需要把自己的数据封装为event进行传输            Event e = new SimpleEvent();            // Store the Event into this Source&#39;s associated Channel(s)            getChannelProcessor().processEvent(e);            status = Status.READY;        } catch (Throwable t) {            // Log exception, handle individual exceptions as needed            status = Status.BACKOFF;            // re-throw all Errors            if (t instanceof Error) {                throw (Error)t;            }        } finally {        }        return status;    }    @Override    public long getBackOffSleepIncrement() {        return 0;    }    @Override    public long getMaxBackOffSleepInterval() {        return 0;    }    @Override    public void start() {        // Initialize the connection to the external client    }    @Override    public void stop () {        // Disconnect from external client and do any additional cleanup        // (e.g. releasing resources or nulling-out field values) ..    }}</code></pre><h4 id="3、自定义interceptor—数据清洗过滤器"><a href="#3、自定义interceptor—数据清洗过滤器" class="headerlink" title="3、自定义interceptor—数据清洗过滤器"></a>3、自定义interceptor—数据清洗过滤器</h4><p><strong>3.1实现Interceptor 接口</strong></p><pre><code>package com.hsiehchou.flume.interceptor;import com.alibaba.fastjson.JSON;import com.hsiehchou.flume.fields.MapFields;import com.hsiehchou.flume.service.DataCheck;import org.apache.commons.io.Charsets;import org.apache.flume.Context;import org.apache.flume.Event;import org.apache.flume.event.SimpleEvent;import org.apache.flume.interceptor.Interceptor;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.ArrayList;import java.util.List;import java.util.Map;/** * 数据清洗过滤器 */public class DataCleanInterceptor implements Interceptor {    private static final Logger LOG = LoggerFactory.getLogger(DataCleanInterceptor.class);    //datatpye.properties    //private static Map&lt;String,ArrayList&lt;String&gt;&gt; dataMap = DataTypeProperties.dataTypeMap;    /**     *  初始化     */    @Override    public void initialize() {    }    /**     * 单条处理     * 拦截方法。数据解析，封装，数据清洗     * @param event     * @return     */    @Override    public Event intercept(Event event) {        SimpleEvent eventNew = new SimpleEvent();        try {            LOG.info(&quot;拦截器Event开始执行&quot;);            Map&lt;String, String&gt; map = parseEvent(event);            if(map == null){                return null;            }            String lineJson = JSON.toJSONString(map);            LOG.info(&quot;拦截器推送数据到channel:&quot; +lineJson);            eventNew.setBody(lineJson.getBytes());        } catch (Exception e) {            LOG.error(null,e);        }        return eventNew;    }    /**     * 批处理     * @param events     * @return     */    @Override    public List&lt;Event&gt; intercept(List&lt;Event&gt; events) {        List&lt;Event&gt; list = new ArrayList&lt;Event&gt;();        for (Event event : events) {            Event intercept = intercept(event);            if (intercept != null) {                list.add(intercept);            }        }        return list;    }    @Override    public void close() {    }    /**     * 数据解析     * @param event     * @return     */    public static Map&lt;String,String&gt; parseEvent(Event event){        if (event == null) {            return null;        }        //000000000000000    000000000000000    24.000000    25.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    andiy    18609765432    judy            1789098763        String line = new String(event.getBody(), Charsets.UTF_8);        //文件名 和 文件绝对路径        String filename = event.getHeaders().get(MapFields.FILENAME);        String absoluteFilename = event.getHeaders().get(MapFields.ABSOLUTE_FILENAME);        //String转map，进行数据校验，检验错误入ES错误表        Map&lt;String, String&gt; map = DataCheck.txtParseAndalidation(line,filename,absoluteFilename);        return map;        //wechat_source1_1111115.txt        //String[] fileNames = filename.split(&quot;_&quot;);        // String转map，并进行数据长度校验，校验错误入ES错误表        //Map&lt;String, String&gt; map = JZDataCheck.txtParse(type, line, source, filename,absoluteFilename);        //Map&lt;String,String&gt; map = new HashMap&lt;&gt;();        //000000000000000    000000000000000    24.000000    25.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    andiy    18609765432    judy            1789098763        //String[] split = line.split(&quot;\t&quot;);        //数据类别        //String dataType = fileNames[0];        //imei,imsi,longitude,latitude,phone_mac,device_mac,device_number,collect_time,username,phone,object_username,send_message,accept_message,message_time        //ArrayList&lt;String&gt; fields = dataMap.get(dataType);        //for (int i = 0; i &lt; split.length; i++) {        //    map.put(fields.get(i),split[i]);        //}        //添加ID        //map.put(MapFields.ID, UUID.randomUUID().toString().replace(&quot;-&quot;,&quot;&quot;));        // map.put(MapFields.TABLE, dataType);        // map.put(MapFields.FILENAME, filename);        // map.put(MapFields.ABSOLUTE_FILENAME, absoluteFilename);//        Map&lt;String, String&gt; map = DataCheck.txtParseAndalidation(line,filename,absoluteFilename);//        return map;    }    /**     * 实例化创建     */    public static class Builder implements Interceptor.Builder {        @Override        public void configure(Context context) {        }        @Override        public Interceptor build() {            return new DataCleanInterceptor();        }    }}</code></pre><h4 id="4、utils工具类"><a href="#4、utils工具类" class="headerlink" title="4、utils工具类"></a>4、utils工具类</h4><p><strong>utils/FileUtilsStronger.java</strong></p><pre><code>package com.hsiehchou.flume.utils;import com.hsiehchou.common.time.TimeTranstationUtils;import com.hsiehchou.flume.fields.MapFields;import org.apache.commons.io.FileUtils;import org.apache.log4j.Logger;import java.io.File;import java.util.*;import static java.io.File.separator;public class FileUtilsStronger {    private static final Logger logger = Logger.getLogger(FileUtilsStronger.class);    /**     * @param file     * @param path     */    public static Map&lt;String,Object&gt; parseFile(File file, String path) {        Map&lt;String,Object&gt; map=new HashMap&lt;String,Object&gt;();        List&lt;String&gt; lines;        String fileNew = path+ TimeTranstationUtils.Date2yyyy_MM_dd()+getDir(file);        try {            if((new File(fileNew+file.getName())).exists()){                try{                    logger.info(&quot;文件名已经存在，开始删除同名已经存在文件&quot;+file.getAbsolutePath());                    file.delete();                    logger.info(&quot;删除同名已经存在文件&quot;+file.getAbsolutePath()+&quot;成功&quot;);                }catch (Exception e){                    logger.error(&quot;删除同名已经存在文件&quot;+file.getAbsolutePath()+&quot;失败&quot;,e);                }            }else{                lines = FileUtils.readLines(file);                map.put(MapFields.ABSOLUTE_FILENAME,fileNew+file.getName());                map.put(MapFields.VALUE,lines);                FileUtils.moveToDirectory(file, new File(fileNew), true);                logger.info(&quot;移动文件到&quot;+file.getAbsolutePath()+&quot;到&quot;+fileNew+&quot;成功&quot;);            }        } catch (Exception e) {            logger.error(&quot;移动文件&quot; + file.getAbsolutePath() + &quot;到&quot; + fileNew + &quot;失败&quot;, e);        }        return map;    }    /**     * @param file     * @param path     */    public static List&lt;String&gt; chanmodName(File file, String path) {        List&lt;String&gt; lines=null;        try {            if((new File(path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;/&quot;+file.getName())).exists()){                logger.warn(&quot;文件名已经存在，开始删除同名文件&quot; +path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;/&quot;+file.getName());                try{                    file.delete();                    logger.warn(&quot;删除同名文件&quot;+file.getAbsolutePath()+&quot;成功&quot;);                }catch (Exception e){                    logger.warn(&quot;删除同名文件&quot;+file.getAbsolutePath()+&quot;失败&quot;,e);                }            }else{                lines = FileUtils.readLines(file);                FileUtils.moveToDirectory(file, new File(path+ TimeTranstationUtils.Date2yyyy_MM_dd()), true);                logger.info(&quot;移动文件到&quot;+file.getAbsolutePath()+&quot;到&quot;+path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;成功&quot;);            }        } catch (Exception e) {            logger.error(&quot;移动文件&quot; + file.getName() + &quot;到&quot; + path+ TimeTranstationUtils.Date2yyyy_MM_dd() + &quot;失败&quot;, e);        }        return lines;    }    /**     * @param file     * @param path     */    public static void moveFile2unmanage(File file, String path) {        try {            if((new File(path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;/&quot;+file.getName())).exists()){                logger.warn(&quot;文件名已经存在，开始删除同名文件&quot; +file.getAbsolutePath());                try{                    file.delete();                    logger.warn(&quot;删除同名文件&quot;+file.getAbsolutePath()+&quot;成功&quot;);                }catch (Exception e){                    logger.warn(&quot;删除同名文件&quot;+file.getAbsolutePath()+&quot;失败&quot;,e);                }            }else{                FileUtils.moveToDirectory(file, new File(path+ TimeTranstationUtils.Date2yyyy_MM_dd()), true);                //logger.info(&quot;移动文件到&quot;+file.getAbsolutePath()+&quot;到&quot;+path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;成功&quot;);            }        } catch (Exception e) {            logger.error(&quot;移动错误文件&quot; + file.getName() + &quot;到&quot; + path+ TimeTranstationUtils.Date2yyyy_MM_dd() + &quot;失败&quot;, e);        }    }    /**     * @param file     * @param path     */    public static void shnegtingChanmodName(File file, String path) {        try {            if((new File(path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;/&quot;+file.getName())).exists()){                logger.warn(&quot;文件名已经存在，开始删除同名文件&quot; +path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;/&quot;+file.getName());                try{                    file.delete();                    logger.warn(&quot;删除同名文件&quot;+file.getAbsolutePath()+&quot;成功&quot;);                }catch (Exception e){                    logger.warn(&quot;删除同名文件&quot;+file.getAbsolutePath()+&quot;失败&quot;,e);                }            }else{                FileUtils.moveToDirectory(file, new File(path+ TimeTranstationUtils.Date2yyyy_MM_dd()), true);                logger.info(&quot;移动文件到&quot;+file.getAbsolutePath()+&quot;到&quot;+path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;成功&quot;);            }        } catch (Exception e) {            logger.error(&quot;移动文件&quot; + file.getName() + &quot;到&quot; + path+ TimeTranstationUtils.Date2yyyy_MM_dd() + &quot;失败&quot;, e);        }    }    /**     * 获取文件父目录     * @param file     * @return     */    public static String getDir(File file){        String dir=file.getParent();        StringTokenizer dirs = new StringTokenizer(dir, separator);        List&lt;String&gt; list=new ArrayList&lt;String&gt;();        while(dirs.hasMoreTokens()){            list.add((String)dirs.nextElement());        }        String str=&quot;&quot;;        for(int i=2;i&lt;list.size();i++){            str=str+separator+list.get(i);        }        return str+&quot;/&quot;;    }}</code></pre><p><strong>utils/Validation.java—验证工具类</strong></p><pre><code>package com.hsiehchou.flume.utils;import java.util.regex.Matcher;import java.util.regex.Pattern;/** * 验证工具类 */@Deprecatedpublic class Validation {     // ------------------常量定义    /**     * Email正则表达式=     * &quot;^([a-z0-9A-Z]+[-|\\.]?)+[a-z0-9A-Z]@([a-z0-9A-Z]+(-[a-z0-9A-Z]+)?\\.)+[a-zA-Z]{2,}$&quot;     * ;     */    // public static final String EMAIL =    // &quot;^([a-z0-9A-Z]+[-|\\.]?)+[a-z0-9A-Z]@([a-z0-9A-Z]+(-[a-z0-9A-Z]+)?\\.)+[a-zA-Z]{2,}$&quot;;;    public static final String EMAIL = &quot;\\w+(\\.\\w+)*@\\w+(\\.\\w+)+&quot;;    /**     * 电话号码正则表达式=     * (^(\d{2,4}[-_－—]?)?\d{3,8}([-_－—]?\d{3,8})?([-_－—]?\d{1,7})?$)|     * (^0?1[35]\d{9}$)     */    public static final String PHONE = &quot;(^(\\d{2,4}[-_－—]?)?\\d{3,8}([-_－—]?\\d{3,8})?([-_－—]?\\d{1,7})?$)|(^0?1[35]\\d{9}$)&quot;;    /**     * 手机号码正则表达式=^(13[0-9]|15[0-9]|18[0-9])\d{8}$     */    public static final String MOBILE = &quot;^((13[0-9])|(14[5-7])|(15[^4])|(17[0-8])|(18[0-9]))\\d{8}$&quot;;    /**     * Integer正则表达式 ^-?(([1-9]\d*$)|0)     */    public static final String INTEGER = &quot;^-?(([1-9]\\d*$)|0)&quot;;    /**     * 正整数正则表达式 &gt;=0 ^[1-9]\d*|0$     */    public static final String INTEGER_NEGATIVE = &quot;^[1-9]\\d*|0$&quot;;    /**     * 负整数正则表达式 &lt;=0 ^-[1-9]\d*|0$     */    public static final String INTEGER_POSITIVE = &quot;^-[1-9]\\d*|0$&quot;;    /**     * Double正则表达式 ^-?([1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0)$     */    public static final String DOUBLE = &quot;^-?([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0)$&quot;;    /**     * 正Double正则表达式 &gt;=0 ^[1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0$　     */    public static final String DOUBLE_NEGATIVE = &quot;^[1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0$&quot;;    /**     * 负Double正则表达式 &lt;= 0 ^(-([1-9]\d*\.\d*|0\.\d*[1-9]\d*))|0?\.0+|0$     */    public static final String DOUBLE_POSITIVE = &quot;^(-([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*))|0?\\.0+|0$&quot;;    /**     * 年龄正则表达式 ^(?:[1-9][0-9]?|1[01][0-9]|120)$ 匹配0-120岁     */    public static final String AGE = &quot;^(?:[1-9][0-9]?|1[01][0-9]|120)$&quot;;    /**     * 邮编正则表达式 [0-9]\d{5}(?!\d) 国内6位邮编     */    public static final String CODE = &quot;[0-9]\\d{5}(?!\\d)&quot;;    /**     * 匹配由数字、26个英文字母或者下划线组成的字符串 ^\w+$     */    public static final String STR_ENG_NUM_ = &quot;^\\w+$&quot;;    /**     * 匹配由数字和26个英文字母组成的字符串 ^[A-Za-z0-9]+$     */    public static final String STR_ENG_NUM = &quot;^[A-Za-z0-9]+&quot;;    /**     * 匹配由26个英文字母组成的字符串 ^[A-Za-z]+$     */    public static final String STR_ENG = &quot;^[A-Za-z]+$&quot;;    /**     * 过滤特殊字符串正则 regEx=     * &quot;[`~!@#$%^&amp;*()+=|{}&#39;:;&#39;,\\[\\].&lt;&gt;/?~！@#￥%……&amp;*（）——+|{}【】‘；：”“’。，、？]&quot;;     */    public static final String STR_SPECIAL = &quot;[`~!@#$%^&amp;*()+=|{}&#39;:;&#39;,\\[\\].&lt;&gt;/?~！@#￥%……&amp;*（）——+|{}【】‘；：”“’。，、？]&quot;;    /***     * 日期正则 支持： YYYY-MM-DD YYYY/MM/DD YYYY_MM_DD YYYYMMDD YYYY.MM.DD的形式     */    public static final String DATE_ALL = &quot;((^((1[8-9]\\d{2})|([2-9]\\d{3}))([-\\/\\._]?)(10|12|0?[13578])([-\\/\\._]?)(3[01]|[12][0-9]|0?[1-9])$)&quot;            + &quot;|(^((1[8-9]\\d{2})|([2-9]\\d{3}))([-\\/\\._]?)(11|0?[469])([-\\/\\._]?)(30|[12][0-9]|0?[1-9])$)&quot;            + &quot;|(^((1[8-9]\\d{2})|([2-9]\\d{3}))([-\\/\\._]?)(0?2)([-\\/\\._]?)(2[0-8]|1[0-9]|0?[1-9])$)|(^([2468][048]00)([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|(^([3579][26]00)&quot;            + &quot;([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)&quot;            + &quot;|(^([1][89][0][48])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|(^([2-9][0-9][0][48])([-\\/\\._]?)&quot;            + &quot;(0?2)([-\\/\\._]?)(29)$)&quot;            + &quot;|(^([1][89][2468][048])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|(^([2-9][0-9][2468][048])([-\\/\\._]?)(0?2)&quot;            + &quot;([-\\/\\._]?)(29)$)|(^([1][89][13579][26])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|&quot;            + &quot;(^([2-9][0-9][13579][26])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$))&quot;;    /***     * 日期正则 支持： YYYY-MM-DD     */    public static final String DATE_FORMAT1 = &quot;(([0-9]{3}[1-9]|[0-9]{2}[1-9][0-9]{1}|[0-9]{1}[1-9][0-9]{2}|[1-9][0-9]{3})-(((0[13578]|1[02])-(0[1-9]|[12][0-9]|3[01]))|((0[469]|11)-(0[1-9]|[12][0-9]|30))|(02-(0[1-9]|[1][0-9]|2[0-8]))))|((([0-9]{2})(0[48]|[2468][048]|[13579][26])|((0[48]|[2468][048]|[3579][26])00))-02-29)&quot;;    /**     * URL正则表达式 匹配 http www ftp     */    public static final String URL = &quot;^(http|www|ftp|)?(://)?(\\w+(-\\w+)*)(\\.(\\w+(-\\w+)*))*((:\\d+)?)(/(\\w+(-\\w+)*))*(\\.?(\\w)*)(\\?)?&quot;            + &quot;(((\\w*%)*(\\w*\\?)*(\\w*:)*(\\w*\\+)*(\\w*\\.)*(\\w*&amp;)*(\\w*-)*(\\w*=)*(\\w*%)*(\\w*\\?)*&quot;            + &quot;(\\w*:)*(\\w*\\+)*(\\w*\\.)*&quot;            + &quot;(\\w*&amp;)*(\\w*-)*(\\w*=)*)*(\\w*)*)$&quot;;    /**     * 身份证正则表达式     */    public static final String IDCARD = &quot;((11|12|13|14|15|21|22|23|31|32|33|34|35|36|37|41|42|43|44|45|46|50|51|52|53|54|61|62|63|64|65)[0-9]{4})&quot;            + &quot;(([1|2][0-9]{3}[0|1][0-9][0-3][0-9][0-9]{3}&quot;            + &quot;[Xx0-9])|([0-9]{2}[0|1][0-9][0-3][0-9][0-9]{3}))&quot;;    /**     * 机构代码     */    public static final String JIGOU_CODE = &quot;^[A-Z0-9]{8}-[A-Z0-9]$&quot;;    /**     * 匹配数字组成的字符串 ^[0-9]+$     */    public static final String STR_NUM = &quot;^[0-9]+$&quot;;    // //------------------验证方法    /**     * 判断字段是否为空 符合返回ture     * @param str     * @return boolean     */    public static synchronized boolean StrisNull(String str) {        return null == str || str.trim().length() &lt;= 0 ? true : false;    }    /**     * 判断字段是非空 符合返回ture     * @param str     * @return boolean     */    public static boolean StrNotNull(String str) {        return !StrisNull(str);    }    /**     * 字符串null转空     * @param str     * @return boolean     */    public static String nulltoStr(String str) {        return StrisNull(str) ? &quot;&quot; : str;    }    /**     * 字符串null赋值默认值     * @param str 目标字符串     * @param defaut 默认值     * @return String     */    public static String nulltoStr(String str, String defaut) {        return StrisNull(str) ? defaut : str;    }    /**     * 判断字段是否为Email 符合返回ture     * @param str     * @return boolean     */    public static boolean isEmail(String str) {        return Regular(str, EMAIL);    }    /**     * 判断是否为电话号码 符合返回ture     * @param str     * @return boolean     */    public static boolean isPhone(String str) {        return Regular(str, PHONE);    }    /**     * 判断是否为手机号码 符合返回ture     * @param str     * @return boolean     */    public static boolean isMobile(String str) {        return RegularSJHM(str, MOBILE);    }    /**     * 判断是否为Url 符合返回ture     * @param str     * @return boolean     */    public static boolean isUrl(String str) {        return Regular(str, URL);    }    /**     * 判断字段是否为数字 正负整数 正负浮点数 符合返回ture     * @param str     * @return boolean     */    public static boolean isNumber(String str) {        return Regular(str, DOUBLE);    }    /**     * 判断字段是否为INTEGER 符合返回ture     * @param str     * @return boolean     */    public static boolean isInteger(String str) {        return Regular(str, INTEGER);    }    /**     * 判断字段是否为正整数正则表达式 &gt;=0 符合返回ture     * @param str     * @return boolean     */    public static boolean isINTEGER_NEGATIVE(String str) {        return Regular(str, INTEGER_NEGATIVE);    }    /**     * 判断字段是否为负整数正则表达式 &lt;=0 符合返回ture     * @param str     * @return boolean     */    public static boolean isINTEGER_POSITIVE(String str) {        return Regular(str, INTEGER_POSITIVE);    }    /**     * 判断字段是否为DOUBLE 符合返回ture     * @param str     * @return boolean     */    public static boolean isDouble(String str) {        return Regular(str, DOUBLE);    }    /**     * 判断字段是否为正浮点数正则表达式 &gt;=0 符合返回ture     * @param str     * @return boolean     */    public static boolean isDOUBLE_NEGATIVE(String str) {        return Regular(str, DOUBLE_NEGATIVE);    }    /**     * 判断字段是否为负浮点数正则表达式 &lt;=0 符合返回ture     * @param str     * @return boolean     */    public static boolean isDOUBLE_POSITIVE(String str) {        return Regular(str, DOUBLE_POSITIVE);    }    /**     * 判断字段是否为日期 符合返回ture     * @param str     * @return boolean     */    public static boolean isDate(String str) {        return Regular(str, DATE_ALL);    }    /**     * 验证     * @param str     * @return     */    public static boolean isDate1(String str) {        return Regular(str, DATE_FORMAT1);    }    /**     * 判断字段是否为年龄 符合返回ture     * @param str     * @return boolean     */    public static boolean isAge(String str) {        return Regular(str, AGE);    }    /**     * 判断字段是否超长 字串为空返回fasle, 超过长度{leng}返回ture 反之返回false     * @param str     * @param leng     * @return boolean     */    public static boolean isLengOut(String str, int leng) {        return StrisNull(str) ? false : str.trim().length() &gt; leng;    }    /**     * 判断字段是否为身份证 符合返回ture     * @param str     * @return boolean     */    public static boolean isIdCard(String str) {        if (StrisNull(str))            return false;        if (str.trim().length() == 15 || str.trim().length() == 18) {            return Regular(str, IDCARD);        } else {            return false;        }    }    /**     * 判断字段是否为邮编 符合返回ture     * @param str     * @return boolean     */    public static boolean isCode(String str) {        return Regular(str, CODE);    }    /**     * 判断字符串是不是全部是英文字母     * @param str     * @return boolean     */    public static boolean isEnglish(String str) {        return Regular(str, STR_ENG);    }    /**     * 判断字符串是不是全部是英文字母+数字     * @param str     * @return boolean     */    public static boolean isENG_NUM(String str) {        return Regular(str, STR_ENG_NUM);    }    /**     * 判断字符串是不是全部是英文字母+数字+下划线     * @param str     * @return boolean     */    public static boolean isENG_NUM_(String str) {        return Regular(str, STR_ENG_NUM_);    }    /**     * 过滤特殊字符串 返回过滤后的字符串     * @param str     * @return boolean     */    public static String filterStr(String str) {        Pattern p = Pattern.compile(STR_SPECIAL);        Matcher m = p.matcher(str);        return m.replaceAll(&quot;&quot;).trim();    }    /**     * 校验机构代码格式     * @return     */    public static boolean isJigouCode(String str) {        return Regular(str, JIGOU_CODE);    }    /**     * 判断字符串是不是数字组成     * @param str     * @return boolean     */    public static boolean isSTR_NUM(String str) {        return Regular(str, STR_NUM);    }    /**     * 匹配是否符合正则表达式pattern 匹配返回true     * @param str 匹配的字符串     * @param pattern 匹配模式     * @return boolean     */    private static boolean Regular(String str, String pattern) {        if (null == str || str.trim().length() &lt;= 0)            return false;        Pattern p = Pattern.compile(pattern);        Matcher m = p.matcher(str);        return m.matches();    }    /**     * 匹配是否符合正则表达式pattern 匹配返回true     * @param str 匹配的字符串     * @param pattern 匹配模式     * @return boolean     */    private static boolean RegularSJHM(String str, String pattern) {        if (null == str || str.trim().length() &lt;= 0){            return false;        }        if(str.contains(&quot;+86&quot;)){            str=str.replace(&quot;+86&quot;,&quot;&quot;);        }        Pattern p = Pattern.compile(pattern);        Matcher m = p.matcher(str);        return m.matches();    }    /**     * description:匹配yyyyMMddHHmmss格式时间     * @param time     * @return boolean     */    public static final String yyyyMMddHHmmss = &quot;[0-9]{14}&quot;;    public static boolean isyyyyMMddHHmmss(String time) {        if (time == null) {            return false;        }        boolean bool = time.matches(yyyyMMddHHmmss);        return bool;    }    /**     * description:匹配yyyyMMddHHmmss格式时间     * @param time     * @return boolean     */    public static final String isMac = &quot;^[A-Fa-f0-9]{2}(-[A-Fa-f0-9]{2}){5}$&quot;;    public static boolean isMac(String mac) {        if (mac == null) {            return false;        }        boolean bool = mac.matches(isMac);        return bool;    }    /**     * description:匹配yyyyMMddHHmmss格式时间     * @param time     * @return boolean     */    public static final String longtime = &quot;[0-9]{10}&quot;;    public static boolean isTimestamp(String timestamp) {        if (timestamp == null) {            return false;        }        boolean bool = timestamp.matches(longtime);        return bool;    }    /**     * 判断字段是否为datatype 符合返回ture     * @param str     * @return boolean     */    public static final String DATATYPE = &quot;^\\d{7}$&quot;;    public static boolean isDATATYPE(String str) {        return Regular(str, DATATYPE);    }    /**     * 判断字段是否为QQ 符合返回ture     * @param str     * @return boolean     */    public static final String QQ = &quot;^\\d{5,15}$&quot;;    public static boolean isQQ(String str) {        return Regular(str, QQ);    }    /**     * 判断字段是否为IMSI 符合返回ture     * @param str     * @return boolean     */    public static final String IMSI = &quot;^4600[0,1,2,3,4,5,6,7,9]\\d{10}|(46011|46020)\\d{10}$&quot;;    public static boolean isIMSI(String str) {        return Regular(str, IMSI);    }    /**     * 判断字段是否为IMEI 符合返回ture     * @param str     * @return boolean     */    public static final String IMEI = &quot;^\\d{8}$|^[a-fA-F0-9]{14}$|^\\d{15}$&quot;;    public static boolean isIMEI(String str) {return Regular(str, IMEI);}    /**     * 判断字段是否为CAPTURETIME 符合返回ture     * @param str     * @return boolean     */    public static final String CAPTURETIME = &quot;^\\d{10}|(20[0-9][0-9])\\d{10}$&quot;;    public static boolean isCAPTURETIME(String str) {return Regular(str, CAPTURETIME);}    /**     * description:检测认证类型     * @param auth     * @return boolean     */    public static final String AUTH_TYPE = &quot;^\\d{7}$&quot;;    public static boolean isAUTH_TYPE(String str) {return Regular(str, CAPTURETIME);}    /**     * description:检测FIRM_CODE     * @param auth     * @return boolean     */    public static final String FIRM_CODE = &quot;^\\d{9}$&quot;;    public static boolean isFIRM_CODE(String str) {return Regular(str, FIRM_CODE);}    /**     * description:检测经度     * @param auth     * @return boolean     */    public static final String LONGITUDE = &quot;^-?(([1-9]\\d?)|(1[0-7]\\d)|180)(\\.\\d{1,6})?$&quot;;    //public static final String LONGITUDE =&quot;^([-]?(\\d|([1-9]\\d)|(1[0-7]\\d)|(180))(\\.\\d*)\\,[-]?(\\d|([1-8]\\d)|(90))(\\.\\d*))$&quot;;    public static boolean isLONGITUDE(String str) {return Regular(str, LONGITUDE);}    /**     * description:检测纬度     *     * @param auth     * @return boolean 2016-7-19 下午4:50:06 by      */    public static final String LATITUDE = &quot;^-?(([1-8]\\d?)|([1-8]\\d)|90)(\\.\\d{1,6})?$&quot;;    public static boolean isLATITUDE(String str) {return Regular(str, LATITUDE);}    public static void main(String[] args) {        boolean bool = isLATITUDE(&quot;25.546685&quot;);        System.out.println(bool);    }}</code></pre><h4 id="5、constant常量"><a href="#5、constant常量" class="headerlink" title="5、constant常量"></a>5、constant常量</h4><p><strong>constant/FlumeConfConstant.java</strong></p><pre><code>package com.hsiehchou.flume.constant;public class FlumeConfConstant {    //flumeSource配置    public static final String UNMANAGE=&quot;unmanage&quot;;    public static final String DIRS=&quot;dirs&quot;;    public static final String SUCCESSFILE=&quot;successfile&quot;;    public static final String ALL=&quot;all&quot;;    public static final String SOURCE=&quot;source&quot;;    public static final String FILENUM=&quot;filenum&quot;;    public static final String SLEEPTIME=&quot;sleeptime&quot;;    //ESSINK配置    public static final String TIMECELL=&quot;timecell&quot;;    public static final String MAXNUM=&quot;maxnum&quot;;    public static final String SINK_SOURCE=&quot;source&quot;;    public static final String THREADNUM=&quot;threadnum&quot;;    public static final String REDISHOST=&quot;redishost&quot;;}</code></pre><p><strong>constant/TxtConstant.java</strong></p><pre><code>package com.hsiehchou.flume.constant;public class TxtConstant {    public static final String TYPE_ES=&quot;TYPE_ES&quot;;    public static final String STATIONCENTER=&quot;STATIONCENTER&quot;;    public static final String APCENTER=&quot;APCENTER&quot;;    public static final String IPLOGINLOG=&quot;IPLOGINLOG&quot;;    public static final String IMSIIMEI=&quot;IMSIIMEI&quot;;    public static final String MACHOUR=&quot;MACHOUR&quot;;    public static final String TYPE_SITEMANAGE=&quot;TYPE_SITEMANAGE&quot;;    public static final String JZWA=&quot;JZWA&quot;;    public static final String FIRMCODE=&quot;FIRMCODE&quot;;    public static final String FILENAME_FIELDS1=&quot;FILENAME_FIELDS1&quot;;    public static final String FILENAME_FIELDS2=&quot;FILENAME_FIELDS2&quot;;    public static final String FILENAME_FIELDS3=&quot;FILENAME_FIELDS3&quot;;    public static final String FILENAME_FIELDS4=&quot;FILENAME_FIELDS4&quot;;    public static final String FILENAME_FIELDS5=&quot;FILENAME_FIELDS5&quot;;    public static final String FILENAME_VALIDATION=&quot;FILENAME_VALIDATION&quot;;    public static final String AUTHTYPE_LIST=&quot;AUTHTYPE_LIST&quot;;    public static final String SOURCE_FEIJING=&quot;SOURCE_FEIJING&quot;;    public static final String SOURCE_650=&quot;SOURCE_650&quot;;    public static final String OFFICE_11=&quot;OFFICE_11&quot;;    public static final String OFFICE_12=&quot;OFFICE_12&quot;;    public static final String WLZK=&quot;WLZK&quot;;    public static final String FEIJING=&quot;FEIJING&quot;;    public static final String HLWZC=&quot;HLWZC&quot;;    public static final String WIFIWL=&quot;WIFIWL&quot;;    // 错误索引    public static final String ERROR_INDEX=&quot;es.errorindex&quot;;    public static final String ERROR_TYPE=&quot;es.errortype&quot;;    //WIFI索引    public static final String WIFILOG_INDEX=&quot;es.index.wifilog&quot;;    public static final String IPLOGINLOG_TYPE=&quot;es.type.iploginlog&quot;;    public static final String EMAIL_TYPE=&quot;es.type.email&quot;;    public static final String FTP_TYPE=&quot;es.type.ftp&quot;;    public static final String GAME_TYPE=&quot;es.type.game&quot;;    public static final String HEARTBEAT_TYPE=&quot;es.type.heartbeat&quot;;    public static final String HTTP_TYPE=&quot;es.type.http&quot;;    public static final String IMINFO_TYPE=&quot;es.type.iminfo&quot;;    public static final String ORGANIZATION_TYPE=&quot;es.type.organization&quot;;    public static final String SEARCH_TYPE=&quot;es.type.search&quot;;    public static final String IMSIIMEI_TYPE=&quot;es.type.imsiimei&quot;;}</code></pre><h4 id="6、field字段"><a href="#6、field字段" class="headerlink" title="6、field字段"></a>6、field字段</h4><p><strong>field/ErrorMapFields.java</strong></p><pre><code>package com.hsiehchou.flume.fields;public class ErrorMapFields {    public static final String RKSJ=&quot;RKSJ&quot;;    public static final String RECORD=&quot;RECORD&quot;;    public static final String LENGTH=&quot;LENGTH&quot;;    public static final String LENGTH_ERROR=&quot;LENGTH_ERROR&quot;;    public static final String LENGTH_ERROR_NUM=&quot;10001&quot;;    public static final String FILENAME=&quot;FILENAME&quot;;    public static final String FILENAME_ERROR=&quot;FILENAME_ERROR&quot;;    public static final String FILENAME_ERROR_NUM=&quot;10010&quot;;    public static final String ABSOLUTE_FILENAME=&quot;ABSOLUTE_FILENAME&quot;;    public static final String SJHM=&quot;SJHM&quot;;    public static final String SJHM_ERROR=&quot;SJHM_ERROR&quot;;    public static final String SJHM_ERRORCODE=&quot;10007&quot;;    public static final String DATA_TYPE=&quot;DATA_TYPE&quot;;    public static final String DATA_TYPE_ERROR=&quot;DATA_TYPE_ERROR&quot;;    public static final String DATA_TYPE_ERRORCODE=&quot;10011&quot;;    public static final String QQ=&quot;QQ&quot;;    public static final String QQ_ERROR=&quot;QQ_ERROR&quot;;    public static final String QQ_ERRORCODE=&quot;10002&quot;;    public static final String IMSI=&quot;IMSI&quot;;    public static final String IMSI_ERROR=&quot;IMSI_ERROR&quot;;    public static final String IMSI_ERRORCODE=&quot;10005&quot;;    public static final String IMEI=&quot;IMEI&quot;;    public static final String IMEI_ERROR=&quot;IMEI_ERROR&quot;;    public static final String IMEI_ERRORCODE=&quot;10006&quot;;    public static final String MAC=&quot;MAC&quot;;    public static final String CLIENTMAC=&quot;CLIENTMAC&quot;;    public static final String STATIONMAC=&quot;STATIONMAC&quot;;    public static final String BSSID=&quot;BSSID&quot;;    public static final String MAC_ERROR=&quot;MAC_ERROR&quot;;    public static final String MAC_ERRORCODE=&quot;10003&quot;;    public static final String DEVICENUM=&quot;DEVICENUM&quot;;    public static final String DEVICENUM_ERROR=&quot;DEVICENUM_ERROR&quot;;    public static final String DEVICENUM_ERRORCODE=&quot;10014&quot;;    public static final String CAPTURETIME=&quot;CAPTURETIME&quot;;    public static final String CAPTURETIME_ERROR=&quot;CAPTURETIME_ERROR&quot;;    public static final String CAPTURETIME_ERRORCODE=&quot;10019&quot;;    public static final String EMAIL=&quot;EMAIL&quot;;    public static final String EMAIL_ERROR=&quot;EMAIL_ERROR&quot;;    public static final String EMAIL_ERRORCODE=&quot;10004&quot;;    public static final String AUTH_TYPE=&quot;AUTH_TYPE&quot;;    public static final String AUTH_TYPE_ERROR=&quot;AUTH_TYPE_ERROR&quot;;    public static final String AUTH_TYPE_ERRORCODE=&quot;10020&quot;;    public static final String FIRM_CODE=&quot;FIRM_CODE&quot;;    public static final String FIRMCODE_NUM=&quot;FIRMCODE_NUM&quot;;    public static final String FIRM_CODE_ERROR=&quot;FIRM_CODE_ERROR&quot;;    public static final String FIRM_CODE_ERRORCODE=&quot;10009&quot;;    public static final String STARTTIME=&quot;STARTTIME&quot;;    public static final String STARTTIME_ERROR=&quot;STARTTIME_ERROR&quot;;    public static final String STARTTIME_ERRORCODE=&quot;10015&quot;;    public static final String ENDTIME=&quot;ENDTIME&quot;;    public static final String ENDTIME_ERROR=&quot;ENDTIME_ERROR&quot;;    public static final String ENDTIME_ERRORCODE=&quot;10016&quot;;    public static final String LOGINTIME=&quot;LOGINTIME&quot;;    public static final String LOGINTIME_ERROR=&quot;LOGINTIME_ERROR&quot;;    public static final String LOGINTIME_ERRORCODE=&quot;10017&quot;;    public static final String LOGOUTTIME=&quot;LOGOUTTIME&quot;;    public static final String LOGOUTTIME_ERROR=&quot;LOGOUTTIME_ERROR&quot;;    public static final String LOGOUTTIME_ERRORCODE=&quot;10018&quot;;    public static final String LONGITUDE=&quot;LONGITUDE&quot;;    public static final String LONGITUDE_ERROR=&quot;LONGITUDE_ERROR&quot;;    public static final String LONGITUDE_ERRORCODE=&quot;10012&quot;;    public static final String LATITUDE=&quot;LATITUDE&quot;;    public static final String LATITUDE_ERROR=&quot;LATITUDE_ERROR&quot;;    public static final String LATITUDE_ERRORCODE=&quot;10013&quot;;    //TODO 其他类型DATA_TYPE  记录    public static final String DATA_TYPE_OTHER=&quot;DATA_TYPE_OTHER&quot;;    public static final String DATA_TYPE_OTHER_ERROR=&quot;DATA_TYPE_OTHER_ERROR&quot;;    public static final String DATA_TYPE_OTHER_ERRORCODE=&quot;10022&quot;;    //TODO USERNAME 错误    public static final String USERNAME=&quot;USERNAME&quot;;    public static final String USERNAME_ERROR=&quot;USERNAME_ERROR&quot;;    public static final String USERNAME_ERRORCODE=&quot;10023&quot;;}</code></pre><p><strong>field/MapFields.java</strong></p><pre><code>package com.hsiehchou.flume.fields;public class MapFields {    public static final String ID=&quot;id&quot;;    public static final String SOURCE=&quot;source&quot;;    public static final String TYPE=&quot;TYPE&quot;;    public static final String TABLE=&quot;table&quot;;    public static final String FILENAME=&quot;filename&quot;;    public static final String RKSJ=&quot;rksj&quot;;    public static final String ABSOLUTE_FILENAME=&quot;absolute_filename&quot;;    public static final String BSSID=&quot;BSSID&quot;;    public static final String USERNAME=&quot;USERNAME&quot;;    public static final String DAYID=&quot;DAYID&quot;;    public static final String FIRMCODE_NUM=&quot;FIRMCODE_NUM&quot;;    public static final String FIRM_CODE=&quot;FIRM_CODE&quot;;    public static final String IMEI=&quot;IMEI&quot;;    public static final String IMSI=&quot;IMSI&quot;;    public static final String DATA_TYPE_NAME=&quot;DATA_TYPE_NAME&quot;;    public static final String AUTH_TYPE=&quot;AUTH_TYPE&quot;;    public static final String AUTH_ACCOUNT=&quot;AUTH_ACCOUNT&quot;;    //TODO 时间类参数    public static final String CAPTURETIME=&quot;CAPTURETIME&quot;;    public static final String LOGINTIME=&quot;LOGINTIME&quot;;    public static final String LOGOUTTIME=&quot;LOGOUTTIME&quot;;    public static final String STARTTIME=&quot;STARTTIME&quot;;    public static final String ENDTIME=&quot;ENDTIME&quot;;    public static final String FIRSTTIME=&quot;FIRSTTIME&quot;;    public static final String LASTTIME=&quot;LASTTIME&quot;;    //TODO 去重参数    public static final String COUNT=&quot;COUNT&quot;;    public static final String DATA_TYPE=&quot;DATA_TYPE&quot;;    public static final String VALUE=&quot;value&quot;;    public static final String SITECODE=&quot;SITECODE&quot;;    public static final String SITECODENEW=&quot;SITECODENEW&quot;;    public static final String DEVICENUM=&quot;DEVICENUM&quot;;    public static final String MAC=&quot;MAC&quot;;    public static final String CLIENTMAC=&quot;CLIENTMAC&quot;;    public static final String STATIONMAC=&quot;STATIONMAC&quot;;    public static final String BRAND=&quot;BRAND&quot;;    public static final String INDEX=&quot;INDEX&quot;;    public static final String ACTION_TYPE=&quot;ACTION_TYPE&quot;;    public static final String CITY_CODE=&quot;CITY_CODE&quot;;    /* public static final String FILENAME_FIELDS1=&quot;FILENAME_FIELDS1&quot;;    public static final String FILENAME_FIELDS1=&quot;FILENAME_FIELDS1&quot;;    public static final String FILENAME_FIELDS1=&quot;FILENAME_FIELDS1&quot;;    public static final String FILENAME_FIELDS1=&quot;FILENAME_FIELDS1&quot;;*/}</code></pre><h4 id="7、自定义sink"><a href="#7、自定义sink" class="headerlink" title="7、自定义sink"></a>7、自定义sink</h4><p><strong>sink/KafkaSink.java—将数据下沉到kafka</strong></p><pre><code>package com.hsiehchou.flume.sink;import com.google.common.base.Throwables;import com.hsiehchou.kafka.producer.StringProducer;import org.apache.flume.*;import org.apache.flume.conf.Configurable;import org.apache.flume.sink.AbstractSink;import org.apache.log4j.Logger;import java.util.ArrayList;import java.util.List;public class KafkaSink extends AbstractSink implements Configurable {    private final Logger logger = Logger.getLogger(KafkaSink.class);    private String[] kafkatopics = null;    //private List&lt;KeyedMessage&lt;String,String&gt;&gt; listKeyedMessage=null;    private List&lt;String&gt; listKeyedMessage=null;    private Long proTimestamp=System.currentTimeMillis();    /**     * 配置读取     * @param context     */    @Override    public void configure(Context context) {        //tier1.sinks.sink1.kafkatopic=chl_test7        //获取 推送kafkatopic参数        kafkatopics = context.getString(&quot;kafkatopics&quot;).split(&quot;,&quot;);        logger.info(&quot;获取kafka topic配置&quot; + context.getString(&quot;kafkatopics&quot;));        listKeyedMessage=new ArrayList&lt;&gt;();    }    @Override    public Status process() throws EventDeliveryException {        logger.info(&quot;sink开始执行&quot;);        Channel channel = getChannel();        Transaction transaction = channel.getTransaction();        transaction.begin();        try {            //从channel中拿到event            Event event = channel.take();            if (event == null) {                transaction.rollback();                return Status.BACKOFF;            }            // 解析记录 获取事件内容            String recourd = new String(event.getBody());            // 发送数据到kafka            try {                //调用kafka的消息推送，将数据推送到kafka                StringProducer.producer(kafkatopics[0],recourd);            /*    if(listKeyedMessage.size()&gt;1000){                    logger.info(&quot;数据大与10000,推送数据到kafka&quot;);                    sendListKeyedMessage();                    logger.info(&quot;数据大与10000,推送数据到kafka成功&quot;);                }else if(System.currentTimeMillis()-proTimestamp&gt;=60*1000){                    logger.info(&quot;时间间隔大与60,推送数据到kafka&quot;);                    sendListKeyedMessage();                    logger.info(&quot;时间间隔大与60,推送数据到kafka成功&quot;+listKeyedMessage.size());                }*/            } catch (Exception e) {                logger.error(&quot;推送数据到kafka失败&quot; , e);                throw Throwables.propagate(e);            }            transaction.commit();            return Status.READY;        } catch (ChannelException e) {            logger.error(e);            transaction.rollback();            return Status.BACKOFF;        } finally {            if(transaction != null){                transaction.close();            }        }    }    @Override    public synchronized void stop() {        super.stop();    }    /*private void sendListKeyedMessage(){        Producer&lt;String, String&gt; producer = new Producer&lt;&gt;(KafkaConfig.getInstance().getProducerConfig());        producer.send(listKeyedMessage);        listKeyedMessage.clear();        proTimestamp=System.currentTimeMillis();        producer.close();    }*/}</code></pre><h4 id="8、service"><a href="#8、service" class="headerlink" title="8、service"></a>8、service</h4><p><strong>DataCheck.java—数据校验</strong></p><pre><code>package com.hsiehchou.flume.service;import com.alibaba.fastjson.JSON;import com.hsiehchou.common.net.HttpRequest;import com.hsiehchou.common.project.datatype.DataTypeProperties;import com.hsiehchou.common.time.TimeTranstationUtils;import com.hsiehchou.flume.fields.ErrorMapFields;import com.hsiehchou.flume.fields.MapFields;import org.apache.log4j.Logger;import java.util.*;/** * 数据校验 */public class DataCheck {    private final static Logger LOG = Logger.getLogger(DataCheck.class);    /**     * 获取数据类型对应的字段  对应的文件     * 结构为 [ 数据类型1 = [字段1，字段2。。。。]，     * 数据类型2 = [字段1，字段2。。。。]]     */    private static Map&lt;String, ArrayList&lt;String&gt;&gt; dataMap = DataTypeProperties.dataTypeMap;    /**     * 数据解析     * @param line     * @param fileName     * @param absoluteFilename     * @return     */    public static Map&lt;String, String&gt; txtParse(String line, String fileName, String absoluteFilename) {        Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();        String[] fileNames = fileName.split(&quot;_&quot;);        String dataType = fileNames[0];        if (dataMap.containsKey(dataType)) {            List&lt;String&gt; fields = dataMap.get(dataType.toLowerCase());            String[] splits = line.split(&quot;\t&quot;);            //长度校验            if (fields.size() == splits.length) {                //添加公共字段                map.put(MapFields.ID, UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;));                map.put(MapFields.TABLE, dataType.toLowerCase());                map.put(MapFields.RKSJ, (System.currentTimeMillis() / 1000) + &quot;&quot;);                map.put(MapFields.FILENAME, fileName);                map.put(MapFields.ABSOLUTE_FILENAME, absoluteFilename);                for (int i = 0; i &lt; splits.length; i++) {                    map.put(fields.get(i), splits[i]);                }            } else {                map = null;                LOG.error(&quot;字段长度不匹配fields&quot;+fields.size()  + &quot;/t&quot; + splits.length);            }        } else {            map = null;            LOG.error(&quot;配置文件中不存在此数据类型&quot;);        }        return map;    }    /**     * 数据长度校验添加必要字段并转map，将长度不符合的插入ES数据库     * @param line     * @param fileName     * @param absoluteFilename     * @return     */    public static Map&lt;String, String&gt; txtParseAndalidation(String line, String fileName, String absoluteFilename) {        Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();        Map&lt;String, Object&gt; errorMap = new HashMap&lt;String, Object&gt;();        //文件名按&quot;_&quot;切分  wechat_source1_1111142.txt        //wechat 数据类型        //source1 数据来源        //1111142  不让文件名相同        String[] fileNames = fileName.split(&quot;_&quot;);        String dataType = fileNames[0];        String source = fileNames[1];        if (dataMap.containsKey(dataType)) {            //获取数据类型字段            // imei,imsi,longitude,latitude,phone_mac,device_mac,device_number,collect_time,username,phone,object_username,send_message,accept_message,message_time            //根据数据类型，获取改类型的字段            List&lt;String&gt; fields = dataMap.get(dataType.toLowerCase());            //line            String[] splits = line.split(&quot;\t&quot;);            //长度校验            if (fields.size() == splits.length) {                for (int i = 0; i &lt; splits.length; i++) {                    map.put(fields.get(i), splits[i]);                }                //添加公共字段                // map.put(SOURCE, source);                map.put(MapFields.ID, UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;));                map.put(MapFields.TABLE, dataType.toLowerCase());                map.put(MapFields.RKSJ, (System.currentTimeMillis() / 1000) + &quot;&quot;);                map.put(MapFields.FILENAME, fileName);                map.put(MapFields.ABSOLUTE_FILENAME, absoluteFilename);                //数据封装完成  开始进行数据校验                errorMap = DataValidation.dataValidation(map);            } else {                errorMap.put(ErrorMapFields.LENGTH, &quot;字段数不匹配 实际&quot; + fields.size() + &quot;\t&quot; + &quot;结果&quot; + splits.length);                errorMap.put(ErrorMapFields.LENGTH_ERROR, ErrorMapFields.LENGTH_ERROR_NUM);                LOG.info(&quot;字段数不匹配 实际&quot; + fields.size() + &quot;\t&quot; + &quot;结果&quot; + splits.length);                map = null;            }            //判断数据是否存在错误            if (null != errorMap &amp;&amp; errorMap.size() &gt; 0) {                LOG.info(&quot;errorMap===&quot; + errorMap);                if (&quot;1&quot;.equals(&quot;1&quot;)) {                    //addErrorMapES(errorMap, map, fileName, absoluteFilename);                    //验证没通过，将错误数据写到ES，并将map置空                    addErrorMapESByHTTP(errorMap, map, fileName, absoluteFilename);                }                map = null;            }        } else {            map = null;            LOG.error(&quot;配置文件中不存在此数据类型&quot;);        }        return map;    }    /**     *  将错误信息写入ES，方便查错     * @param errorMap     * @param map     * @param fileName     * @param absoluteFilename     */    public static void addErrorMapESByHTTP(Map&lt;String, Object&gt; errorMap, Map&lt;String, String&gt; map, String fileName, String absoluteFilename) {        String errorType = fileName.split(&quot;_&quot;)[0];        errorMap.put(MapFields.TABLE, errorType);        errorMap.put(MapFields.ID, UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;));        errorMap.put(ErrorMapFields.RECORD, map);        errorMap.put(ErrorMapFields.FILENAME, fileName);        errorMap.put(ErrorMapFields.ABSOLUTE_FILENAME, absoluteFilename);        errorMap.put(ErrorMapFields.RKSJ, TimeTranstationUtils.Date2yyyy_MM_dd_HH_mm_ss());        String url=&quot;http://192.168.116.201:9200/error_recourd/error_recourd/&quot;+ errorMap.get(MapFields.ID).toString();        String json = JSON.toJSONString(errorMap);        HttpRequest.sendPost(url,json);        //HttpRequest.sendPostMessage(url, errorMap);    }    /*    public static void addErrorMapES(Map&lt;String, Object&gt; errorMap, Map&lt;String, String&gt; map, String fileName, String absoluteFilename) {        String errorType = fileName.split(&quot;_&quot;)[0];        errorMap.put(MapFields.TABLE, errorType);        errorMap.put(MapFields.ID, UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;));        errorMap.put(ErrorMapFields.RECORD, map);        errorMap.put(ErrorMapFields.FILENAME, fileName);        errorMap.put(ErrorMapFields.ABSOLUTE_FILENAME, absoluteFilename);        errorMap.put(ErrorMapFields.RKSJ, TimeTranstationUtils.Date2yyyy_MM_dd_HH_mm_ss());        TransportClient client = null;        try {            LOG.info(&quot;开始获取客户端===============================&quot; + errorMap);            client = ESClientUtils.getClient();        } catch (Throwable t) {            if (t instanceof Error) {                throw (Error)t;            }            LOG.error(null,t);        }        //JestClient jestClient = JestService.getJestClient();        //boolean bool = JestService.indexOne(jestClient,TxtConstant.ERROR_INDEX, TxtConstant.ERROR_TYPE,errorMap.get(MapFields.ID).toString(),errorMap);        LOG.info(&quot;开始写入错误数据到ES===============================&quot; + errorMap);        boolean bool = IndexUtil.putIndexData(TxtConstant.ERROR_INDEX, TxtConstant.ERROR_TYPE, errorMap.get(MapFields.ID).toString(), errorMap,client);        if(bool){            LOG.info(&quot;写入错误数据到ES===============================&quot; + errorMap);        }else{            LOG.info(&quot;写入错误数据到ES===============================失败&quot;);        }    }*/    public static void main(String[] args) {    }}</code></pre><p><strong>DataValidation.java</strong></p><pre><code>package com.hsiehchou.flume.service;import com.hsiehchou.flume.fields.ErrorMapFields;import com.hsiehchou.flume.fields.MapFields;import com.hsiehchou.flume.utils.Validation;import org.apache.commons.lang.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.HashMap;import java.util.List;import java.util.Map;public class DataValidation {    private static final Logger LOG = LoggerFactory.getLogger(DataValidation.class);   //  private static final TxtConfigurationFileReader reader = TxtConfigurationFileReader.getInstance();   //  private static final DataTypeConfigurationFileReader datatypereader = DataTypeConfigurationFileReader.getInstance();   //  private static final ValidationConfigurationFileReader readerValidation = ValidationConfigurationFileReader.getInstance();    private static Map&lt;String,String&gt;  dataTypeMap;    private static List&lt;String&gt; listAuthType;    private static String isErrorES;    private static final String USERNAME=ErrorMapFields.USERNAME;    private static final String DATA_TYPE=ErrorMapFields.DATA_TYPE;    private static final String DATA_TYPE_ERROR=ErrorMapFields.DATA_TYPE_ERROR;    private static final String DATA_TYPE_ERRORCODE=ErrorMapFields.DATA_TYPE_ERRORCODE;    private static final String SJHM=ErrorMapFields.SJHM;    private static final String SJHM_ERROR=ErrorMapFields.SJHM_ERROR;    private static final String SJHM_ERRORCODE=ErrorMapFields.SJHM_ERRORCODE;    private static final String QQ=ErrorMapFields.QQ;    private static final String QQ_ERROR=ErrorMapFields.QQ_ERROR;    private static final String QQ_ERRORCODE=ErrorMapFields.QQ_ERRORCODE;    private static final String IMSI=ErrorMapFields.IMSI;    private static final String IMSI_ERROR=ErrorMapFields.IMSI_ERROR;    private static final String IMSI_ERRORCODE=ErrorMapFields.IMSI_ERRORCODE;    private static final String IMEI=ErrorMapFields.IMEI;    private static final String IMEI_ERROR=ErrorMapFields.IMEI_ERROR;    private static final String IMEI_ERRORCODE=ErrorMapFields.IMEI_ERRORCODE;    private static final String MAC=ErrorMapFields.MAC;    private static final String CLIENTMAC=ErrorMapFields.CLIENTMAC;    private static final String STATIONMAC=ErrorMapFields.STATIONMAC;    private static final String BSSID=ErrorMapFields.BSSID;    private static final String MAC_ERROR=ErrorMapFields.MAC_ERROR;    private static final String MAC_ERRORCODE=ErrorMapFields.MAC_ERRORCODE;    private static final String DEVICENUM=ErrorMapFields.DEVICENUM;    private static final String DEVICENUM_ERROR=ErrorMapFields.DEVICENUM_ERROR;    private static final String DEVICENUM_ERRORCODE=ErrorMapFields.DEVICENUM_ERRORCODE;    private static final String CAPTURETIME=ErrorMapFields.CAPTURETIME;    private static final String CAPTURETIME_ERROR=ErrorMapFields.CAPTURETIME_ERROR;    private static final String CAPTURETIME_ERRORCODE=ErrorMapFields.CAPTURETIME_ERRORCODE;    private static final String EMAIL=ErrorMapFields.EMAIL;    private static final String EMAIL_ERROR=ErrorMapFields.EMAIL_ERROR;    private static final String EMAIL_ERRORCODE=ErrorMapFields.EMAIL_ERRORCODE;    private static final String AUTH_TYPE=ErrorMapFields.AUTH_TYPE;    private static final String AUTH_TYPE_ERROR=ErrorMapFields.AUTH_TYPE_ERROR;    private static final String AUTH_TYPE_ERRORCODE=ErrorMapFields.AUTH_TYPE_ERRORCODE;    private static final String FIRM_CODE=ErrorMapFields.FIRM_CODE;    private static final String FIRM_CODE_ERROR=ErrorMapFields.FIRM_CODE_ERROR;    private static final String FIRM_CODE_ERRORCODE=ErrorMapFields.FIRM_CODE_ERRORCODE;    private static final String STARTTIME=ErrorMapFields.STARTTIME;    private static final String STARTTIME_ERROR=ErrorMapFields.STARTTIME_ERROR;    private static final String STARTTIME_ERRORCODE=ErrorMapFields.STARTTIME_ERRORCODE;    private static final String ENDTIME=ErrorMapFields.ENDTIME;    private static final String ENDTIME_ERROR=ErrorMapFields.ENDTIME_ERROR;    private static final String ENDTIME_ERRORCODE=ErrorMapFields.ENDTIME_ERRORCODE;    private static final String LOGINTIME=ErrorMapFields.LOGINTIME;    private static final String LOGINTIME_ERROR=ErrorMapFields.LOGINTIME_ERROR;    private static final String LOGINTIME_ERRORCODE=ErrorMapFields.LOGINTIME_ERRORCODE;    private static final String LOGOUTTIME=ErrorMapFields.LOGOUTTIME;    private static final String LOGOUTTIME_ERROR=ErrorMapFields.LOGOUTTIME_ERROR;    private static final String LOGOUTTIME_ERRORCODE=ErrorMapFields.LOGOUTTIME_ERRORCODE;    public static Map&lt;String, Object&gt; dataValidation( Map&lt;String, String&gt; map){        if(map == null){            return null;        }        Map&lt;String,Object&gt; errorMap = new HashMap&lt;String,Object&gt;();        //验证手机号码        sjhmValidation(map,errorMap);        //验证MAC        macValidation(map,errorMap);        //验证经纬度        longlaitValidation(map,errorMap);        //定义自己的清洗规则        //TODO 大小写统一        //TODO 时间类型统一        //TODO 数据字段统一        //TODO 业务字段转换        //TODO 数据矫正        //TODO 验证MAC不能为空        //TODO 验证IMSI不能为空        //TODO 验证 QQ IMSI IMEI        //TODO 验证DEVICENUM是否为空 为空返回错误        /*devicenumValidation(map,errorMap);        //TODO 验证CAPTURETIME是否为空 为空过滤   不为10，14位数字过滤        capturetimeValidation(map,errorMap);        //TODO 验证EMAIL        emailValidation(map,errorMap);        //TODO 验证STARTTIME ENDTIME LOGINTIME LOGOUTTIME        timeValidation(map,errorMap);        */        return errorMap;    }    /**     * 手机号码验证     * @param map     * @param errorMap     */    public static void sjhmValidation(Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){        if(map.containsKey(&quot;phone&quot;)){            String sjhm=map.get(&quot;phone&quot;);            //调用正则做手机号码验证，是否是正确的一个，检验            boolean ismobile = Validation.isMobile(sjhm);            if(!ismobile){                errorMap.put(SJHM,sjhm);                errorMap.put(SJHM_ERROR,SJHM_ERRORCODE);            }        }    }    //TODO QQ验证  10002  QQ编码 1030001    需要根据DATATYPE来判断数据类型的一起验证    public static void virtualValidation(String dataType, Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){        //TODO USERNAME验证  10023  长度》=2        if(map.containsKey(ErrorMapFields.USERNAME)){            String username=map.get(ErrorMapFields.USERNAME);            if(StringUtils.isNotBlank(username)){                if(username.length()&lt;2){                    errorMap.put(ErrorMapFields.USERNAME,username);                    errorMap.put(ErrorMapFields.USERNAME_ERROR,ErrorMapFields.USERNAME_ERRORCODE);                }            }        }        //TODO QQ验证  10002  QQ编码 1030001        if(&quot;1030001&quot;.equals(dataType)&amp;&amp; map.containsKey(USERNAME)){            String qqnum= map.get(USERNAME);            boolean bool = Validation.isQQ(qqnum);            if(!bool){                errorMap.put(QQ,qqnum);                errorMap.put(QQ_ERROR,QQ_ERRORCODE);            }        }        //TODO IMSI验证  10005  IMSI编码 1429997        if(&quot;1429997&quot;.equals(dataType)&amp;&amp; map.containsKey(IMSI)){            String imsi= map.get(IMSI);            boolean bool = Validation.isIMSI(imsi);            if(!bool){                errorMap.put(IMSI,imsi);                errorMap.put(IMSI_ERROR,IMSI_ERRORCODE);            }        }        //TODO IMEI验证  10006  IMEI编码 1429998        if(&quot;1429998&quot;.equals(dataType)&amp;&amp; map.containsKey(IMEI)){            String imei= map.get(IMEI);            boolean bool = Validation.isIMEI(imei);            if(!bool){                errorMap.put(IMEI,imei);                errorMap.put(IMEI_ERROR,IMEI_ERRORCODE);            }        }    }    //MAC验证  10003    public static void macValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){        if(map == null){            return ;        }        if(map.containsKey(&quot;phone_mac&quot;)){            String mac=map.get(&quot;phone_mac&quot;);            if(StringUtils.isNotBlank(mac)){                boolean bool = Validation.isMac(mac);                if(!bool){                    LOG.info(&quot;MAC验证失败&quot;);                    errorMap.put(MAC,mac);                    errorMap.put(MAC_ERROR,MAC_ERRORCODE);                }            }else{                LOG.info(&quot;MAC验证失败&quot;);                errorMap.put(MAC,mac);                errorMap.put(MAC_ERROR,MAC_ERRORCODE);            }        }    }    /**     * TODO DEVICENUM 验证 为空过滤     * @param map     * @param errorMap     */    public static void devicenumValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){        if(map == null){            return ;        }        if(map.containsKey(&quot;device_number&quot;)){            String devicenum=map.get(&quot;device_number&quot;);            if(StringUtils.isBlank(devicenum)){                errorMap.put(DEVICENUM,&quot;设备编码不能为空&quot;);                errorMap.put(DEVICENUM_ERROR,DEVICENUM_ERRORCODE);            }        }    }    /**     * TODO CAPTURETIME验证 为空过滤  10019  验证时间长度为10或14位     * @param map     * @param errorMap     */    public static void capturetimeValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){        if(map == null){            return ;        }        if(map.containsKey(CAPTURETIME)){            String capturetime=map.get(CAPTURETIME);            if(StringUtils.isBlank(capturetime)){                errorMap.put(CAPTURETIME,&quot;CAPTURETIME不能为空&quot;);                errorMap.put(CAPTURETIME_ERROR,CAPTURETIME_ERRORCODE);            }else{                boolean bool = Validation.isCAPTURETIME(capturetime);                if(!bool){                    errorMap.put(CAPTURETIME,capturetime);                    errorMap.put(CAPTURETIME_ERROR,CAPTURETIME_ERRORCODE);                }            }        }    }    //TODO EMAIL验证 为空过滤 为错误过滤  10004  通过TABLE取USERNAME验证    public static void emailValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){        if(map == null){            return ;        }        if(map.get(&quot;TABLE&quot;).equals(EMAIL)){            String email=map.get(USERNAME);            if(StringUtils.isNotBlank(email)){                boolean bool = Validation.isEmail(email);                if(!bool){                    errorMap.put(EMAIL,email);                    errorMap.put(EMAIL_ERROR,EMAIL_ERRORCODE);                }            }else{                errorMap.put(EMAIL,&quot;EMAIL不能为空&quot;);                errorMap.put(EMAIL_ERROR,EMAIL_ERRORCODE);            }        }    }    //TODO EMAIL验证 为空过滤 为错误过滤  10004  通过TABLE取USERNAME验证    public static void timeValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){        if(map == null){            return ;        }        if(map.containsKey(STARTTIME)&amp;&amp;map.containsKey(ENDTIME)){            String starttime=map.get(STARTTIME);            String endtime=map.get(ENDTIME);            if(StringUtils.isBlank(starttime)&amp;&amp;StringUtils.isBlank(endtime)){                errorMap.put(STARTTIME,&quot;STARTTIME和ENDTIME不能同时为空&quot;);                errorMap.put(STARTTIME_ERROR,STARTTIME_ERRORCODE);                errorMap.put(ENDTIME,&quot;STARTTIME和ENDTIME不能同时为空&quot;);                errorMap.put(ENDTIME_ERROR,ENDTIME_ERRORCODE);            }else{                Boolean bool1 = istime(starttime, STARTTIME, STARTTIME_ERROR, STARTTIME_ERRORCODE, errorMap);                Boolean bool2 = istime(endtime, ENDTIME, ENDTIME_ERROR, ENDTIME_ERRORCODE, errorMap);                if(bool1&amp;&amp;bool2&amp;&amp;(starttime.length()!=endtime.length())){                    errorMap.put(STARTTIME,&quot;STARTTIME和ENDTIME长度不等 STARTTIME：&quot;+starttime + &quot;\t&quot;+&quot;ENDTIME:&quot; + endtime);                    errorMap.put(STARTTIME_ERROR,STARTTIME_ERRORCODE);                    errorMap.put(ENDTIME,&quot;STARTTIME和ENDTIME长度不等 STARTTIME：&quot;+starttime + &quot;\t&quot;+&quot;ENDTIME:&quot; + endtime);                    errorMap.put(ENDTIME_ERROR,ENDTIME_ERRORCODE);                }                else if(bool1&amp;&amp;bool2&amp;&amp;(endtime.compareTo(starttime)&lt;0)){                    errorMap.put(STARTTIME,&quot;ENDTIME必须大于STARTTIME  STARTTIME:&quot;+starttime + &quot;\t&quot;+&quot;ENDTIME:&quot; + endtime);                    errorMap.put(STARTTIME_ERROR,STARTTIME_ERRORCODE);                    errorMap.put(ENDTIME,&quot;ENDTIME必须大于STARTTIME  STARTTIME:&quot;+starttime + &quot;\t&quot;+&quot;ENDTIME:&quot; + endtime);                    errorMap.put(ENDTIME_ERROR,ENDTIME_ERRORCODE);                }            }        }else if(map.containsKey(LOGINTIME)&amp;&amp;map.containsKey(LOGOUTTIME)){            String logintime=map.get(LOGINTIME);            String logouttime=map.get(LOGOUTTIME);            if(StringUtils.isBlank(logintime)&amp;&amp;StringUtils.isBlank(logouttime)){                errorMap.put(LOGINTIME,&quot;LOGINTIME和LOGOUTTIME不能同时为空&quot;);                errorMap.put(LOGINTIME_ERROR,LOGINTIME_ERRORCODE);                errorMap.put(LOGOUTTIME,&quot;LOGINTIME和LOGOUTTIME不能同时为空&quot;);                errorMap.put(LOGOUTTIME_ERROR,LOGOUTTIME_ERRORCODE);            }else{                Boolean bool1 = istime(logintime, LOGINTIME, LOGINTIME_ERROR, LOGINTIME_ERRORCODE, errorMap);                Boolean bool2 = istime(logouttime, LOGOUTTIME, LOGOUTTIME_ERROR, LOGOUTTIME_ERRORCODE, errorMap);                if(bool1&amp;&amp;bool2&amp;&amp;(logintime.length()!=logouttime.length())){                    errorMap.put(LOGINTIME,&quot;LOGOUTTIME LOGINTIME长度不等 LOGINTIME：&quot;+logintime + &quot;\t&quot;+&quot;LOGOUTTIME:&quot; + logouttime);                    errorMap.put(LOGINTIME_ERROR,LOGINTIME_ERRORCODE);                    errorMap.put(LOGOUTTIME,&quot;LOGOUTTIME LOGINTIME长度不等 LOGINTIME：&quot;+logintime + &quot;\t&quot;+&quot;LOGOUTTIME:&quot; + logouttime);                    errorMap.put(LOGOUTTIME_ERROR,LOGOUTTIME_ERRORCODE);                }                else if(bool1&amp;&amp;bool2&amp;&amp;(logouttime.compareTo(logintime)&lt;0)){                    errorMap.put(LOGINTIME,&quot;LOGOUTTIME必须大于LOGINTIME  LOGINTIME:&quot;+logintime + &quot;\t&quot;+&quot;LOGOUTTIME:&quot; + logouttime);                    errorMap.put(LOGINTIME_ERROR,LOGINTIME_ERRORCODE);                    errorMap.put(LOGOUTTIME,&quot;LOGOUTTIME必须大于LOGINTIME  LOGINTIME:&quot;+logintime + &quot;\t&quot;+&quot;LOGOUTTIME:&quot; + logouttime);                    errorMap.put(LOGOUTTIME_ERROR,LOGOUTTIME_ERRORCODE);                }            }        }    }    //TODO AUTH_TYPE验证  为空过滤 为错误过滤  10020    public static void authtypeValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){        if(map == null){            return ;        }        String fileName=map.get(MapFields.FILENAME);        if(fileName.split(&quot;_&quot;).length&lt;=2){            map = null;            return;        }        if(StringUtils.isNotBlank(fileName)){            if(&quot;bh&quot;.equals(fileName.split(&quot;_&quot;)[2])||&quot;wy&quot;.equals(fileName.split(&quot;_&quot;)[2])||&quot;yc&quot;.equals(fileName.split(&quot;_&quot;)[2])){                return ;            }else if(map.containsKey(AUTH_TYPE)){                String authtype=map.get(AUTH_TYPE);                if(StringUtils.isNotBlank(authtype)){                    if(listAuthType.contains(authtype)){                        if(&quot;1020004&quot;.equals(authtype)){                            String sjhm=map.get(MapFields.AUTH_ACCOUNT);                            boolean ismobile = Validation.isMobile(sjhm);                            if(!ismobile){                                errorMap.put(SJHM,sjhm);                                errorMap.put(SJHM_ERROR,SJHM_ERRORCODE);                            }                        }                        if(&quot;1020002&quot;.equals(authtype)){                            String mac=map.get(MapFields.AUTH_ACCOUNT);                            boolean ismac = Validation.isMac(mac);                            if(!ismac){                                errorMap.put(MAC,mac);                                errorMap.put(MAC_ERROR,MAC_ERRORCODE);                            }                        }                    }else{                        errorMap.put(AUTH_TYPE,&quot;AUTHTYPE_LIST 影射里没有&quot;+ &quot;\t&quot;+ &quot;[&quot;+ authtype+&quot;]&quot;);                        errorMap.put(AUTH_TYPE_ERROR,AUTH_TYPE_ERRORCODE);                    }                }else{                    errorMap.put(AUTH_TYPE,&quot;AUTH_TYPE 不能为空&quot;);                    errorMap.put(AUTH_TYPE_ERROR,AUTH_TYPE_ERRORCODE);                }            }        }    }    private static final String LONGITUDE = &quot;longitude&quot;;    private static final String LATITUDE = &quot;latitude&quot;;    private static final String LONGITUDE_ERROR=ErrorMapFields.LONGITUDE_ERROR;    private static final String LONGITUDE_ERRORCODE=ErrorMapFields.LONGITUDE_ERRORCODE;    private static final String LATITUDE_ERROR=ErrorMapFields.LATITUDE_ERROR;    private static final String LATITUDE_ERRORCODE=ErrorMapFields.LATITUDE_ERRORCODE;    /**     * 经纬度验证  错误过滤  10012  10013     * @param map     * @param errorMap     */    public static void longlaitValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){        if(map == null){            return ;        }        if(map.containsKey(LONGITUDE)&amp;&amp;map.containsKey(LATITUDE)){            String longitude=map.get(LONGITUDE);            String latitude=map.get(LATITUDE);            boolean bool1 = Validation.isLONGITUDE(longitude);            boolean bool2 = Validation.isLATITUDE(latitude);            if(!bool1){                errorMap.put(LONGITUDE,longitude);                errorMap.put(LONGITUDE_ERROR,LONGITUDE_ERRORCODE);            }            if(!bool2){                errorMap.put(LATITUDE,latitude);                errorMap.put(LATITUDE_ERROR,LATITUDE_ERRORCODE);            }        }    }    public static Boolean istime(String time,String str1,String str2,String str3,Map&lt;String,Object&gt; errorMap){        if(StringUtils.isNotBlank(time)){            boolean bool = Validation.isCAPTURETIME(time);            if(!bool){                errorMap.put(str1,time);                errorMap.put(str2,str3);                return false;            }            return true;        }        return false;    }}</code></pre><h4 id="9、配置CDH上的Agent文件—-跟FolderSource等里面读取配置文件相对应"><a href="#9、配置CDH上的Agent文件—-跟FolderSource等里面读取配置文件相对应" class="headerlink" title="9、配置CDH上的Agent文件—-跟FolderSource等里面读取配置文件相对应"></a>9、配置CDH上的Agent文件—-跟FolderSource等里面读取配置文件相对应</h4><p><img src="/medias/Flume%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6.PNG" alt="Flume配置文件"></p><p><strong>Flume配置：</strong></p><pre><code>tier1.sources= source1tier1.channels=channel1tier1.sinks=sink1#定义source1tier1.sources.source1.type = com.hsiehchou.flume.source.FolderSource#读取文件之后睡眠时间tier1.sources.source1.sleeptime=5tier1.sources.source1.filenum=3000tier1.sources.source1.dirs =/usr/chl/data/filedir/tier1.sources.source1.successfile=/usr/chl/data/filedir_successful/tier1.sources.source1.deserializer.outputCharset=UTF-8tier1.sources.source1.channels = channel1# 定义拦截器1tier1.sources.source1.interceptors=i1tier1.sources.source1.interceptors.i1.type=com.hsiehchou.flume.interceptor.DataCleanInterceptor$Builder#定义channeltier1.channels.channel1.type = memorytier1.channels.channel1.keep-alive= 300tier1.channels.channel1.capacity = 1000000tier1.channels.channel1.transactionCapacity = 5000tier1.channels.channel1.byteCapacityBufferPercentage = 200tier1.channels.channel1.byteCapacity = 80000#定义sink1tier1.sinks.sink1.type = com.hsiehchou.flume.sink.KafkaSinktier1.sinks.sink1.kafkatopics = chl_test7tier1.sinks.sink1.channel = channel1</code></pre><p><img src="/medias/ftp%E7%9B%91%E6%8E%A7%E6%96%87%E4%BB%B6.PNG" alt="ftp监控文件"></p><p><strong>flumesource 不断监控 ftp 文件目录，通过自定义拦截器拦截，然后推送到flumechannel，然后通过flumesink下沉到kafka</strong></p><h4 id="10、flume打包到服务器执行"><a href="#10、flume打包到服务器执行" class="headerlink" title="10、flume打包到服务器执行"></a>10、flume打包到服务器执行</h4><p><img src="/medias/flume%E6%8F%92%E4%BB%B6%E7%9B%AE%E5%BD%95.PNG" alt="flume插件目录"></p><p><strong>不能放在默认的/usr/lib/flume-ng/plugins.d下面</strong></p><p>mkdir -p /var/lib/flume-ng/plugins.d/chl/lib<br>mkdir -p /usr/chl/data/filedir/<br>mkdir -p /usr/chl/data/filedir_successful/</p><p><strong>要设置777，flume启动的时候是以flume权限启动的，所以要更改权限</strong><br><strong>chmod 777 /usr/chl/data/filedir/</strong></p><p>kafka-topics –zookeeper hadoop1:2181 –topic chl_test7 –create –replication-factor 1 –partitions 3</p><p>kafka-topics –zookeeper hadoop1:2181 –list</p><p>kafka-topics –zookeeper hadoop1:2181 –delete –topic chl_test7</p><p>kafka-console-consumer –bootstrap-server hadoop1:9092 –topic chl_test7 –from-beginning<br><img src="/medias/%E6%B6%88%E8%B4%B9kafka.PNG" alt="消费kafka"></p><h3 id="六、Kafka开发"><a href="#六、Kafka开发" class="headerlink" title="六、Kafka开发"></a>六、Kafka开发</h3><p><strong>xz_bigdata_kafka</strong></p><h4 id="1、pom-xml"><a href="#1、pom-xml" class="headerlink" title="1、pom.xml"></a>1、pom.xml</h4><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_kafka&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_kafka&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;        &lt;poi.version&gt;3.14&lt;/poi.version&gt;        &lt;kafka.version&gt;0.9.0-kafka-2.0.2&lt;/kafka.version&gt;        &lt;mysql.connector.version&gt;5.1.46&lt;/mysql.connector.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;            &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;            &lt;version&gt;${zookeeper.version}-${cdh.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;            &lt;artifactId&gt;kafka_2.10&lt;/artifactId&gt;            &lt;version&gt;${kafka.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.poi&lt;/groupId&gt;            &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt;            &lt;version&gt;${poi.version}&lt;/version&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;            &lt;artifactId&gt;scala-library&lt;/artifactId&gt;            &lt;version&gt;${scala.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;artifactId&gt;scala-reflect&lt;/artifactId&gt;            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;            &lt;version&gt;${scala.version}&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><h4 id="2、config-KafkaConfig-java—kafka配置文件-解析器"><a href="#2、config-KafkaConfig-java—kafka配置文件-解析器" class="headerlink" title="2、config/KafkaConfig.java—kafka配置文件 解析器"></a>2、config/KafkaConfig.java—kafka配置文件 解析器</h4><pre><code>package com.hsiehchou.kafka.config;import com.hsiehchou.common.config.ConfigUtil;import kafka.producer.ProducerConfig;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.text.SimpleDateFormat;import java.util.Date;import java.util.Properties;/** * kafka配置文件 解析器 */public class KafkaConfig {    private static final Logger LOG = LoggerFactory.getLogger(KafkaConfig.class);    private static final String DEFUALT_CONFIG_PATH = &quot;kafka/kafka-server-config.properties&quot;;    private volatile static KafkaConfig kafkaConfig = null;    private ProducerConfig config;    private Properties properties;    private KafkaConfig() throws IOException{        try {            properties = ConfigUtil.getInstance().getProperties(DEFUALT_CONFIG_PATH);        } catch (Exception e) {            IOException ioException = new IOException();            ioException.addSuppressed(e);            throw ioException;        }        config = new ProducerConfig(properties);    }    public static KafkaConfig getInstance(){        if(kafkaConfig == null){            synchronized (KafkaConfig.class) {                if(kafkaConfig == null){                    try {                        kafkaConfig = new KafkaConfig();                    } catch (IOException e) {                        LOG.error(&quot;实例化kafkaConfig失败&quot;, e);                    }                }            }        }        return kafkaConfig;    }    public ProducerConfig getProducerConfig(){        return config;    }    /**      * 获取当前时间的字符串       格式为：yyyy-MM-dd HH:mm:ss      * @return String     */    public static String nowStr(){        return new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;).format( new Date() );    }}</code></pre><h4 id="3、producer-StringProducer-java—生产者"><a href="#3、producer-StringProducer-java—生产者" class="headerlink" title="3、producer/StringProducer.java—生产者"></a>3、producer/StringProducer.java—生产者</h4><pre><code>package com.hsiehchou.kafka.producer;import com.hsiehchou.common.thread.ThreadPoolManager;import com.hsiehchou.kafka.config.KafkaConfig;import kafka.javaapi.producer.Producer;import kafka.producer.KeyedMessage;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.ArrayList;import java.util.List;import java.util.concurrent.CountDownLatch;import java.util.concurrent.ExecutorService;public class StringProducer {    private static final Logger LOG = LoggerFactory.getLogger(StringProducer.class);    public static void main(String[] args) {        StringProducer.producer(&quot;chl_test2&quot;,&quot;{\&quot;rksj\&quot;:\&quot;1558177156\&quot;,\&quot;latitude\&quot;:\&quot;24.000000\&quot;,\&quot;imsi\&quot;:\&quot;000000000000000\&quot;,\&quot;accept_message\&quot;:\&quot;\&quot;,\&quot;phone_mac\&quot;:\&quot;aa-aa-aa-aa-aa-aa\&quot;,\&quot;device_mac\&quot;:\&quot;bb-bb-bb-bb-bb-bb\&quot;,\&quot;message_time\&quot;:\&quot;1789098762\&quot;,\&quot;filename\&quot;:\&quot;wechat_source1_1111119.txt\&quot;,\&quot;absolute_filename\&quot;:\&quot;/usr/chl/data/filedir_successful/2019-05-18/data/filedir/wechat_source1_1111119.txt\&quot;,\&quot;phone\&quot;:\&quot;18609765432\&quot;,\&quot;device_number\&quot;:\&quot;32109231\&quot;,\&quot;imei\&quot;:\&quot;000000000000000\&quot;,\&quot;id\&quot;:\&quot;1792d6529e2143fa85717e706403c83c\&quot;,\&quot;collect_time\&quot;:\&quot;1557305988\&quot;,\&quot;send_message\&quot;:\&quot;\&quot;,\&quot;table\&quot;:\&quot;wechat\&quot;,\&quot;object_username\&quot;:\&quot;judy\&quot;,\&quot;longitude\&quot;:\&quot;23.000000\&quot;,\&quot;username\&quot;:\&quot;andiy\&quot;}&quot;);    }    private static int threadSize = 6;    /**     * 生产单条消息 单条推送     * @param topic     * @param recourd     */    public static void producer(String topic,String recourd){        Producer&lt;String, String&gt; producer = new Producer&lt;&gt;(KafkaConfig.getInstance().getProducerConfig());        KeyedMessage&lt;String, String&gt; keyedMessage = new KeyedMessage&lt;&gt;(topic, recourd);        producer.send(keyedMessage);        LOG.info(&quot;发送数据&quot;+recourd+&quot;到kafka成功&quot;);        producer.close();    }    /**     * 批量推送     * @param topic     * @param listRecourd     */    public static void producerList(String topic,List&lt;String&gt; listRecourd){        Producer&lt;String, String&gt; producer = new Producer&lt;&gt;(KafkaConfig.getInstance().getProducerConfig());        List&lt;KeyedMessage&lt;String, String&gt;&gt; listKeyedMessage= new ArrayList&lt;&gt;();        listRecourd.forEach(recourd-&gt;{            listKeyedMessage.add(new KeyedMessage&lt;&gt;(topic, recourd));        });        producer.send(listKeyedMessage);        producer.close();    }    /**     * 多线程推送     * @param topic  kafka  topic     * @param listMessage 消息     * @throws Exception     */    public void producer(String topic,List&lt;String&gt; listMessage) throws Exception{        //  int size = listMessage.size();        List&lt;List&lt;String&gt;&gt; lists = splitList(listMessage, 5);        int threadNum = lists.size();        long t1 = System.currentTimeMillis();        CountDownLatch cdl = new CountDownLatch(threadNum);        //使用线程池        ExecutorService executorService = ThreadPoolManager.getInstance().getExecutorService();        LOG.info(&quot;开启 &quot; + threadNum + &quot; 个线程来向  topic &quot; + topic + &quot; 生产数据 . &quot;);        for (int i = 0; i &lt; threadNum; i++) {            try {                executorService.execute(new ProducerTask(topic,lists.get(i)));            } catch (Exception e) {                LOG.error(&quot;&quot;, e);            }        }        cdl.await();        long t = System.currentTimeMillis() - t1;        LOG.info(  &quot; 一共耗时  ：&quot; + t + &quot;  毫秒 ... &quot; );        executorService.shutdown();    }    /**     * 拆分消息集合,计算使用多少个线程执行运算     * @param mtList     */    public static List&lt;List&lt;String&gt;&gt; splitList(List&lt;String&gt; mtList, int splitSize){        if(mtList == null || mtList.size()==0){            return null;        }        int length = mtList.size();        // 计算可以分成多少组        int num = ( length + splitSize - 1 )/splitSize ;        List&lt;List&lt;String&gt;&gt; spiltList = new ArrayList&lt;&gt;(num);        for (int i = 0; i &lt; num; i++) {            // 开始位置            int fromIndex = i * splitSize;            // 结束位置            int toIndex = (i+1) * splitSize &lt; length ? ( i+1 ) * splitSize : length ;            spiltList.add(mtList.subList(fromIndex,toIndex)) ;        }        return  spiltList;    }    class ProducerTask implements Runnable{        private String topic;        private List&lt;String&gt; listRecourd;        public ProducerTask( String topic, List&lt;String&gt; listRecourd){            this.topic = topic;            this.listRecourd = listRecourd;        }        public void run() {            producerList(topic,listRecourd);        }    }   /* public static void producer(String topic,List&lt;KeyedMessage&lt;String,String&gt;&gt; listMessage) throws Exception{        int size = listMessage.size();        int threads = ( ( size - 1  ) / threadSize ) + 1;        long t1 = System.currentTimeMillis();        CountDownLatch cdl = new CountDownLatch(threads);        //使用线程池        ExecutorService executorService = ThreadPoolManager.getInstance().getExecutorService();        LOG.info(&quot;开启 &quot; + threads + &quot; 个线程来向  topic &quot; + topic + &quot; 生产数据 . &quot;);      *//*  for( int i = 0 ; i &lt; threads ; i++ ){            executorService.execute( new StringProducer.ChildProducer( start , end ,  topic , id,  cdl ));        }*//*        cdl.await();        long t = System.currentTimeMillis() - t1;        LOG.info(  &quot; 一共耗时  ：&quot; + t + &quot;  毫秒 ... &quot; );        executorService.shutdown();    }    static class ChildProducer implements Runnable{        public ChildProducer( int start , int end ,  String topic , String id,  CountDownLatch cdl ){        }        public void run() {        }    }    */}</code></pre><h3 id="七、Spark—kafka2es开发"><a href="#七、Spark—kafka2es开发" class="headerlink" title="七、Spark—kafka2es开发"></a>七、Spark—kafka2es开发</h3><h4 id="Cloudera查找对应的maven依赖地址"><a href="#Cloudera查找对应的maven依赖地址" class="headerlink" title="Cloudera查找对应的maven依赖地址"></a>Cloudera查找对应的maven依赖地址</h4><p><a href="https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo_514x.html#concept_flv_nwn_yk" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo_514x.html#concept_flv_nwn_yk</a></p><p><strong>SparkStreaming+Kafka的两种模式receiver模式和Direct模式</strong></p><h4 id="Sparkstreming-kafka-receiver模式理解"><a href="#Sparkstreming-kafka-receiver模式理解" class="headerlink" title="Sparkstreming + kafka receiver模式理解"></a>Sparkstreming + kafka receiver模式理解</h4><p><img src="/medias/kafka%E7%9A%84receiver%E6%A8%A1%E5%BC%8F.PNG" alt="kafka的receiver模式"></p><p> <strong>receiver模式理解</strong><br>在SparkStreaming程序运行起来后，Executor中会有receiver tasks接收kafka推送过来的数据。数据会被持久化，默认级别为MEMORY_AND_DISK_SER_2,这个级别也可以修改。receiver task对接收过来的数据进行存储和备份，这个过程会有节点之间的数据传输。备份完成后去zookeeper中更新消费偏移量，然后向Driver中的receiver tracker汇报数据的位置。最后Driver根据数据本地化将task分发到不同节点上执行。</p><p><strong>receiver模式中存在的问题</strong><br>当Driver进程挂掉后，Driver下的Executor都会被杀掉，当更新完zookeeper消费偏移量的时候，Driver如果挂掉了，就会存在找不到数据的问题，相当于丢失数据。</p><p><strong>如何解决这个问题？</strong><br>开启WAL(write ahead log)预写日志机制,在接受过来数据备份到其他节点的时候，同时备份到HDFS上一份（我们需要将接收来的数据的持久化级别降级到MEMORY_AND_DISK），这样就能保证数据的安全性。不过，因为写HDFS比较消耗性能，要在备份完数据之后才能进行更新zookeeper以及汇报位置等，这样会增加job的执行时间，这样对于任务的执行提高了延迟度。</p><p><strong>注意</strong><br>1）开启WAL之后，接受数据级别要降级，有效率问题<br>2）开启WAL要checkpoint<br>3）开启WAL(write ahead log),往HDFS中备份一份数据</p><h4 id="Sparkstreming-kafka-receiver模式理解-1"><a href="#Sparkstreming-kafka-receiver模式理解-1" class="headerlink" title="Sparkstreming + kafka receiver模式理解"></a>Sparkstreming + kafka receiver模式理解</h4><p><img src="/medias/kafka%E7%9A%84direct%E6%A8%A1%E5%BC%8F.PNG" alt="kafka的direct模式"></p><ol><li>简化数据处理流程</li><li>自己定义offset存储，保证数据0丢失，但是会存在重复消费问题。（解决消费等幂问题）</li><li>不用接收数据，自己去kafka中拉取</li></ol><h4 id="1、spark下的pom-xml"><a href="#1、spark下的pom-xml" class="headerlink" title="1、spark下的pom.xml"></a>1、spark下的pom.xml</h4><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_spark&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_spark&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;        &lt;spark.version&gt;1.6.0&lt;/spark.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_es&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_redis&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_hbase&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;servlet-api&lt;/artifactId&gt;                    &lt;groupId&gt;javax.servlet&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;gson&lt;/artifactId&gt;                    &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;            &lt;version&gt;${spark.version}-${cdh.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-hive_2.10&lt;/artifactId&gt;            &lt;version&gt;${spark.version}-${cdh.version}&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;httpcore&lt;/artifactId&gt;                    &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;httpclient&lt;/artifactId&gt;                    &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;                    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;gson&lt;/artifactId&gt;                    &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt;            &lt;version&gt;${spark.version}-${cdh.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-streaming-kafka_2.10&lt;/artifactId&gt;            &lt;version&gt;${spark.version}-${cdh.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;            &lt;artifactId&gt;elasticsearch-spark-13_2.10&lt;/artifactId&gt;            &lt;version&gt;6.2.3&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;                &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;                &lt;version&gt;2.15.2&lt;/version&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;goals&gt;                            &lt;goal&gt;compile&lt;/goal&gt;                            &lt;goal&gt;testCompile&lt;/goal&gt;                        &lt;/goals&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;plugin&gt;&lt;!--打包依赖的jar包--&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;outputDirectory&gt;${project.build.directory}/lib&lt;/outputDirectory&gt;                    &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt; &lt;!-- 表示是否不包含间接依赖的包 --&gt;                    &lt;stripVersion&gt;false&lt;/stripVersion&gt; &lt;!-- 去除版本信息 --&gt;                &lt;/configuration&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;copy-dependencies&lt;/id&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;!-- 拷贝项目依赖包到lib/目录下 --&gt;                            &lt;outputDirectory&gt;${project.build.directory}/jars&lt;/outputDirectory&gt;                            &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt;                            &lt;stripVersion&gt;false&lt;/stripVersion&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><h4 id="2、spark中的文件结构"><a href="#2、spark中的文件结构" class="headerlink" title="2、spark中的文件结构"></a>2、spark中的文件结构</h4><p><img src="/medias/spark%E4%B8%AD%E7%9A%84%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84.PNG" alt="spark中的文件结构"></p><p><img src="/medias/%E8%AE%A9IDEA%E8%83%BD%E6%96%B0%E5%BB%BAscala.class.PNG" alt="让IDEA能新建scala.class"></p><p>点击”+”号，选择Scala SDK，点击Browse，选择本地下载的scala-sdk-2.10.4</p><h4 id="3、xz-bigdata-spark-spark-common"><a href="#3、xz-bigdata-spark-spark-common" class="headerlink" title="3、xz_bigdata_spark/spark/common"></a>3、xz_bigdata_spark/spark/common</h4><p><strong>SparkContextFactory.scala</strong></p><pre><code>package com.hsiehchou.spark.commonimport org.apache.spark.streaming.{Seconds, StreamingContext}import org.apache.spark.{Accumulator, SparkContext}object SparkContextFactory {  def newSparkBatchContext(appName:String = &quot;sparkBatch&quot;) : SparkContext = {    val sparkConf = SparkConfFactory.newSparkBatchConf(appName)    new SparkContext(sparkConf)  }  def newSparkLocalBatchContext(appName:String = &quot;sparkLocalBatch&quot; , threads : Int = 2) : SparkContext = {    val sparkConf = SparkConfFactory.newSparkLoalConf(appName, threads)    sparkConf.set(&quot;&quot;,&quot;&quot;)    new SparkContext(sparkConf)  }  def getAccumulator(appName:String = &quot;sparkBatch&quot;) : Accumulator[Int] = {    val sparkConf = SparkConfFactory.newSparkBatchConf(appName)    val accumulator: Accumulator[Int] = new SparkContext(sparkConf).accumulator(0,&quot;&quot;)    accumulator  }  /**    * 创建本地流streamingContext    * @param appName             appName    * @param batchInterval      多少秒读取一次    * @param threads            开启多少个线程    * @return    */  def newSparkLocalStreamingContext(appName:String = &quot;sparkStreaming&quot; ,                                    batchInterval:Long = 30L ,                                    threads : Int = 4) : StreamingContext = {    val sparkConf =  SparkConfFactory.newSparkLocalConf(appName, threads)    // sparkConf.set(&quot;spark.streaming.receiver.maxRate&quot;,&quot;10000&quot;)    sparkConf.set(&quot;spark.streaming.kafka.maxRatePerPartition&quot;,&quot;1&quot;)    new StreamingContext(sparkConf, Seconds(batchInterval))  }  /**    * 创建集群模式streamingContext    * 这里不设置线程数，在submit中指定    * @param appName    * @param batchInterval    * @return    */  def newSparkStreamingContext(appName:String = &quot;sparkStreaming&quot; , batchInterval:Long = 30L) : StreamingContext = {    val sparkConf = SparkConfFactory.newSparkStreamingConf(appName)    new StreamingContext(sparkConf, Seconds(batchInterval))  }  def startSparkStreaming(ssc:StreamingContext){    ssc.start()      ssc.awaitTermination()      ssc.stop()  }}</code></pre><p><strong>convert/DataConvert.scala</strong></p><pre><code>package com.hsiehchou.spark.common.convertimport java.utilimport com.hsiehchou.common.config.ConfigUtilimport org.apache.spark.Loggingimport scala.collection.JavaConversions._/**  * 数据类型转换  */object DataConvert extends Serializable with Logging {  val fieldMappingPath = &quot;es/mapping/fieldmapping.properties&quot;  private val typeFieldMap: util.HashMap[String, util.HashMap[String, String]] = getEsFieldtypeMap()  /**    * 将Map&lt;String,String&gt;转化为Map&lt;String,Object&gt;    */  def strMap2esObjectMap(map:util.Map[String,String]):util.Map[String,Object] ={    //获取配置文件中的数据类型    val dataType = map.get(&quot;table&quot;)    //获取配置文件中的数据类型的 字段类型    val fieldMap = typeFieldMap.get(dataType)    //获取数据类型的所有字段，配置文件里的字段    val keySet = fieldMap.keySet()    //var objectMap:util.HashMap[String,Object] = new util.HashMap[String,Object]()    var objectMap = new java.util.HashMap[String, Object]()    //数据里的字段    val set = map.keySet().iterator()    try {      //遍历真实数据的所有字段      while (set.hasNext()) {        val key = set.next()        var dataType:String = &quot;string&quot;        //如果在配置文件中的key包含真实数据的key        if (keySet.contains(key)) {          //则获取真实数据字段的数据类型          dataType = fieldMap.get(key)        }        dataType match {          case &quot;long&quot; =&gt; objectMap = BaseDataConvert.mapString2Long(map, key, objectMap)          case &quot;string&quot; =&gt; objectMap = BaseDataConvert.mapString2String(map, key, objectMap)          case &quot;double&quot; =&gt; objectMap = BaseDataConvert.mapString2Double(map, key, objectMap)          case _ =&gt; objectMap = BaseDataConvert.mapString2String(map, key, objectMap)        }      }    }catch {      case e: Exception =&gt; logInfo(&quot;转换异常&quot;, e)    }    println(&quot;转换后&quot; + objectMap)    objectMap  }  /**    * 读取 &quot;es/mapping/fieldmapping.properties 配置文件    * 主要作用是将 真实数据 根据配置来作数据类型转换 转换为和ES mapping结构保持一致    * @return    */  def getEsFieldtypeMap(): util.HashMap[String, util.HashMap[String, String]] = {    // [&quot;wechat&quot;:[&quot;phone_mac&quot;:&quot;string&quot;,&quot;latitude&quot;:&quot;long&quot;]]    //定义返回Map    val mapMap = new util.HashMap[String, util.HashMap[String, String]]    val properties = ConfigUtil.getInstance().getProperties(fieldMappingPath)    val tables = properties.get(&quot;tables&quot;).toString.split(&quot;,&quot;)    val tableFields = properties.keySet()    tables.foreach(table =&gt; {      val map = new util.HashMap[String, String]()      tableFields.foreach(tableField =&gt; {        if (tableField.toString.startsWith(table)) {          val key = tableField.toString.split(&quot;\\.&quot;)(1)          val value = properties.get(tableField).toString          map.put(key, value)        }      })      mapMap.put(table, map)    })    mapMap  }}</code></pre><p><img src="/medias/scala%E4%B8%AD%E7%9A%84scala%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84.PNG" alt="scala中的scala文件结构"></p><h4 id="4、org-apache-spark-streaming-kafka-KafkaManager-scala"><a href="#4、org-apache-spark-streaming-kafka-KafkaManager-scala" class="headerlink" title="4、org/apache/spark/streaming/kafka/KafkaManager.scala"></a>4、org/apache/spark/streaming/kafka/KafkaManager.scala</h4><p>构建Kafka时用到，KafkaCluster在org.apache.spark.streaming.kafka下面，而且只能在spark里面使用，这时候我们就可以新建相同的目录结构，就可以引用了，如下图所示：</p><p><img src="/medias/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%96%B0%E5%BB%BAorg.apache.spark.streaming.kafka.PNG" alt="为什么要新建org.apache.spark.streaming.kafka"></p><pre><code>package org.apache.spark.streaming.kafkaimport com.alibaba.fastjson.TypeReferenceimport kafka.common.TopicAndPartitionimport kafka.message.MessageAndMetadataimport kafka.serializer.{Decoder, StringDecoder}import org.apache.spark.Loggingimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.StreamingContextimport org.apache.spark.streaming.dstream.{DStream, InputDStream}import scala.reflect.ClassTag/**  * 包名说明 ：KafkaCluster是私有类，只能在spark包中使用，  *           所以包名保持和 KafkaCluster 一致才能调用  * @param kafkaParams  * @param autoUpdateoffset  */class KafkaManager(val kafkaParams:Map[String, String],                   val autoUpdateoffset:Boolean =true) extends Serializable with Logging {  //构造一个KafkaCluster  @transient  private var cluster = new KafkaCluster(kafkaParams)  //定义一个单例  def kc(): KafkaCluster = {    if (cluster == null) {      cluster = new KafkaCluster(kafkaParams)    }    cluster  }  /**    * 泛型流读取器    * @param ssc    * @param topics kafka topics,多个topic按&quot;,&quot;分割    * @tparam K  泛型 K    * @tparam V  泛型 V    * @tparam KD scala泛型 KD &lt;: Decoder[K] 说明KD 的类型必须是Decoder[K]的子类型  上下界    * @tparam VD scala泛型 VD &lt;: Decoder[V] 说明VD 的类型必须是Decoder[V]的子类型  上下界    * @return    */  def createDirectStream[K: ClassTag, V: ClassTag,  KD &lt;: Decoder[K] : ClassTag,  VD &lt;: Decoder[V] : ClassTag](ssc: StreamingContext, topics: Set[String]): InputDStream[(K, V)] = {    //获取消费者组ID    //val groupId = &quot;test&quot;    val groupId = kafkaParams.get(&quot;group.id&quot;).getOrElse(&quot;default&quot;)    // 在zookeeper上读取offsets前先根据实际情况更新offsets    setOrUpdateOffsets(topics, groupId)    //把所有的offsets处理完成，就可以从zookeeper上读取offset开始消费message    val messages = {      //获取kafka分区信息  为了打印信息      val partitionsE = kc.getPartitions(topics)      require(partitionsE.isRight, s&quot;获取 kafka topic ${topics}`s partition 失败。&quot;)      val partitions = partitionsE.right.get      println(&quot;打印分区信息&quot;)      partitions.foreach(println(_))      //获取分区的offset      val consumerOffsetsE = kc.getConsumerOffsets(groupId, partitions)      require(consumerOffsetsE.isRight, s&quot;获取 kafka topic ${topics}`s consumer offsets 失败。&quot;)      val consumerOffsets = consumerOffsetsE.right.get      println(&quot;打印消费者分区偏移信息&quot;)      consumerOffsets.foreach(println(_))      //读取数据      KafkaUtils.createDirectStream[K, V, KD, VD, (K, V)](        ssc, kafkaParams, consumerOffsets, (mmd: MessageAndMetadata[K, V]) =&gt; (mmd.key, mmd.message))    }    if (autoUpdateoffset) {      //更新offset      messages.foreachRDD(rdd =&gt; {        logInfo(&quot;RDD 消费成功，开始更新zookeeper上的偏移&quot;)        updateZKOffsets(rdd)      })    }    messages  }  /**    * 创建数据流前，根据实际消费情况更新消费offsets    * @param topics    * @param groupId    */  private def setOrUpdateOffsets(topics: Set[String], groupId: String): Unit = {    topics.foreach(topic =&gt; {      //先获取Kafka offset信息  Kafka partions的节点信息      //获取kafka本身的偏移量, Either类型可以认为就是封装了2种信息      val partitionsE = kc.getPartitions(Set(topic))      logInfo(partitionsE + &quot;&quot;)      //require(partitionsE.isRight, &quot;获取partition失败&quot;)      require(partitionsE.isRight, s&quot;获取 kafka topic ${topic}`s partition 失败。&quot;)      println(&quot;partitionsE=&quot; + partitionsE)      val partitions = partitionsE.right.get      println(&quot;打印分区信息&quot;)      partitions.foreach(println(_))      //获取kafka partions最早的offsets      val earliestLeader = kc.getEarliestLeaderOffsets(partitions)      require(earliestLeader.isRight, &quot;获取earliestLeader失败&quot;)      val earliestLeaderOffsets = earliestLeader.right.get      println(&quot;kafka最早的消息偏移量&quot;)      earliestLeaderOffsets.foreach(println(_))      //获取kafka最末尾的offsets      val latestLeader = kc.getLatestLeaderOffsets(partitions)      //require(latestLeader.isRight, &quot;获取latestLeader失败&quot;)      val latestLeaderOffsets = latestLeader.right.get      println(&quot;kafka最末尾的消息偏移量&quot;)      latestLeaderOffsets.foreach(println(_))      //获取消费者的offsets      val consumerOffsetsE = kc.getConsumerOffsets(groupId, partitions)      //判断消费者是否消费过,消费者offset存在      if (consumerOffsetsE.isRight) {        /**          * 如果zk上保存的offsets已经过时了，即kafka的定时清理策略已经将包含该offsets的文件删除。          * 针对这种情况，只要判断一下zk上的consumerOffsets和earliestLeaderOffsets的大小，          * 如果consumerOffsets比earliestLeaderOffsets还小的话，说明consumerOffsets已过时,          * 这时把consumerOffsets更新为earliestLeaderOffsets          */        //如果消费过，直接取过来的kafka消费，，earliestLeader 存在        if (earliestLeader.isRight) {          //获取到最早的offset  也就是最小的offset          require(earliestLeader.isRight, &quot;获取earliestLeader失败&quot;)          val earliestLeaderOffsets = earliestLeader.right.get          //获取消费者组的offset          val consumerOffsets = consumerOffsetsE.right.get          // 将 consumerOffsets 和 earliestLeaderOffsets 的offsets 做比较          // 可能只是存在部分分区consumerOffsets过时，所以只更新过时分区的consumerOffsets为earliestLeaderOffsets          var offsets: Map[TopicAndPartition, Long] = Map()          consumerOffsets.foreach({ case (tp, n) =&gt;            val earliestLeaderOffset = earliestLeaderOffsets(tp).offset            //如果消費者的偏移小于 kafka中最早的offset,那么，將最早的offset更新到zk            if (n &lt; earliestLeaderOffset) {              logWarning(&quot;consumer group:&quot; + groupId + &quot;,topic:&quot; + tp.topic + &quot;,partition:&quot; + tp.partition +                &quot; offsets已经过时，更新为&quot; + earliestLeaderOffset)              offsets += (tp -&gt; earliestLeaderOffset)            }          })          //设置offsets          setOffsets(groupId, offsets)        }      } else {        //如果没有消费过，那么就去取kafka获取earliestLeader写到zk中        // 消费者还没有消费过  也就是zookeeper中还没有消费者的信息        if (earliestLeader.isLeft)          logError(s&quot;${topic} hasConsumed but earliestLeaderOffsets is null。&quot;)        //看是从头消费还是从末开始消费  smallest表示从头开始消费        val reset = kafkaParams.get(&quot;auto.offset.reset&quot;).map(_.toLowerCase).getOrElse(&quot;smallest&quot;)        //往zk中去写，构建消费者 偏移        var leaderOffsets: Map[TopicAndPartition, Long] = Map.empty        //从头消费        if (reset.equals(&quot;smallest&quot;)) {          //分为 存在 和 不存在 最早的消费记录 两种情况          //如果kafka 最小偏移存在，则将消费者偏移设置为和kafka偏移一样          if (earliestLeader.isRight) {            leaderOffsets = earliestLeader.right.get.map {              case (tp, offset) =&gt; (tp, offset.offset)            }          } else {            //如果不存在，则从新构建偏移全部为0 offsets            leaderOffsets = partitions.map(tp =&gt; (tp, 0L)).toMap          }        } else {          //直接获取最新的offset          leaderOffsets = kc.getLatestLeaderOffsets(partitions).right.get.map {            case (tp, offset) =&gt; (tp, offset.offset)          }        }        //设置offsets 写到zk中        setOffsets(groupId, leaderOffsets)      }    })  }  /**    * 设置消费者组的offsets    * @param groupId    * @param offsets    */  private def setOffsets(groupId: String, offsets: Map[TopicAndPartition, Long]): Unit = {    if (offsets.nonEmpty) {      //更新offset      val o = kc.setConsumerOffsets(groupId, offsets)      logInfo(s&quot;更新zookeeper中消费组为：${groupId} 的 topic offset信息为： ${offsets}&quot;)      if (o.isLeft) {        logError(s&quot;Error updating the offset to Kafka cluster: ${o.left.get}&quot;)      }    }  }  /**    * 通过spark的RDD 更新zookeeper上的消费offsets    * @param rdd    */  def updateZKOffsets[K: ClassTag, V: ClassTag](rdd: RDD[(K, V)]) : Unit = {    //获取消费者组    val groupId = kafkaParams.get(&quot;group.id&quot;).getOrElse(&quot;default&quot;)    //spark使用kafka低阶API进行消费的时候,每个partion的offset是保存在 spark的RDD中，所以这里可以直接在    //RDD的 HasOffsetRanges 中获取倒offsets信息。因为这个信息spark不会把则个信息存储到zookeeper中，所以    //我们需要自己实现将这部分offsets信息存储到zookeeper中    val offsetsList = rdd.asInstanceOf[HasOffsetRanges].offsetRanges    //打印出spark中保存的offsets信息    offsetsList.foreach(x=&gt;{      println(&quot;获取spark 中的偏移信息&quot;+x)    })    for (offsets &lt;- offsetsList) {      //根据topic和partition 构建topicAndPartition      val topicAndPartition = TopicAndPartition(offsets.topic, offsets.partition)      logInfo(&quot;将SPARK中的 偏移信息 存到zookeeper中&quot;)      //将消费者组的offsets更新到zookeeper中      setOffsets(groupId, Map((topicAndPartition, offsets.untilOffset)))    }  }  //(null,{&quot;rksj&quot;:&quot;1558178497&quot;,&quot;latitude&quot;:&quot;24.000000&quot;,&quot;imsi&quot;:&quot;000000000000000&quot;})  //读取kafka流，并将json数据转为map  def createJsonToJMapObjectDirectStreamWithOffset(ssc:StreamingContext, topicsSet:Set[String]): DStream[java.util.Map[String,Object]] = {    //一个转换器    val converter = {json:String =&gt;      println(json)      var res : java.util.Map[String,Object] = null      try {        //JSON转map的操作        res = com.alibaba.fastjson.JSON.parseObject(json,          new TypeReference[java.util.Map[String, Object]]() {})      } catch {        case e: Exception =&gt; logError(s&quot;解析topic ${topicsSet}, 的记录 ${json} 失败。&quot;, e)      }      res    }    createDirectStreamWithOffset(ssc, topicsSet, converter).filter(_ != null)  }  /**    * 根据converter创建流数据    * @param ssc    * @param topicsSet    * @param converter    * @tparam T    * @return    */  def createDirectStreamWithOffset[T:ClassTag](ssc:StreamingContext,                                               topicsSet:Set[String], converter:String =&gt; T): DStream[T] = {    createDirectStream[String, String, StringDecoder, StringDecoder](ssc, topicsSet)      .map(pair =&gt;converter(pair._2))  }  def createJsonToJMapDirectStreamWithOffset(ssc:StreamingContext,                                             topicsSet:Set[String]): DStream[java.util.Map[String,String]] = {    val converter = {json:String =&gt;      var res : java.util.Map[String,String] = null      try {        res = com.alibaba.fastjson.JSON.parseObject(json, new TypeReference[java.util.Map[String, String]]() {})      } catch {        case e: Exception =&gt; logError(s&quot;解析topic ${topicsSet}, 的记录 ${json} 失败。&quot;, e)      }      res    }    createDirectStreamWithOffset(ssc, topicsSet, converter).filter(_ != null)  }  /*    /**      * @param ssc      * @param topicsSet      * @return      */    def createJsonToJavaBeanDirectStreamWithOffset(ssc:StreamingContext ,                                                   topicsSet:Set[String]): DStream[Object] = {      val converter = {json:String =&gt;        var res : Object = null        try {          res = com.alibaba.fastjson.JSON.parseObject(json, new TypeReference[Object]() {})        } catch {          case e: Exception =&gt; logError(s&quot;解析topic ${topicsSet}, 的记录 ${json} 失败。&quot;, e)        }        res      }      createDirectStreamWithOffset(ssc, topicsSet, converter).filter(_ != null)    }  */  /*    def createStringDirectStreamWithOffset(ssc:StreamingContext ,                                           topicsSet:Set[String]): DStream[String] = {      val converter = {json:String =&gt;        json      }      createDirectStreamWithOffset(ssc, topicsSet, converter).filter(_ != null)    }  */  /**    * 读取JSON的流 并将JSON流 转为MAP流  并且这个流支持RDD向zookeeper中记录消费信息    * @param ssc   spark ssc    * @param topicsSet topic 集合 支持从多个kafka topic同时读取数据    * @return  DStream[java.util.Map[String,String    */  def createJsonToJMapStringDirectStreamWithOffset(ssc:StreamingContext , topicsSet:Set[String]): DStream[java.util.Map[String,String]] = {    val converter = {json:String =&gt;      var res : java.util.Map[String,String] = null      try {        res = com.alibaba.fastjson.JSON.parseObject(json, new TypeReference[java.util.Map[String, String]]() {})      } catch {        case e: Exception =&gt; logError(s&quot;解析topic ${topicsSet}, 的记录 ${json} 失败。&quot;, e)      }      res    }    createDirectStreamWithOffset(ssc, topicsSet, converter).filter(_ != null)  }  /**    * 读取JSON的流 并将JSON流 转为MAP流  并且这个流支持RDD向zookeeper中记录消费信息    * @param ssc   spark ssc    * @param topicsSet topic 集合 支持从多个kafka topic同时读取数据    * @return  DStream[java.util.Map[String,String    */  def createJsonToJMapStringDirectStreamWithoutOffset(ssc:StreamingContext , topicsSet:Set[String]): DStream[java.util.Map[String,String]] = {    val converter = {json:String =&gt;      var res : java.util.Map[String,String] = null      try {        res = com.alibaba.fastjson.JSON.parseObject(json, new TypeReference[java.util.Map[String, String]]() {})      } catch {        case e: Exception =&gt; logError(s&quot;解析topic ${topicsSet}, 的记录 ${json} 失败。&quot;, e)      }      res    }    createDirectStreamWithOffset(ssc, topicsSet, converter).filter(_ != null)  }}object KafkaManager extends Logging{  def apply(broker:String, groupId:String = &quot;default&quot;,            numFetcher:Int = 1, offset:String = &quot;smallest&quot;,            autoUpdateoffset:Boolean = true): KafkaManager ={    new KafkaManager(      createKafkaParam(broker, groupId, numFetcher, offset),      autoUpdateoffset)  }  def createKafkaParam(broker:String, groupId:String = &quot;default&quot;,                       numFetcher:Int = 1, offset:String = &quot;smallest&quot;): Map[String, String] ={    //创建 stream 时使用的 topic 名字集合    Map[String, String](      &quot;metadata.broker.list&quot; -&gt; broker,      &quot;auto.offset.reset&quot; -&gt; offset,      &quot;group.id&quot; -&gt; groupId,      &quot;num.consumer.fetchers&quot; -&gt; numFetcher.toString)  }}</code></pre><h4 id="5、resources-log4j-properties"><a href="#5、resources-log4j-properties" class="headerlink" title="5、resources/log4j.properties"></a>5、resources/log4j.properties</h4><pre><code>### 设置###log4j.rootLogger = error,stdout,D,E### 输出信息到控制抬 ###log4j.appender.stdout = org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.Target = System.outlog4j.appender.stdout.layout = org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern = [%-5p] %d{yyyy-MM-dd HH:mm:ss,SSS} method:%l%n%m%n### 输出DEBUG 级别以上的日志到=E://logs/error.log ###log4j.appender.D = org.apache.log4j.DailyRollingFileAppenderlog4j.appender.D.File = E://logs/log.loglog4j.appender.D.Append = truelog4j.appender.D.Threshold = stdout log4j.appender.D.layout = org.apache.log4j.PatternLayoutlog4j.appender.D.layout.ConversionPattern = %-d{yyyy-MM-dd HH:mm:ss}  [ %t:%r ] - [ %p ]  %m%n###输出ERROR 级别以上的日志到=E://logs/error.log ###log4j.appender.E = org.apache.log4j.DailyRollingFileAppenderlog4j.appender.E.File =E://logs/error.log log4j.appender.E.Append = truelog4j.appender.E.Threshold = ERROR log4j.appender.E.layout = org.apache.log4j.PatternLayoutlog4j.appender.E.layout.ConversionPattern = %-d{yyyy-MM-dd HH:mm:ss}  [ %t:%r ] - [ %p ]  %m%n</code></pre><h4 id="6、xz-bigdata-spark-spark-streaming-kafka"><a href="#6、xz-bigdata-spark-spark-streaming-kafka" class="headerlink" title="6、xz_bigdata_spark/spark/streaming/kafka"></a>6、xz_bigdata_spark/spark/streaming/kafka</h4><p><strong>Spark_Es_ConfigUtil.scala</strong></p><pre><code>package com.hsiehchou.spark.streaming.kafkaimport org.apache.spark.Loggingobject Spark_Es_ConfigUtil extends Serializable with Logging{ // val ES_NODES = &quot;es.cluster.nodes&quot; // val ES_PORT = &quot;es.cluster.http.port&quot; // val ES_CLUSTERNAME = &quot;es.cluster.name&quot;  val ES_NODES = &quot;es.nodes&quot;  val ES_PORT = &quot;es.port&quot;  val ES_CLUSTERNAME = &quot;es.clustername&quot;  def getEsParam(id_field : String): Map[String,String] ={    Map[String ,String](&quot;es.mapping.id&quot; -&gt; id_field,      ES_NODES -&gt; &quot;hadoop1,hadoop2,hadoop3&quot;,      //ES_NODES -&gt; &quot;hadoop1&quot;,      ES_PORT -&gt; &quot;9200&quot;,      ES_CLUSTERNAME -&gt; &quot;xz_es&quot;,      &quot;es.batch.size.entries&quot;-&gt;&quot;6000&quot;,      /*   &quot;es.nodes.wan.only&quot;-&gt;&quot;true&quot;,*/      &quot;es.nodes.discovery&quot;-&gt;&quot;true&quot;,      &quot;es.batch.size.bytes&quot;-&gt;&quot;300000000&quot;,      &quot;es.batch.write.refresh&quot;-&gt;&quot;false&quot;    )  }}</code></pre><p><strong>Spark_Kafka_ConfigUtil.scala</strong></p><pre><code>package com.hsiehchou.spark.streaming.kafkaimport org.apache.spark.Loggingobject Spark_Kafka_ConfigUtil extends Serializable with Logging{  def getKafkaParam(brokerList:String,groupId : String): Map[String,String]={    val kafkaParam=Map[String,String](      &quot;metadata.broker.list&quot; -&gt; brokerList,      &quot;auto.offset.reset&quot; -&gt; &quot;smallest&quot;,      &quot;group.id&quot; -&gt; groupId,      &quot;refresh.leader.backoff.ms&quot; -&gt; &quot;1000&quot;,      &quot;num.consumer.fetchers&quot; -&gt; &quot;8&quot;)    kafkaParam  }}</code></pre><h4 id="7、kafka2es"><a href="#7、kafka2es" class="headerlink" title="7、kafka2es"></a>7、kafka2es</h4><p><strong>Kafka2esJob.scala</strong></p><pre><code>package com.hsiehchou.spark.streaming.kafka.kafka2esimport com.hsiehchou.es.admin.AdminUtilimport com.hsiehchou.es.client.ESClientUtilsimport com.hsiehchou.spark.common.convert.DataConvertimport com.hsiehchou.spark.streaming.kafka.Spark_Es_ConfigUtilimport org.apache.spark.Loggingimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.dstream.DStreamimport org.elasticsearch.client.transport.TransportClientimport org.elasticsearch.spark.rdd.EsSparkobject Kafka2esJob extends Serializable with Logging {  /**    * 按日期分组写入ES    * @param dataType    * @param typeDS    */  def insertData2EsBydate(dataType:String,typeDS:DStream[java.util.Map[String,String]]): Unit ={    //通过 dataType + 日期来动态创建 分索引。 日期格式为 yyyyMMdd    //主要就是时间混杂  通过时间分组就行了 groupby       filter    //index前缀  通过对日期进行过滤 避免shuffle操作    val index_prefix = dataType    val client: TransportClient = ESClientUtils.getClient    typeDS.foreachRDD(rdd=&gt;{      //如果时少量数据可以这样处理      //rdd.groupBy()      //吧所有的日期拿到      val days = getDays(dataType,rdd)      //我们使用日期对数据进行过滤  par时scala并发集合      days.par.foreach(day=&gt;{        //通过前缀+日期组成一个动态的索引   比例  qq + &quot;_&quot; + &quot;20190508&quot;        val index = index_prefix + &quot;_&quot; + day        //判断索引是否存在        val bool = AdminUtil.indexExists(client,index)        if(!bool){          //如果不存在，创建          val mappingPath = s&quot;es/mapping/${index_prefix}.json&quot;          AdminUtil.buildIndexAndTypes(index, index, mappingPath, 5, 1)        }        //构建RDD，数据类型 某一天的数据RDD        //返回一个map[String,obJECT] 的RDD   //就是一个单一类型  单一天数的RDD        val tableRDD = rdd.filter(map=&gt;{          day.equals(map.get(&quot;index_date&quot;))        }).map(x=&gt;{          //将map[String,String] 转为map[String,obJECT]          DataConvert.strMap2esObjectMap(x)        })        EsSpark.saveToEs(tableRDD,index+ &quot;/&quot;+index,Spark_Es_ConfigUtil.getEsParam(&quot;id&quot;))      })    })    //日期为后  }  /**    * 获取日期的集合    * @param dataType    * @param rdd    * @return    */  def getDays(dataType:String,rdd:RDD[java.util.Map[String,String]]): Array[String] ={    //对日期去重，然后集中到driver    return  rdd.map(x=&gt;{x.get(&quot;index_date&quot;)}).distinct().collect()  }  /**    * 将RDD转换之后写入ES    * @param dataType    * @param typeRDD    */  def insertData2Es(dataType:String,typeRDD:RDD[java.util.Map[String,String]]): Unit = {    val index = dataType    val esRDD =  typeRDD.map(x=&gt;{      DataConvert.strMap2esObjectMap(x)    })    EsSpark.saveToEs(esRDD,index+ &quot;/&quot;+index,Spark_Es_ConfigUtil.getEsParam(&quot;id&quot;))    println(&quot;写入ES&quot; + esRDD.count() + &quot;条数据成功&quot;)  }  /**    * 将RDD转换后写入ES    * @param dataType    * @param typeDS    */  def insertData2Es(dataType:String, typeDS:DStream[java.util.Map[String, String]]): Unit = {    val index = dataType    typeDS.foreachRDD(rdd=&gt;{      val esRDD = rdd.map(x=&gt;{        DataConvert.strMap2esObjectMap(x)      })      EsSpark.saveToEs(rdd, dataType+&quot;/&quot;+dataType, Spark_Es_ConfigUtil.getEsParam(&quot;id&quot;))      println(&quot;写入ES&quot; + esRDD.count() + &quot;条数据成功&quot;)    })  }}</code></pre><p><strong>Kafka2esStreaming.scala</strong></p><pre><code>package com.hsiehchou.spark.streaming.kafka.kafka2esimport java.utilimport java.util.Propertiesimport com.hsiehchou.common.config.ConfigUtilimport com.hsiehchou.common.project.datatype.DataTypePropertiesimport com.hsiehchou.common.time.TimeTranstationUtilsimport com.hsiehchou.spark.common.SparkContextFactoryimport com.hsiehchou.spark.streaming.kafka.Spark_Kafka_ConfigUtilimport org.apache.commons.lang3.StringUtilsimport org.apache.spark.Loggingimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.kafka.KafkaManagerimport scala.collection.JavaConversions._object Kafka2esStreaming extends Serializable with Logging {  //获取数据类型  private val dataTypes: util.Set[String] = DataTypeProperties.dataTypeMap.keySet()  val kafkaConfig: Properties = ConfigUtil.getInstance().getProperties(&quot;kafka/kafka-server-config.properties&quot;)  def main(args: Array[String]): Unit = {    //val topics = &quot;chl_test7&quot;.split(&quot;,&quot;)    val topics = args(1).split(&quot;,&quot;)    //   val ssc = SparkConfFactory.newSparkLocalStreamingContext(&quot;XZ_kafka2es&quot;, java.lang.Long.valueOf(10),1)    val ssc = SparkContextFactory.newSparkStreamingContext(&quot;Kafka2esStreaming&quot;, java.lang.Long.valueOf(10))    //构建kafkaManager    val kafkaManager = new KafkaManager(      Spark_Kafka_ConfigUtil.getKafkaParam(kafkaConfig.getProperty(&quot;metadata.broker.list&quot;), &quot;XZ3&quot;)    )    //使用kafkaManager创建DStreaming流    val kafkaDS = kafkaManager.createJsonToJMapStringDirectStreamWithOffset(ssc, topics.toSet)      //添加一个日期分组字段      //如果数据其他的转换，可以先在这里进行统一转换      .map(map=&gt;{      map.put(&quot;index_date&quot;,TimeTranstationUtils.Date2yyyyMMddHHmmss(java.lang.Long.valueOf(map.get(&quot;collect_time&quot;)+&quot;000&quot;)))      map    }).persist(StorageLevel.MEMORY_AND_DISK)    //使用par并发集合可以是任务并发执行。在资源充足的情况下    dataTypes.foreach(datatype=&gt;{      //过滤出单个类别的数据种类      val tableDS = kafkaDS.filter(x=&gt;{datatype.equals(x.get(&quot;table&quot;))})      Kafka2esJob.insertData2Es(datatype,tableDS)    })    ssc.start()    ssc.awaitTermination()  }  /**    * 启动参数检查    * @param args    */  def sparkParamCheck(args: Array[String]): Unit ={    if (args.length == 4) {      if (StringUtils.isBlank(args(1))) {        logInfo(&quot;kafka集群地址不能为空&quot;)        logInfo(&quot;kafka集群地址格式为     主机1名：9092,主机2名：9092,主机3名：9092...&quot;)        logInfo(&quot;格式为     主机1名：9092,主机2名：9092,主机3名：9092...&quot;)        System.exit(-1)      }      if (StringUtils.isBlank(args(2))) {        logInfo(&quot;kafka topic1不能为空&quot;)        System.exit(-1)      }      if (StringUtils.isBlank(args(3))) {        logInfo(&quot;kafka topic2不能为空&quot;)        System.exit(-1)      }    }else{      logError(&quot;启动参数个数错误&quot;)    }  }  def startJob(ds:DStream[String]): Unit ={  }}</code></pre><p><strong>java/com/hsiehchou/spark/common/convert/BaseDataConvert.java</strong></p><pre><code>package com.hsiehchou.spark.common.convert;import org.apache.commons.lang.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.HashMap;import java.util.Map;public class BaseDataConvert {    private static final Logger LOG = LoggerFactory.getLogger(BaseDataConvert.class);    public static HashMap&lt;String,Object&gt; mapString2Long(Map&lt;String,String&gt; map, String key, HashMap&lt;String,Object&gt; objectMap) {        String logouttime = map.get(key);        if (StringUtils.isNotBlank(logouttime)) {            objectMap.put(key, Long.valueOf(logouttime));        } else {            objectMap.put(key, 0L);        }        return objectMap;    }    public static HashMap&lt;String,Object&gt; mapString2Double(Map&lt;String,String&gt; map, String key, HashMap&lt;String,Object&gt; objectMap) {        String logouttime = map.get(key);        if (StringUtils.isNotBlank(logouttime)) {            objectMap.put(key, Double.valueOf(logouttime));        } else {            objectMap.put(key, 0.000000);        }        return objectMap;    }    public static HashMap&lt;String,Object&gt; mapString2String(Map&lt;String,String&gt; map, String key, HashMap&lt;String,Object&gt; objectMap) {        String logouttime = map.get(key);        if (StringUtils.isNotBlank(logouttime)) {            objectMap.put(key, logouttime);        } else {            objectMap.put(key, &quot;&quot;);        }        return objectMap;    }}</code></pre><h4 id="8、ES动态索引创建"><a href="#8、ES动态索引创建" class="headerlink" title="8、ES动态索引创建"></a>8、ES动态索引创建</h4><pre><code>/**    * 按日期分组写入ES    * @param dataType    * @param typeDS    */  def insertData2EsBydate(dataType:String,typeDS:DStream[java.util.Map[String,String]]): Unit ={    //通过 dataType + 日期来动态创建 分索引。 日期格式为 yyyyMMdd    //主要就是时间混杂  通过时间分组就行了 groupby       filter    //index前缀  通过对日期进行过滤 避免shuffle操作    val index_prefix = dataType    val client: TransportClient = ESClientUtils.getClient    typeDS.foreachRDD(rdd=&gt;{      //如果时少量数据可以这样处理      //rdd.groupBy()      //吧所有的日期拿到      val days = getDays(dataType,rdd)      //我们使用日期对数据进行过滤  par时scala并发集合      days.par.foreach(day=&gt;{        //通过前缀+日期组成一个动态的索引   比例  qq + &quot;_&quot; + &quot;20190508&quot;        val index = index_prefix + &quot;_&quot; + day        //判断索引是否存在        val bool = AdminUtil.indexExists(client,index)        if(!bool){          //如果不存在，创建          val mappingPath = s&quot;es/mapping/${index_prefix}.json&quot;          AdminUtil.buildIndexAndTypes(index, index, mappingPath, 5, 1)        }        //构建RDD，数据类型 某一天的数据RDD        //返回一个map[String,obJECT] 的RDD   //就是一个单一类型  单一天数的RDD        val tableRDD = rdd.filter(map=&gt;{          day.equals(map.get(&quot;index_date&quot;))        }).map(x=&gt;{          //将map[String,String] 转为map[String,obJECT]          DataConvert.strMap2esObjectMap(x)        })        EsSpark.saveToEs(tableRDD,index+ &quot;/&quot;+index,Spark_Es_ConfigUtil.getEsParam(&quot;id&quot;))      })    })    //日期为后  }</code></pre><p><strong>xz_bigdata_es下一节展示代码</strong><br><img src="/medias/%E5%85%A5ES%E4%BD%BF%E7%94%A8%E5%8A%A8%E6%80%81%E7%B4%A2%E5%BC%95.PNG" alt="入ES使用动态索引"></p><h4 id="9、CDH的java配置和Elasticsearch的配置"><a href="#9、CDH的java配置和Elasticsearch的配置" class="headerlink" title="9、CDH的java配置和Elasticsearch的配置"></a>9、CDH的java配置和Elasticsearch的配置</h4><p><strong>cdh的jdk设置</strong><br>/usr/local/jdk1.8</p><p><strong>kafka配置</strong></p><p>Default Number of Partitions：num.partitions 8</p><p>Offset Commit Topic Number of Partitions：180天</p><p>Log Compaction Delete Record Retention Time：log.cleaner.delete.retention.ms 30天</p><p>Data Log Roll Hours：log.retention.hours 30天  log.roll.hours 30天</p><p>Java Heap Size of Broker：broker_max_heap_size  1吉字节</p><p><strong>YARN</strong><br>容器内存 5g 5g 1g 10g</p><p><strong>这里的CDH安装另一篇文章介绍</strong></p><p><strong>前提安装好elasticsearch</strong></p><p>mkdir /opt/software/elasticsearch/data/</p><p>mkdir /opt/software/elasticsearch/logs/</p><p>chmod 777 /opt/software/elasticsearch/data/</p><p>useradd elasticsearch<br>passwd elasticsearch</p><p>chown -R elasticsearch elasticsearch/</p><p><strong>vim /etc/security/limits.conf</strong><br>添加如下内容:<br><code>*</code> <strong>soft nofile 65536</strong><br><code>*</code> <strong>hard nofile 131072</strong><br><code>*</code> <strong>soft nproc 2048</strong><br><code>*</code> <strong>hard nproc 4096</strong></p><p>进入limits.d目录下修改配置文件<br><strong>vim /etc/security/limits.d/90-nproc.conf</strong></p><p>修改如下内容：<br><strong>soft nproc 4096（修改为此参数，6版本的默认就是4096）</strong></p><p>修改配置sysctl.conf<br><strong>vim /etc/sysctl.conf</strong></p><p>添加下面配置：<br><strong>vm.max_map_count=655360</strong></p><p>并执行命令：<br><strong>sysctl -p</strong></p><p><strong>hadoop1的conf配置</strong><br><strong>elasticsearch.yml</strong></p><pre><code># ======================== Elasticsearch Configuration =========================## NOTE: Elasticsearch comes with reasonable defaults for most settings.#       Before you set out to tweak and tune the configuration, make sure you#       understand what are you trying to accomplish and the consequences.## The primary way of configuring a node is via this file. This template lists# the most important settings you may want to configure for a production cluster.## Please consult the documentation for further information on configuration options:# https://www.elastic.co/guide/en/elasticsearch/reference/index.html## ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:##cluster.name: my-applicationcluster.name: xz_es## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:##node.name: node-1#node.name: node-1node.master: truenode.data: true# Add custom attributes to the node:##node.attr.rack: r1## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):##path.data: /path/to/datapath.data: /opt/software/elasticsearch/data## Path to log files:##path.logs: /path/to/logspath.logs: /opt/software/elasticsearch/logs## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:##bootstrap.memory_lock: truebootstrap.memory_lock: falsebootstrap.system_call_filter: false## Make sure that the heap size is set to about half the memory available# on the system and that the owner of the process is allowed to use this# limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):##network.host: 192.168.0.1network.host: 192.168.116.201## Set a custom port for HTTP:##http.port: 9200## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when new node is started:# The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]##discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2&quot;]discovery.zen.ping.unicast.hosts: [&quot;hadoop1&quot;, &quot;hadoop2&quot;, &quot;hadoop3&quot;]## Prevent the &quot;split brain&quot; by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):##discovery.zen.minimum_master_nodes: ## For more information, consult the zen discovery module documentation.## ---------------------------------- Gateway -----------------------------------## Block initial recovery after a full cluster restart until N nodes are started:##gateway.recover_after_nodes: 3## For more information, consult the gateway module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: true</code></pre><p><strong>jvm.options</strong><br>修改下<br>-Xms64m<br>-Xmx64m</p><p><strong>hadoop2的conf配置</strong><br><strong>elasticsearch.yml</strong></p><pre><code># ======================== Elasticsearch Configuration =========================## NOTE: Elasticsearch comes with reasonable defaults for most settings.#       Before you set out to tweak and tune the configuration, make sure you#       understand what are you trying to accomplish and the consequences.## The primary way of configuring a node is via this file. This template lists# the most important settings you may want to configure for a production cluster.## Please consult the documentation for further information on configuration options:# https://www.elastic.co/guide/en/elasticsearch/reference/index.html## ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:##cluster.name: my-applicationcluster.name: xz_es## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:##node.name: node-1node.name: node-2node.master: falsenode.data: true## Add custom attributes to the node:##node.attr.rack: r1## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):##path.data: /path/to/datapath.data: /opt/software/elasticsearch/data## Path to log files:##path.logs: /path/to/logspath.logs: /opt/software/elasticsearch/logs## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:##bootstrap.memory_lock: truebootstrap.memory_lock: falsebootstrap.system_call_filter: false## Make sure that the heap size is set to about half the memory available# on the system and that the owner of the process is allowed to use this# limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):##network.host: 192.168.0.1network.host: 192.168.116.202## Set a custom port for HTTP:##http.port: 9200## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when new node is started:# The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]##discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2&quot;]discovery.zen.ping.unicast.hosts: [&quot;hadoop1&quot;, &quot;hadoop2&quot;, &quot;hadoop3&quot;]## Prevent the &quot;split brain&quot; by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):##discovery.zen.minimum_master_nodes: ## For more information, consult the zen discovery module documentation.## ---------------------------------- Gateway -----------------------------------## Block initial recovery after a full cluster restart until N nodes are started:##gateway.recover_after_nodes: 3## For more information, consult the gateway module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: true</code></pre><p><strong>jvm.options</strong><br>修改下<br>-Xms64m<br>-Xmx64m</p><p><strong>hadoop3的conf配置</strong><br><strong>elasticsearch.yml</strong></p><pre><code># ======================== Elasticsearch Configuration =========================## NOTE: Elasticsearch comes with reasonable defaults for most settings.#       Before you set out to tweak and tune the configuration, make sure you#       understand what are you trying to accomplish and the consequences.## The primary way of configuring a node is via this file. This template lists# the most important settings you may want to configure for a production cluster.## Please consult the documentation for further information on configuration options:# https://www.elastic.co/guide/en/elasticsearch/reference/index.html## ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:##cluster.name: my-applicationcluster.name: xz_es## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:##node.name: node-1node.name: node-3node.master: falsenode.data: true## Add custom attributes to the node:##node.attr.rack: r1## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):##path.data: /path/to/datapath.data: /opt/software/elasticsearch/data## Path to log files:##path.logs: /path/to/logspath.logs: /opt/software/elasticsearch/logs## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:##bootstrap.memory_lock: truebootstrap.memory_lock: falsebootstrap.system_call_filter: false## Make sure that the heap size is set to about half the memory available# on the system and that the owner of the process is allowed to use this# limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):##network.host: 192.168.0.1network.host: 192.168.116.203## Set a custom port for HTTP:##http.port: 9200## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when new node is started:# The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]##discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2&quot;]discovery.zen.ping.unicast.hosts: [&quot;hadoop1&quot;, &quot;hadoop2&quot;, &quot;hadoop3&quot;]## Prevent the &quot;split brain&quot; by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):##discovery.zen.minimum_master_nodes: ## For more information, consult the zen discovery module documentation.## ---------------------------------- Gateway -----------------------------------## Block initial recovery after a full cluster restart until N nodes are started:##gateway.recover_after_nodes: 3## For more information, consult the gateway module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##action.destructive_requires_name: true</code></pre><p><strong>jvm.options</strong><br>修改下<br>-Xms64m<br>-Xmx64m</p><p><strong>Kibana的conf配置</strong></p><p><strong>kibana.yml</strong></p><pre><code># Kibana is served by a back end server. This setting specifies the port to use.server.port: 5601# Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values.# The default is &#39;localhost&#39;, which usually means remote machines will not be able to connect.# To allow connections from remote users, set this parameter to a non-loopback address.#server.host: &quot;localhost&quot;server.host: &quot;192.168.116.202&quot;# Enables you to specify a path to mount Kibana at if you are running behind a proxy. This only affects# the URLs generated by Kibana, your proxy is expected to remove the basePath value before forwarding requests# to Kibana. This setting cannot end in a slash.#server.basePath: &quot;&quot;# The maximum payload size in bytes for incoming server requests.#server.maxPayloadBytes: 1048576# The Kibana server&#39;s name.  This is used for display purposes.#server.name: &quot;your-hostname&quot;# The URL of the Elasticsearch instance to use for all your queries.#elasticsearch.url: &quot;http://localhost:9200&quot;elasticsearch.url: &quot;http://192.168.116.201:9200&quot;</code></pre><p><strong>运行Elasticsearch</strong><br>cd /opt/software/elasticsearch<br>su elasticsearch<br>bin/elasticsearch &amp;</p><p><strong>运行Kibana</strong><br>cd /opt/software/kibana/<br>bin/kibana &amp;</p><h4 id="10、kafka2es打包到集群执行"><a href="#10、kafka2es打包到集群执行" class="headerlink" title="10、kafka2es打包到集群执行"></a>10、kafka2es打包到集群执行</h4><p><strong>打包</strong><br>使用maven工具点击install</p><p><strong>放入集群</strong><br>将打包完成的jar文件和xz_bigdata_spark-1.0-SNAPSHOT.jar 一起放入/usr/chl/spark7/目录下面</p><p><strong>执行</strong><br>spark-submit <code>--</code>master yarn-cluster <code>--</code>num-executors 1 <code>--</code>driver-memory 500m <code>--</code>executor-memory 1g <code>--</code>executor-cores 1 <code>--</code>jars $(echo /usr/chl/spark7/jars/*.jar | tr ‘ ‘ ‘,’) <code>--</code>class com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar chl_test7 chl_test7</p><p>spark-submit<br><code>--</code>master yarn-cluster    //集群启动<br><code>--</code>num-executors 1        //分配多少个进程<br><code>--</code>driver-memory 500m  //driver内存<br><code>--</code>executor-memory 1g //进程内存<br><code>--</code>executor-cores 1       //开多少个核，线程<br><code>--</code>jars $(echo /usr/chl/spark8/jars/*.jar | tr ‘ ‘ ‘,’) //加载jar<br><code>--</code>class com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</p><h4 id="11、运行截图"><a href="#11、运行截图" class="headerlink" title="11、运行截图"></a>11、运行截图</h4><p><img src="/medias/kafka2esstreaming%E6%88%AA%E5%9B%BE.PNG" alt="kafka2esstreaming截图"></p><p><img src="/medias/Elasticsearch%E5%90%84%E4%B8%AA%E8%8A%82%E7%82%B9%E7%8A%B6%E5%86%B5.PNG" alt="Elasticsearch各个节点状况"></p><h4 id="12、冲突查找快捷键"><a href="#12、冲突查找快捷键" class="headerlink" title="12、冲突查找快捷键"></a>12、冲突查找快捷键</h4><p><strong>Ctrl+Alt+Shift+N</strong></p><h3 id="八、xz-bigdata-es开发"><a href="#八、xz-bigdata-es开发" class="headerlink" title="八、xz_bigdata_es开发"></a>八、xz_bigdata_es开发</h3><h4 id="1、pom-xml-1"><a href="#1、pom-xml-1" class="headerlink" title="1、pom.xml"></a>1、pom.xml</h4><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_es&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_es&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;            &lt;artifactId&gt;transport&lt;/artifactId&gt;            &lt;version&gt;6.2.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;io.searchbox&lt;/groupId&gt;            &lt;artifactId&gt;jest&lt;/artifactId&gt;            &lt;version&gt;6.3.1&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><h4 id="2、admin"><a href="#2、admin" class="headerlink" title="2、admin"></a>2、admin</h4><p><strong>AdminUtil.java</strong></p><pre><code>package com.hsiehchou.es.admin;import com.hsiehchou.common.file.FileCommon;import com.hsiehchou.es.client.ESClientUtils;import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;import org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsResponse;import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.common.settings.Settings;import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class AdminUtil {    private static Logger LOG = LoggerFactory.getLogger(AdminUtil.class);    public static void main(String[] args) throws Exception{        //创建索引核mapping        AdminUtil.buildIndexAndTypes(&quot;tanslator_test1111&quot;,&quot;tanslator_test1111&quot;, &quot;es/mapping/test.json&quot;,3,1);        //index = 类型+日期        //查找类  Ctrl+Shift+Alt+N    }    /**     * @param index     * @param type     * @param path     * @param shard     * @param replication     * @return     * @throws Exception     */    public static boolean buildIndexAndTypes(String index,String type,String path,int shard,int replication) throws Exception{        boolean flag ;        TransportClient client = ESClientUtils.getClient();        String mappingJson = FileCommon.getAbstractPath(path);        boolean indices = AdminUtil.createIndices(client, index, shard, replication);        if(indices){            LOG.info(&quot;创建索引&quot;+ index + &quot;成功&quot;);            flag = MappingUtil.addMapping(client, index, type, mappingJson);        }        else{            LOG.error(&quot;创建索引&quot;+ index + &quot;失败&quot;);            flag = false;        }        return flag;    }    /**     * @desc 判断需要创建的index是否存在     * */    public static boolean indexExists(TransportClient client,String index){        boolean ifExists = false;        try {            System.out.println(&quot;client===&quot; + client);            IndicesExistsResponse existsResponse = client.admin().indices().prepareExists(index).execute().actionGet();            ifExists = existsResponse.isExists();        } catch (Exception e) {            e.printStackTrace();            LOG.error(&quot;判断index是否存在失败...&quot;);            return ifExists;        }        return ifExists;    }    /**     * 创建索引     * @param client     * @param index     * @param shard     * @param replication     * @return     */    public static boolean createIndices(TransportClient client, String index, int shard , int replication){        if(!indexExists(client,index)) {            LOG.info(&quot;该index不存在，创建...&quot;);            CreateIndexResponse createIndexResponse =null;            try {                createIndexResponse = client.admin().indices().prepareCreate(index)                        .setSettings(Settings.builder()                                .put(&quot;index.number_of_shards&quot;, shard)                                .put(&quot;index.number_of_replicas&quot;, replication)                                .put(&quot;index.codec&quot;, &quot;best_compression&quot;)                                .put(&quot;refresh_interval&quot;, &quot;30s&quot;))                        .execute().actionGet();                return createIndexResponse.isAcknowledged();            } catch (Exception e) {                LOG.error(null, e);                return false;            }        }        LOG.warn(&quot;该index &quot; + index + &quot; 已经存在...&quot;);        return false;    }}</code></pre><p><strong>MappingUtil.java</strong></p><pre><code>package com.hsiehchou.es.admin;import com.alibaba.fastjson.JSON;import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;import org.elasticsearch.action.admin.indices.mapping.put.PutMappingResponse;import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.common.xcontent.XContentBuilder;import org.elasticsearch.common.xcontent.XContentFactory;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;public class MappingUtil {    private static Logger LOG = LoggerFactory.getLogger(MappingUtil.class);    //关闭自动添加字段，关闭后索引数据中如果有多余字段不会修改mapping,默认true    private boolean dynamic = true;    public static XContentBuilder buildMapping(String tableName) throws IOException {        XContentBuilder builder = null;        try {            builder = XContentFactory.jsonBuilder().startObject()                    .startObject(tableName)                    .startObject(&quot;_source&quot;).field(&quot;enabled&quot;, true).endObject()                    .startObject(&quot;properties&quot;)                    .startObject(&quot;id&quot;).field(&quot;type&quot;, &quot;long&quot;).endObject()                    .startObject(&quot;sn&quot;).field(&quot;type&quot;, &quot;text&quot;).endObject()                    .endObject()                  .endObject()                  .endObject();        } catch (IOException e) {            e.printStackTrace();        }        return builder;    }    public static boolean addMapping(TransportClient client, String index, String type, String jsonString){        PutMappingResponse putMappingResponse = null;        try {            PutMappingRequest mappingRequest = new PutMappingRequest(index)                    .type(type).source(JSON.parseObject(jsonString));            putMappingResponse = client.admin().indices().putMapping(mappingRequest).actionGet();        } catch (Exception e) {            LOG.error(null,e);            e.printStackTrace();            LOG.error(&quot;添加&quot; + type + &quot;的mapping失败....&quot;,e);            return false;        }        boolean success = putMappingResponse.isAcknowledged();        if (success){            LOG.info(&quot;创建&quot; + type + &quot;的mapping成功....&quot;);            return success;        }        return success;    }    public static void main(String[] args) throws Exception {        /*String singleConf = ConsulConfigUtil.getSingleConf(&quot;es6.1.0/mapping/http&quot;);        int i = singleConf.length() / 2;        System.out.println(i);*/    }}</code></pre><h4 id="3、client"><a href="#3、client" class="headerlink" title="3、client"></a>3、client</h4><p><strong>ESClientUtils.java</strong></p><pre><code>package com.hsiehchou.es.client;import com.hsiehchou.common.config.ConfigUtil;import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.common.settings.Settings;import org.elasticsearch.common.transport.TransportAddress;import org.elasticsearch.transport.client.PreBuiltTransportClient;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.Serializable;import java.net.InetAddress;import java.util.Properties;/** * ES 客户端获取 */public class ESClientUtils implements Serializable{    private static Logger LOG = LoggerFactory.getLogger(ESClientUtils.class);    private volatile static TransportClient esClusterClient;    private ESClientUtils(){}    private static Properties properties;    static {        properties = ConfigUtil.getInstance().getProperties(&quot;es/es_cluster.properties&quot;);    }    public static TransportClient getClient(){        System.setProperty(&quot;es.set.netty.runtime.available.processors&quot;, &quot;false&quot;);        String clusterName = properties.getProperty(&quot;es.cluster.name&quot;);        String clusterNodes1 = properties.getProperty(&quot;es.cluster.nodes1&quot;);        String clusterNodes2 = properties.getProperty(&quot;es.cluster.nodes2&quot;);        String clusterNodes3 = properties.getProperty(&quot;es.cluster.nodes3&quot;);        LOG.info(&quot;clusterName:&quot;+ clusterName);        LOG.info(&quot;clusterNodes:&quot;+ clusterNodes1);        LOG.info(&quot;clusterNodes:&quot;+ clusterNodes2);        LOG.info(&quot;clusterNodes:&quot;+ clusterNodes3);        if(esClusterClient==null){            synchronized (ESClientUtils.class){                if(esClusterClient==null){                    try{                        Settings settings = Settings.builder()                                .put(&quot;cluster.name&quot;, clusterName)                                //.put(&quot;searchguard.ssl.transport.enabled&quot;, false)                                //.put(&quot;xpack.security.user&quot;, &quot;sc_xy_mn_es:xy@66812.com&quot;)                               // .put(&quot;transport.type&quot;,&quot;netty3&quot;)                               // .put(&quot;http.type&quot;,&quot;netty3&quot;)                                .put(&quot;client.transport.sniff&quot;,true).build();//开启自动嗅探功能                        esClusterClient = new PreBuiltTransportClient(settings)                                .addTransportAddress(new TransportAddress(InetAddress.getByName(clusterNodes1), 9300))                                .addTransportAddress(new TransportAddress(InetAddress.getByName(clusterNodes2), 9300))                                .addTransportAddress(new TransportAddress(InetAddress.getByName(clusterNodes3), 9300));                        LOG.info(&quot;esClusterClient========&quot; + esClusterClient.listedNodes());                    }catch (Exception e){                        LOG.error(&quot;获取客户端失败&quot;,e);                    }finally {                    }                }            }        }        return esClusterClient;    }    public static void main(String[] args) {        TransportClient client = ESClientUtils.getClient();        System.out.println(client);    }}</code></pre><h4 id="4、jest-service"><a href="#4、jest-service" class="headerlink" title="4、jest/service"></a>4、jest/service</h4><p><strong>IndexTypeUtil.java</strong></p><pre><code>package com.hsiehchou.es.jest.service;import com.hsiehchou.common.config.JsonReader;import io.searchbox.client.JestClient;public class IndexTypeUtil {    public static void main(String[] args) {        IndexTypeUtil.createIndexAndType(&quot;tanslator&quot;,&quot;es/mapping/tanslator.json&quot;);       // IndexTypeUtil.createIndexAndType(&quot;task&quot;);      //  IndexTypeUtil.createIndexAndType(&quot;ability&quot;);       // IndexTypeUtil.createIndexAndType(&quot;paper&quot;);    }    public static void createIndexAndType(String index,String jsonPath){        try{            JestClient jestClient = JestService.getJestClient();            JestService.createIndex(jestClient, index);            JestService.createIndexMapping(jestClient,index,index,getSourceFromJson(jsonPath));        }catch (Exception e){            e.printStackTrace();            //LOG.error(&quot;创建索引失败&quot;,e);        }    }    public static String getSourceFromJson(String path){        return JsonReader.readJson(path);    }    public static String getSource(String index){        if(index.equals(&quot;task&quot;)){            return &quot;{\&quot;_source\&quot;: {\n&quot; +                    &quot;    \&quot;enabled\&quot;: true\n&quot; +                    &quot;  },\n&quot; +                    &quot;  \&quot;properties\&quot;: {\n&quot; +                    &quot;    \&quot;taskwordcount\&quot;: {\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +                    &quot;    },\n&quot; +                    &quot;    \&quot;taskprice\&quot;: {\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;float\&quot;\n&quot; +                    &quot;    }\n&quot; +                    &quot;  }\n&quot; +                    &quot;}&quot;;        }        if(index.equals(&quot;tanslator&quot;)){            return &quot;{\n&quot; +                    &quot;  \&quot;_source\&quot;: {\n&quot; +                    &quot;    \&quot;enabled\&quot;: true\n&quot; +                    &quot;  },\n&quot; +                    &quot;  \&quot;properties\&quot;: {\n&quot; +                    &quot;    \&quot;birthday\&quot;: {\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;text\&quot;,\n&quot; +                    &quot;      \&quot;fields\&quot;: {\n&quot; +                    &quot;        \&quot;keyword\&quot;: {\n&quot; +                    &quot;          \&quot;ignore_above\&quot;: 256,\n&quot; +                    &quot;          \&quot;type\&quot;: \&quot;keyword\&quot;\n&quot; +                    &quot;        }\n&quot; +                    &quot;      }\n&quot; +                    &quot;    },\n&quot; +                    &quot;    \&quot;createtime\&quot;:{\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +                    &quot;    },\n&quot; +                    &quot;    \&quot;updatetime\&quot;:{\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +                    &quot;    },\n&quot; +                    &quot;    \&quot;avgcooperation\&quot;:{\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +                    &quot;    },\n&quot; +                    &quot;    \&quot;cooperationwordcount\&quot;:{\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +                    &quot;    },\n&quot; +                    &quot;    \&quot;cooperation\&quot;:{\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +                    &quot;    },\n&quot; +                    &quot;    \&quot;cooperationtime\&quot;:{\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +                    &quot;    },\n&quot; +                    &quot;    \&quot;age\&quot;:{\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +                    &quot;    },\n&quot; +                    &quot;    \&quot;industry\&quot;: {\n&quot; +                    &quot;      \&quot;type\&quot;: \&quot;nested\&quot;,\n&quot; +                    &quot;      \&quot;properties\&quot;: {\n&quot; +                    &quot;        \&quot;industryname\&quot;: {\n&quot; +                    &quot;          \&quot;type\&quot;: \&quot;text\&quot;,\n&quot; +                    &quot;          \&quot;fields\&quot;: {\n&quot; +                    &quot;            \&quot;keyword\&quot;: {\n&quot; +                    &quot;              \&quot;ignore_above\&quot;: 256,\n&quot; +                    &quot;              \&quot;type\&quot;: \&quot;keyword\&quot;\n&quot; +                    &quot;            }\n&quot; +                    &quot;          }\n&quot; +                    &quot;        },\n&quot; +                    &quot;        \&quot;count\&quot;: {\n&quot; +                    &quot;          \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +                    &quot;        },\n&quot; +                    &quot;        \&quot;industryid\&quot;: {\n&quot; +                    &quot;          \&quot;type\&quot;: \&quot;text\&quot;,\n&quot; +                    &quot;          \&quot;fields\&quot;: {\n&quot; +                    &quot;            \&quot;keyword\&quot;: {\n&quot; +                    &quot;              \&quot;ignore_above\&quot;: 256,\n&quot; +                    &quot;              \&quot;type\&quot;: \&quot;keyword\&quot;\n&quot; +                    &quot;            }\n&quot; +                    &quot;          }\n&quot; +                    &quot;        }\n&quot; +                    &quot;      }\n&quot; +                    &quot;    }\n&quot; +                    &quot;\n&quot; +                    &quot;  }\n&quot; +                    &quot;}&quot;;        }        return &quot;&quot;;    }}</code></pre><p><strong>JestService.java</strong></p><pre><code>package com.hsiehchou.es.jest.service;import com.hsiehchou.common.file.FileCommon;import com.google.gson.GsonBuilder;import io.searchbox.action.Action;import io.searchbox.client.JestClient;import io.searchbox.client.JestClientFactory;import io.searchbox.client.JestResult;import io.searchbox.client.config.HttpClientConfig;import io.searchbox.core.*;import io.searchbox.indices.CreateIndex;import io.searchbox.indices.DeleteIndex;import io.searchbox.indices.IndicesExists;import io.searchbox.indices.mapping.GetMapping;import io.searchbox.indices.mapping.PutMapping;import org.apache.commons.lang.StringUtils;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.search.aggregations.AggregationBuilder;import org.elasticsearch.search.aggregations.AggregationBuilders;import org.elasticsearch.search.builder.SearchSourceBuilder;import org.elasticsearch.search.sort.SortOrder;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.util.List;import java.util.Map;public class JestService {    private static Logger LOG = LoggerFactory.getLogger(JestService.class);    /**     * 获取JestClient对象     *     * @return     */    public static JestClient getJestClient() {        JestClientFactory factory = new JestClientFactory();        factory.setHttpClientConfig(new HttpClientConfig                .Builder(&quot;http://hadoop1:9200&quot;)                //.defaultCredentials(&quot;sc_xy_mn_es&quot;,&quot;xy@66812.com&quot;)                .gson(new GsonBuilder().setDateFormat(&quot;yyyy-MM-dd&#39;T&#39;hh:mm:ss&quot;).create())                .connTimeout(1500)                .readTimeout(3000)                .multiThreaded(true)                .build());        return factory.getObject();    }    public static void main(String[] args) throws Exception {        JestClient jestClient = null;//        Map&lt;String, Long&gt; stringLongMap = null;        List&lt;Map&lt;String, Object&gt;&gt; maps = null;        try {            jestClient = JestService.getJestClient();           /* SearchResult aggregation = JestService.aggregation(jestClient,                    &quot;wechat&quot;,                    &quot;wechat&quot;,                    &quot;collect_time&quot;);            stringLongMap = ResultParse.parseAggregation(aggregation);*/           /* SearchResult search = search(jestClient,                    &quot;wechat&quot;,                    &quot;wechat&quot;,                    &quot;id&quot;,                    &quot;65a3d548bd3e42b1972191bc2bd2829b&quot;,                    &quot;collect_time&quot;,                    &quot;desc&quot;,                    1,                    2);*/            /*SearchResult search = search(jestClient,                    &quot;&quot;,                    &quot;&quot;,                    &quot;phone_mac&quot;,                    &quot;aa-aa-aa-aa-aa-aa&quot;,                    &quot;collect_time&quot;,                    &quot;asc&quot;,                    1,                    1000);*///            System.out.println(indexExists(jestClient,&quot;wechat&quot;));            System.out.println(&quot;wechat数据量：&quot;+count(jestClient,&quot;wechat&quot;,&quot;wechat&quot;));            System.out.println(aggregation(jestClient,&quot;wechat&quot;,&quot;wechat&quot;, &quot;phone&quot;));            String[] includes = new String[]{&quot;latitude&quot;,&quot;longitude&quot;,&quot;collect_time&quot;};//            try{            SearchResult search = JestService.search(jestClient,                        &quot;&quot;,                        &quot;&quot;,                        &quot;phone_mac.keyword&quot;,                        &quot;aa-aa-aa-aa-aa-aa&quot;,                        &quot;collect_time&quot;,                        &quot;asc&quot;,                        1,                        2000);                maps = ResultParse.parseSearchResultOnly(search);                System.out.println(maps.size());                System.out.println(maps);            } catch (Exception e) {                e.printStackTrace();            } finally {                JestService.closeJestClient(jestClient);            }        System.out.println(maps);//        } catch (Exception e) {//            e.printStackTrace();//        }finally {//            JestService.closeJestClient(jestClient);//        }//        System.out.println(stringLongMap);    }    /**     * 统计一个索引所有数据     * @param jestClient     * @param indexName     * @param typeName     * @return     * @throws Exception     */    public static Long count(JestClient jestClient,                             String indexName,                             String typeName) throws Exception {        Count count = new Count.Builder()                .addIndex(indexName)                .addType(typeName)                .build();        CountResult results = jestClient.execute(count);        return results.getCount().longValue();    }    /**     * 聚合分组查询     * @param jestClient     * @param indexName     * @param typeName     * @param field     * @return     * @throws Exception     */    public static SearchResult  aggregation(JestClient jestClient, String indexName, String typeName, String field) throws Exception {        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();        //分组聚合API        AggregationBuilder group1 = AggregationBuilders.terms(&quot;group1&quot;).field(field);        //group1.subAggregation(AggregationBuilders.terms(&quot;group2&quot;).field(query));        searchSourceBuilder.aggregation(group1);        searchSourceBuilder.size(0);        System.out.println(searchSourceBuilder.toString());        Search search = new Search.Builder(searchSourceBuilder.toString())                .addIndex(indexName)                .addType(typeName).build();        SearchResult result = jestClient.execute(search);        return result;    }    //基础封装    public static SearchResult search(            JestClient jestClient,            String indexName,            String typeName,            String field,            String fieldValue,            String sortField,            String sortValue,            int pageNumber,            int pageSize,            String[] includes) {        //构造一个查询体  封装的就是查询语句        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();        searchSourceBuilder.fetchSource(includes,new String[0]);        //查询构造器        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();        if(StringUtils.isEmpty(field)){            boolQueryBuilder = boolQueryBuilder.must(QueryBuilders.matchAllQuery());        }else{            boolQueryBuilder = boolQueryBuilder.must(QueryBuilders.termQuery(field,fieldValue));        }        searchSourceBuilder.query(boolQueryBuilder);        //定义分页        //从什么时候开始        searchSourceBuilder.from((pageNumber-1)*pageSize);        searchSourceBuilder.size(pageSize);        //设置排序        if(&quot;desc&quot;.equals(sortValue)){            searchSourceBuilder.sort(sortField,SortOrder.DESC);        }else{            searchSourceBuilder.sort(sortField,SortOrder.ASC);        }        System.out.println(&quot;sql =====&quot; + searchSourceBuilder.toString());        //构造一个查询执行器        Search.Builder builder = new Search.Builder(searchSourceBuilder.toString());        //设置indexName typeName        if(StringUtils.isNotBlank(indexName)){            builder.addIndex(indexName);        }        if(StringUtils.isNotBlank(typeName)){            builder.addType(typeName);        }        Search build = builder.build();        SearchResult searchResult = null;        try {            searchResult = jestClient.execute(build);        } catch (IOException e) {            LOG.error(&quot;查询失败&quot;,e);        }        return searchResult;    }    //基础封装    public static SearchResult search(            JestClient jestClient,            String indexName,            String typeName,            String field,            String fieldValue,            String sortField,            String sortValue,            int pageNumber,            int pageSize) {        //构造一个查询体  封装的就是查询语句        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();        //查询构造器        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();        if(StringUtils.isEmpty(field)){            boolQueryBuilder = boolQueryBuilder.must(QueryBuilders.matchAllQuery());        }else{            boolQueryBuilder = boolQueryBuilder.must(QueryBuilders.termQuery(field,fieldValue));        }        searchSourceBuilder.query(boolQueryBuilder);        //定义分页        //从什么时候开始        searchSourceBuilder.from((pageNumber-1)*pageSize);        searchSourceBuilder.size(pageSize);        //设置排序        if(&quot;desc&quot;.equals(sortValue)){            searchSourceBuilder.sort(sortField,SortOrder.DESC);        }else{            searchSourceBuilder.sort(sortField,SortOrder.ASC);        }        System.out.println(&quot;sql =====&quot; + searchSourceBuilder.toString());        //构造一个查询执行器        Search.Builder builder = new Search.Builder(searchSourceBuilder.toString());        //设置indexName typeName        if(StringUtils.isNotBlank(indexName)){            builder.addIndex(indexName);        }        if(StringUtils.isNotBlank(typeName)){            builder.addType(typeName);        }        Search build = builder.build();        SearchResult searchResult = null;        try {            searchResult = jestClient.execute(build);        } catch (IOException e) {            LOG.error(&quot;查询失败&quot;,e);        }        return searchResult;    }   /* //基础封装    public static SearchResult search(            JestClient jestClient,            String indexName,            String typeName,            String field,            String fieldValue,            String sortField,            String sortValue,            int pageNumber,            int pageSize) {        //构造一个查询体  封装的就是查询语句        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();        //查询构造器        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();        if(StringUtils.isEmpty(field)){            boolQueryBuilder = boolQueryBuilder.must(QueryBuilders.matchAllQuery());        }else{            boolQueryBuilder = boolQueryBuilder.must(QueryBuilders.termQuery(field,fieldValue));        }        searchSourceBuilder.query(boolQueryBuilder);        //定义分页        //从什么时候开始        searchSourceBuilder.from((pageNumber-1)*pageSize);        searchSourceBuilder.size(pageSize);        //设置排序        if(&quot;desc&quot;.equals(sortValue)){            searchSourceBuilder.sort(sortField,SortOrder.DESC);        }else{            searchSourceBuilder.sort(sortField,SortOrder.ASC);        }        System.out.println(&quot;sql =====&quot; + searchSourceBuilder.toString());        //构造一个查询执行器        Search.Builder builder = new Search.Builder(searchSourceBuilder.toString());        //设置indexName typeName        if(StringUtils.isNotBlank(indexName)){            builder.addIndex(indexName);        }        if(StringUtils.isNotBlank(typeName)){            builder.addType(typeName);        }        Search build = builder.build();        SearchResult searchResult = null;        try {            searchResult = jestClient.execute(build);        } catch (IOException e) {            LOG.error(&quot;查询失败&quot;,e);        }        return searchResult;    }*/    /**     * 判断索引是否存在     *     * @param jestClient     * @param indexName     * @return     * @throws Exception     */    public static boolean indexExists(JestClient jestClient, String indexName) {        JestResult result = null;        try {            Action action = new IndicesExists.Builder(indexName).build();            result = jestClient.execute(action);        } catch (IOException e) {            LOG.error(null, e);        }        return result.isSucceeded();    }    /**     * 创建索引     *     * @param jestClient     * @param indexName     * @return     * @throws Exception     */    public static boolean createIndex(JestClient jestClient, String indexName) throws Exception {        if (!JestService.indexExists(jestClient, indexName)) {            JestResult jr = jestClient.execute(new CreateIndex.Builder(indexName).build());            return jr.isSucceeded();        } else {            LOG.info(&quot;该索引已经存在&quot;);            return false;        }    }    public static boolean createIndexWithSettingsMapAndMappingsString(JestClient jestClient, String indexName, String type, String path) throws Exception {        // String mappingJson = &quot;{\&quot;type1\&quot;: {\&quot;_source\&quot;:{\&quot;enabled\&quot;:false},\&quot;properties\&quot;:{\&quot;field1\&quot;:{\&quot;type\&quot;:\&quot;keyword\&quot;}}}}&quot;;        String mappingJson = FileCommon.getAbstractPath(path);        String realMappingJson = &quot;{&quot; + type + &quot;:&quot; + mappingJson + &quot;}&quot;;        System.out.println(realMappingJson);        CreateIndex createIndex = new CreateIndex.Builder(indexName)                .mappings(realMappingJson)                .build();        JestResult jr = jestClient.execute(createIndex);        return jr.isSucceeded();    }    /**     * Put映射     *     * @param jestClient     * @param indexName     * @param typeName     * @param source     * @return     * @throws Exception     */    public static boolean createIndexMapping(JestClient jestClient, String indexName, String typeName, String source) throws Exception {        PutMapping putMapping = new PutMapping.Builder(indexName, typeName, source).build();        JestResult jr = jestClient.execute(putMapping);        return jr.isSucceeded();    }    /**     * Get映射     *     * @param jestClient     * @param indexName     * @param typeName     * @return     * @throws Exception     */    public static String getIndexMapping(JestClient jestClient, String indexName, String typeName) throws Exception {        GetMapping getMapping = new GetMapping.Builder().addIndex(indexName).addType(typeName).build();        JestResult jr = jestClient.execute(getMapping);        return jr.getJsonString();    }    /**     * 索引文档     *     * @param jestClient     * @param indexName     * @param typeName     * @return     * @throws Exception     */    public static boolean index(JestClient jestClient, String indexName, String typeName, String idField, List&lt;Map&lt;String, Object&gt;&gt; listMaps) throws Exception {        Bulk.Builder bulk = new Bulk.Builder().defaultIndex(indexName).defaultType(typeName);        for (Map&lt;String, Object&gt; map : listMaps) {            if (map != null &amp;&amp; map.containsKey(idField)) {                Object o = map.get(idField);                Index index = new Index.Builder(map).id(map.get(idField).toString()).build();                bulk.addAction(index);            }        }        BulkResult br = jestClient.execute(bulk.build());        return br.isSucceeded();    }    /**     * 索引文档     *     * @param jestClient     * @param indexName     * @param typeName     * @return     * @throws Exception     */    public static boolean indexString(JestClient jestClient, String indexName, String typeName, String idField, List&lt;Map&lt;String, String&gt;&gt; listMaps) throws Exception {        if (listMaps != null &amp;&amp; listMaps.size() &gt; 0) {            Bulk.Builder bulk = new Bulk.Builder().defaultIndex(indexName).defaultType(typeName);            for (Map&lt;String, String&gt; map : listMaps) {                if (map != null &amp;&amp; map.containsKey(idField)) {                    Index index = new Index.Builder(map).id(map.get(idField)).build();                    bulk.addAction(index);                }            }            BulkResult br = jestClient.execute(bulk.build());            return br.isSucceeded();        } else {            return false;        }    }    /**     * 索引文档     *     * @param jestClient     * @param indexName     * @param typeName     * @return     * @throws Exception     */    public static boolean indexOne(JestClient jestClient, String indexName, String typeName, String id, Map&lt;String, Object&gt; map) {        Index.Builder builder = new Index.Builder(map);        builder.id(id);        builder.refresh(true);        Index index = builder.index(indexName).type(typeName).build();        try {            JestResult result = jestClient.execute(index);            if (result != null &amp;&amp; !result.isSucceeded()) {                throw new RuntimeException(result.getErrorMessage() + &quot;插入更新索引失败!&quot;);            }        } catch (Exception e) {            e.printStackTrace();            return false;        }        return true;    }    /**     * 搜索文档     *     * @param jestClient     * @param indexName     * @param typeName     * @param query     * @return     * @throws Exception     */    public static SearchResult search(JestClient jestClient, String indexName, String typeName, String query) throws Exception {        Search search = new Search.Builder(query)                .addIndex(indexName)                .addType(typeName)                .build();        return jestClient.execute(search);    }    /**     * Get文档     *     * @param jestClient     * @param indexName     * @param typeName     * @param id     * @return     * @throws Exception     */    public static JestResult get(JestClient jestClient, String indexName, String typeName, String id) throws Exception {        Get get = new Get.Builder(indexName, id).type(typeName).build();        return jestClient.execute(get);    }    /**     * Delete索引     *     * @param jestClient     * @param indexName     * @return     * @throws Exception     */    public boolean delete(JestClient jestClient, String indexName) throws Exception {        JestResult jr = jestClient.execute(new DeleteIndex.Builder(indexName).build());        return jr.isSucceeded();    }    /**     * Delete文档     *     * @param jestClient     * @param indexName     * @param typeName     * @param id     * @return     * @throws Exception     */    public static boolean delete(JestClient jestClient, String indexName, String typeName, String id) throws Exception {        DocumentResult dr = jestClient.execute(new Delete.Builder(id).index(indexName).type(typeName).build());        return dr.isSucceeded();    }    /**     * 关闭JestClient客户端     *     * @param jestClient     * @throws Exception     */    public static void closeJestClient(JestClient jestClient) {        if (jestClient != null) {            jestClient.shutdownClient();        }    }    public static String query = &quot;{\n&quot; +            &quot;  \&quot;size\&quot;: 1,\n&quot; +            &quot;  \&quot;query\&quot;: {\n&quot; +            &quot;     \&quot;match\&quot;: {\n&quot; +            &quot;       \&quot;taskexcuteid\&quot;: \&quot;89899143\&quot;\n&quot; +            &quot;     }\n&quot; +            &quot;  },\n&quot; +            &quot;  \&quot;aggs\&quot;: {\n&quot; +            &quot;    \&quot;count\&quot;: {\n&quot; +            &quot;      \&quot;terms\&quot;: {\n&quot; +            &quot;        \&quot;field\&quot;: \&quot;source.keyword\&quot;\n&quot; +            &quot;      },\n&quot; +            &quot;      \&quot;aggs\&quot;: {\n&quot; +            &quot;        \&quot;sum_price\&quot;: {\n&quot; +            &quot;          \&quot;sum\&quot;: {\n&quot; +            &quot;            \&quot;field\&quot;: \&quot;taskprice\&quot;\n&quot; +            &quot;          }\n&quot; +            &quot;        },\n&quot; +            &quot;        \&quot;sum_wordcount\&quot;: {\n&quot; +            &quot;          \&quot;sum\&quot;: {\n&quot; +            &quot;            \&quot;field\&quot;: \&quot;taskwordcount\&quot;\n&quot; +            &quot;          }\n&quot; +            &quot;        },\n&quot; +            &quot;        \&quot;avg_taskprice\&quot;: {\n&quot; +            &quot;          \&quot;avg\&quot;: {\n&quot; +            &quot;            \&quot;field\&quot;: \&quot;taskprice\&quot;\n&quot; +            &quot;          }\n&quot; +            &quot;        }\n&quot; +            &quot;      }\n&quot; +            &quot;    }\n&quot; +            &quot;  }\n&quot; +            &quot;}&quot;;}</code></pre><p><strong>ResultParse.java</strong></p><pre><code>package com.hsiehchou.es.jest.service;import com.google.gson.Gson;import com.google.gson.JsonElement;import com.google.gson.JsonObject;import com.google.gson.JsonPrimitive;import io.searchbox.client.JestClient;import io.searchbox.client.JestResult;import io.searchbox.core.SearchResult;import io.searchbox.core.search.aggregation.MetricAggregation;import io.searchbox.core.search.aggregation.TermsAggregation;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.*;public class ResultParse {    private static Logger LOG = LoggerFactory.getLogger(ResultParse.class);    public static void main(String[] args) throws Exception {        JestClient jestClient = JestService.getJestClient();        /*long l = System.currentTimeMillis();        JestClient jestClient = JestClientUtil.getJestClient();        System.out.println(jestClient);        String json =&quot;{\n&quot; +                &quot;  \&quot;size\&quot;: 1, \n&quot; +                &quot;  \&quot;query\&quot;: {\n&quot; +                &quot;    \&quot;query_string\&quot;: {\n&quot; +                &quot;      \&quot;query\&quot;: \&quot;中文\&quot;\n&quot; +                &quot;    }\n&quot; +                &quot;  },\n&quot; +                &quot;  \&quot;highlight\&quot;: {\n&quot; +                &quot;    \&quot;pre_tags\&quot; : [ \&quot;&lt;red&gt;\&quot; ],\n&quot; +                &quot;    \&quot;post_tags\&quot; : [ \&quot;&lt;/red&gt;\&quot; ],\n&quot; +                &quot;    \&quot;fields\&quot;:{\n&quot; +                &quot;      \&quot;secondlanguage\&quot;: {}\n&quot; +                &quot;      ,\&quot;firstlanguage\&quot;: {}\n&quot; +                &quot;    }\n&quot; +                &quot;  }\n&quot; +                &quot;}&quot;;        SearchResult search = JestService.search(jestClient, ES_INDEX.TANSLATOR_TEST, ES_INDEX.TANSLATOR_TEST,json);        ResultParse.parseSearchResult(search);        jestClient.shutdownClient();        long l1 = System.currentTimeMillis();        System.out.println(l1-l);*/    }    public static Map&lt;String,Object&gt; parseGet(JestResult getResult){        Map&lt;String,Object&gt; map = null;        JsonObject jsonObject = getResult.getJsonObject().getAsJsonObject(&quot;_source&quot;);        if(jsonObject != null){            map = new HashMap&lt;String,Object&gt;();            //System.out.println(jsonObject);            Set&lt;Map.Entry&lt;String, JsonElement&gt;&gt; entries = jsonObject.entrySet();            for(Map.Entry&lt;String, JsonElement&gt; entry:entries){                JsonElement value = entry.getValue();                if(value.isJsonPrimitive()){                    JsonPrimitive value1 = (JsonPrimitive) value;                  //  LOG.error(&quot;转换前==========&quot; + value1);                    if( value1.isString() ){                       // LOG.error(&quot;转换后==========&quot; + value1.getAsString());                        map.put(entry.getKey(),value1.getAsString());                    }else{                        map.put(entry.getKey(),value1);                    }                }else{                    map.put(entry.getKey(),value);                }             }        }        return map;    }    public static Map&lt;String,Object&gt; parseGet2map(JestResult getResult){        JsonObject source = getResult.getJsonObject().getAsJsonObject(&quot;_source&quot;);        Gson gson = new Gson();        Map map = gson.fromJson(source, Map.class);        return map;    }    /**     * 解析listMap     * 结果格式为  {hits=0, total=0, data=[]}     * @param search     * @return     */    public static List&lt;Map&lt;String,Object&gt;&gt; parseSearchResultOnly(SearchResult search){        List&lt;Map&lt;String,Object&gt;&gt; list = new ArrayList&lt;Map&lt;String,Object&gt;&gt;();        List&lt;SearchResult.Hit&lt;Object, Void&gt;&gt; hits = search.getHits(Object.class);        for(SearchResult.Hit&lt;Object, Void&gt; hit : hits){            Map&lt;String,Object&gt; source = (Map&lt;String,Object&gt;)hit.source;            list.add(source);        }        return list;    }    /**     * 解析listMap     * 结果格式为  {hits=0, total=0, data=[]}     * @param search     * @return     */    public static Map&lt;String,Long&gt; parseAggregation(SearchResult search){        Map&lt;String,Long&gt; mapResult = new HashMap&lt;&gt;();        MetricAggregation aggregations = search.getAggregations();        TermsAggregation group1 = aggregations.getTermsAggregation(&quot;group1&quot;);        List&lt;TermsAggregation.Entry&gt; buckets = group1.getBuckets();        buckets.forEach(x-&gt;{            String key = x.getKey();            Long count = x.getCount();            mapResult.put(key,count);        });        return mapResult;    }    /**     * 解析listMap     * 结果格式为  {hits=0, total=0, data=[]}     * @param search     * @return     */    public static Map&lt;String,Object&gt; parseSearchResult(SearchResult search){        Map&lt;String,Object&gt; map = new HashMap&lt;String,Object&gt;();        List&lt;Map&lt;String,Object&gt;&gt; list = new ArrayList&lt;Map&lt;String,Object&gt;&gt;();        Long total = search.getTotal();        map.put(&quot;total&quot;,total);        List&lt;SearchResult.Hit&lt;Object, Void&gt;&gt; hits = search.getHits(Object.class);        map.put(&quot;hits&quot;,hits.size());        for(SearchResult.Hit&lt;Object, Void&gt; hit : hits){            Map&lt;String, List&lt;String&gt;&gt; highlight = hit.highlight;            Map&lt;String,Object&gt; source = (Map&lt;String,Object&gt;)hit.source;            source.put(&quot;highlight&quot;,highlight);            list.add(source);        }        map.put(&quot;data&quot;,list);        return map;    }}</code></pre><h4 id="5、search"><a href="#5、search" class="headerlink" title="5、search"></a>5、search</h4><p><strong>BuilderUtil.java</strong></p><pre><code>package com.hsiehchou.es.search;import org.apache.commons.lang.StringUtils;import org.elasticsearch.action.search.SearchRequestBuilder;import org.elasticsearch.client.transport.TransportClient;import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class BuilderUtil {    private static Logger LOG = LoggerFactory.getLogger(BuilderUtil.class);    public static SearchRequestBuilder getSearchBuilder(TransportClient client, String index, String type){        SearchRequestBuilder builder = null;        try {            if (StringUtils.isNotBlank(index)) {                builder = client.prepareSearch(index.split(&quot;,&quot;));            } else {                builder = client.prepareSearch();            }            if (StringUtils.isNotBlank(type)) {                builder.setTypes(type.split(&quot;,&quot;));            }        } catch (Exception e) {            LOG.error(null, e);        }        return builder;    }    public static SearchRequestBuilder getSearchBuilder(TransportClient client, String[] indexs, String type){        SearchRequestBuilder builder = null;        try {            if (indexs.length&gt;0) {                for(String index:indexs){                    builder = client.prepareSearch(index);                }            } else {                builder = client.prepareSearch();            }            if (StringUtils.isNotBlank(type)) {                builder.setTypes(type);            }        } catch (Exception e) {            LOG.error(null, e);        }        return builder;    }}</code></pre><p><strong>QueryUtil.java</strong></p><pre><code>package com.hsiehchou.es.search;import com.hsiehchou.es.utils.UnicodeUtil;import org.apache.lucene.queryparser.classic.QueryParser;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.index.query.QueryStringQueryBuilder;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.Map;public class QueryUtil {    private static Logger LOG = LoggerFactory.getLogger(QueryUtil.class);    /**     * EQ   等於     * NEQ  不等於     * GE   大于等于     * GT   大于     * LE   小于等于     * LT   小于     * RANGE 区间范围     */    public static enum OPREATOR {EQ, NEQ,WILDCARD, GE, LE, GT, LT, FUZZY, RANGE, IN, PREFIX}    /**     * @param paramMap     * @return     */    public static BoolQueryBuilder getSearchParam(Map&lt;OPREATOR, Map&lt;String, Object&gt;&gt; paramMap) {        BoolQueryBuilder qb = QueryBuilders.boolQuery();        if (null != paramMap &amp;&amp; !paramMap.isEmpty()) {            for (Map.Entry&lt;OPREATOR, Map&lt;String, Object&gt;&gt; paramEntry : paramMap.entrySet()) {                OPREATOR key = paramEntry.getKey();                Map&lt;String, Object&gt; fieldMap = paramEntry.getValue();                for (Map.Entry&lt;String, Object&gt; fieldEntry : fieldMap.entrySet()) {                    String field = fieldEntry.getKey();                    Object value = fieldEntry.getValue();                    switch (key) {                        case EQ:/**等於查詢 equale**/                            qb.must(QueryBuilders.matchPhraseQuery(field, value).slop(0));                            break;                        case NEQ:/**不等於查詢 not equale**/                            qb.mustNot(QueryBuilders.matchQuery(field, value));                            break;                        case GE: /**大于等于查詢  great than or equal to**/                            qb.must(QueryBuilders.rangeQuery(field).gte(value));                            break;                        case LE: /**小于等于查詢 less than or equal to**/                            qb.must(QueryBuilders.rangeQuery(field).lte(value));                            break;                        case GT: /**大于查詢**/                            qb.must(QueryBuilders.rangeQuery(field).gt(value));                            break;                        case LT: /**小于查詢**/                            qb.must(QueryBuilders.rangeQuery(field).lt(value));                            break;                        case FUZZY:                            String text = String.valueOf(value);                            if (!UnicodeUtil.hasChinese(text)) {                                text = &quot;*&quot; + text + &quot;*&quot;;                            }                            text = QueryParser.escape(text);                            qb.must(new QueryStringQueryBuilder(text).field(field));                            break;                        case RANGE: /**区间查詢**/                            String[] split = value.toString().split(&quot;,&quot;);                            if(split.length==2){                                qb.must(QueryBuilders.rangeQuery(field).from(Long.valueOf(split[0]))                                        .to(Long.valueOf(split[1])));                            }                             /*  if (value instanceof Map) {                                Map&lt;String, Object&gt; rangMap = (Map&lt;String, Object&gt;) value;                                qb.must(QueryBuilders.rangeQuery(field).from(rangMap.get(&quot;ge&quot;))                                        .to(rangMap.get(&quot;le&quot;)));                            }*/                            break;                        case PREFIX: /**前缀查詢**/                            qb.must(QueryBuilders.prefixQuery(field, String.valueOf(value)));                            break;                        case IN:                            qb.must(QueryBuilders.termsQuery(field, (Object[]) value));                            break;                        default:                            qb.must(QueryBuilders.matchQuery(field, value));                            break;                    }                }            }        }        return qb;    }}</code></pre><p><strong>ResponseParse.java</strong></p><pre><code>package com.hsiehchou.es.search;import org.elasticsearch.action.get.GetResponse;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.Map;public class ResponseParse {    private static Logger LOG = LoggerFactory.getLogger(BuilderUtil.class);    public static Map&lt;String, Object&gt; parseGetResponse(GetResponse getResponse){        Map&lt;String, Object&gt; source = null;        try {            source = getResponse.getSource();        } catch (Exception e) {            LOG.error(null,e);        }        return source;    }}</code></pre><p><strong>SearchUtil.java</strong></p><pre><code>package com.hsiehchou.es.search;import com.hsiehchou.es.client.ESClientUtils;import org.elasticsearch.action.get.GetRequestBuilder;import org.elasticsearch.action.get.GetResponse;import org.elasticsearch.action.search.SearchRequestBuilder;import org.elasticsearch.action.search.SearchResponse;import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.MatchQueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.search.SearchHit;import org.elasticsearch.search.SearchHits;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.ArrayList;import java.util.List;import java.util.Map;public class SearchUtil {    private static Logger LOG = LoggerFactory.getLogger(SearchUtil.class);    private static TransportClient client = ESClientUtils.getClient();    public static void main(String[] args) {        TransportClient client = ESClientUtils.getClient();        List&lt;Map&lt;String, Object&gt;&gt; maps = searchSingleData(client, &quot;wechat&quot;, &quot;wechat&quot;, &quot;phone_mac&quot;, &quot;aa-aa-aa-aa-aa-aa&quot;);        System.out.println(maps);        /* long l = System.currentTimeMillis();        searchSingleData(&quot;tanslator&quot;, &quot;tanslator&quot;,&quot;4e1117d7-c434-48a7-9134-45f7c90f94ee_TR1100397895_2&quot;);        System.out.println(&quot;消耗时间&quot; + (System.currentTimeMillis() - l));        long lll = System.currentTimeMillis();        searchSingleData(&quot;tanslator&quot;, &quot;tanslator&quot;,&quot;4e1117d7-c434-48a7-9134-45f7c90f94ee_TR1100397895_2&quot;);        System.out.println(&quot;消耗时间&quot; + (System.currentTimeMillis() - lll));        long ll = System.currentTimeMillis();        List&lt;Map&lt;String, Object&gt;&gt; maps = searchSingleData(client,&quot;tanslator&quot;, &quot;tanslator&quot;, &quot;iolid&quot;, &quot;TR1100397895&quot;);        System.out.println(&quot;消耗时间&quot; + (System.currentTimeMillis() - ll));        System.out.println(maps);*/    }    /**     * 查询单条数据     * @param index  索引     * @param type   表名     * @param id     字段     * @return     */    public static GetResponse searchSingleData(String index, String type, String id) {        GetResponse response = null;        try {            GetRequestBuilder builder = null;            builder = client.prepareGet(index, type, id);            response = builder.execute().actionGet();        } catch (Exception e) {            LOG.error(null, e);        }        return response;    }    /**     * @param index     * @param type     * @param field     * @param value     * @return     */    public static List&lt;Map&lt;String, Object&gt;&gt; searchSingleData(TransportClient client,String index, String type,String field, String value) {        List&lt;Map&lt;String, Object&gt;&gt; result = new ArrayList&lt;&gt;();        try {            SearchRequestBuilder builder = BuilderUtil.getSearchBuilder(client,index,type);            MatchQueryBuilder matchQueryBuilder = QueryBuilders.matchQuery(field, value);            builder.setQuery(matchQueryBuilder).setExplain(false);            SearchResponse searchResponse = builder.execute().actionGet();            SearchHits hits = searchResponse.getHits();            SearchHit[] searchHists = hits.getHits();            for (SearchHit sh : searchHists) {                result.add(sh.getSourceAsMap());            }        } catch (Exception e) {            e.printStackTrace();            LOG.error(null, e);        }        return result;    }    /**     * 多条件查詢     * @param index     * @param type     * @param paramMap 组合查询条件     * @return     */    public static SearchResponse searchListData(String index, String type,                                                Map&lt;QueryUtil.OPREATOR,Map&lt;String,Object&gt;&gt; paramMap) {        SearchRequestBuilder builder = BuilderUtil.getSearchBuilder(client,index,type);        builder.setQuery(QueryUtil.getSearchParam(paramMap)).setExplain(false);        SearchResponse searchResponse = builder.get();        return searchResponse;    }    /**     * 多条件查詢     * @param index     * @param type     * @param paramMap 组合查询条件     * @return     */    public static SearchResponse searchListData1(String index, String type, Map&lt;String,String&gt; paramMap) {        BoolQueryBuilder qb = QueryBuilders.boolQuery();        qb.must(QueryBuilders.matchQuery(&quot;&quot;, &quot;&quot;));        BoolQueryBuilder qb1 = QueryBuilders.boolQuery();        qb1.should(QueryBuilders.matchQuery(&quot;&quot;,&quot;&quot;));        qb1.should(QueryBuilders.matchQuery(&quot;&quot;,&quot;&quot;));        qb.must(qb1);        return null;    }}</code></pre><h4 id="6、utils"><a href="#6、utils" class="headerlink" title="6、utils"></a>6、utils</h4><p><strong>ESresultUtil.java</strong></p><pre><code>package com.hsiehchou.es.utils;import org.apache.commons.lang.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.Map;public class ESresultUtil {    private static Logger LOG = LoggerFactory.getLogger(ESresultUtil.class);    public static Long getLong(Map&lt;String,Object&gt; esMAp,String field){        Long valueLong = 0L;        if(esMAp!=null &amp;&amp; esMAp.size()&gt;0){            if(esMAp.containsKey(field)){                 Object value = esMAp.get(field);                 if(value!=null &amp;&amp; StringUtils.isNotBlank(value.toString())){                     valueLong = Long.valueOf(value.toString());                 }            }        }        return valueLong;    }}</code></pre><p><strong>UnicodeUtil.java</strong></p><pre><code>package com.hsiehchou.es.utils;import java.util.regex.Pattern;public class UnicodeUtil {    // 根据Unicode编码完美的判断中文汉字和符号    private static boolean isChinese(char c) {        Character.UnicodeBlock ub = Character.UnicodeBlock.of(c);        if (ub == Character.UnicodeBlock.CJK_UNIFIED_IDEOGRAPHS || ub == Character.UnicodeBlock.CJK_COMPATIBILITY_IDEOGRAPHS                || ub == Character.UnicodeBlock.CJK_UNIFIED_IDEOGRAPHS_EXTENSION_A || ub == Character.UnicodeBlock.CJK_UNIFIED_IDEOGRAPHS_EXTENSION_B                || ub == Character.UnicodeBlock.CJK_SYMBOLS_AND_PUNCTUATION || ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS                || ub == Character.UnicodeBlock.GENERAL_PUNCTUATION) {            return true;        }        return false;    }    // 完整的判断中文汉字和符号    public static boolean isChinese(String strName) {        char[] ch = strName.toCharArray();        for (int i = 0; i &lt; ch.length; i++) {            char c = ch[i];            if (isChinese(c)) {                return true;            }        }        return false;    }    // 完整的判断中文汉字和符号    public static boolean hasChinese(String strName) {        char[] ch = strName.toCharArray();        for (int i = 0; i &lt; ch.length; i++) {            char c = ch[i];            if (isChinese(c)) {                return true;            }        }        return false;    }    // 只能判断部分CJK字符（CJK统一汉字）    public static boolean isChineseByREG(String str) {        if (str == null) {            return false;        }        Pattern pattern = Pattern.compile(&quot;[\\u4E00-\\u9FBF]+&quot;);        return pattern.matcher(str.trim()).find();    }    // 只能判断部分CJK字符（CJK统一汉字）    /*    public static boolean isChineseByName(String str) {        if (str == null) {            return false;        }        // 大小写不同：\\p 表示包含，\\P 表示不包含        // \\p{Cn} 的意思为 Unicode 中未被定义字符的编码，\\P{Cn} 就表示 Unicode中已经被定义字符的编码        String reg = &quot;\\p{InCJK Unified Ideographs}&amp;&amp;\\P{Cn}&quot;;        Pattern pattern = Pattern.compile(reg);        return pattern.matcher(str.trim()).find();    }*/    public static void main(String[] args) {        System.out.println(hasChinese(&quot;aa表aa&quot;));    }}</code></pre><h4 id="7、V2"><a href="#7、V2" class="headerlink" title="7、V2"></a>7、V2</h4><p><strong>ElasticSearchService.java</strong></p><pre><code>package com.hsiehchou.es.V2;import com.hsiehchou.es.client.ESClientUtils;import org.apache.commons.collections.map.HashedMap;import org.apache.commons.lang.StringUtils;import org.elasticsearch.action.admin.indices.create.CreateIndexRequest;import org.elasticsearch.action.admin.indices.delete.DeleteIndexResponse;import org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsRequest;import org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsResponse;import org.elasticsearch.action.bulk.BulkRequestBuilder;import org.elasticsearch.action.search.SearchRequestBuilder;import org.elasticsearch.action.search.SearchResponse;import org.elasticsearch.action.update.UpdateRequest;import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.common.text.Text;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.search.SearchHit;import org.elasticsearch.search.SearchHits;import org.elasticsearch.search.aggregations.AggregationBuilder;import org.elasticsearch.search.aggregations.AggregationBuilders;import org.elasticsearch.search.aggregations.bucket.terms.Terms;import org.elasticsearch.search.fetch.subphase.highlight.HighlightBuilder;import org.elasticsearch.search.fetch.subphase.highlight.HighlightField;import org.elasticsearch.search.sort.SortBuilder;import org.elasticsearch.search.sort.SortOrder;import java.util.*;/** *  ES检索封装 */public class ElasticSearchService {    private final static int MAX = 10000;    private static TransportClient client = ESClientUtils.getClient();    /**     * 功能描述：新建索引     * @param indexName 索引名     */    public void createIndex(String indexName) {        client.admin().indices().create(new CreateIndexRequest(indexName))                .actionGet();    }    /**     * 功能描述：新建索引     * @param index 索引名     * @param type 类型     */    public void createIndex(String index, String type) {        client.prepareIndex(index, type).setSource().get();    }    /**     * 功能描述：删除索引     * @param index 索引名     */    public void deleteIndex(String index) {        if (indexExist(index)) {            DeleteIndexResponse dResponse = client.admin().indices().prepareDelete(index)                    .execute().actionGet();            if (!dResponse.isAcknowledged()) {            }        } else {        }    }    /**     * 功能描述：验证索引是否存在     * @param index 索引名     */    public boolean indexExist(String index) {        IndicesExistsRequest inExistsRequest = new IndicesExistsRequest(index);        IndicesExistsResponse inExistsResponse = client.admin().indices()                .exists(inExistsRequest).actionGet();        return inExistsResponse.isExists();    }    /**     * 功能描述：插入数据     * @param index 索引名     * @param type 类型     * @param json 数据     */    public void insertData(String index, String type, String json) {       client.prepareIndex(index, type)                .setSource(json)                .get();    }    /**     * 功能描述：插入数据     * @param index 索引名     * @param type 类型     * @param _id 数据id     * @param json 数据     */    public void insertData(String index, String type, String _id, String json) {        client.prepareIndex(index, type).setId(_id)                .setSource(json)                .get();    }    /**     * 功能描述：更新数据     * @param index 索引名     * @param type 类型     * @param _id 数据id     * @param json 数据     */    public void updateData(String index, String type, String _id, String json) throws Exception {        try {            UpdateRequest updateRequest = new UpdateRequest(index, type, _id)                    .doc(json);            client.update(updateRequest).get();        } catch (Exception e) {            //throw new MessageException(&quot;update data failed.&quot;, e);        }    }    /**     * 功能描述：删除数据     * @param index 索引名     * @param type 类型     * @param _id 数据id     */    public void deleteData(String index, String type, String _id) {        client.prepareDelete(index, type, _id)                .get();    }    /**     * 功能描述：批量插入数据     * @param index 索引名     * @param type 类型     * @param data (_id 主键, json 数据)     */    public void bulkInsertData(String index, String type, Map&lt;String, String&gt; data) {        BulkRequestBuilder bulkRequest = client.prepareBulk();        data.forEach((param1, param2) -&gt; {            bulkRequest.add(client.prepareIndex(index, type, param1)                    .setSource(param2)            );        });        bulkRequest.get();    }    /**     * 功能描述：批量插入数据     * @param index 索引名     * @param type 类型     * @param jsonList 批量数据     */    public void bulkInsertData(String index, String type, List&lt;String&gt; jsonList) {        BulkRequestBuilder bulkRequest = client.prepareBulk();        jsonList.forEach(item -&gt; {            bulkRequest.add(client.prepareIndex(index, type)                    .setSource(item)            );        });        bulkRequest.get();    }    /**     * 功能描述：查询     * @param index 索引名     * @param type 类型     * @param constructor 查询构造     */    public List&lt;Map&lt;String, Object&gt;&gt; search(String index, String type, ESQueryBuilderConstructor constructor) {        List&lt;Map&lt;String, Object&gt;&gt; list = new ArrayList&lt;&gt;();        SearchRequestBuilder searchRequestBuilder = client.prepareSearch(index).setTypes(type);        //排序        if (StringUtils.isNotEmpty(constructor.getAsc()))            searchRequestBuilder.addSort(constructor.getAsc(), SortOrder.ASC);        if (StringUtils.isNotEmpty(constructor.getDesc()))            searchRequestBuilder.addSort(constructor.getDesc(), SortOrder.DESC);        //设置查询体        searchRequestBuilder.setQuery(constructor.listBuilders());        //返回条目数        int size = constructor.getSize();        if (size &lt; 0) {            size = 0;        }        if (size &gt; MAX) {            size = MAX;        }        //返回条目数        searchRequestBuilder.setSize(size);        searchRequestBuilder.setFrom(constructor.getFrom() &lt; 0 ? 0 : constructor.getFrom());        SearchResponse searchResponse = searchRequestBuilder.execute().actionGet();        SearchHits hits = searchResponse.getHits();        SearchHit[] searchHists = hits.getHits();        for (SearchHit sh : searchHists) {            list.add(sh.getSourceAsMap());        }        return list;    }    /**     * 功能描述：查询     * @param index 索引名     * @param type 类型     * @param constructor 查询构造     */    public Map&lt;String,Object&gt; searchCountAndMessage(String index, String type, ESQueryBuilderConstructor constructor) {        Map&lt;String,Object&gt; map = new HashMap&lt;String,Object&gt;();        List&lt;Map&lt;String, Object&gt;&gt; list = new ArrayList&lt;&gt;();        SearchRequestBuilder searchRequestBuilder = client.prepareSearch(index).setTypes(type);        //排序        if (StringUtils.isNotEmpty(constructor.getAsc()))            searchRequestBuilder.addSort(constructor.getAsc(), SortOrder.ASC);        if (StringUtils.isNotEmpty(constructor.getDesc()))            searchRequestBuilder.addSort(constructor.getDesc(), SortOrder.DESC);        //设置查询体        searchRequestBuilder.setQuery(constructor.listBuilders());        //返回条目数        int size = constructor.getSize();        if (size &lt; 0) {            size = 0;        }        if (size &gt; MAX) {            size = MAX;        }        //返回条目数        searchRequestBuilder.setSize(size);        searchRequestBuilder.setFrom(constructor.getFrom() &lt; 0 ? 0 : constructor.getFrom());        SearchResponse searchResponse = searchRequestBuilder.execute().actionGet();        long totalHits = searchResponse.getHits().getTotalHits();        SearchHits hits = searchResponse.getHits();        SearchHit[] searchHists = hits.getHits();        for (SearchHit sh : searchHists) {            list.add(sh.getSourceAsMap());        }        map.put(&quot;total&quot;,(long)searchHists.length);        map.put(&quot;count&quot;,totalHits);        map.put(&quot;data&quot;,list);        return map;    }    /**     * 功能描述：查询     * @param index 索引名     * @param type 类型     * @param constructor 查询构造     */    public Map&lt;String,Object&gt; searchCountAndMessageNew(String index, String type, ESQueryBuilderConstructorNew constructor) {        Map&lt;String,Object&gt; map = new HashMap&lt;String,Object&gt;();        List&lt;Map&lt;String, Object&gt;&gt; list = new ArrayList&lt;&gt;();        SearchRequestBuilder searchRequestBuilder = client.prepareSearch(index).setTypes(type);        //排序        List&lt;SortBuilder&gt; sortBuilderList = constructor.getSortBuilderList();        if(sortBuilderList!=null &amp;&amp; sortBuilderList.size()&gt;0){            sortBuilderList.forEach(sortBuilder-&gt;{                searchRequestBuilder.addSort(sortBuilder);            });        }        //设置查询体        searchRequestBuilder.setQuery(constructor.listBuilders());        //返回条目数        int size = constructor.getSize();        if (size &lt; 0) {            size = 0;        }        if (size &gt; MAX) {            size = MAX;        }        //返回条目数        searchRequestBuilder.setSize(size);        searchRequestBuilder.setFrom(constructor.getFrom() &lt; 0 ? 0 : constructor.getFrom());        //设置高亮        HighlightBuilder highlightBuilder = new HighlightBuilder();        List&lt;String&gt; highLighterFields = constructor.getHighLighterFields();        if(highLighterFields.size()&gt;0){            highLighterFields.forEach(field -&gt; {                highlightBuilder.field(field);            });        }        highlightBuilder.preTags(&quot;&lt;font color=\&quot;red\&quot;&gt;&quot;);        highlightBuilder.postTags(&quot;&lt;/font&gt;&quot;);        SearchResponse searchResponse = searchRequestBuilder.highlighter(highlightBuilder).execute().actionGet();        long totalHits = searchResponse.getHits().getTotalHits();        SearchHits hits = searchResponse.getHits();        SearchHit[] searchHists = hits.getHits();        for (SearchHit hit : searchHists) {            Map&lt;String, Object&gt; sourceAsMap = hit.getSourceAsMap();            Map&lt;String, HighlightField&gt; highlightFields = hit.getHighlightFields();            //获取高亮结果            Set&lt;String&gt; set = highlightFields.keySet();            for (String str : set) {                Text[] fragments = highlightFields.get(str).getFragments();                String st1r=&quot;&quot;;                for(Text text:fragments){                    st1r = st1r + text.toString();                }                sourceAsMap.put(str,st1r);                System.out.println(&quot;str(==============&quot; + st1r);            }            list.add(sourceAsMap);        }        map.put(&quot;total&quot;,(long)searchHists.length);        map.put(&quot;count&quot;,totalHits);        map.put(&quot;data&quot;,list);        return map;    }    /**     * 功能描述：统计查询     * @param index 索引名     * @param type 类型     * @param constructor 查询构造     * @param groupBy 统计字段     */    public Map&lt;Object, Object&gt; statSearch(String index, String type, ESQueryBuilderConstructor constructor, String groupBy) {        Map&lt;Object, Object&gt; map = new HashedMap();        SearchRequestBuilder searchRequestBuilder = client.prepareSearch(index).setTypes(type);        //排序        if (StringUtils.isNotEmpty(constructor.getAsc()))            searchRequestBuilder.addSort(constructor.getAsc(), SortOrder.ASC);        if (StringUtils.isNotEmpty(constructor.getDesc()))            searchRequestBuilder.addSort(constructor.getDesc(), SortOrder.DESC);        //设置查询体        if (null != constructor) {            searchRequestBuilder.setQuery(constructor.listBuilders());        } else {            searchRequestBuilder.setQuery(QueryBuilders.matchAllQuery());        }        int size = constructor.getSize();        if (size &lt; 0) {            size = 0;        }        if (size &gt; MAX) {            size = MAX;        }        //返回条目数        searchRequestBuilder.setSize(size);        searchRequestBuilder.setFrom(constructor.getFrom() &lt; 0 ? 0 : constructor.getFrom());        SearchResponse sr = searchRequestBuilder.addAggregation(                AggregationBuilders.terms(&quot;agg&quot;).field(groupBy)        ).get();        Terms stateAgg = sr.getAggregations().get(&quot;agg&quot;);        Iterator&lt;? extends Terms.Bucket&gt; iter = stateAgg.getBuckets().iterator();        while (iter.hasNext()) {            Terms.Bucket gradeBucket = iter.next();            map.put(gradeBucket.getKey(), gradeBucket.getDocCount());        }        return map;    }    /**     * 功能描述：统计查询     * @param index 索引名     * @param type 类型     * @param constructor 查询构造     * @param agg 自定义计算     */    public Map&lt;Object, Object&gt; statSearch(String index, String type, ESQueryBuilderConstructor constructor, AggregationBuilder agg) {        if (agg == null) {            return null;        }        Map&lt;Object, Object&gt; map = new HashedMap();        SearchRequestBuilder searchRequestBuilder = client.prepareSearch(index).setTypes(type);        //排序        if (StringUtils.isNotEmpty(constructor.getAsc()))            searchRequestBuilder.addSort(constructor.getAsc(), SortOrder.ASC);        if (StringUtils.isNotEmpty(constructor.getDesc()))            searchRequestBuilder.addSort(constructor.getDesc(), SortOrder.DESC);        //设置查询体        if (null != constructor) {            searchRequestBuilder.setQuery(constructor.listBuilders());        } else {            searchRequestBuilder.setQuery(QueryBuilders.matchAllQuery());        }        int size = constructor.getSize();        if (size &lt; 0) {            size = 0;        }        if (size &gt; MAX) {            size = MAX;        }        //返回条目数        searchRequestBuilder.setSize(size);        searchRequestBuilder.setFrom(constructor.getFrom() &lt; 0 ? 0 : constructor.getFrom());        SearchResponse sr = searchRequestBuilder.addAggregation(                agg        ).get();        Terms stateAgg = sr.getAggregations().get(&quot;agg&quot;);        Iterator&lt;? extends Terms.Bucket&gt; iter = stateAgg.getBuckets().iterator();        while (iter.hasNext()) {            Terms.Bucket gradeBucket = iter.next();            map.put(gradeBucket.getKey(), gradeBucket.getDocCount());        }        return map;    }    /**     * 功能描述：关闭链接     */    public void close() {        client.close();    }    public static void test() {        try{            ElasticSearchService service = new ElasticSearchService();            ESQueryBuilderConstructorNew constructor = new ESQueryBuilderConstructorNew();            constructor.must(new ESQueryBuilders().bool(QueryBuilders.boolQuery()));            constructor.must(new ESQueryBuilders().match(&quot;secondlanguage&quot;, &quot;4&quot;));            constructor.must(new ESQueryBuilders().match(&quot;secondlanguage&quot;, &quot;4&quot;));            constructor.should(new ESQueryBuilders().match(&quot;source&quot;, &quot;5&quot;));            constructor.should(new ESQueryBuilders().match(&quot;source&quot;, &quot;5&quot;));            service.searchCountAndMessageNew(&quot;&quot;, &quot;&quot;, constructor);        }catch (Exception e){            e.printStackTrace();        }    }    public static void main(String[] args) {        try {            ElasticSearchService service = new ElasticSearchService();            ESQueryBuilderConstructor constructor = new ESQueryBuilderConstructor();         /*   constructor.must(new ESQueryBuilders().term(&quot;gender&quot;, &quot;f&quot;).range(&quot;age&quot;, 20, 50));            constructor.should(new ESQueryBuilders().term(&quot;gender&quot;, &quot;f&quot;).range(&quot;age&quot;, 20, 50).fuzzy(&quot;age&quot;, 20));            constructor.mustNot(new ESQueryBuilders().term(&quot;gender&quot;, &quot;m&quot;));            constructor.setSize(15);  //查询返回条数，最大 10000            constructor.setFrom(11);  //分页查询条目起始位置， 默认0            constructor.setAsc(&quot;age&quot;); //排序            List&lt;Map&lt;String, Object&gt;&gt; list = service.search(&quot;bank&quot;, &quot;account&quot;, constructor);            Map&lt;Object, Object&gt; map = service.statSearch(&quot;bank&quot;, &quot;account&quot;, constructor, &quot;state&quot;);*/            constructor.must(new ESQueryBuilders().match(&quot;id&quot;, &quot;WE16000190TR&quot;));            List&lt;Map&lt;String, Object&gt;&gt; list = service.search(&quot;test01&quot;, &quot;test01&quot;, constructor);             for(Map&lt;String, Object&gt; map : list){                 System.out.println(map);             }        } catch (Exception e) {            e.printStackTrace();        }    }}</code></pre><p><strong>ESCriterion.java</strong></p><pre><code>package com.hsiehchou.es.V2;import org.elasticsearch.index.query.QueryBuilder;import java.util.List;/** * 条件接口 */public interface ESCriterion {    public enum Operator {        PREFIX,             /**根据字段前缀查询**/        MATCH,              /**匹配查询**/        MATCH_PHRASE,       /**精确匹配**/        MULTI_MATCH,        /**多字段匹配**/        TERM,               /**term查询**/        TERMS,              /**term查询**/        RANGE,              /**范围查询**/        GTE,                 /**大于等于查询**/        LTE,        FUZZY,              /**根据字段前缀查询**/        QUERY_STRING,       /**根据字段前缀查询**/        MISSING ,           /**根据字段前缀查询**/        BOOL    }    public enum MatchMode {        START, END, ANYWHERE    }    public enum Projection {        MAX, MIN, AVG, LENGTH, SUM, COUNT    }    public List&lt;QueryBuilder&gt; listBuilders();}</code></pre><p><strong>ESQueryBuilderConstructor.java</strong></p><pre><code>package com.hsiehchou.es.V2;import org.apache.commons.collections.CollectionUtils;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.QueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import java.util.ArrayList;import java.util.List;/** * 查询条件容器 */public class ESQueryBuilderConstructor {    private int size = Integer.MAX_VALUE;    private int from = 0;    private String asc;    private String desc;    //查询条件容器    private List&lt;ESCriterion&gt; mustCriterions = new ArrayList&lt;ESCriterion&gt;();    private List&lt;ESCriterion&gt; shouldCriterions = new ArrayList&lt;ESCriterion&gt;();    private List&lt;ESCriterion&gt; mustNotCriterions = new ArrayList&lt;ESCriterion&gt;();    //构造builder    public QueryBuilder listBuilders() {        int count = mustCriterions.size() + shouldCriterions.size() + mustNotCriterions.size();        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();        QueryBuilder queryBuilder = null;        if (count &gt;= 1) {            //must容器            if (!CollectionUtils.isEmpty(mustCriterions)) {                for (ESCriterion criterion : mustCriterions) {                    for (QueryBuilder builder : criterion.listBuilders()) {                        queryBuilder = boolQueryBuilder.must(builder);                    }                }            }            //should容器            if (!CollectionUtils.isEmpty(shouldCriterions)) {                for (ESCriterion criterion : shouldCriterions) {                    for (QueryBuilder builder : criterion.listBuilders()) {                        queryBuilder = boolQueryBuilder.should(builder);                    }                }            }            //must not 容器            if (!CollectionUtils.isEmpty(mustNotCriterions)) {                for (ESCriterion criterion : mustNotCriterions) {                    for (QueryBuilder builder : criterion.listBuilders()) {                        queryBuilder = boolQueryBuilder.mustNot(builder);                    }                }            }            return queryBuilder;        } else {            return null;        }    }    /**     * 增加简单条件表达式     */    public ESQueryBuilderConstructor must(ESCriterion criterion){        if(criterion!=null){            mustCriterions.add(criterion);        }        return this;    }    /**     * 增加简单条件表达式     */    public ESQueryBuilderConstructor should(ESCriterion criterion){        if(criterion!=null){            shouldCriterions.add(criterion);        }        return this;    }    /**     * 增加简单条件表达式     */    public ESQueryBuilderConstructor mustNot(ESCriterion criterion){        if(criterion!=null){            mustNotCriterions.add(criterion);        }        return this;    }    public int getSize() {        return size;    }    public void setSize(int size) {        this.size = size;    }    public String getAsc() {        return asc;    }    public void setAsc(String asc) {        this.asc = asc;    }    public String getDesc() {        return desc;    }    public void setDesc(String desc) {        this.desc = desc;    }    public int getFrom() {        return from;    }    public void setFrom(int from) {        this.from = from;    }}</code></pre><p><strong>ESQueryBuilderConstructorNew.java</strong></p><pre><code>package com.hsiehchou.es.V2;import org.apache.commons.collections.CollectionUtils;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.QueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.search.sort.SortBuilder;import java.util.ArrayList;import java.util.List;import java.util.Map;/** * 查询条件容器 */public class ESQueryBuilderConstructorNew {    private List&lt;String&gt; highLighterFields = new ArrayList&lt;String&gt;();    private int size = Integer.MAX_VALUE;    private int from = 0;    private List&lt;SortBuilder&gt; sortBuilderList;    public List&lt;SortBuilder&gt; getSortBuilderList() {        return sortBuilderList;    }    public void setSortBuilderList(List&lt;SortBuilder&gt; sortBuilderList) {        this.sortBuilderList = sortBuilderList;    }    private Map&lt;String,List&lt;String&gt;&gt; sortMap;    //查询条件容器    private List&lt;ESCriterion&gt; mustCriterions = new ArrayList&lt;ESCriterion&gt;();    private List&lt;ESCriterion&gt; shouldCriterions = new ArrayList&lt;ESCriterion&gt;();    private List&lt;ESCriterion&gt; mustNotCriterions = new ArrayList&lt;ESCriterion&gt;();    //构造builder    public QueryBuilder listBuilders() {        int count = mustCriterions.size() + shouldCriterions.size() + mustNotCriterions.size();        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();        QueryBuilder queryBuilder = null;        if (count &gt;= 1) {            //must容器            if (!CollectionUtils.isEmpty(mustCriterions)) {                for (ESCriterion criterion : mustCriterions) {                    for (QueryBuilder builder : criterion.listBuilders()) {                        queryBuilder = boolQueryBuilder.must(builder);                    }                }            }            //should容器            if (!CollectionUtils.isEmpty(shouldCriterions)) {                for (ESCriterion criterion : shouldCriterions) {                    for (QueryBuilder builder : criterion.listBuilders()) {                        queryBuilder = boolQueryBuilder.should(builder);                    }                }            }            //must not 容器            if (!CollectionUtils.isEmpty(mustNotCriterions)) {                for (ESCriterion criterion : mustNotCriterions) {                    for (QueryBuilder builder : criterion.listBuilders()) {                        queryBuilder = boolQueryBuilder.mustNot(builder);                    }                }            }            return queryBuilder;        } else {            return null;        }    }    /**     * 增加简单条件表达式     */    public ESQueryBuilderConstructorNew must(ESCriterion criterion){        if(criterion!=null){            mustCriterions.add(criterion);        }        return this;    }    /**     * 增加简单条件表达式     */    public ESQueryBuilderConstructorNew should(ESCriterion criterion){        if(criterion!=null){            shouldCriterions.add(criterion);        }        return this;    }    /**     * 增加简单条件表达式     */    public ESQueryBuilderConstructorNew mustNot(ESCriterion criterion){        if(criterion!=null){            mustNotCriterions.add(criterion);        }        return this;    }    public List&lt;String&gt; getHighLighterFields() {        return highLighterFields;    }    public void setHighLighterFields(List&lt;String&gt; highLighterFields) {        this.highLighterFields = highLighterFields;    }    public int getSize() {        return size;    }    public void setSize(int size) {        this.size = size;    }    public Map&lt;String, List&lt;String&gt;&gt; getSortMap() {        return sortMap;    }    public void setSortMap(Map&lt;String, List&lt;String&gt;&gt; sortMap) {        this.sortMap = sortMap;    }    public int getFrom() {        return from;    }    public void setFrom(int from) {        this.from = from;    }}</code></pre><p><strong>ESQueryBuilders.java</strong></p><pre><code>package com.hsiehchou.es.V2;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.NestedQueryBuilder;import org.elasticsearch.index.query.QueryBuilder;import java.util.ArrayList;import java.util.Collection;import java.util.List;/** * 条件构造器 */public class ESQueryBuilders implements ESCriterion{    private List&lt;QueryBuilder&gt; list = new ArrayList&lt;QueryBuilder&gt;();    /**     * 功能描述：match 查询     * @param field 字段名     * @param value 值     */    public ESQueryBuilders match(String field, Object value) {        list.add(new ESSimpleExpression (field, value, Operator.MATCH).toBuilder());        return this;    }    /**     * 功能描述：match 查询     * @param field 字段名     * @param value 值     */    public ESQueryBuilders match_phrase(String field, Object value) {        list.add(new ESSimpleExpression (field, value, Operator.MATCH_PHRASE).toBuilder());        return this;    }    /**     * 功能描述：match 查询     * @param fieldNames 字段名     * @param value 值     */    public ESQueryBuilders multi_match(Object value , String... fieldNames ) {        String[] fields = fieldNames;        list.add(new ESSimpleExpression (value, Operator.MULTI_MATCH,fields).toBuilder());        return this;    }    /**     * 功能描述：Term 查询     * @param field 字段名     * @param value 值     */    public ESQueryBuilders term(String field, Object value) {        list.add(new ESSimpleExpression (field, value, Operator.TERM).toBuilder());        return this;    }    /**     * 功能描述：Terms 查询     * @param field 字段名     * @param values 集合值     */    public ESQueryBuilders terms(String field, Collection&lt;Object&gt; values) {        list.add(new ESSimpleExpression (field, values).toBuilder());        return this;    }    /**     * 功能描述：fuzzy 查询     * @param field 字段名     * @param value 值     */    public ESQueryBuilders fuzzy(String field, Object value) {        list.add(new ESSimpleExpression (field, value, Operator.FUZZY).toBuilder());        return this;    }    /**     * 功能描述：Range 查询     * @param from 起始值     * @param to 末尾值     */    public ESQueryBuilders range(String field, Object from, Object to) {        list.add(new ESSimpleExpression (field, from, to).toBuilder());        return this;    }    /**     * 功能描述：GTE 大于等于查询     * @param     */    public ESQueryBuilders gte(String field, Object num) {        list.add(new ESSimpleExpression (field, num,Operator.GTE).toBuilder());        return this;    }    /**     * 功能描述：LTE 小于等于查询     * @param     */    public ESQueryBuilders lte(String field, Object num) {        list.add(new ESSimpleExpression (field, num,Operator.LTE).toBuilder());        return this;    }    /**     * 功能描述：prefix 查询     * @param field 字段名     * @param value 值     */    public ESQueryBuilders prefix(String field, Object value) {        list.add(new ESSimpleExpression (field, value, Operator.PREFIX).toBuilder());        return this;    }    /**     * 功能描述：Range 查询     * @param queryString 查询语句     */    public ESQueryBuilders queryString(String queryString) {        list.add(new ESSimpleExpression (queryString, Operator.QUERY_STRING).toBuilder());        return this;    }    /**     * 功能描述：Range 查询     * @param     */    public ESQueryBuilders bool(BoolQueryBuilder boolQueryBuilder) {        list.add(boolQueryBuilder);        return this;    }    public ESQueryBuilders nested(NestedQueryBuilder nestedQueryBuilder) {        list.add(nestedQueryBuilder);        return this;    }    public List&lt;QueryBuilder&gt; listBuilders() {        return list;    }}</code></pre><p><strong>ESSimpleExpression.java</strong></p><pre><code>package com.hsiehchou.es.V2;import org.elasticsearch.index.query.BoolQueryBuilder;import org.elasticsearch.index.query.QueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import java.util.Collection;import com.hsiehchou.es.V2.ESCriterion.Operator;import static org.elasticsearch.index.search.MatchQuery.Type.PHRASE;/** * 条件表达式 */public class ESSimpleExpression {    private String[] fieldNames;         //属性名    private String fieldName;         //属性名    private Object value;             //对应值    private Collection&lt;Object&gt; values;//对应值    private Operator operator;        //计算符    private Object from;    private Object to;    protected  ESSimpleExpression() {    }    protected  ESSimpleExpression(Object value, Operator operator,String... fieldNames) {        this.fieldNames = fieldNames;        this.value = value;        this.operator = operator;    }    protected  ESSimpleExpression(String fieldName, Object value, Operator operator) {        this.fieldName = fieldName;        this.value = value;        this.operator = operator;    }    protected  ESSimpleExpression(String value, Operator operator) {        this.value = value;        this.operator = operator;    }    protected ESSimpleExpression(String fieldName, Collection&lt;Object&gt; values) {        this.fieldName = fieldName;        this.values = values;        this.operator = Operator.TERMS;    }    protected ESSimpleExpression(String fieldName, Object from, Object to) {        this.fieldName = fieldName;        this.from = from;        this.to = to;        this.operator = Operator.RANGE;    }    public BoolQueryBuilder toBoolQueryBuilder(){        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();        boolQueryBuilder.mustNot(QueryBuilders.matchQuery(&quot;&quot;,&quot;&quot;));        boolQueryBuilder.mustNot(QueryBuilders.matchQuery(&quot;&quot;,&quot;&quot;));        return null;    }    public QueryBuilder toBuilder() {        QueryBuilder qb = null;        switch (operator) {            case MATCH:                qb = QueryBuilders.matchQuery(fieldName, value);                break;            case MATCH_PHRASE:                qb = QueryBuilders.matchPhraseQuery(fieldName, value);                break;            case MULTI_MATCH:                qb = QueryBuilders.multiMatchQuery(value,fieldNames).type(PHRASE);                break;            case TERM:                qb = QueryBuilders.termQuery(fieldName, value);                break;            case TERMS:                qb = QueryBuilders.termsQuery(fieldName, values);                break;            case RANGE:                qb = QueryBuilders.rangeQuery(fieldName).from(from).to(to).includeLower(true).includeUpper(true);                break;            case GTE:                qb = QueryBuilders.rangeQuery(fieldName).gte(value);                break;            case LTE:                qb = QueryBuilders.rangeQuery(fieldName).lte(value);                break;            case FUZZY:                qb = QueryBuilders.fuzzyQuery(fieldName, value);                break;            case PREFIX:                qb = QueryBuilders.prefixQuery(fieldName, value.toString());                break;            case QUERY_STRING:                qb = QueryBuilders.queryStringQuery(value.toString());                default:        }        return qb;    }}</code></pre><h3 id="九、预警"><a href="#九、预警" class="headerlink" title="九、预警"></a>九、预警</h3><p>通过后台或者界面设置规则，保存到mysql，然后同步到redis。</p><p>数据量大的话，用mysql是非常慢的，使用内存数据库redis进行规则缓存，使用时直接比对预警。</p><p><img src="/medias/%E9%A2%84%E8%AD%A6%E6%B5%81%E7%A8%8B.PNG" alt="预警流程"></p><p><img src="/medias/%E9%A2%84%E8%AD%A6%E8%BF%87%E7%A8%8B.PNG" alt="预警过程"></p><p>MySQL 需要2张表<br>一张是规则表   用来存储规则<br>一张是消息表   存储告警消息</p><h4 id="1、创建规则表（由界面控制规则发布）"><a href="#1、创建规则表（由界面控制规则发布）" class="headerlink" title="1、创建规则表（由界面控制规则发布）"></a>1、创建规则表（由界面控制规则发布）</h4><p>规则首先存放在mysql中，会使用一个定时任务将mysql中的规则同步到redis<br>       直接在test库中创建<br>       创建脚本<br><strong>xz_rule.sql</strong></p><pre><code>SET FOREIGN_KEY_CHECKS=0;DROP TABLE IF EXISTS `xz_rule`;CREATE TABLE `xz_rule` (  `id` int(11) NOT NULL AUTO_INCREMENT,  `warn_fieldname` varchar(20) DEFAULT NULL,  `warn_fieldvalue` varchar(255) DEFAULT NULL,  `publisher` varchar(255) DEFAULT NULL,  `send_type` varchar(255) CHARACTER SET utf8 DEFAULT NULL,  `send_mobile` varchar(255) DEFAULT NULL,  `send_mail` varchar(255) DEFAULT NULL,  `send_dingding` varchar(255) DEFAULT NULL,  `create_time` date DEFAULT NULL,  PRIMARY KEY (`id`)) ENGINE=MyISAM AUTO_INCREMENT=2 DEFAULT CHARSET=latin1;INSERT INTO `xz_rule` VALUES (&#39;1&#39;, &#39;phone&#39;, &#39;18609765432&#39;, &#39;?????1&#39;, &#39;2&#39;, &#39;13724536789&#39;, &#39;1782324@qq.com&#39;, &#39;32143243&#39;, &#39;2019-06-28&#39;);</code></pre><h4 id="2、创建消息表"><a href="#2、创建消息表" class="headerlink" title="2、创建消息表"></a>2、创建消息表</h4><ol><li>用于存放预警的消息，供界面定时刷新预警消息 或者是滚屏预警</li><li>预警消息统计</li></ol><p><strong>warn_message.sql</strong></p><pre><code>SET FOREIGN_KEY_CHECKS=0;DROP TABLE IF EXISTS `warn_message`;CREATE TABLE `warn_message` (  `id` int(11) NOT NULL AUTO_INCREMENT,  `alarmRuleid` varchar(255) DEFAULT NULL,  `alarmType` varchar(255) DEFAULT NULL,  `sendType` varchar(255) DEFAULT NULL,  `sendMobile` varchar(255) DEFAULT NULL,  `sendEmail` varchar(255) DEFAULT NULL,  `sendStatus` varchar(255) DEFAULT NULL,  `senfInfo` varchar(255) CHARACTER SET utf8 DEFAULT NULL,  `hitTime` datetime DEFAULT NULL,  `checkinTime` datetime DEFAULT NULL,  `isRead` varchar(255) DEFAULT NULL,  `readAccounts` varchar(255) DEFAULT NULL,  `alarmaccounts` varchar(255) DEFAULT NULL,  `accountid` varchar(11) DEFAULT NULL,  PRIMARY KEY (`id`)) ENGINE=MyISAM AUTO_INCREMENT=31 DEFAULT CHARSET=latin1;</code></pre><h4 id="3、创建数据库连接工具类"><a href="#3、创建数据库连接工具类" class="headerlink" title="3、创建数据库连接工具类"></a>3、创建数据库连接工具类</h4><p><strong>新建com.hsiehchou.common.netb.db包</strong><br><strong>创建DBCommon类</strong></p><p><strong>DBCommon.java</strong></p><pre><code>package com.hsiehchou.common.netb.db;import com.hsiehchou.common.config.ConfigUtil;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.sql.*;import java.util.Properties;public class DBCommon {    private static Logger LOG = LoggerFactory.getLogger(DBCommon.class);    private static String MYSQL_PATH = &quot;common/mysql.properties&quot;;    private static Properties properties = ConfigUtil.getInstance().getProperties(MYSQL_PATH);    private static Connection conn ;    private DBCommon(){}    public static void main(String[] args) {        System.out.println(properties);        Connection xz_bigdata = DBCommon.getConn(&quot;test&quot;);        System.out.println(xz_bigdata);    }    //TODO  配置文件    private static final String JDBC_DRIVER = &quot;com.mysql.jdbc.Driver&quot;;    private static final String USER_NAME = properties.getProperty(&quot;user&quot;);    private static final String PASSWORD = properties.getProperty(&quot;password&quot;);    private static final String IP = properties.getProperty(&quot;db_ip&quot;);    private static final String PORT = properties.getProperty(&quot;db_port&quot;);    private static final String DB_CONFIG = &quot;?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull&amp;autoReconnect=true&amp;failOverReadOnly=false&quot;;    static {        try {            Class.forName(JDBC_DRIVER);        } catch (ClassNotFoundException e) {            LOG.error(null, e);        }    }    /**     * 获取数据库连接     * @param dbName     * @return     */    public static Connection getConn(String dbName) {        Connection conn = null;        String  connstring = &quot;jdbc:mysql://&quot;+IP+&quot;:&quot;+PORT+&quot;/&quot;+dbName+DB_CONFIG;        try {            conn = DriverManager.getConnection(connstring, USER_NAME, PASSWORD);        } catch (SQLException e) {            e.printStackTrace();            LOG.error(null, e);        }        return conn;    }    /**     * @param url eg:&quot;jdbc:oracle:thin:@172.16.1.111:1521:d406&quot;     * @param driver eg:&quot;oracle.jdbc.driver.OracleDriver&quot;     * @param user eg:&quot;ucase&quot;     * @param password eg:&quot;ucase123&quot;     * @return     * @throws ClassNotFoundException     * @throws SQLException     */    public static Connection getConn(String url, String driver, String user,                                     String password) throws ClassNotFoundException, SQLException{        Class.forName(driver);        conn = DriverManager.getConnection(url, user, password);        return  conn;    }    public static void close(Connection conn){        try {            if( conn != null ){                conn.close();            }        } catch (SQLException e) {            LOG.error(null,e);        }    }    public static void close(Statement statement){        try {            if( statement != null ){                statement.close();            }        } catch (SQLException e) {            LOG.error(null,e);        }    }    public static void close(Connection conn,PreparedStatement statement){        try {            if( conn != null ){                conn.close();            }            if( statement != null ){                statement.close();            }        } catch (SQLException e) {            LOG.error(null,e);        }    }    public static void close(Connection conn,Statement statement,ResultSet resultSet) throws SQLException{        if( resultSet != null ){            resultSet.close();        }        if( statement != null ){            statement.close();        }        if( conn != null ){            conn.close();        }    }}</code></pre><p><strong>引入maven依赖</strong></p><pre><code>&lt;dependency&gt;    &lt;groupId&gt;commons-dbutils&lt;/groupId&gt;    &lt;artifactId&gt;commons-dbutils&lt;/artifactId&gt;    &lt;version&gt;${commons-dbutils.version}&lt;/version&gt;&lt;/dependency&gt;</code></pre><h4 id="4、创建实体类和dao"><a href="#4、创建实体类和dao" class="headerlink" title="4、创建实体类和dao"></a>4、创建实体类和dao</h4><p><strong>新建com.hsiehchou.spark.warn.domain包</strong><br><strong>新建 XZ_RuleDomain，WarningMessage</strong></p><p><strong>XZ_RuleDomain.java</strong></p><pre><code>package com.hsiehchou.spark.warn.domain;import java.sql.Date;public class XZ_RuleDomain {    private int id;    private String warn_fieldname;   //预警字段    private String warn_fieldvalue; //预警内容    private String publisher;       //发布者    private String send_type;       //消息接收方式    private String send_mobile;     //接收手机号    private String send_mail;       //接收邮箱    private String send_dingding;   //接收钉钉    private Date create_time;       //创建时间    public int getId() {        return id;    }    public void setId(int id) {        this.id = id;    }    public String getWarn_fieldname() {        return warn_fieldname;    }    public void setWarn_fieldname(String warn_fieldname) {        this.warn_fieldname = warn_fieldname;    }    public String getWarn_fieldvalue() {        return warn_fieldvalue;    }    public void setWarn_fieldvalue(String warn_fieldvalue) {        this.warn_fieldvalue = warn_fieldvalue;    }    public String getPublisher() {        return publisher;    }    public void setPublisher(String publisher) {        this.publisher = publisher;    }    public String getSend_type() {        return send_type;    }    public void setSend_type(String send_type) {        this.send_type = send_type;    }    public String getSend_mobile() {        return send_mobile;    }    public void setSend_mobile(String send_mobile) {        this.send_mobile = send_mobile;    }    public String getSend_mail() {        return send_mail;    }    public void setSend_mail(String send_mail) {        this.send_mail = send_mail;    }    public String getSend_dingding() {        return send_dingding;    }    public void setSend_dingding(String send_dingding) {        this.send_dingding = send_dingding;    }    public Date getCreate_time() {        return create_time;    }    public void setCreate_time(Date create_time) {        this.create_time = create_time;    }}</code></pre><p><strong>WarningMessage.java</strong></p><pre><code>package com.hsiehchou.spark.warn.domain;import java.sql.Date;public class WarningMessage {    private String id;            //主键id    private String alarmRuleid;   //规则id    private String alarmType;     //告警类型    private String sendType;      //发送方式    private String sendMobile;    //发送至手机    private String sendEmail;     //发送至邮箱    private String sendStatus;    //发送状态    private String senfInfo;      //发送内容    private Date hitTime;         //命中时间    private Date checkinTime;     //入库时间    private String isRead;        //是否已读    private String readAccounts;  //已读用户    private String alarmaccounts;    private String accountid;    public String getId() {        return id;    }    public void setId(String id) {        this.id = id;    }    public String getAlarmRuleid() {        return alarmRuleid;    }    public void setAlarmRuleid(String alarmRuleid) {        this.alarmRuleid = alarmRuleid;    }    public String getAlarmType() {        return alarmType;    }    public void setAlarmType(String alarmType) {        this.alarmType = alarmType;    }    public String getSendType() {        return sendType;    }    public void setSendType(String sendType) {        this.sendType = sendType;    }    public String getSendMobile() {        return sendMobile;    }    public void setSendMobile(String sendMobile) {        this.sendMobile = sendMobile;    }    public String getSendEmail() {        return sendEmail;    }    public void setSendEmail(String sendEmail) {        this.sendEmail = sendEmail;    }    public String getSendStatus() {        return sendStatus;    }    public void setSendStatus(String sendStatus) {        this.sendStatus = sendStatus;    }    public String getSenfInfo() {        return senfInfo;    }    public void setSenfInfo(String senfInfo) {        this.senfInfo = senfInfo;    }    public Date getHitTime() {        return hitTime;    }    public void setHitTime(Date hitTime) {        this.hitTime = hitTime;    }    public Date getCheckinTime() {        return checkinTime;    }    public void setCheckinTime(Date checkinTime) {        this.checkinTime = checkinTime;    }    public String getIsRead() {        return isRead;    }    public void setIsRead(String isRead) {        this.isRead = isRead;    }    public String getReadAccounts() {        return readAccounts;    }    public void setReadAccounts(String readAccounts) {        this.readAccounts = readAccounts;    }    public String getAlarmaccounts() {        return alarmaccounts;    }    public void setAlarmaccounts(String alarmaccounts) {        this.alarmaccounts = alarmaccounts;    }    public String getAccountid() {        return accountid;    }    public void setAccountid(String accountid) {        this.accountid = accountid;    }    @Override    public String toString() {        return &quot;WarningMessage{&quot; +                &quot;id=&#39;&quot; + id + &#39;\&#39;&#39; +                &quot;, alarmRuleid=&#39;&quot; + alarmRuleid + &#39;\&#39;&#39; +                &quot;, alarmType=&#39;&quot; + alarmType + &#39;\&#39;&#39; +                &quot;, sendType=&#39;&quot; + sendType + &#39;\&#39;&#39; +                &quot;, sendMobile=&#39;&quot; + sendMobile + &#39;\&#39;&#39; +                &quot;, sendEmail=&#39;&quot; + sendEmail + &#39;\&#39;&#39; +                &quot;, sendStatus=&#39;&quot; + sendStatus + &#39;\&#39;&#39; +                &quot;, senfInfo=&#39;&quot; + senfInfo + &#39;\&#39;&#39; +                &quot;, hitTime=&quot; + hitTime +                &quot;, checkinTime=&quot; + checkinTime +                &quot;, isRead=&#39;&quot; + isRead + &#39;\&#39;&#39; +                &quot;, readAccounts=&#39;&quot; + readAccounts + &#39;\&#39;&#39; +                &quot;, alarmaccounts=&#39;&quot; + alarmaccounts + &#39;\&#39;&#39; +                &quot;, accountid=&#39;&quot; + accountid + &#39;\&#39;&#39; +                &#39;}&#39;;    }}</code></pre><p><strong>新建com.hsiehchou.spark.warn.dao包</strong><br><strong>新建 XZ_RuleDao，WarningMessageDao</strong></p><p><strong>XZ_RuleDao.java</strong></p><pre><code>package com.hsiehchou.spark.warn.dao;import com.hsiehchou.common.netb.db.DBCommon;import com.hsiehchou.spark.warn.domain.XZ_RuleDomain;import org.apache.commons.dbutils.QueryRunner;import org.apache.commons.dbutils.handlers.BeanListHandler;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.sql.Connection;import java.sql.SQLException;import java.util.List;public class XZ_RuleDao {    private static final Logger LOG = LoggerFactory.getLogger(XZ_RuleDao.class);    /**     *  获取所有的规则     * @return     */    public static List&lt;XZ_RuleDomain&gt; getRuleList(){        List&lt;XZ_RuleDomain&gt; listRules = null;        //获取连接        Connection conn = DBCommon.getConn(&quot;test&quot;);        //执行器        QueryRunner query = new QueryRunner();        String sql = &quot;select * from xz_rule&quot;;        try {            listRules = query.query(conn,sql,new BeanListHandler&lt;&gt;(XZ_RuleDomain.class));        } catch (SQLException e) {            LOG.error(null,e);        }finally {            DBCommon.close(conn);        }        return listRules;    }    public static void main(String[] args) {        List&lt;XZ_RuleDomain&gt; ruleList = XZ_RuleDao.getRuleList();        System.out.println(ruleList.size());        ruleList.forEach(x-&gt;{            System.out.println(x);        });    }}</code></pre><p><strong>WarningMessageDao.java</strong></p><pre><code>package com.hsiehchou.spark.warn.dao;import com.hsiehchou.common.netb.db.DBCommon;import com.hsiehchou.spark.warn.domain.WarningMessage;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.sql.*;public class WarningMessageDao {    private static final Logger LOG = LoggerFactory.getLogger(WarningMessageDao.class);    /**     * 写入消息到mysql     * @param warningMessage     * @return     */    public static Integer insertWarningMessageReturnId(WarningMessage warningMessage) {        Connection conn= DBCommon.getConn(&quot;test&quot;);        String sql=&quot;insert into warn_message(alarmruleid,sendtype,senfinfo,hittime,sendmobile,alarmtype) &quot; +                &quot;values(?,?,?,?,?,?)&quot;;        PreparedStatement stmt=null;        ResultSet resultSet=null;        int id=-1;        try{            stmt = conn.prepareStatement(sql);            stmt.setString(1,warningMessage.getAlarmRuleid());            stmt.setInt(2,Integer.valueOf(warningMessage.getSendType()));            stmt.setString(3,warningMessage.getSenfInfo());            stmt.setTimestamp(4,new Timestamp(System.currentTimeMillis()));            stmt.setString(5,warningMessage.getSendMobile());            stmt.setInt(6,Integer.valueOf(warningMessage.getAlarmType()));            stmt.executeUpdate();        }catch(Exception e) {            LOG.error(null,e);        }finally {            try {                DBCommon.close(conn,stmt,resultSet);            } catch (SQLException e) {                e.printStackTrace();            }        }        return id;    }}</code></pre><h4 id="5、告警工具类"><a href="#5、告警工具类" class="headerlink" title="5、告警工具类"></a>5、告警工具类</h4><p><strong>新建com.hsiehchou.spark.warn.service包</strong><br><strong>新建 BlackRuleWarning，WarningMessageSendUtil</strong></p><p><strong>BlackRuleWarning.java</strong></p><pre><code>package com.hsiehchou.spark.warn.service;import com.hsiehchou.spark.warn.dao.WarningMessageDao;import com.hsiehchou.spark.warn.domain.WarningMessage;import org.apache.commons.lang3.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import redis.clients.jedis.Jedis;import java.text.SimpleDateFormat;import java.util.ArrayList;import java.util.List;import java.util.Map;public class BlackRuleWarning {    private static final Logger LOG = LoggerFactory.getLogger(BlackRuleWarning.class);    //可以通过数据库，配置文件加载    //为了遍历所有预警字段    private static List&lt;String&gt; listWarnFields = new ArrayList&lt;&gt;();    static {        listWarnFields.add(&quot;phone&quot;);        listWarnFields.add(&quot;mac&quot;);    }    /**     * 预警流程处理     * @param map     * @param jedis15     */    public static void blackWarning(Map&lt;String, Object&gt; map, Jedis jedis15) {        listWarnFields.forEach(warnField -&gt; {            if (map.containsKey(warnField) &amp;&amp; StringUtils.isNotBlank(map.get(warnField).toString())) {                //获取预警字段核预警值  相当于手机号                String warnFieldValue = map.get(warnField).toString();                //去redis中进行比对                //数据中  通过   &quot;字段&quot; + &quot;字段值&quot; 去拼接key                //            phone       :    186XXXXXX                String key = warnField + &quot;:&quot; + warnFieldValue;                //redis中的key是   phone:18609765435                System.out.println(&quot;拼接数据流中的key=======&quot; + key);                if (jedis15.exists(key)) {                    //对比命中之后 就可以发送消息提醒                    System.out.println(&quot;命中REDIS中的&quot; + key + &quot;===========开始预警&quot;);                    beginWarning(jedis15, key);                } else {                    //直接过                    System.out.println(&quot;未命中&quot; + key + &quot;===========不进行预警&quot;);                }            }        });    }    /**     * 规则已经命中，开始预警     * @param jedis15     * @param key     */    private static void beginWarning( Jedis jedis15, String key) {        System.out.println(&quot;============MESSAGE -1- =========&quot;);        //封装告警  信息及告警消息        WarningMessage warningMessage = getWarningMessage(jedis15, key);        System.out.println(&quot;============MESSAGE -4- =========&quot;);        if (warningMessage != null) {            //将预警信息写入预警信息表            WarningMessageDao.insertWarningMessageReturnId(warningMessage);            //String accountid = warningMessage.getAccountid();            //String readAccounts = warningMessage.getAlarmaccounts();            // WarnService.insertRead_status(messageId, accountid);            if (warningMessage.getSendType().equals(&quot;2&quot;)) {                //手机短信告警 默认告警方式                WarningMessageSendUtil.messageWarn(warningMessage);            }        }    }    /**     * 封装告警信息及告警消息     * @param jedis15     * @param key     * @return     */    private static WarningMessage getWarningMessage(Jedis jedis15, String key) {        System.out.println(&quot;============MESSAGE -2- =========&quot;);        //封装消息        String[] split = key.split(&quot;:&quot;);        if (split.length == 2) {            WarningMessage warningMessage = new WarningMessage();            String time = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;).toString();            String clew_type = split[0];//告警字段            String rulecontent = split[1];//告警字段值            //从redis中获取消息信息进行封装            Map&lt;String, String&gt; valueMap = jedis15.hgetAll(key);            //规则ID (是哪条规则命中的)            warningMessage.setAlarmRuleid(valueMap.get(&quot;id&quot;));            //预警方式            warningMessage.setSendType(valueMap.get(&quot;send_type&quot;));//告警方式，0：界面 1：邮件 2：短信 3：邮件+短信            //预警信息接收手机号            warningMessage.setSendMobile(valueMap.get(&quot;send_mobile&quot;));            //arningMessage.setSendEmail(valueMap.get(&quot;sendemail&quot;));            /*arningMessage.setAlarmaccounts(valueMap.get(&quot;alarmaccounts&quot;));*/            //规则发布人            warningMessage.setAccountid(valueMap.get(&quot;publisher&quot;));            warningMessage.setAlarmType(&quot;2&quot;);            StringBuffer warn_content = new StringBuffer();            //预警内容 信息   时间  地点  人物            //预警字段来进行设置  phone            //我们有手机号            //数据关联            // 手机  MAC  身份证， 车牌  人脸。。URL 姓名            // 全部设在推送消息里面            warn_content.append(&quot;【网络告警】：手机号为:&quot; + &quot;[&quot; + rulecontent + &quot;]在时间&quot; + time + &quot;出现在&quot; + &quot;&gt;附近,设备号&quot;            );            String content = warn_content.toString();            warningMessage.setSenfInfo(content);            System.out.println(&quot;============MESSAGE -3- =========&quot;);            return warningMessage;        } else {            return null;        }    }}</code></pre><p><strong>WarningMessageSendUtil.java</strong></p><pre><code>package com.hsiehchou.spark.warn.service;import com.hsiehchou.common.regex.Validation;import com.hsiehchou.spark.warn.domain.WarningMessage;import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class WarningMessageSendUtil {    private static final Logger LOG = LoggerFactory.getLogger(WarningMessageSendUtil.class);    public static void messageWarn(WarningMessage warningMessage) {        String[] mobiles = warningMessage.getSendMobile().split(&quot;,&quot;);        for(String phone:mobiles){            if(Validation.isMobile(phone)){                System.out.println(&quot;开始向手机号为&quot; + phone + &quot;发送告警消息====&quot; + warningMessage);                StringBuffer sb= new StringBuffer();                String content=warningMessage.getSenfInfo().toString();                //TODO  调用短信接口发送消息                //TODO  怎么通过短信发送  这个是需要公司开通接口                //TODO  DINGDING                // 专门的接口             /*   sb.append(ClusterProperties.https_url + &quot;username=&quot; + ClusterProperties.https_username +                        &quot;&amp;password=&quot; + ClusterProperties.https_password + &quot;&amp;mobile=&quot; + phone +                        &quot;&amp;apikey=&quot; + ClusterProperties.https_apikey+                        &quot;&amp;content=&quot; + URLEncoder.encode(content));*/               // sendMessage(sb.toString());            }        }    }}</code></pre><h4 id="6、创建redis子项目"><a href="#6、创建redis子项目" class="headerlink" title="6、创建redis子项目"></a>6、创建redis子项目</h4><p><strong>操作redis 使用</strong></p><p><strong>新建xz_bigdata_redis子模块</strong></p><p><strong>pom.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_redis&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_redis&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;        &lt;jedis.version&gt;2.7.0&lt;/jedis.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;redis.clients&lt;/groupId&gt;            &lt;artifactId&gt;jedis&lt;/artifactId&gt;            &lt;version&gt;${jedis.version}&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><p><strong>新建com.hsiehchou.redis.client包</strong><br> <strong>创建redis连接类—JedisSingle</strong></p><p><strong>JedisSingle.java</strong></p><pre><code>package com.hsiehchou.redis.client;import com.hsiehchou.common.config.ConfigUtil;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import redis.clients.jedis.Jedis;import redis.clients.jedis.exceptions.JedisConnectionException;import java.net.SocketTimeoutException;import java.util.Map;import java.util.Properties;public class JedisSingle {    private static final Logger LOG = LoggerFactory.getLogger(JedisSingle.class);    private static Properties redisConf;    /**     * 读取redis配置文件     * redis.hostname = 192.168.247.103     * redis.port  = 6379     */    static {        redisConf = ConfigUtil.getInstance().getProperties(&quot;redis/redis.properties&quot;);        System.out.println(redisConf);    }    public static Jedis getJedis(int db){        Jedis jedis = JedisSingle.getJedis();        if(jedis!=null){            jedis.select(db);        }        return jedis;    }    public static void main(String[] args) {        Jedis jedis = JedisSingle.getJedis(15);        Map&lt;String, String&gt; Map = jedis.hgetAll(&quot;phone:18609765435&quot;);        System.out.println(Map.toString());    }    public static Jedis getJedis(){        int timeoutCount = 0;        while (true) {// 如果是网络超时则多试几次            try            {                 Jedis jedis = new Jedis(redisConf.get(&quot;redis.hostname&quot;).toString(),                         Integer.valueOf(redisConf.get(&quot;redis.port&quot;).toString()));                return jedis;            } catch (Exception e)            {                if (e instanceof JedisConnectionException || e instanceof SocketTimeoutException)                {                    timeoutCount++;                    LOG.warn(&quot;获取jedis连接超时次数:&quot; +timeoutCount);                    if (timeoutCount &gt; 4)                    {                        LOG.error(&quot;获取jedis连接超时次数a:&quot; +timeoutCount);                        LOG.error(null,e);                        break;                    }                }else                {                    LOG.error(&quot;getJedis error&quot;, e);                    break;                }            }        }        return null;    }    public static void close(Jedis jedis){        if(jedis!=null){            jedis.close();        }    }}</code></pre><h4 id="7、创建定时任务，将规则同步到redis"><a href="#7、创建定时任务，将规则同步到redis" class="headerlink" title="7、创建定时任务，将规则同步到redis"></a>7、创建定时任务，将规则同步到redis</h4><p><strong>新建 com.hsiehchou.spark.warn.timer 包</strong><br><strong>新建 SyncRule2Redis，WarnHelper</strong></p><p><strong>SyncRule2Redis.java</strong></p><pre><code>package com.hsiehchou.spark.warn.timer;import java.util.TimerTask;public class SyncRule2Redis extends TimerTask {    @Override    public void run() {        //这里定义同步方法        //就是读取mysql的数据 然后写入到redis中        System.out.println(&quot;========开始同步MYSQL规则到redis=======&quot;);        WarnHelper.syncRuleFromMysql2Redis();        System.out.println(&quot;============开始同步规则成功===========&quot;);    }}</code></pre><p><strong>WarnHelper.java</strong></p><pre><code>package com.hsiehchou.spark.warn.timer;import com.hsiehchou.redis.client.JedisSingle;import com.hsiehchou.spark.warn.dao.XZ_RuleDao;import com.hsiehchou.spark.warn.domain.XZ_RuleDomain;import org.apache.commons.lang3.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import redis.clients.jedis.Jedis;import java.util.List;public class WarnHelper {    private static final Logger LOG = LoggerFactory.getLogger(WarnHelper.class);    /**     * 同步mysql规则数据到redis     */    public static void syncRuleFromMysql2Redis(){        //获取所有的规则        List&lt;XZ_RuleDomain&gt; ruleList = XZ_RuleDao.getRuleList();        Jedis jedis = null;        try {            //获取redis 客户端            jedis = JedisSingle.getJedis(15);            for (int i = 0; i &lt; ruleList.size(); i++) {                XZ_RuleDomain rule = ruleList.get(i);                String id = rule.getId()+&quot;&quot;;                String publisher = rule.getPublisher();                String warn_fieldname = rule.getWarn_fieldname();                String warn_fieldvalue = rule.getWarn_fieldvalue();                String send_mobile = rule.getSend_mobile();                String send_type = rule.getSend_type();                //拼接redis key值                String redisKey = warn_fieldname +&quot;:&quot; + warn_fieldvalue;                //通过redis hash结构   hashMap                jedis.hset(redisKey,&quot;id&quot;,StringUtils.isNoneBlank(id) ? id : &quot;&quot;);                jedis.hset(redisKey,&quot;publisher&quot;,StringUtils.isNoneBlank(publisher) ? publisher : &quot;&quot;);                jedis.hset(redisKey,&quot;warn_fieldname&quot;,StringUtils.isNoneBlank(warn_fieldname) ? warn_fieldname : &quot;&quot;);                jedis.hset(redisKey,&quot;warn_fieldvalue&quot;,StringUtils.isNoneBlank(warn_fieldvalue) ? warn_fieldvalue : &quot;&quot;);                jedis.hset(redisKey,&quot;send_mobile&quot;,StringUtils.isNoneBlank(send_mobile) ? send_mobile : &quot;&quot;);                jedis.hset(redisKey,&quot;send_type&quot;,StringUtils.isNoneBlank(send_type) ? send_type : &quot;&quot;);            }        } catch (Exception e) {           LOG.error(&quot;同步规则到es失败&quot;,e);        } finally {            JedisSingle.close(jedis);        }    }    public static void main(String[] args)    {        WarnHelper.syncRuleFromMysql2Redis();    }}</code></pre><h4 id="8、创建streaming流任务"><a href="#8、创建streaming流任务" class="headerlink" title="8、创建streaming流任务"></a>8、创建streaming流任务</h4><p><strong>scala/com/hsiehchou/spark/streaming/kafka/warn</strong><br><strong>WarningStreamingTask.scala</strong></p><pre><code>package com.hsiehchou.spark.streaming.kafka.warnimport java.util.Timerimport com.hsiehchou.redis.client.JedisSingleimport com.hsiehchou.spark.common.SparkContextFactoryimport com.hsiehchou.spark.streaming.kafka.Spark_Kafka_ConfigUtilimport com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming.kafkaConfigimport com.hsiehchou.spark.warn.service.BlackRuleWarningimport com.hsiehchou.spark.warn.timer.SyncRule2Redisimport org.apache.spark.Loggingimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.StreamingContextimport org.apache.spark.streaming.kafka.KafkaManagerimport redis.clients.jedis.Jedisobject WarningStreamingTask extends Serializable with Logging{  def main(args: Array[String]): Unit = {     //定义一个定时器去定时同步 MYSQL到REDIS     val timer : Timer = new Timer    //SyncRule2Redis 任务类    //0 第一次开始执行    //1*60*1000  隔多少时间执行一次    timer.schedule(new SyncRule2Redis,0,1*60*1000)     //从kafka中获取数据流     //val topics = &quot;chl_test7&quot;.split(&quot;,&quot;)     //kafka topic     val topics = &quot;chl_test7&quot;.split(&quot;,&quot;)     //val ssc = SparkContextFactory.newSparkLocalStreamingContext(&quot;WarningStreamingTask1&quot;, java.lang.Long.valueOf(10),1)     val ssc:StreamingContext = SparkContextFactory.newSparkStreamingContext(&quot;Kafka2esStreaming&quot;, java.lang.Long.valueOf(10))    //构建kafkaManager    val kafkaManager = new KafkaManager(      Spark_Kafka_ConfigUtil.getKafkaParam(kafkaConfig.getProperty(&quot;metadata.broker.list&quot;), &quot;WarningStreamingTask111&quot;)    )    //使用kafkaManager创建DStreaming流    val kafkaDS = kafkaManager.createJsonToJMapStringDirectStreamWithOffset(ssc, topics.toSet)      //添加一个日期分组字段      //如果数据其他的转换，可以先在这里进行统一转换       .persist(StorageLevel.MEMORY_AND_DISK)    kafkaDS.foreachRDD(rdd=&gt;{      //流量预警      //if(!rdd.isEmpty()){/*      val count_flow = rdd.map(x=&gt;{          val flow = java.lang.Long.valueOf(x.get(&quot;collect_time&quot;))          flow        }).reduce(_+_)      if(count_flow &gt; 1719179595L){        println(&quot;流量预警: 阈值[1719179595L] 实际值:&quot;+ count_flow)      }*/      //}      //客户端连接之类的 最好不要放在RDD外面，因为在处理partion时，数据需要分发到各个节点上去      //数据分发必须需要序列化才可以，如果不能序列化，分发会报错      //如果这个数据 包括他里面的内容 都可以序列化，那么可以直接放在RDD外面      var jedis:Jedis = null      try {        //jedis = JedisSingle.getJedis(15)        rdd.foreachPartition(partion =&gt; {          jedis = JedisSingle.getJedis(15)          while (partion.hasNext) {            val map = partion.next()            val table = map.get(&quot;table&quot;)            val mapObject = map.asInstanceOf[java.util.Map[String,Object]]            println(table)            //开始比对            BlackRuleWarning.blackWarning(mapObject,jedis)          }        })      } catch {        case e =&gt; e.printStackTrace()      } finally {        JedisSingle.close(jedis)      } /*       rdd.foreachPartition(partion =&gt; {          var jedis: Jedis = null          try {            jedis = JedisSingle.getJedis(15)            while (partion.hasNext) {              val map = partion.next()              val mapObject = map.asInstanceOf[java.util.Map[String, Object]]              //开始比对              BlackRuleWarning.blackWarning(mapObject, jedis)            }          } catch {            case e =&gt; logError(null,e)          }finally {            JedisSingle.close(jedis)          }        })*/    })    ssc.start()    ssc.awaitTermination()  }}</code></pre><h4 id="9、执行"><a href="#9、执行" class="headerlink" title="9、执行"></a>9、执行</h4><p>spark-submit <code>--</code>master local[1] <code>--</code>num-executors 1 <code>--</code>driver-memory 300m <code>--</code>executor-memory 500m <code>--</code>executor-cores 1 <code>--</code>jars $(echo /usr/chl/spark7/jars/*.jar | tr ‘ ‘ ‘,’) <code>--</code>class com.hsiehchou.spark.streaming.kafka.warn.WarningStreamingTask /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</p><h4 id="10、截图"><a href="#10、截图" class="headerlink" title="10、截图"></a>10、截图</h4><p><img src="/medias/redis%E5%AE%89%E8%A3%85%E6%88%90%E5%8A%9F.PNG" alt="redis安装成功"></p><p><img src="/medias/%E9%A2%84%E8%AD%A6.PNG" alt="预警"></p><p><img src="/medias/RedisManager.PNG" alt="RedisManager"></p><p><img src="/medias/mysql-xz_rule.PNG" alt="mysql-xz_rule"></p><p><img src="/medias/%E5%8F%91%E9%80%81%E9%A2%84%E8%AD%A6.PNG" alt="发送预警"></p><h4 id="11、redis安装"><a href="#11、redis安装" class="headerlink" title="11、redis安装"></a>11、redis安装</h4><p>解压：tar -zxvf redis-3.0.5.tar.gz<br>cd redis-3.0.5/<br>make<br>make PREFIX=/opt/software/redis install</p><p><strong>redis-benchmark</strong> ： Redis提供的压力测试工具。模拟产生客户端的压力<br><strong>redis-check-aof</strong> ： 检查aof日志文件<br><strong>redis-check-dump</strong> ： 检查rdb文件<br><strong>redis-cli</strong> ： Redis客户端脚本<br><strong>redis-sentinel</strong> ： 哨兵<br><strong>redis-server</strong> ： Redis服务器脚本</p><p><strong>核心配置文件:redis.conf</strong><br>[root@hsiehchou202 redis-3.0.5]# cp redis.conf /opt/software/redis<br>[root@hsiehchou202 redis]# mkdir conf<br>[root@hsiehchou202 redis]# mv redis.conf conf/<br>[root@hsiehchou202 conf]# vi redis.conf</p><p>42行 <strong>daemonize yes //后台方式运行</strong><br>50行 <strong>port 6379</strong></p><p>启动<strong>redis ./bin/redis-server conf/redis.conf</strong></p><p><strong>检测是否启动好</strong><br>[root@hsiehchou202 redis]# <strong>bin/redis-server conf/redis.conf</strong></p><h3 id="十、Spark—kafka2hive"><a href="#十、Spark—kafka2hive" class="headerlink" title="十、Spark—kafka2hive"></a>十、Spark—kafka2hive</h3><h4 id="1、CDH启用Hive-on-spark"><a href="#1、CDH启用Hive-on-spark" class="headerlink" title="1、CDH启用Hive on spark"></a>1、CDH启用Hive on spark</h4><p><strong>设置 hive on spark 参数</strong><br>原来的HIVE执行引擎使用的hadoop的mapreduce，Hive on Spark 就是讲执行引擎换为spark 引擎</p><h4 id="2、hive配置文件"><a href="#2、hive配置文件" class="headerlink" title="2、hive配置文件"></a>2、hive配置文件</h4><p><strong>scala/com/hsiehchou/spark/streaming/kafka/kafka2hdfs/</strong></p><p><strong>HiveConfig.scala</strong></p><pre><code>package com.hsiehchou.spark.streaming.kafka.kafka2hdfsimport java.utilimport org.apache.commons.configuration.{CompositeConfiguration, ConfigurationException, PropertiesConfiguration}import org.apache.spark.Loggingimport org.apache.spark.sql.types.{StringType, StructField, StructType}import scala.collection.mutable.ArrayBufferimport scala.collection.JavaConversions._object HiveConfig extends Serializable with Logging {  //HIVE 文件根目录  var hive_root_path = &quot;/apps/hive/warehouse/external/&quot;  var hiveFieldPath = &quot;es/mapping/fieldmapping.properties&quot;  var config: CompositeConfiguration = null  //所有的表  var tables: util.List[_] = null  //表对应所有的字段映射,可以通过table名获取 这个table的所有字段  var tableFieldsMap: util.Map[String, util.HashMap[String, String]] = null  //StructType  var mapSchema: util.Map[String, StructType] = null  //建表语句  var hiveTableSQL: util.Map[String, String] = null  /**    * 主要就是创建mapSchema  和  hiveTableSQL    */  initParams()  def main(args: Array[String]): Unit = {  }  /**    * 初始化HIVE参数    */  def initParams(): Unit = {    //加载es/mapping/fieldmapping.properties 配置文件    config = HiveConfig.readCompositeConfiguration(hiveFieldPath)    println(&quot;==========================config====================================&quot;)    config.getKeys.foreach(key =&gt; {      println(key + &quot;:&quot; + config.getProperty(key.toString))    })    println(&quot;==========================tables====================================&quot;)    //wechat,mail,qq    tables = config.getList(&quot;tables&quot;)    tables.foreach(table =&gt; {      println(table)    })    var tables1 = config.getProperty(&quot;tables&quot;)    println(&quot;======================tableFieldsMap================================&quot;)    //(qq,{qq.imsi=string, qq.id=string, qq.send_message=string, qq.filename=string})    tableFieldsMap = HiveConfig.getKeysByType()    tableFieldsMap.foreach(x =&gt; {      println(x)    })    println(&quot;=========================mapSchema===================================&quot;)    mapSchema = HiveConfig.createSchema()    mapSchema.foreach(x =&gt; {//      val structType = x._2//      println(&quot;-----------&quot;)//      println(structType)//////      val names = structType.fieldNames//      names.foreach(field =&gt; {//        println(field)//      })      println(x)    })    println(&quot;=========================hiveTableSQL===================================&quot;)    hiveTableSQL = HiveConfig.getHiveTables()    hiveTableSQL.foreach(x =&gt; {      println(x)    })  }  /**    * 读取hive 字段配置文件    * @param path    * @return    */  def readCompositeConfiguration(path: String): CompositeConfiguration = {    logInfo(&quot;加载配置文件 &quot; + path)    //多配置工具    val compositeConfiguration = new CompositeConfiguration    try {      val configuration = new PropertiesConfiguration(path)      compositeConfiguration.addConfiguration(configuration)    } catch {      case e: ConfigurationException =&gt; {        logError(&quot;加载配置文件 &quot; + path + &quot;失败&quot;, e)      }    }    logInfo(&quot;加载配置文件&quot; + path + &quot;成功。 &quot;)    compositeConfiguration  }  /**    * 获取table-字段 对应关系    * 使用 util.Map[String,util.HashMap[String, String结构保存    * @return    */  def getKeysByType(): util.Map[String, util.HashMap[String, String]] = {    val map = new util.HashMap[String, util.HashMap[String, String]]()    println(&quot;__________________tables_____________________&quot;+tables)    //wechat, mail, qq    val iteratorTable = tables.iterator()    //对每个表进行遍历    while (iteratorTable.hasNext) {      //使用一个MAP保存一种对应关系      val fieldMap = new util.HashMap[String, String]()      //获取一个表      val table: String = iteratorTable.next().toString      //获取这个表的所有字段      val fields = config.getKeys(table)      //获取通用字段  这里暂时没有      val commonKeys: util.Iterator[String] = config.getKeys(&quot;common&quot;).asInstanceOf[util.Iterator[String]]      //将通用字段放到map结构中去      while (commonKeys.hasNext) {        val key = commonKeys.next()        fieldMap.put(key.replace(&quot;common&quot;, table), config.getString(key))      }      //将每种表的私有字段放到map中去      while (fields.hasNext) {        val field = fields.next().toString        fieldMap.put(field, config.getString(field))        println(&quot;__________________field_____________________&quot;+&quot;\n&quot;+field)      }      map.put(table, fieldMap)    }    map  }  /**    * 构建建表语句    * 例如CREATE external TABLE IF NOT EXISTS qq (imei string,imsi string,longitude string,latitude string,phone_mac string,device_mac string,device_number string,collect_time string,username string,phone string,object_username string,send_message string,accept_message string,message_time string,id string,table string,filename string,absolute_filename string)    * @return    */  def getHiveTables(): util.Map[String, String] = {    val hiveTableSqlMap: util.Map[String, String] = new util.HashMap[String, String]()    //获取没中数据的建表语句    tables.foreach(table =&gt; {      var sql: String = s&quot;CREATE external TABLE IF NOT EXISTS ${table} (&quot;      val tableFields = config.getKeys(table.toString)      tableFields.foreach(tableField =&gt; {        //qq.imsi=string, qq.id=string, qq.send_message=string        val fieldType = config.getProperty(tableField.toString)        val field = tableField.toString.split(&quot;\\.&quot;)(1)        sql = sql + field        fieldType match {          //就是将配置中的类型映射为HIVE 建表语句中的类型          case &quot;string&quot; =&gt; sql = sql + &quot; string,&quot;          case &quot;long&quot; =&gt; sql = sql + &quot; string,&quot;          case &quot;double&quot; =&gt; sql = sql + &quot; string,&quot;          case _ =&gt; println(&quot;Nothing Matched!!&quot; + fieldType)        }      })      sql = sql.substring(0, sql.length - 1)      //sql = sql + s&quot;)STORED AS PARQUET location &#39;${hive_root_path}${table}&#39;&quot;      sql = sql + s&quot;) partitioned by(year string,month string,day string) STORED AS PARQUET &quot; + s&quot;location &#39;${hive_root_path}${table}&#39;&quot;      hiveTableSqlMap.put(table.toString, sql)    })    hiveTableSqlMap  }  /**    * 使用tableFieldsMap    * 对每种类型数据创建对应的Schema    * @return    */  def createSchema(): util.Map[String, StructType] = {    // schema  表结构    /*   CREATE TABLE `warn_message` (         //arrayStructType         `id` int(11) NOT NULL AUTO_INCREMENT,         `alarmRuleid` varchar(255) DEFAULT NULL,         `alarmType` varchar(255) DEFAULT NULL,         `sendType` varchar(255) DEFAULT NULL,         `sendMobile` varchar(255) DEFAULT NULL,         `sendEmail` varchar(255) DEFAULT NULL,         `sendStatus` varchar(255) DEFAULT NULL,         `senfInfo` varchar(255) CHARACTER SET utf8 DEFAULT NULL,         `hitTime` datetime DEFAULT NULL,         `checkinTime` datetime DEFAULT NULL,         `isRead` varchar(255) DEFAULT NULL,         `readAccounts` varchar(255) DEFAULT NULL,         `alarmaccounts` varchar(255) DEFAULT NULL,         `accountid` varchar(11) DEFAULT NULL,         PRIMARY KEY (`id`)       ) ENGINE=MyISAM AUTO_INCREMENT=528 DEFAULT CHARSET=latin1;*/    val mapStructType: util.Map[String, StructType] = new util.HashMap[String, StructType]()    for (table &lt;- tables) {      //通过tableFieldsMap 拿到这个表的所有字段      val tableFields = tableFieldsMap.get(table)      //对这个字段进行遍历      val keyIterator = tableFields.keySet().iterator()      //创建ArrayBuffer      var arrayStructType = ArrayBuffer[StructField]()      while (keyIterator.hasNext) {        val key = keyIterator.next()        val value = tableFields.get(key)        //将key拆分 获取 &quot;.&quot;后面的部分作为数据字段        val field = key.split(&quot;\\.&quot;)(1)        value match {          /* case &quot;string&quot; =&gt; arrayStructType += StructField(field, StringType, true)           case &quot;long&quot;   =&gt; arrayStructType += StructField(field, LongType, true)           case &quot;double&quot;   =&gt; arrayStructType += StructField(field, DoubleType, true)*/          case &quot;string&quot; =&gt; arrayStructType += StructField(field, StringType, true)          case &quot;long&quot; =&gt; arrayStructType += StructField(field, StringType, true)          case &quot;double&quot; =&gt; arrayStructType += StructField(field, StringType, true)          case _ =&gt; println(&quot;Nothing Matched!!&quot; + value)        }      }      val schema = StructType(arrayStructType)      mapStructType.put(table.toString, schema)    }    mapStructType  }}</code></pre><h4 id="3、kafka写hdfs和创建hive表"><a href="#3、kafka写hdfs和创建hive表" class="headerlink" title="3、kafka写hdfs和创建hive表"></a>3、kafka写hdfs和创建hive表</h4><p><strong>Kafka2HiveTest.scala</strong> </p><pre><code>package com.hsiehchou.spark.streaming.kafka.kafka2hdfsimport java.utilimport com.hsiehchou.hdfs.HdfsAdminimport com.hsiehchou.hive.HiveConfimport com.hsiehchou.spark.common.{SparkContextFactory}import com.hsiehchou.spark.streaming.kafka.Spark_Kafka_ConfigUtilimport com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming.kafkaConfigimport org.apache.hadoop.fs.Pathimport org.apache.spark.{Logging}import org.apache.spark.rdd.RDDimport org.apache.spark.sql.hive.HiveContextimport org.apache.spark.sql.{DataFrame, Row, SaveMode}import org.apache.spark.sql.types.StructTypeimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.kafka.KafkaManagerimport scala.collection.JavaConversions._object Kafka2HiveTest extends Serializable with Logging{  val topics = &quot;chl_test7&quot;.split(&quot;,&quot;)  //获取所有数据类型  //获取所有数据的Schema  def main(args: Array[String]): Unit = {    //val ssc = SparkContextFactory.newSparkLocalStreamingContext(&quot;XZ_kafka2es&quot;, java.lang.Long.valueOf(10),1)    val ssc = SparkContextFactory.newSparkStreamingContext(&quot;Kafka2HiveTest&quot;, java.lang.Long.valueOf(10))    //1.创建HIVE表  hiveSQL已經創建好了    val sc = ssc.sparkContext    val hiveContext: HiveContext = HiveConf.getHiveContext(sc)    hiveContext.setConf(&quot;spark.sql.parquet.mergeSchema&quot;, &quot;true&quot;)    createHiveTable(hiveContext)    //kafka拿到流数据    val kafkaDS = new KafkaManager(Spark_Kafka_ConfigUtil                                    .getKafkaParam(kafkaConfig.getProperty(&quot;metadata.broker.list&quot;),                                      &quot;Kafka2HiveTest&quot;))                                    .createJsonToJMapStringDirectStreamWithOffset(ssc, topics.toSet)                                    .persist(StorageLevel.MEMORY_AND_DISK)    HiveConfig.tables.foreach(table=&gt;{      //过滤出单一数据类型(获取和table相同类型的所有数据)       val tableDS = kafkaDS.filter(x =&gt; {table.equals(x.get(&quot;table&quot;))})      //获取数据类型的schema 表结构      val schema = HiveConfig.mapSchema.get(table)      //获取这个表的所有字段      val schemaFields: Array[String] = schema.fieldNames      tableDS.foreachRDD(rdd=&gt;{        //TODO 数据写入HDFS        /* val sc = rdd.sparkContext        val hiveContext = HiveConf.getHiveContext(sc)        hiveContext.sql(s&quot;USE DEFAULT&quot;)*/        //将RDD转为DF   原因：要加字段描述，写比较方便        val tableDF = rdd2DF(rdd,schemaFields,hiveContext,schema)        //多种数据一起处理        val path_all = s&quot;hdfs://hadoop1:8020${HiveConfig.hive_root_path}${table}&quot;        val exists = HdfsAdmin.get().getFs.exists(new Path(path_all))        //2.写到HDFS   不管存不存在我们都要把数据写入进去 通过追加的方式        //每10秒写一次，写一次会生成一个文件        tableDF.write.mode(SaveMode.Append).parquet(path_all)        //3.加载数据到HIVE        if (!exists) {          //如果不存在 进行首次加载          System.out.println(&quot;===================开始加载数据到分区=============&quot;)          hiveContext.sql(s&quot;ALTER TABLE ${table} LOCATION &#39;${path_all}&#39;&quot;)        }      })    })    ssc.start()    ssc.awaitTermination()  }  /**    * 创建HIVE表    * @param hiveContext    */  def createHiveTable(hiveContext: HiveContext): Unit ={    val keys = HiveConfig.hiveTableSQL.keySet()    keys.foreach(key=&gt;{      val sql = HiveConfig.hiveTableSQL.get(key)      //通过hiveContext 和已经创建好的SQL语句去创建HIVE表      hiveContext.sql(sql)      println(s&quot;创建表${key}成功&quot;)    })  }  /**    * 将RDD转为DF    * @param rdd    * @param schemaFields    * @param hiveContext    * @param schema    * @return    */  def rdd2DF(rdd:RDD[util.Map[String,String]],             schemaFields: Array[String],             hiveContext:HiveContext,             schema:StructType): DataFrame ={      //将RDD[Map[String,String]]转为RDD[ROW]      val rddRow = rdd.map(recourd =&gt; {        val listRow: util.ArrayList[Object] = new util.ArrayList[Object]()          for (schemaField &lt;- schemaFields) {            listRow.add(recourd.get(schemaField))          }          Row.fromSeq(listRow)          //所有分区合并成一个      }).repartition(1)    //构建DF    //def createDataFrame(rowRDD: RDD[Row], schema: StructType)    val typeDF = hiveContext.createDataFrame(rddRow, schema)    typeDF  }}</code></pre><h4 id="4、Kafka2HiveTest-执行"><a href="#4、Kafka2HiveTest-执行" class="headerlink" title="4、Kafka2HiveTest 执行"></a>4、Kafka2HiveTest 执行</h4><p>spark-submit <code>--</code>master local[1] <code>--</code>num-executors 1 <code>--</code>driver-memory 300m <code>--</code>executor-memory 500m <code>--</code>executor-cores 1 <code>--</code>jars $(echo /usr/chl/spark7/jars/*.jar | tr ‘ ‘ ‘,’) <code>--</code>class com.hsiehchou.spark.streaming.kafka.kafka2hdfs.Kafka2HiveTest /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</p><p><img src="/medias/%E5%AD%98%E5%88%B0hdfs%E4%B8%AD.PNG" alt="存到hdfs中"></p><p><img src="/medias/hive%E6%9F%A5%E8%AF%A21.PNG" alt="hive查询1"></p><h4 id="5、xz-bigdata-spark-src-java"><a href="#5、xz-bigdata-spark-src-java" class="headerlink" title="5、xz_bigdata_spark/src/java/"></a>5、xz_bigdata_spark/src/java/</h4><p><strong>com/hsiehchou/hdfs</strong><br><strong>HdfsAdmin.java—HDFS 文件操作类</strong></p><pre><code>package com.hsiehchou.hdfs;import com.hsiehchou.common.adjuster.StringAdjuster;import com.hsiehchou.common.file.FileCommon;import com.google.common.base.Preconditions;import com.google.common.collect.Lists;import org.apache.commons.io.IOUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.*;import org.apache.log4j.Logger;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStream;import java.io.InputStreamReader;import java.lang.reflect.Array;import java.util.Collection;import java.util.List;/** * HDFS 文件操作类 */public class HdfsAdmin {    private static Logger LOG;    private static final String HDFS_SITE = &quot;/hadoop/hdfs-site.xml&quot;;    private static final String CORE_SITE = &quot;/hadoop/core-site.xml&quot;;    private volatile static HdfsAdmin hdfsAdmin;    private  FileSystem fs;    private HdfsAdmin(Configuration conf, Logger logger){        try {            if(conf == null) conf = newConf();            conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://hadoop1:8020&quot;);            fs = FileSystem.get(conf);        } catch (IOException e) {            LOG.error(&quot;获取 hdfs的FileSystem出现异常。&quot;, e);        }        Preconditions.checkNotNull(fs, &quot;没有获取到可用的Hdfs的FileSystem&quot;);        this.LOG = logger;        if(this.LOG == null)            this.LOG = Logger.getLogger(HdfsAdmin.class);    }    private Configuration newConf(){        Configuration conf = new Configuration();        if(FileCommon.exist(HDFS_SITE)) conf.addResource(HDFS_SITE);        if(FileCommon.exist(CORE_SITE)) conf.addResource(CORE_SITE);        return conf;    }    public static HdfsAdmin get(){        return get(null);    }    /**     * 获取hdfsAdmin     * @param logger     * @return     */    public static HdfsAdmin get(Logger logger){        if(hdfsAdmin == null){            synchronized (HdfsAdmin.class){                if(hdfsAdmin == null) hdfsAdmin = new HdfsAdmin(null, logger);            }        }        return hdfsAdmin;    }    public static HdfsAdmin get(Configuration conf, Logger logger){        if(hdfsAdmin == null){            synchronized (HdfsAdmin.class){                if(hdfsAdmin == null) hdfsAdmin = new HdfsAdmin(conf, logger);            }        }        return hdfsAdmin;    }    public FileStatus getFileStatus(String dir) {        FileStatus fileStatus = null;        try {            fileStatus = fs.getFileStatus(new Path(dir));        } catch (IOException e) {            LOG.error(String.format(&quot;获取文件 %s信息失败。&quot;, dir), e);        }        return fileStatus;    }    public void createFile(String dst , byte[] contents){        //目标路径        Path dstPath = new Path(dst);        //打开一个输出流        FSDataOutputStream outputStream;        try {            outputStream = fs.create(dstPath);            outputStream.write(contents);            outputStream.flush();            outputStream.close();        } catch (IOException e) {            LOG.error(String.format(&quot;创建文件 %s 失败。&quot;, dst), e);        }        LOG.info(String.format(&quot;文件: %s 创建成功！&quot;, dst));    }    //上传本地文件    public void uploadFile(String src,String dst){        //原路径        Path srcPath = new Path(src);        //目标路径        Path dstPath = new Path(dst);        //调用文件系统的文件复制函数,前面参数是指是否删除原文件，true为删除，默认为false        try {            fs.copyFromLocalFile(false,srcPath, dstPath);        } catch (IOException e) {            LOG.error(String.format(&quot;上传文件 %s 到 %s 失败。&quot;, src, dst), e);        }        //打印文件路径        LOG.info(String.format(&quot;上传文件 %s 到 %s 完成。&quot;, src, dst));    }    public void downloadFile(String src , String dst){        Path dstPath = new Path(dst) ;        try {            fs.copyToLocalFile(false, new Path(src), dstPath);        } catch (IOException e) {            LOG.error(String.format(&quot;下载文件 %s 到 %s 失败。&quot;, src, dst), e);        }        LOG.info(String.format(&quot;下载文件 %s 到 %s 完成&quot;, src, dst));    }    //文件重命名    public void rename(String oldName,String newName){        Path oldPath = new Path(oldName);        Path newPath = new Path(newName);        boolean isok = false;        try {            isok = fs.rename(oldPath, newPath);        } catch (IOException e) {            LOG.error(String.format(&quot;重命名文件 %s 为 %s 失败。&quot;, oldName, newName), e);        }        if(isok){            LOG.info(String.format(&quot;重命名文件 %s 为 %s 完成。&quot;, oldName, newName));        }else{            LOG.error(String.format(&quot;重命名文件 %s 为 %s 失败。&quot;, oldName, newName));        }    }    public void delete(String path){        delete(path, true);    }    //删除文件    public void delete(String path, boolean recursive){        Path deletePath = new Path(path);        boolean isok = false;        try {            isok = fs.delete(deletePath, recursive);        } catch (IOException e) {            LOG.error(String.format(&quot;删除文件 %s 失败。&quot;, path), e);        }        if(isok){            LOG.info(String.format(&quot;删除文件 %s 完成。&quot;, path));        }else{            LOG.error(String.format(&quot;删除文件 %s 失败。&quot;, path));        }    }    //创建目录    public void mkdir(String path){        Path srcPath = new Path(path);        boolean isok = false;        try {            isok = fs.mkdirs(srcPath);        } catch (IOException e) {            LOG.error(String.format(&quot;创建目录 %s 失败。&quot;, path), e);        }        if(isok){            LOG.info(String.format(&quot;创建目录 %s 完成。&quot;, path));        }else{            LOG.error(String.format(&quot;创建目录 %s 失败。&quot;, path));        }    }    //读取文件的内容    public InputStream readFile(String filePath){        Path srcPath = new Path(filePath);        InputStream in = null;        try {           in = fs.open(srcPath);        } catch (IOException e) {            LOG.error(String.format(&quot;读取文件  %s 失败。&quot;, filePath), e);        }        return in;    }    public &lt;T&gt; void readFile(String filePath, StringAdjuster&lt;T&gt; adjuster, Collection&lt;T&gt; result){        InputStream inputStream = readFile(filePath);        if(inputStream != null){            InputStreamReader reader = new InputStreamReader(inputStream);            BufferedReader bufferedReader = new BufferedReader(reader);            String line;            try {                T t;                while((line = bufferedReader.readLine()) != null){                    t = adjuster.doAdjust(line);                    if(t != null)result.add(t);                }            } catch (IOException e) {                LOG.error(String.format(&quot;利用缓冲流读取文件  %s 失败。&quot;, filePath), e);            }finally {                IOUtils.closeQuietly(bufferedReader);                IOUtils.closeQuietly(reader);                IOUtils.closeQuietly(inputStream);            }        }    }    public List&lt;String&gt; readLines(String filePath){        return readLines(filePath, &quot;UTF-8&quot;);    }    public  List&lt;String&gt; readLines(String filePath, String encoding){        InputStream inputStream = readFile(filePath);        List&lt;String&gt; lines = null;        if(inputStream != null) {            try {                lines = IOUtils.readLines(inputStream, encoding);            } catch (IOException e) {                LOG.error(String.format(&quot;按行读取文件 %s 失败。&quot;, filePath), e);            }finally {                IOUtils.closeQuietly(inputStream);            }        }        return lines;    }    public List&lt;FileStatus&gt; findNewFileOrDirInDir(String dir, HdfsFileFilter filter,                                                final boolean onlyFile, final boolean onlyDir){       return findNewFileOrDirInDir(dir, filter, onlyFile, onlyDir, false);    }    public List&lt;FileStatus&gt; findNewFileOrDirInDir(String dir, HdfsFileFilter filter,                          final boolean onlyFile, final boolean onlyDir, boolean recursive){        if(onlyFile &amp;&amp; onlyDir){            FileStatus fileStatus = getFileStatus(dir);            if(fileStatus == null)return Lists.newArrayList();            if(isAccepted(fileStatus,filter)){                return Lists.newArrayList(fileStatus);            }            return Lists.newArrayList();        }       if(onlyFile){           return findNewFileInDir(dir, filter, recursive);       }       if(onlyDir){           return findNewDirInDir(dir, filter, recursive);       }       return Lists.newArrayList();    }    /**     * 查找一个文件夹中 新建的目录     * @param dir     * @param filter     * @return     */    public List&lt;FileStatus&gt; findNewDirInDir(String dir, HdfsFileFilter filter){        return findNewDirInDir(new Path(dir), filter, false);    }    public List&lt;FileStatus&gt; findNewDirInDir(Path path, HdfsFileFilter filter){        return findNewDirInDir(path, filter, false);    }    public List&lt;FileStatus&gt; findNewDirInDir(String dir, HdfsFileFilter filter, boolean recursive){        return findNewDirInDir(new Path(dir), filter, recursive);    }    public List&lt;FileStatus&gt; findNewDirInDir(Path path, HdfsFileFilter filter, boolean recursive){        FileStatus[] files = null;        try {            files = fs.listStatus(path);        } catch (IOException e) {            LOG.error(String.format(&quot;获取目录 %s下的文件列表失败。&quot;, path), e);        }        if(files == null)return Lists.newArrayList();        List&lt;FileStatus&gt; paths = Lists.newArrayList();        List&lt;String&gt; res = Lists.newArrayList();        for(FileStatus fileStatus : files){            if (fileStatus.isDirectory()) {                if (isAccepted(fileStatus, filter)) {                    paths.add(fileStatus);                    res.add(fileStatus.getPath().toString());                }else if(recursive){                    paths.addAll(findNewDirInDir(fileStatus.getPath(), filter, recursive));                }            }        }        LOG.info(String.format(&quot;从目录%s 找到满足条件%s 有如下 %s 个文件： %s&quot;,                path, filter,res.size(), res));        return paths;    }    /**     * 查找一个文件夹中 新建的文件     * @param dir     * @param filter     * @return     */    public List&lt;FileStatus&gt; findNewFileInDir(String dir, HdfsFileFilter filter){        return  findNewFileInDir(new Path(dir), filter, false);    }    public List&lt;FileStatus&gt; findNewFileInDir(String dir, HdfsFileFilter filter, boolean recursive){        return  findNewFileInDir(new Path(dir), filter, recursive);    }    public List&lt;FileStatus&gt; findNewFileInDir(Path path, HdfsFileFilter filter){        return  findNewFileInDir(path, filter, false);    }    public List&lt;FileStatus&gt; findNewFileInDir(Path path, HdfsFileFilter filter, boolean recursive){        FileStatus[] files = null;        try {            files = fs.listStatus(path);        } catch (IOException e) {            LOG.error(String.format(&quot;获取目录 %s下的文件列表失败。&quot;, path), e);        }        if(files == null)return Lists.newArrayList();        List&lt;FileStatus&gt; paths = Lists.newArrayList();        List&lt;String&gt; res = Lists.newArrayList();        for(FileStatus fileStatus : files){            if (fileStatus.isFile()) {                if (isAccepted(fileStatus, filter)) {                    paths.add(fileStatus);                    res.add(fileStatus.getPath().toString());                }            }else if(recursive){                paths.addAll(findNewFileInDir(fileStatus.getPath(), filter, recursive));            }        }        LOG.info(String.format(&quot;从目录%s 找到满足条件%s 有如下 %s 个文件： %s&quot;, path, filter,res.size(), res));        return paths;    }    private boolean isAccepted(String file, HdfsFileFilter filter) {        if(filter == null) return true;        FileStatus fileStatus = getFileStatus(file);        if(fileStatus == null)return false;        return isAccepted(fileStatus, filter);    }    private boolean isAccepted(FileStatus fileStatus, HdfsFileFilter filter) {        return  filter == null ? true : filter.filter(fileStatus);    }    public long getModificationTime(Path path){        try {            FileStatus status = fs.getFileStatus(path);            return status.getModificationTime();        } catch (IOException e) {            LOG.error(String.format(&quot;获取路径 %s信息失败。&quot;, path), e);        }        return -1L;    }    public FileSystem getFs() {        return fs;    }    public static void main(String[] args) throws Exception {        // HdfsAdmin hdfsAdmin = HdfsAdmin.get();       // hdfsAdmin.mkdir(&quot;hdfs://hdp04.ultiwill.com:8020/test1111&quot;);        //System.out.println(hdfsAdmin.getFs().exists(new Path(&quot;hdfs://hdp04.ultiwill.com:8020/test&quot;)));        //hdfsAdmin.delete(&quot;hdfs://hdp04.ultiwill.com:8020/test1111&quot;);        //System.out.println(&quot;hdfsAdmin = &quot; + );       // List&lt;FileStatus&gt; status = hdfsAdmin.findNewDirInDir(&quot;hdfs://hdp04.ultiwill.com:50070/hdp&quot;, null);        //System.out.println(&quot;status = &quot; + status.size());    }}</code></pre><p><strong>HdfsFileFilter.java</strong></p><pre><code>package com.hsiehchou.hdfs;import com.hsiehchou.common.filter.Filter;import org.apache.hadoop.fs.FileStatus;public abstract class HdfsFileFilter implements Filter&lt;FileStatus&gt; {}</code></pre><p><strong>com/hsiehchou/hive</strong><br><strong>HiveConf.java</strong></p><pre><code>package com.hsiehchou.hive;import org.apache.hadoop.conf.Configuration;import org.apache.spark.SparkContext;import org.apache.spark.sql.hive.HiveContext;import java.util.Iterator;import java.util.Map;public class HiveConf {    //private static String DEFUALT_CONFIG = &quot;spark/hive/hive-server-config&quot;;    private static HiveConf hiveConf;    private static HiveContext hiveContext;    private HiveConf(){    }    public static HiveConf getHiveConf(){        if(hiveConf==null){            synchronized (HiveConf.class){                if(hiveConf==null){                    hiveConf=new  HiveConf();                }            }        }        return hiveConf;    }    public static HiveContext getHiveContext(SparkContext sparkContext){        if(hiveContext==null){            synchronized (HiveConf.class){                if(hiveContext==null){                    hiveContext = new  HiveContext(sparkContext);                    Configuration conf = new Configuration();                    conf.addResource(&quot;spark/hive/hive-site.xml&quot;);                    Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator = conf.iterator();                    while (iterator.hasNext()) {                        Map.Entry&lt;String, String&gt; next = iterator.next();                        hiveContext.setConf(next.getKey(), next.getValue());                    }                    hiveContext.setConf(&quot;spark.sql.parquet.mergeSchema&quot;, &quot;true&quot;);                }            }        }        return hiveContext;    }}</code></pre><h4 id="6、小文件合并"><a href="#6、小文件合并" class="headerlink" title="6、小文件合并"></a>6、小文件合并</h4><p><strong>scala/com/hsiehchou/spark/streaming/kafka/kafka2hdfs</strong></p><p><strong>CombineHdfs.scala—合并HDFS小文件任务</strong></p><pre><code>package com.hsiehchou.spark.streaming.kafka.kafka2hdfsimport com.hsiehchou.hdfs.HdfsAdminimport com.hsiehchou.spark.common.SparkContextFactoryimport org.apache.hadoop.fs.{FileSystem, FileUtil, Path}import org.apache.spark.Loggingimport org.apache.spark.sql.{SQLContext, SaveMode}import scala.collection.JavaConversions._/**  * 合并HDFS小文件任务  */object CombineHdfs extends Serializable with Logging{  def main(args: Array[String]): Unit = {    //  val sparkContext = SparkContextFactory.newSparkBatchContext(&quot;CombineHdfs&quot;)    val sparkContext = SparkContextFactory.newSparkLocalBatchContext(&quot;CombineHdfs&quot;)    //创建一个 sparkSQL    val sqlContext: SQLContext = new SQLContext(sparkContext)    //遍历表 就是遍历HIVE表    HiveConfig.tables.foreach(table=&gt;{      //获取HDFS文件目录      //apps/hive/warehouse/external/mail类似      //apps/hive/warehouse/external/mail      val table_path =s&quot;${HiveConfig.hive_root_path}$table&quot;       //通过sparkSQL 加载 这些目录的文件      val tableDF = sqlContext.read.load(table_path)      //先获取原来数据种的所有文件  HDFS文件 API      val fileSystem:FileSystem = HdfsAdmin.get().getFs      //通过globStatus 获取目录下的正则匹配文件      //fileSystem.listFiles()      val arrayFileStatus = fileSystem.globStatus(new Path(table_path+&quot;/part*&quot;))      //stat2Paths将文件状态转为文件路径   这个文件路径是用来删除的      val paths = FileUtil.stat2Paths(arrayFileStatus)      //写入合并文件   //repartition 需要根据生产中实际情况去定义      tableDF.repartition(1).write.mode(SaveMode.Append).parquet(table_path)      println(&quot;写入&quot; + table_path +&quot;成功&quot;)      //删除小文件      paths.foreach(path =&gt;{        HdfsAdmin.get().getFs.delete(path)        println(&quot;删除文件&quot; + path + &quot;成功&quot;)      })    })  }}</code></pre><h4 id="7、定时任务"><a href="#7、定时任务" class="headerlink" title="7、定时任务"></a>7、定时任务</h4><p><strong>命令行输入：crontab -e</strong></p><p><strong>内容：</strong><br><code>0 1 * * *</code> spark-submit <code>--</code>master local[1] <code>--</code>num-executors 1 <code>--</code>driver-memory 300m <code>--</code>executor-memory 500m <code>--</code>executor-cores 1 <code>--</code>jars $(echo /usr/chl/spark7/jars/*.jar | tr ‘ ‘ ‘,’) <code>--</code>class com.hsiehchou.spark.streaming.kafka.kafka2hdfs.CombineHdfs /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</p><p><strong>说明：</strong><br><code>* * * * *</code> 执行的任务</p><table><thead><tr><th align="center">项目</th><th align="center">含义</th><th align="center">范围</th></tr></thead><tbody><tr><td align="center">第一个“*”</td><td align="center">一小时当中的第几分钟（分）</td><td align="center">0-59</td></tr><tr><td align="center">第二个“*”</td><td align="center">一天当中的第几小时（时）</td><td align="center">0-23</td></tr><tr><td align="center">第三个“*”</td><td align="center">一个月当中的第几天（天）</td><td align="center">1-31</td></tr><tr><td align="center">第四个“*”</td><td align="center">一年当中的第几月（月）</td><td align="center">1-12</td></tr><tr><td align="center">第五个“*”</td><td align="center">一周当中的星期几（周）</td><td align="center">0-7（0和7都代表星期日）</td></tr></tbody></table><h4 id="8、合并小文件截图"><a href="#8、合并小文件截图" class="headerlink" title="8、合并小文件截图"></a>8、合并小文件截图</h4><p><img src="/medias/%E5%90%88%E5%B9%B6%E5%B0%8F%E6%96%87%E4%BB%B6.PNG" alt="合并小文件">        </p><h4 id="9、hive命令"><a href="#9、hive命令" class="headerlink" title="9、hive命令"></a>9、hive命令</h4><p>show tales;</p><p>hdfs dfs -ls /apps/hive/warehouse/external</p><p>hdfs dfs -rm -r /apps/hive/warehouse/external/mail</p><p>drop table mail;</p><p>desc qq;</p><p>select * from qq limit 1;<br>select count(*) from qq;</p><p>/usr/bin下面的启动zookeeper客户端<br>zookeeper-client</p><p>删除zookeeper里面的消费者数据<br>rmr /consumers/WarningStreamingTask2/offsets</p><p>rmr /consumers/Kafka2HiveTest/offsets</p><p>rmr /consumers/DataRelationStreaming1/offsets</p><h3 id="十一、Spark—Kafka2Hbase"><a href="#十一、Spark—Kafka2Hbase" class="headerlink" title="十一、Spark—Kafka2Hbase"></a>十一、Spark—Kafka2Hbase</h3><h4 id="1、数据关联"><a href="#1、数据关联" class="headerlink" title="1、数据关联"></a>1、数据关联</h4><p><strong>（1）为什么需要关联</strong><br><strong>问题</strong>：我们不能充分了解数据之间的关联关系。</p><p><strong>公司中应用的非常多</strong><br><strong>离线关联</strong>，传通数据 mysql 通过关联字段去关联。<br>但是，如果数据量非常大，关联表非常多。处理不了。</p><p>数据零散，只能从单一维度去看数据，看的面比较窄。<br>如果需要从多个维度分析，关联成本比较大。</p><p>建立数据之间的关联关系，实现<strong>关联查询</strong>的<strong>毫秒级响应</strong>；<br>另一个方面，可以为数据挖掘，机器学习<strong>提供训练数据</strong>。</p><p>后面进行机器学习的时候，都需要从<strong>多维度</strong>对数据进行<strong>分析和建模</strong>。</p><p><strong>（2）HBASE 只要rowkey一样，那么他们就是一条数据</strong><br>QQ<br>aa-aa-aa-aa-aa-aa 666666</p><p>微信<br>aa-aa-aa-aa-aa-aa weixin</p><p>邮箱<br>aa-aa-aa-aa-aa-aa <a href="mailto:666666@qq.com">666666@qq.com</a></p><p><strong>（3）如何关联</strong><br>一对一的情况 :<br><a href="https://blog.csdn.net/shujuelin/article/details/83657485" target="_blank" rel="noopener">https://blog.csdn.net/shujuelin/article/details/83657485</a></p><p><strong>使用HBASE写入特性</strong><br>比如 MAC1  1789932321<br> MAC1  <a href="mailto:88888@qq.com">88888@qq.com</a><br> MAC1  88888 </p><p>一对多的情况怎么处理<br> <strong>使用多版本</strong><br>aa-aa-aa-aa-aa-aa 666666<br>aa-aa-aa-aa-aa-aa 777777</p><p><strong>（4）一对多</strong><br>使用多版本存一堆多的关系<br>多版本 插入了一个777777 一个版本<br>再插入一个777777   一个版本</p><p>所以需要自定义版本号 确定版本唯一<br>通过 “888888”.hashCode() &amp; Integer.MAX_VALUE</p><p><strong>（5）如果实现hbase多字段查询</strong><br>往主关联表 test:relation 里面写入数据  rowkey=&gt;aa-aa-aa-aa-aa-aa version=&gt;1637094383 类型phone_mac value=&gt;aa-aa-aa-aa-aa-aa<br>往二级索表 test:phone_mac里面写入数据  rowkey=&gt;aa-aa-aa-aa-aa-aa version=&gt;1736188717 value=&gt;aa-aa-aa-aa-aa-aa</p><p><img src="/medias/Hbase%E5%85%B3%E8%81%94.PNG" alt="Hbase关联"></p><p>查询不直接查主关联表，因为查询字段不在主键里面，没办法查或者性能非常低下。</p><p>查询是分2步rowkey查询<br>第一步， 通过查询字段取对应的二级索引表里面去找主关联表的ROWKEY<br>第二步， 通过主关联表的ROWKEY 获取HBASE中的全量数据</p><p> WIFI 已经入库的情况下，手机号也必须已经入库了，才能找到<br> 加入WIFI的手机号还没有入库</p><p>如果是基础数据先过来   没有mac 没有主键</p><table><thead><tr><th align="center">Card</th><th align="center">phone</th></tr></thead><tbody><tr><td align="center">400000000000000</td><td align="center">18612345678</td></tr></tbody></table><p>关联</p><table><thead><tr><th align="center">Phone</th><th align="center">value （识别这个字段是身份证才可以）</th></tr></thead><tbody><tr><td align="center">18612345678</td><td align="center">400000000000000</td></tr></tbody></table><p>1）因为检索的时候都是通过索引表直接找MAC，混入了身份证<br>2）要进行一个合并</p><p><strong>（6）关联及二级索引示意</strong></p><p><img src="/medias/%E5%85%B3%E8%81%94%E5%8F%8A%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95%E7%A4%BA%E6%84%8F.PNG" alt="关联及二级索引示意"></p><p><img src="/medias/Hbase%E5%85%B3%E8%81%94%E8%A1%A8%E7%A4%BA%E6%84%8F%E5%9B%BE.PNG" alt="Hbase关联表示意图"></p><p><strong>（7）如果使用ES建立二级索引</strong></p><p><img src="/medias/%E4%BD%BF%E7%94%A8ES%E5%BB%BA%E7%AB%8B%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95.PNG" alt="使用ES建立二级索引"></p><p>如果hbase 里面有100个字段，存放的是全量信息，但是只有20个字段参与查询、检索，那么我们可以把这个20个字段单独提出来存放到es中，因为ES是对对字段，多条件查询非常灵活。所以我们可以先在ES中对条件进行检索，根据检索的结果拿到hbaSe的rowkey，然后再通过rowkey到hbase里面获取全量信息。      </p><p><strong>（8）Hbase 预分区</strong><br>主要是根据rowkey分布来进行预分区</p><p>分区主要是为了防止热点问题</p><p>relation表为例<br>这个表的rowkey 是不是就是 mac</p><p>phone_mac 都是以0-9  a-f开头的<br>device_mac 都是以0-9  a-z开头的<br>Hbase 是按字典序排序</p><p><strong>（9）自定义版本号</strong><br>通过这样的一个转换我们可以精确定位数据的多版本号，，然后可以根据版本号对数据进行多版本删除。<br>156511 aaaaaaaa</p><h4 id="2、DataRelationStreaming—数据关联"><a href="#2、DataRelationStreaming—数据关联" class="headerlink" title="2、DataRelationStreaming—数据关联"></a>2、DataRelationStreaming—数据关联</h4><p><strong>DataRelationStreaming.scala</strong></p><pre><code>package com.hsiehchou.spark.streaming.kafka.kafka2hbaseimport java.util.Propertiesimport com.hsiehchou.common.config.ConfigUtilimport com.hsiehchou.hbase.config.HBaseTableUtilimport com.hsiehchou.hbase.insert.HBaseInsertHelperimport com.hsiehchou.hbase.spilt.SpiltRegionUtilimport com.hsiehchou.spark.common.SparkContextFactoryimport com.hsiehchou.spark.streaming.kafka.Spark_Kafka_ConfigUtilimport org.apache.hadoop.hbase.client.Putimport org.apache.hadoop.hbase.util.Bytesimport org.apache.spark.Loggingimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.kafka.KafkaManagerobject DataRelationStreaming extends Serializable with Logging{  // 读取需要关联的配置文件字段  // phone_mac,phone,username,send_mail,imei,imsi  val relationFields = ConfigUtil.getInstance()    .getProperties(&quot;spark/relation.properties&quot;)    .get(&quot;relationfield&quot;)    .toString    .split(&quot;,&quot;)  def main(args: Array[String]): Unit = {    //初始化hbase表    //initRelationHbaseTable(relationFields)    val ssc = SparkContextFactory.newSparkLocalStreamingContext(&quot;DataRelationStreaming&quot;, java.lang.Long.valueOf(10),1)    //  val ssc = SparkContextFactory.newSparkStreamingContext(&quot;DataRelationStreaming&quot;, java.lang.Long.valueOf(10))    val kafkaConfig: Properties = ConfigUtil.getInstance().getProperties(&quot;kafka/kafka-server-config.properties&quot;)    val topics = &quot;chl_test7&quot;.split(&quot;,&quot;)    val kafkaDS = new KafkaManager(Spark_Kafka_ConfigUtil      .getKafkaParam(kafkaConfig.getProperty(&quot;metadata.broker.list&quot;),        &quot;DataRelationStreaming2&quot;))      .createJsonToJMapStringDirectStreamWithOffset(ssc, topics.toSet)      .persist(StorageLevel.MEMORY_AND_DISK)    kafkaDS.foreachRDD(rdd=&gt;{      rdd.foreachPartition(partion=&gt;{        //对partion进行遍历        while (partion.hasNext){          //获取每一条流数据          val map = partion.next()          //获取mac 主键          var phone_mac:String = map.get(&quot;phone_mac&quot;)          //获取所有关联字段 //phone_mac,phone,username,send_mail,imei,imsi          relationFields.foreach(relationFeild =&gt;{            //relationFields 是关联字段，需要进行关联处理的，所有判断            //map中是不是包含这个字段，如果包含的话，取出来进行处理            if(map.containsKey(relationFeild)){              //创建主关联，并遍历关联字段进行关联              val put = new Put(phone_mac.getBytes())              //取关联字段的值              //TODO  到这里  主关联表的 主键和值都有了  然后封装成PUT写入hbase主关联表就行了              val value = map.get(relationFeild)              //自定义版本号  通过 (表字段名 + 字段值 取hashCOde)              //因为值有可能是字符串，但是版本号必须是long类型，所以这里我们需要              //将字符串影射唯一数字，而且必须是正整数              val versionNum = (relationFeild+value).hashCode() &amp; Integer.MAX_VALUE              put.addColumn(&quot;cf&quot;.getBytes(), Bytes.toBytes(relationFeild),versionNum ,Bytes.toBytes(value.toString))              HBaseInsertHelper.put(&quot;test:relation&quot;,put)              println(s&quot;往主关联表 test:relation 里面写入数据  rowkey=&gt;${phone_mac} version=&gt;${versionNum} 类型${relationFeild} value=&gt;${value}&quot;)              // 建立二级索引              // 使用关联字段的值最为二级索引的rowkey              // 二级索引就是把这个字段的值作为索引表rowkey              // 把这个字段的mac做为索引表的值              val put_2 = new Put(value.getBytes())//把这个字段的值作为索引表rowkey              val table_name = s&quot;test:${relationFeild}&quot;//往索引表里面取写              //使用主表的rowkey  就是 取hash作为二级索引的版本号              val versionNum_2 = phone_mac.hashCode() &amp; Integer.MAX_VALUE              put_2.addColumn(&quot;cf&quot;.getBytes(), Bytes.toBytes(&quot;phone_mac&quot;),versionNum_2 ,Bytes.toBytes(phone_mac.toString))              HBaseInsertHelper.put(table_name,put_2)              println(s&quot;往二级索表 ${table_name}里面写入数据  rowkey=&gt;${value} version=&gt;${versionNum_2} value=&gt;${phone_mac}&quot;)            }          })        }      })    })    ssc.start()    ssc.awaitTermination()  }  def initRelationHbaseTable(relationFields:Array[String]): Unit ={    //初始化总关联表    val relation_table = &quot;test:relation&quot;    HBaseTableUtil.createTable(relation_table,      &quot;cf&quot;,      true,      -1,      100,      SpiltRegionUtil.getSplitKeysBydinct)    //HBaseTableUtil.deleteTable(relation_table)    //遍历所有关联字段，根据字段创建二级索引表    relationFields.foreach(field=&gt;{      val hbase_table = s&quot;test:${field}&quot;      HBaseTableUtil.createTable(hbase_table, &quot;cf&quot;, true, -1, 100, SpiltRegionUtil.getSplitKeysBydinct)      // HBaseTableUtil.deleteTable(hbase_table)    })  }}</code></pre><h4 id="3、com-hsiehchou-spark-streaming"><a href="#3、com-hsiehchou-spark-streaming" class="headerlink" title="3、com.hsiehchou.spark.streaming"></a>3、com.hsiehchou.spark.streaming</h4><p><strong>common/SparkContextFactory.scala</strong></p><pre><code>package com.hsiehchou.spark.commonimport org.apache.spark.streaming.{Seconds, StreamingContext}import org.apache.spark.{Accumulator, SparkContext}object SparkContextFactory {  def newSparkBatchContext(appName:String = &quot;sparkBatch&quot;) : SparkContext = {    val sparkConf = SparkConfFactory.newSparkBatchConf(appName)    new SparkContext(sparkConf)  }  def newSparkLocalBatchContext(appName:String = &quot;sparkLocalBatch&quot; , threads : Int = 2) : SparkContext = {    val sparkConf = SparkConfFactory.newSparkLoalConf(appName, threads)    sparkConf.set(&quot;&quot;,&quot;&quot;)    new SparkContext(sparkConf)  }  def getAccumulator(appName:String = &quot;sparkBatch&quot;) : Accumulator[Int] = {    val sparkConf = SparkConfFactory.newSparkBatchConf(appName)    val accumulator: Accumulator[Int] = new SparkContext(sparkConf).accumulator(0,&quot;&quot;)    accumulator  }  /**    * 创建本地流streamingContext    * @param appName             appName    * @param batchInterval      多少秒读取一次    * @param threads            开启多少个线程    * @return    */  def newSparkLocalStreamingContext(appName:String = &quot;sparkStreaming&quot; ,                                    batchInterval:Long = 30L ,                                    threads : Int = 4) : StreamingContext = {    val sparkConf =  SparkConfFactory.newSparkLocalConf(appName, threads)    // sparkConf.set(&quot;spark.streaming.receiver.maxRate&quot;,&quot;10000&quot;)    sparkConf.set(&quot;spark.streaming.kafka.maxRatePerPartition&quot;,&quot;1&quot;)    new StreamingContext(sparkConf, Seconds(batchInterval))  }  /**    * 创建集群模式streamingContext    * 这里不设置线程数，在submit中指定    * @param appName    * @param batchInterval    * @return    */  def newSparkStreamingContext(appName:String = &quot;sparkStreaming&quot; , batchInterval:Long = 30L) : StreamingContext = {    val sparkConf = SparkConfFactory.newSparkStreamingConf(appName)    new StreamingContext(sparkConf, Seconds(batchInterval))  }  def startSparkStreaming(ssc:StreamingContext){    ssc.start()      ssc.awaitTermination()      ssc.stop()  }}</code></pre><p><strong>streaming/kafka/Spark_Kafka_ConfigUtil.scala</strong></p><pre><code>package com.hsiehchou.spark.streaming.kafkaimport org.apache.spark.Loggingobject Spark_Kafka_ConfigUtil extends Serializable with Logging{  def getKafkaParam(brokerList:String,groupId : String): Map[String,String]={    val kafkaParam=Map[String,String](      &quot;metadata.broker.list&quot; -&gt; brokerList,      &quot;auto.offset.reset&quot; -&gt; &quot;smallest&quot;,      &quot;group.id&quot; -&gt; groupId,      &quot;refresh.leader.backoff.ms&quot; -&gt; &quot;1000&quot;,      &quot;num.consumer.fetchers&quot; -&gt; &quot;8&quot;)    kafkaParam  }}</code></pre><h4 id="4、com-hsiehchou-common-config-ConfigUtil"><a href="#4、com-hsiehchou-common-config-ConfigUtil" class="headerlink" title="4、com/hsiehchou/common/config/ConfigUtil"></a>4、com/hsiehchou/common/config/ConfigUtil</h4><p><strong>ConfigUtil.java</strong></p><pre><code>package com.hsiehchou.common.config;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.io.InputStream;import java.util.Properties;public class ConfigUtil {    private static Logger LOG = LoggerFactory.getLogger(ConfigUtil.class);    private static ConfigUtil configUtil;    public static ConfigUtil getInstance(){        if(configUtil == null){            configUtil = new ConfigUtil();        }        return configUtil;    }    public Properties getProperties(String path){        Properties properties = new Properties();        try {            LOG.info(&quot;开始加载配置文件&quot; + path);            InputStream insss = this.getClass().getClassLoader().getResourceAsStream(path);            properties = new Properties();            properties.load(insss);        } catch (IOException e) {            LOG.info(&quot;加载配置文件&quot; + path + &quot;失败&quot;);            LOG.error(null,e);        }        LOG.info(&quot;加载配置文件&quot; + path + &quot;成功&quot;);        System.out.println(&quot;文件内容：&quot;+properties);        return properties;    }    public static void main(String[] args) {        ConfigUtil instance = ConfigUtil.getInstance();        Properties properties = instance.getProperties(&quot;common/datatype.properties&quot;);        //Properties properties = instance.getProperties(&quot;spark/relation.properties&quot;);       // properties.get(&quot;relationfield&quot;);        System.out.println(properties);    }}</code></pre><h4 id="5、构建模块—xz-bigdata-hbase"><a href="#5、构建模块—xz-bigdata-hbase" class="headerlink" title="5、构建模块—xz_bigdata_hbase"></a>5、构建模块—xz_bigdata_hbase</h4><p><strong>pom.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_hbase&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_hbase&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;        &lt;hbase.version&gt;1.2.0&lt;/hbase.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;            &lt;version&gt;${hbase.version}-${cdh.version}&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;guava&lt;/artifactId&gt;                    &lt;groupId&gt;com.google.guava&lt;/groupId&gt;                &lt;/exclusion&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;                    &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;            &lt;artifactId&gt;hbase-server&lt;/artifactId&gt;            &lt;version&gt;${hbase.version}-${cdh.version}&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;servlet-api-2.5&lt;/artifactId&gt;                    &lt;groupId&gt;org.mortbay.jetty&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><p><strong>com/hsiehchou/hbase/config/HBaseConf.java</strong></p><pre><code>package com.hsiehchou.hbase.config;import com.hsiehchou.hbase.spilt.SpiltRegionUtil;import org.apache.commons.configuration.CompositeConfiguration;import org.apache.commons.configuration.ConfigurationException;import org.apache.commons.configuration.PropertiesConfiguration;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.BufferedMutator;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import org.apache.log4j.Logger;import java.io.IOException;import java.io.Serializable;public class HBaseConf implements Serializable {    private static final long serialVersionUID = 1L;    private static final Logger LOG = Logger.getLogger(HBaseConf.class);    private static final String HBASE_SERVER_CONFIG = &quot;hbase/hbase-server-config.properties&quot;;    private static final String HBASE_SITE = &quot;hbase/hbase-site.xml&quot;;    private volatile static HBaseConf hbaseConf;    private CompositeConfiguration hbase_server_config;    public CompositeConfiguration getHbase_server_config() {        return hbase_server_config;    }    public void setHbase_server_config(CompositeConfiguration hbase_server_config) {        this.hbase_server_config = hbase_server_config;    }    //hbase 配置文件    private  Configuration configuration;    //hbase 连接    private volatile transient Connection conn;    /**     * 初始化HBaseConf的时候加载配置文件     */    private HBaseConf() {        hbase_server_config = new CompositeConfiguration();        //加载配置文件        loadConfig(HBASE_SERVER_CONFIG,hbase_server_config);        //初始化连接        getHconnection();    }    //获取连接    public Configuration getConfiguration(){        if(configuration==null){            configuration = HBaseConfiguration.create();            configuration.addResource(HBASE_SITE);            LOG.info(&quot;加载配置文件&quot; + HBASE_SITE + &quot;成功&quot;);        }        return configuration;    }    public BufferedMutator getBufferedMutator(String tableName) throws IOException {        return getHconnection().getBufferedMutator(TableName.valueOf(tableName));    }    public Connection getHconnection(){        if(conn==null){            //获取配置文件            getConfiguration();            synchronized (HBaseConf.class) {                if (conn == null) {                    try {                        conn = ConnectionFactory.createConnection(configuration);                    } catch (IOException e) {                        LOG.error(String.format(&quot;获取hbase的连接失败  参数为： %s&quot;, toString()), e);                    }                }            }        }        return conn;    }    /**     * 加载配置文件     * @param path     * @param configuration     */    private void loadConfig(String path,CompositeConfiguration configuration) {        try {            LOG.info(&quot;加载配置文件 &quot; + path);            configuration.addConfiguration(new PropertiesConfiguration(path));            LOG.info(&quot;加载配置文件&quot; + path +&quot;成功。 &quot;);        } catch (ConfigurationException e) {            LOG.error(&quot;加载配置文件 &quot; + path + &quot;失败&quot;, e);        }    }    /**     * 单例 初始化HBaseConf     * @return     */    public static HBaseConf getInstance() {        if (hbaseConf == null) {            synchronized (HBaseConf.class) {                if (hbaseConf == null) {                    hbaseConf = new HBaseConf();                }            }        }        return hbaseConf;    }    public static void main(String[] args) {        String hbase_table = &quot;test:chl_test2&quot;;        HBaseTableUtil.createTable(hbase_table, &quot;cf&quot;, true, -1, 1, SpiltRegionUtil.getSplitKeysBydinct());      /*  Connection hconnection = HBaseConf.getInstance().getHconnection();        Connection hconnection1 = HBaseConf.getInstance().getHconnection();        System.out.println(hconnection);        System.out.println(hconnection1);*/    }}</code></pre><p><strong>com/hsiehchou/hbase/config/HBaseTableFactory.java</strong></p><pre><code>package com.hsiehchou.hbase.config;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.BufferedMutator;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.HTable;import org.apache.hadoop.hbase.client.Table;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.io.Serializable;public class HBaseTableFactory implements Serializable {    private static final long serialVersionUID = -1071596337076137201L;    private static final Logger LOG = LoggerFactory.getLogger(HBaseTableFactory.class);    private HBaseConf conf;    private transient Connection conn  ;    private boolean isReady = true;    public HBaseTableFactory(){        conf = HBaseConf.getInstance();        if(true){            conn = conf.getHconnection();        }else{            isReady = false;            LOG.warn(&quot;HBase 连接没有启动。&quot;);        }    }    public HBaseTableFactory(Connection conn){        this.conn = conn;    }    /**      * 根据表名创建 表的实例      * @param tableName      * @return      * @throws IOException      * HTableInterface     */    public Table getHBaseTableInstance(String tableName) throws IOException{        if(conn == null){            if(conf == null){                conf = HBaseConf.getInstance();                isReady = true;                LOG.warn(&quot;HBaseConf为空，重新初始化。&quot;);            }            synchronized (HBaseTableFactory.class) {                if(conn == null) {                    conn = conf.getHconnection();                    LOG.warn(&quot;初始 hbase Connection 为空 ， 获取  Connection成功。&quot;);                }            }        }        return  isReady ? conn.getTable(TableName.valueOf(tableName)) : null;    }    public HTable getHTable(String tableName) throws IOException{        return  (HTable) getHBaseTableInstance(tableName);    }    public BufferedMutator getBufferedMutator(String tableName) throws IOException {        return getConf().getBufferedMutator(tableName);    }    public boolean isReady() {        return isReady;    }    private HBaseConf getConf(){        if(conf == null){            conf = HBaseConf.getInstance();        }        return conf;    }    public void close() throws IOException{        conn.close();        conn = null;    }}</code></pre><p><strong>com/hsiehchou/hbase/config/HBaseTableUtil</strong></p><pre><code>package com.hsiehchou.hbase.config;import com.google.common.collect.Sets;import org.apache.hadoop.fs.Path;import org.apache.hadoop.hbase.*;import org.apache.hadoop.hbase.client.Admin;import org.apache.hadoop.hbase.client.Table;import org.apache.hadoop.hbase.io.compress.Compression;import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;import org.apache.hadoop.hbase.regionserver.BloomType;import org.apache.hadoop.hbase.util.Bytes;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.util.*;import static com.google.common.base.Preconditions.checkArgument;public class HBaseTableUtil {    private static final Logger LOG = LoggerFactory.getLogger(HBaseTableUtil.class);    private static final String COPROCESSORCLASSNAME =  &quot;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&quot;;    private static HBaseConf conf = HBaseConf.getInstance() ;    private HBaseTableUtil(){}    /**     * 获取hbase 表连接     * @param tableName     * @return     */    public static Table getTable(String tableName){        Table table =null;        if(tableExists(tableName)){            try {                table = conf.getHconnection().getTable(TableName.valueOf(tableName));            } catch (IOException e) {                LOG.error(null,e);            }        }        return table;    }    public static void close(Table table){        if(table != null) {            try {                table.close();            } catch (IOException e) {                e.printStackTrace();            }        }    }    /**     * 判断   HBase中是否存在  名为  tableName 的表     * @param tableName     * @return  boolean     */    public static boolean tableExists(String tableName){        boolean  isExists = false;        try {            isExists = conf.getHconnection().getAdmin().tableExists(TableName.valueOf(tableName));        } catch (MasterNotRunningException e) {            LOG.error(&quot;HBase  master  未运行 。 &quot;, e);        } catch (ZooKeeperConnectionException e) {            LOG.error(&quot;zooKeeper 连接异常。 &quot;, e);        } catch (IOException e) {            LOG.error(&quot;&quot;, e);        }        return isExists;    }    /**     * 删除表     * @param tableName     * @return     */    public static boolean deleteTable(String tableName){        boolean status = false;        TableName name = TableName.valueOf(tableName);        try {            Admin admin = conf.getHconnection().getAdmin();            if(admin.tableExists(name)){                if(!admin.isTableDisabled(name)){                    admin.disableTable(name);                }                admin.deleteTable(name);            }else{                LOG.warn(&quot; HBase中不存在 表 &quot; + tableName);            }            admin.close();            status = true;        } catch (MasterNotRunningException e) {            LOG.error(&quot;HBase  master  未运行 。 &quot;, e);        } catch (ZooKeeperConnectionException e) {            LOG.error(&quot;zooKeeper 连接异常。 &quot;, e);        } catch (IOException e) {            LOG.error(&quot;&quot;, e);        }        return status;    }    /**     * 清空表     * @param tableName     * @return     */    public static boolean truncateTable(String tableName){        boolean status = false;        TableName name = TableName.valueOf(tableName);        try {            Admin admin = conf.getHconnection().getAdmin();            if(admin.tableExists(name)){                if(admin.isTableAvailable(name)){                    admin.disableTable(name);                }                admin.truncateTable(name, true);            }else{                LOG.warn(&quot; HBase中不存在 表 &quot; + tableName);            }            admin.close();            status = true;        } catch (MasterNotRunningException e) {            LOG.error(&quot;HBase  master  未运行 。 &quot;, e);        } catch (ZooKeeperConnectionException e) {            LOG.error(&quot;zooKeeper 连接异常。 &quot;, e);        } catch (IOException e) {            LOG.error(&quot;&quot;, e);        }        return status;    }    /**     * 创建HBase表     * @param tableName     * @param cf       列族名     * @param inMemory     * @param ttl    ttl &lt; 0     则为永久保存     */    public static boolean createTable(String tableName, String cf, boolean inMemory, int ttl, int maxVersion){        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, COPROCESSORCLASSNAME);        return createTable(htd);    }    public static boolean createTable(String tableName, String cf, boolean inMemory, int ttl, int maxVersion,  boolean useSNAPPY){        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, useSNAPPY , COPROCESSORCLASSNAME);        return createTable(htd);    }    public static boolean createTable(String tableName, String cf, boolean inMemory, int ttl, int maxVersion,  boolean useSNAPPY, byte[][] splits){        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, useSNAPPY, COPROCESSORCLASSNAME);        return createTable(htd , splits);    }    /**     * @param tableName    表名     * @param cf           列簇     * @param inMemory     是否存在内存     * @param ttl          数据过期时间     * @param maxVersion   最大版本     * @param splits       分区     * @return     */    public static boolean createTable(String tableName,                                      String cf,                                      boolean inMemory,                                      int ttl,                                      int maxVersion,                                      byte[][] splits){        //返回表说明        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, COPROCESSORCLASSNAME);        //通过HTableDescriptor 和 splits 分区策略来定义表        return createTable(htd , splits);    }    public static List&lt;String&gt; listTables(){        List&lt;String&gt; list = new ArrayList&lt;String&gt;();        Admin admin = null;        try {            admin = conf.getHconnection().getAdmin();            TableName[] listTableNames = admin.listTableNames();            for( TableName t :  listTableNames ){                list.add( t.getNameAsString() );            }        } catch(IOException e )  {            LOG.error(&quot;创建HBase表失败。&quot;, e);        }finally{            try {                if(admin!=null){                    admin.close();                }            } catch (IOException e) {                LOG.error(&quot;&quot;, e);            }        }        return list;    }    /**     * 列出所有表     * @param reg     * @return     */    public static List&lt;String&gt; listTables(String reg){        List&lt;String&gt; list = new ArrayList&lt;String&gt;();        Admin admin = null;        try {            admin = conf.getHconnection().getAdmin();            TableName[] listTableNames = admin.listTableNames(reg);            for(TableName t :  listTableNames){                list.add(t.getNameAsString());            }        } catch(IOException e)  {            LOG.error(&quot;创建HBase表失败。&quot;, e);        }finally{            try {                if(admin!=null){                    admin.close();                }            } catch (IOException e) {                LOG.error(&quot;&quot;, e);            }        }        return list;    }    /**     * 创建HBase表     * @param tableName     * @param cf       列族名     * @param inMemory     * @param ttl      ttl &lt; 0     则为永久保存     */    public static boolean  createTable(String tableName, String cf, boolean inMemory, int ttl , int maxVersion, String ... coprocessorClassNames){        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, coprocessorClassNames);        return createTable(htd);    }    public static boolean  createTable( String tableName, String cf, boolean inMemory, int ttl, int maxVersion, boolean useSNAPPY, String ... coprocessorClassNames){        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, useSNAPPY, coprocessorClassNames);        return createTable(htd);    }    public static boolean  createTable( String tableName,String cf,boolean inMemory, int ttl ,int maxVersion ,  boolean useSNAPPY ,byte[][] splits, String ... coprocessorClassNames){        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, useSNAPPY ,coprocessorClassNames);        return createTable(htd,splits );    }    public static boolean  createTable(String tableName, String cf, boolean inMemory, int ttl, int maxVersion, byte[][] splits, String ... coprocessorClassNames){        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, coprocessorClassNames);        return createTable(htd,splits );    }    /**     * 通过HTableDescriptor 和 分区 来构建hbase     * @param htd     * @param splits     * @return     */    public static boolean createTable(HTableDescriptor htd, byte[][] splits){        Admin admin = null;        try {            admin = conf.getHconnection().getAdmin();            TableName tableName = htd.getTableName();            boolean exist = admin.tableExists(tableName);            if(exist){                LOG.error(&quot;表&quot;+tableName.getNameAsString() + &quot;已经存在&quot;);            }else{                //使用Admin进行创建表                admin.createTable(htd, splits);            }        } catch(IOException e )  {            LOG.error(&quot;创建HBase表失败。&quot;, e);            return false;        }finally{            try {                if(admin!=null){                    admin.close();                }            } catch (IOException e) {                LOG.error(&quot;&quot;, e);            }        }        return true;    }    public static boolean createTable(HTableDescriptor htd){        Admin admin = null;        try {            admin = conf.getHconnection().getAdmin();            if(admin.tableExists(htd.getTableName())){                LOG.info(&quot;表&quot; + htd.getTableName() + &quot;已经存在&quot;);            }else{                admin.createTable(htd);            }        } catch(IOException e )  {            LOG.error(&quot;创建HBase表失败。&quot;, e);            return false;        }finally{            try {                if(admin!=null){                    admin.close();                }            } catch (IOException e) {                LOG.error(&quot;&quot;, e);            }        }        return true;    }    /**     * 创建命名空间     * @param nameSpace     * @return     */    public static boolean createNameSpace(String nameSpace){        Admin admin = null;        try {            admin = conf.getHconnection().getAdmin();            NamespaceDescriptor[] listNamespaceDescriptors = admin.listNamespaceDescriptors();            boolean exist = false;            for(NamespaceDescriptor namespaceDescriptor : listNamespaceDescriptors){                if(namespaceDescriptor.getName().equals(nameSpace)){                    exist = true;                }            }            if(!exist) admin.createNamespace(NamespaceDescriptor.create(nameSpace).build());        } catch(IOException e )  {            LOG.error(&quot;创建HBase命名空间失败。&quot;, e);            return false;        }finally{            try {                if(admin!=null){                    admin.close();                }            } catch (IOException e) {                LOG.error(&quot;&quot;, e);            }        }        return true;    }    /**     * 为 HBase中的表  tableName添加 协处理器  coprocessorClassName     * @param tableName     * @param coprocessorClassName    必须是已经存在与HBase集群中     * @return  boolean     */    public static boolean addCoprocessorClassForTable(String tableName,String coprocessorClassName){        boolean status = false;        TableName name = TableName.valueOf(tableName);        Admin admin = null;        try {            admin = conf.getHconnection().getAdmin();            HTableDescriptor htd = admin.getTableDescriptor(name);            if(!htd.hasCoprocessor(coprocessorClassName)){                htd.addCoprocessor(coprocessorClassName);                admin.disableTable(name);                admin.modifyTable(name, htd);                admin.enableTable(name);            }else{                LOG.warn(String.format(&quot;表 %s中已经存在协处理器%s&quot;, tableName, coprocessorClassName));            }            status = true;        } catch (MasterNotRunningException e) {            LOG.error(&quot;HBase  master  未运行 。 &quot;, e);        } catch (ZooKeeperConnectionException e) {            LOG.error(&quot;zooKeeper 连接异常。 &quot;, e);        } catch (IOException e) {            LOG.error(&quot;&quot;, e);        }finally{            try {                if(admin!=null){                    admin.close();                }            } catch (IOException e) {                LOG.error(&quot;&quot;, e);            }        }        return status;    }    /**     * 为HBase中的表 tableName添加指定位置的 协处理器 jar     * @param tableName     * @param coprocessorClassName   jar中的具体的协处理器     * @param jarPath     hdfs的路径     * @param level       执行级别     * @param kvs         运行参数    可以为 null     * @return   boolean     */    public static boolean addCoprocessorJarForTable(String  tableName, String coprocessorClassName,String jarPath,int level ,Map&lt;String, String&gt; kvs ){        boolean status = false;        TableName name = TableName.valueOf(tableName);        Admin admin = null;        try {            admin = conf.getHconnection().getAdmin();            HTableDescriptor htd = admin.getTableDescriptor(name);            if(!htd.hasCoprocessor(coprocessorClassName)){                admin.disableTable(name);                htd.addCoprocessor(coprocessorClassName, new Path(jarPath), level, kvs);                admin.modifyTable(name, htd);                admin.enableTable(name);            }else{                LOG.warn(String.format(&quot;表 %s中已经存在协处理器%s&quot;, tableName, coprocessorClassName));            }            status = true;        } catch (MasterNotRunningException e) {            LOG.error(&quot;HBase  master  未运行 。 &quot;, e);        } catch (ZooKeeperConnectionException e) {            LOG.error(&quot;zooKeeper 连接异常。 &quot;, e);        } catch (IOException e) {            LOG.error(&quot;&quot;, e);        }finally{            try {                if(admin!=null){                    admin.close();                }            } catch (IOException e) {                LOG.error(&quot;&quot;, e);            }        }        return status;    }    /**     * @param tableName     * @param cf     * @param inMemory     * @param ttl     * @param maxVersion     * @param coprocessorClassNames     * @return     */    public static HTableDescriptor createHTableDescriptor( String tableName,String cf,boolean inMemory, int ttl ,int maxVersion ,String ... coprocessorClassNames ){        return createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, true , COPROCESSORCLASSNAME);    }    /**     * @param tableName     * @param cf     * @param inMemory     * @param ttl     * @param maxVersion     * @param useSNAPPY     * @param coprocessorClassNames     * @return     */    public static HTableDescriptor createHTableDescriptor( String tableName,String cf,boolean inMemory, int ttl ,int maxVersion , boolean useSNAPPY , String ... coprocessorClassNames ){        // 1.创建命名空间        String[] split = tableName.split(&quot;:&quot;);        if(split.length==2){            createNameSpace(split[0]);        }        // 2.添加协处理器        HTableDescriptor htd = new HTableDescriptor(TableName.valueOf(tableName));        for( String coprocessorClassName : coprocessorClassNames ){            try {                htd.addCoprocessor(coprocessorClassName);            } catch (IOException e1) {                LOG.error(&quot;为表&quot; + tableName + &quot; 添加协处理器失败。 &quot;, e1);            }        }        // 创建HColumnDescriptor        HColumnDescriptor hcd = new HColumnDescriptor(cf);        if( maxVersion &gt; 0 )            //定义最大版本号            hcd.setMaxVersions(maxVersion);        /**         * 设置布隆过滤器         * 默认是NONE 是否使用布隆过虑及使用何种方式         * 布隆过滤可以每列族单独启用         * Default = ROW 对行进行布隆过滤。         * 对 ROW，行键的哈希在每次插入行时将被添加到布隆。         * 对 ROWCOL，行键 + 列族 + 列族修饰的哈希将在每次插入行时添加到布隆         * 使用方法: create ‘table’,{BLOOMFILTER =&gt;’ROW’}         * 启用布隆过滤可以节省读磁盘过程，可以有助于降低读取延迟         * */        hcd.setBloomFilterType(BloomType.ROWCOL);        /**         * hbase在LRU缓存基础之上采用了分层设计，整个blockcache分成了三个部分，分别是single、multi和inMemory。三者区别如下：         * single：如果一个block第一次被访问，放在该优先队列中；         * multi：如果一个block被多次访问，则从single队列转移到multi队列         * inMemory：优先级最高，常驻cache，因此一般只有hbase系统的元数据，如meta表之类的才会放到inMemory队列中。普通的hbase列族也可以指定IN_MEMORY属性，方法如下：         * create &#39;table&#39;, {NAME =&gt; &#39;f&#39;, IN_MEMORY =&gt; true}         * 修改上表的inmemory属性，方法如下：         * alter &#39;table&#39;,{NAME=&gt;&#39;f&#39;,IN_MEMORY=&gt;true}         * */        hcd.setInMemory(inMemory);        hcd.setScope(1);        /**         * 数据量大，边压边写也会提升性能的，毕竟IO是大数据的最严重的瓶颈，         * 哪怕使用了SSD也是一样。众多的压缩方式中，推荐使用SNAPPY。从压缩率和压缩速度来看，         * 性价比最高。         **/        if(useSNAPPY)hcd.setCompressionType(Compression.Algorithm.SNAPPY);        //默认为NONE        //如果数据存储时设置了编码， 在缓存到内存中的时候是不会解码的，这样和不编码的情况相比，相同的数据块，编码后占用的内存更小， 即提高了内存的使用率        //如果设置了编码，用户必须在取数据的时候进行解码， 因此在内存充足的情况下会降低读写性能。        //在任何情况下开启PREFIX_TREE编码都是安全的        //不要同时开启PREFIX_TREE和SNAPPY        //通常情况下 SNAPPY并不能比 PREFIX_TREE取得更好的优化效果        //hcd.setDataBlockEncoding(DataBlockEncoding.PREFIX_TREE);        //默认为64k     65536        //随着blocksize的增大， 系统随机读的吞吐量不断的降低，延迟也不断的增大，        //64k大小比16k大小的吞吐量大约下降13%，延迟增大13%        //128k大小比64k大小的吞吐量大约下降22%，延迟增大27%        //对于随机读取为主的业务，可以考虑调低blocksize的大小        //随着blocksize的增大， scan的吞吐量不断的增大，延迟也不断降低，        //64k大小比16k大小的吞吐量大约增加33%，延迟降低24%        //128k大小比64k大小的吞吐量大约增加7%，延迟降低7%        //对于scan为主的业务，可以考虑调大blocksize的大小        //如果业务请求以Get为主，则可以适当的减小blocksize的大小        //如果业务是以scan请求为主，则可以适当的增大blocksize的大小        //系统默认为64k, 是一个scan和get之间取的平衡值        //hcd.setBlocksize(s)        //设置表中数据的存储生命期，过期数据将自动被删除，        // 例如如果只需要存储最近两天的数据，        // 那么可以设置setTimeToLive(2 * 24 * 60 * 60)        if( ttl &lt; 0 ) ttl = HConstants.FOREVER;        hcd.setTimeToLive(ttl);        htd.addFamily( hcd);        return htd;    }    public static boolean createTable(HBaseTableParam param){        String nameSpace = param.getNameSpace();        if(!&quot;default&quot;.equalsIgnoreCase(nameSpace)){            checkArgument(createNameSpace(nameSpace), String.format(&quot;创建命名空间%s失败。&quot;, nameSpace));        }        HTableDescriptor desc = createHTableDescriptor(param);        byte[][] splits = param.getSplits();        if(splits == null){            return createTable(desc);        }else{            return createTable(desc, splits);        }    }    public static HTableDescriptor createHTableDescriptor(HBaseTableParam param){        String tableName = String.format(&quot;%s:%s&quot;, param.getNameSpace(), param.getTableName());        HTableDescriptor htd = new HTableDescriptor(TableName.valueOf(tableName));        for(String coprocessorClassName : param.getCoprocessorClazz()){            try {                htd.addCoprocessor(coprocessorClassName);            } catch (IOException e) {                LOG.error(String.format(&quot;为表  %s 添加协处理器失败。&quot;, tableName), e);            }        }        HColumnDescriptor hcd = new HColumnDescriptor(param.getCf());        hcd.setBloomFilterType(param.getBloomType());        hcd.setMaxVersions(param.getMaxVersions());        hcd.setScope(param.getReplicationScope());        hcd.setBlocksize(param.getBlocksize());        hcd.setInMemory(param.isInMemory());        hcd.setTimeToLive(param.getTtl());        /* 数据量大，边压边写也会提升性能的，毕竟IO是大数据的最严重的瓶颈，哪怕使用了SSD也是一样。众多的压缩方式中，推荐使用SNAPPY。从压缩率和压缩速度来看，性价比最高。  */        if(param.isUsePrefix_tree())hcd.setDataBlockEncoding(DataBlockEncoding.PREFIX_TREE);        if(param.isUseSnappy())hcd.setCompressionType(Compression.Algorithm.SNAPPY);        htd.addFamily( hcd);        return htd;    }    public static void closeTable( Table table ){        if( table != null ){            try {                table.close();            } catch (IOException e) {                LOG.error(&quot; &quot;, e);            }            table = null;        }    }    public static byte[][] getSplitKeys() {        //String[] keys = new String[]{&quot;50|&quot;};        //String[] keys = new String[]{&quot;25|&quot;,&quot;50|&quot;,&quot;75|&quot;};        //String[] keys = new String[]{&quot;13|&quot;,&quot;26|&quot;,&quot;39|&quot;, &quot;52|&quot;,&quot;65|&quot;,&quot;78|&quot;,&quot;90|&quot;};        String[] keys = new String[]{ &quot;06|&quot;,&quot;13|&quot;,&quot;20|&quot;, &quot;26|&quot;,&quot;33|&quot;, &quot;39|&quot;,&quot;46|&quot;, &quot;52|&quot;,&quot;58|&quot;, &quot;65|&quot;,&quot;72|&quot;,&quot;78|&quot;, &quot;84|&quot;,&quot;90|&quot;,&quot;95|&quot;};        //String[] keys = new String[]{&quot;10|&quot;, &quot;20|&quot;, &quot;30|&quot;, &quot;40|&quot;, &quot;50|&quot;, &quot;60|&quot;, &quot;70|&quot;, &quot;80|&quot;, &quot;90|&quot;};        byte[][] splitKeys = new byte[keys.length][];        TreeSet&lt;byte[]&gt; rows = new TreeSet&lt;byte[]&gt;(Bytes.BYTES_COMPARATOR);//升序排序        for (int i = 0; i &lt; keys.length; i++) {            rows.add(Bytes.toBytes(keys[i]));        }        Iterator&lt;byte[]&gt; rowKeyIter = rows.iterator();        int i = 0;        while (rowKeyIter.hasNext()) {            byte[] tempRow = rowKeyIter.next();            rowKeyIter.remove();            splitKeys[i] = tempRow;            i++;        }        return splitKeys;    }    public static class HBaseTableParam{        private final String nameSpace; //命名空间        private final String tableName; //表名        private final String cf;        //列簇        private Set&lt;String&gt;  coprocessorClazz = Sets.newHashSet(&quot;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&quot;);        private int maxVersions = 1;    //版本号 默认为1        private BloomType bloomType = BloomType.ROWCOL;        private boolean inMemory = false;        private int replicationScope = 1;        private boolean useSnappy = false; //默认不使用压缩        private boolean usePrefix_tree = false;        private int blocksize = 65536;        private int ttl = HConstants.FOREVER;        private byte[][] splits;        public HBaseTableParam(String nameSpace, String tableName, String cf) {            super();            this.nameSpace = nameSpace == null ? &quot;default&quot; : nameSpace;            this.tableName = tableName;            this.cf = cf;        }        public String getNameSpace() {            return nameSpace;        }        public String getTableName() {            return tableName;        }        public String getCf() {            return cf;        }        public Set&lt;String&gt; getCoprocessorClazz() {            return coprocessorClazz;        }        public void clearCoprocessor(){            coprocessorClazz.clear();        }        public void addCoprocessorClazz(String clazz) {            this.coprocessorClazz.add(clazz);        }        public void addCoprocessorClazz(String ... clazz) {            addCoprocessorClazz(Arrays.asList(clazz));        }        public void addCoprocessorClazz(Collection&lt;String&gt;  clazz) {            this.coprocessorClazz.addAll(clazz);        }        public int getMaxVersions() {            return maxVersions;        }        public void setMaxVersions(int maxVersions) {            this.maxVersions = maxVersions &lt;= 0 ? 1 : maxVersions;        }        public BloomType getBloomType() {            return bloomType;        }        public void setBloomType(BloomType bloomType) {            this.bloomType = bloomType == null ? BloomType.ROWCOL : bloomType;        }        public boolean isInMemory() {            return inMemory;        }        public void setInMemory(boolean inMemory) {            this.inMemory = inMemory;        }        public int getReplicationScope() {            return replicationScope;        }        public void setReplicationScope(int replicationScope) {            this.replicationScope = replicationScope &lt; 0 ? 1 : replicationScope;        }        public boolean isUseSnappy() {            return useSnappy;        }        /**         * 控制是否使用 snappy 压缩数据， 默认是不启用         * @param useSnappy         */        public void setUseSnappy(boolean useSnappy) {            this.useSnappy = useSnappy;        }        public boolean isUsePrefix_tree() {            return usePrefix_tree;        }        /**         * 控制是否使用数据编码，默认是不使用         *         * 如果数据存储时设置了编码， 在缓存到内存中的时候是不会解码的，这样和不编码的情况相比，相同的数据块，编码后占用的内存更小， 即提高了内存的使用率         * 如果设置了编码，用户必须在取数据的时候进行解码， 因此在内存充足的情况下会降低读写性能。         * 在任何情况下开启PREFIX_TREE编码都是安全的         * 不要同时开启PREFIX_TREE和SNAPPY         * 通常情况下 SNAPPY并不能比 PREFIX_TREE取得更好的优化效果         */        public void setUsePrefix_tree(boolean usePrefix_tree) {            this.usePrefix_tree = usePrefix_tree;        }        public int getBlocksize() {            return blocksize;        }        /**         *默认为64k     65536         *随着blocksize的增大， 系统随机读的吞吐量不断的降低，延迟也不断的增大，         *64k大小比16k大小的吞吐量大约下降13%，延迟增大13%         *128k大小比64k大小的吞吐量大约下降22%，延迟增大27%         *对于随机读取为主的业务，可以考虑调低blocksize的大小         *         *随着blocksize的增大， scan的吞吐量不断的增大，延迟也不断降低，         *64k大小比16k大小的吞吐量大约增加33%，延迟降低24%         *128k大小比64k大小的吞吐量大约增加7%，延迟降低7%         *对于scan为主的业务，可以考虑调大blocksize的大小         *         *如果业务请求以Get为主，则可以适当的减小blocksize的大小         *如果业务是以scan请求为主，则可以适当的增大blocksize的大小         *系统默认为64k, 是一个scan和get之间取的平衡值         *         */        public void setBlocksize(int blocksize) {            this.blocksize = blocksize &lt;= 0 ? 65536 : blocksize;        }        public int getTtl() {            return ttl;        }        /**         * 默认是永久保存         * @param ttl  大于 零的整数，  &lt;= 0 ? tt 为  永久保存         */        public void setTtl(int ttl) {            this.ttl = ttl &lt;= 0 ? HConstants.FOREVER : ttl;        }        public byte[][] getSplits() {            return splits;        }        /*         * 预分区的rowKey范围配置         * @param splits         */        /*        public void setSplits(byte[][] splits) {            this.splits = splits;        }*/    }    public static void main(String[] args) throws Exception{        Admin admin = conf.getHconnection().getAdmin();        System.out.println(admin);        //deleteTable(&quot;test:user&quot;);        // HBaseTableUtil.createTable(&quot;aaaaa&quot;,&quot;info1&quot;,true,-1,1);        //  HBaseTableUtil.truncateTable(&quot;aaaaa&quot;);     /*   boolean b = tableExists(&quot;test:user2&quot;);        Table table = getTable(&quot;test:user2&quot;);        System.out.println(&quot;==================&quot;+table);        System.out.println(&quot;==================&quot;+table.getName());*/        //HBaseTableUtil.deleteTable(&quot;aaaaa&quot;);       /* Table table = HBaseTableUtil.getTable(&quot;countform:typecount&quot;);        System.out.println(table);*//*        boolean b = HBaseTableUtil.tableExists(&quot;countform:typecount&quot;);        System.out.println(b);*/        HBaseTableUtil.deleteTable(&quot;tanslator&quot;);        HBaseTableUtil.deleteTable(&quot;ability&quot;);        HBaseTableUtil.deleteTable(&quot;task&quot;);        HBaseTableUtil.deleteTable(&quot;paper&quot;);        //  HbaseSearchService hbaseSearchService=new HbaseSearchService();        //  Map&lt;String, String&gt; stringStringMap = hbaseSearchService.get(&quot;countform:bsid&quot;,&quot;&quot;, new BaseMapRowExtrator());        // Map&lt;String, String&gt; aaaaa = hbaseSearchService.get(&quot;countform:bsid&quot;, &quot;aaaaa&quot;, new BaseMapRowExtrator());        // System.out.println(aaaaa);    }}</code></pre><p><strong>com/hsiehchou/hbase/entity/AbstractRow.java</strong></p><pre><code>package com.hsiehchou.hbase.entity;import com.google.common.collect.HashMultimap;import com.google.common.collect.Sets;import java.util.Collection;import java.util.Map;import java.util.Set;public abstract class AbstractRow&lt;T extends HBaseCell&gt; {    protected String rowKey;    protected HashMultimap&lt;String, T&gt; cells;    protected Set&lt;String&gt; fields;    protected long maxCapTime;    public AbstractRow(String rowKey){        this.rowKey = rowKey;        cells = HashMultimap.create();        fields = Sets.newHashSet();    }    public boolean addCell(String field, String value, long capTime){        return addCell(field, createCell(field, value, capTime));    }    public boolean addCell(String field, T cell){        fields.add(cell.getField());        if(cell.getCapTime() &gt; maxCapTime)            maxCapTime = cell.getCapTime();        return cells.put(field, cell);    }    public boolean[] addCell(String field, Collection&lt;T&gt; cells){        boolean[] status = new boolean[cells.size()];        int n = 0;        for(T cell : cells){            status[n] = addCell(field, cell);            n++;        }        return status;    }    public String getRowKey() {        return rowKey;    }    protected abstract T createCell(String field, String value, long capTime);    public Map&lt;String, Collection&lt;T&gt;&gt; getCell() {        return cells.asMap();    }    public Collection&lt;T&gt; getCellByField(String field){        return cells.get(field);    }    public Set&lt;Map.Entry&lt;String, T&gt;&gt; entries(){        return  cells.entries();    }    @Override    public String toString() {        return &quot;AbstractRow [rowKey=&quot; + rowKey + &quot;, cells=&quot; + cells + &quot;]&quot;;    }    public boolean equals(Object obj) {       if(this == obj)return true ;       if(!(obj instanceof AbstractRow))return false ;       @SuppressWarnings(&quot;unchecked&quot;)       AbstractRow&lt;T&gt; row = (AbstractRow&lt;T&gt;) obj;       if(rowKey.equals(row.getRowKey()))return true;       return false;    }    public int hashCode(){        return this.rowKey.hashCode();    }    public long getMaxCapTime() {        return maxCapTime;    }    public Set&lt;String&gt; getFields() {        return Sets.newHashSet(fields);    }}</code></pre><p><strong>com/hsiehchou/hbase/entity/HBaseCell.java</strong></p><pre><code>package com.hsiehchou.hbase.entity;public class HBaseCell implements Comparable&lt;HBaseCell&gt;{    protected String field;               protected String value;    protected Long capTime;    public HBaseCell(String field, String value, long capTime){        this.field = field;        this.capTime = capTime;        this.value = value;    }    public String getField(){        return field;    }    public String getValue(){        return value;    }    public void setCapTime(long capTime) {        this.capTime = capTime;    }    public Long getCapTime() {        return capTime;    }    public String toString(){        return String.format(&quot;%s_[%s]_%s&quot;, field, capTime, value);    }    public int compareTo(HBaseCell o) {        return o.getCapTime().compareTo(this.capTime);    }    public boolean equals(Object obj) {       if(this == obj)return true ;       if(!(obj instanceof HBaseCell))return false ;       HBaseCell cell = (HBaseCell)obj;       if(field.equals(cell.getField()) &amp;&amp; value.equals(cell.getValue())){           if(cell.getCapTime() &lt; capTime){               cell.setCapTime(this.capTime);           }           return true;       }       return false;    }    public int hashCode(){        return this.field.hashCode() +  31*this.value.hashCode();    }}</code></pre><p><strong>com/hsiehchou/hbase/entity/HBaseRow.java</strong></p><pre><code>package com.hsiehchou.hbase.entity;public class HBaseRow extends AbstractRow&lt;HBaseCell&gt; {    public HBaseRow(String rowKey){        super(rowKey);    }    public boolean[] addCell(String field, HBaseCell ... cells){        boolean[] status = new boolean[cells.length];        for(int i = 0; i &lt; cells.length; i++){            status[i] = addCell(field, cells[i]);        }        return status;    }    protected HBaseCell createCell(String field, String value, long capTime) {        return new HBaseCell(field, value, capTime);    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/BaseListRowExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.util.ArrayList;import java.util.List;public class BaseListRowExtrator implements RowExtractor&lt;List&lt;String&gt;&gt;{    private List&lt;String&gt; row;    public Long lastcjtime = 0l;    public Long firstcjtime = 0l;    @Override    public List&lt;String&gt; extractRowData(Result result, int rowNum)            throws IOException {        row = new ArrayList&lt;String&gt;();        for(Cell cell :  result.listCells()) {            String column = Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength());            String value = Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());            if(column.equalsIgnoreCase(&quot;cjtime&quot;)) {                Long v = Long.parseLong(value);                if(lastcjtime &lt; v) {                    lastcjtime = v;                }else if(firstcjtime &gt; v) {                    firstcjtime = v;                }            }            row.add(value);        }        return row;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/BaseMapRowExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.text.SimpleDateFormat;import java.util.Date;import java.util.HashMap;import java.util.List;import java.util.Map;public class BaseMapRowExtrator implements RowExtractor&lt;Map&lt;String,String&gt;&gt; {    private Map&lt;String,String&gt; row;    private List&lt;byte[]&gt; rows;    private String longTimeField;    private SimpleDateFormat format;    private String field;    private String value;    private long time;    public BaseMapRowExtrator(){}    /**     * @param rows   需要提取 所有的 rowKey  , null 则不提取     */    public BaseMapRowExtrator(List&lt;byte[]&gt; rows){        this.rows = rows;    }    /**     * @param rows             需要提取 所有的 rowKey  , null 则不提取     * @param longTimeField    long类型的时间字段   表示需要将其转换称 String 类型     */    public BaseMapRowExtrator(List&lt;byte[]&gt; rows,String longTimeField){        this.rows = rows;        this.longTimeField = longTimeField;    }    /**     * @param rows                  需要提取 所有的 rowKey  , null 则不提取     * @param longTimeField         long类型的时间字段     * @param timePattern           表示需要已该指定的格式  将时间字段的值转换成字符串     */    public BaseMapRowExtrator(List&lt;byte[]&gt; rows,String longTimeField,String timePattern){        this.rows = rows;        this.longTimeField = longTimeField;        if(StringUtils.isNotBlank(timePattern)){            format = new SimpleDateFormat(timePattern);        }    }    public Map&lt;String, String&gt; extractRowData(Result result, int rowNum) throws IOException {            row = new HashMap&lt;String,String&gt;();            List&lt;Cell&gt; cells = result.listCells();            for(Cell cell :  cells) {                field = Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength());                if( field.equals(longTimeField)  ){                    time = Bytes.toLong(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());                    if( format != null ){                        value = format.format(new Date(time));                    }else{                        value = String.valueOf(time);                    }                }else{                    value = Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());                }                row.put(field,value);            }            if( rows != null ){                rows.add(result.getRow());            }        return row;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/BaseMapWithRowKeyExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.util.HashMap;import java.util.Map;public class BaseMapWithRowKeyExtrator implements RowExtractor&lt;Map&lt;String,String&gt;&gt; {    private Map&lt;String,String&gt; row;    /* (non-Javadoc)     * @see com.bh.d406.bigdata.hbase.extractor.RowExtractor#extractRowData(org.apache.hadoop.hbase.client.Result, int)     */    @Override    public Map&lt;String, String&gt; extractRowData(Result result, int rowNum)            throws IOException {        row = new HashMap&lt;String,String&gt;();        row.put(&quot;rowKey&quot;, Bytes.toString( result.getRow() ));        for(Cell cell :  result.listCells()) {            row.put(Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()),Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));        }        return row;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/BeanRowExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import com.google.common.collect.Maps;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.lang.reflect.Field;import java.util.Map;public class BeanRowExtrator&lt;T&gt; implements RowExtractor&lt;T&gt; {    private static final Logger LOG = LoggerFactory.getLogger(BeanRowExtrator.class);    private Class&lt;T&gt; clazz;    private Map&lt;String,Field&gt; fieldMap;    public BeanRowExtrator(Class&lt;T&gt; clazz){        this.clazz = clazz;        this.fieldMap = getDeclaredFields(clazz);    }    public T extractRowData(Result result, int rowNum) throws IOException {        return resultReflectToClass(result, rowNum);    }    private T resultReflectToClass(Result result, int rowNum){        String column = null;        Field field = null;        T obj = null;        try {            obj = clazz.newInstance();            for(Cell cell : result.listCells()){                column = Bytes.toString(cell.getQualifierArray(),                        cell.getQualifierOffset(), cell.getQualifierLength());                /*检查该列是否在实体类中存在对应的属性,若存在则 为其赋值*/                if((field = fieldMap.get(column.toLowerCase())) != null){                    field.set(obj, Bytes.toString(cell.getValueArray(),                            cell.getValueOffset(), cell.getValueLength()));                }            }        } catch (InstantiationException e) {            LOG.error(String.format(&quot;解析第%个满足条件的记录%s失败。&quot;, rowNum, result), e);        } catch (IllegalAccessException e) {            LOG.error(String.format(&quot;解析第%s个满足条件的记录%s失败。&quot;, rowNum, result), e);        }        return obj;    }    private  Map&lt;String,Field&gt;  getDeclaredFields(Class&lt;?&gt; clazz){        Field[] fields = clazz.getDeclaredFields();        Field field = null;        Map&lt;String,Field&gt; fieldMap = Maps.newHashMapWithExpectedSize(fields.length);        for(int i = 0; i &lt; fields.length; i++){            field = fields[i];            if(field.getModifiers() == 2){                field.setAccessible(true);                fieldMap.put(field.getName().toLowerCase(), field);            }        }        fields = null;        return fieldMap;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/CellNumExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.client.Result;import java.io.IOException;public class CellNumExtrator implements RowExtractor&lt;Integer&gt; {    public Integer extractRowData(Result result, int rowNum) throws IOException {        return  result.listCells().size();    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/MapLongRowExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.util.HashMap;import java.util.Map;public class MapLongRowExtrator implements RowExtractor&lt;Map&lt;String,Long&gt;&gt; {    private Map&lt;String,Long&gt; row;    @Override    public Map&lt;String, Long&gt; extractRowData(Result result, int rowNum) throws IOException {        row = new HashMap&lt;String,Long&gt;();        for(Cell cell :  result.listCells()) {            row.put(Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()),Bytes.toLong(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));        }        return row;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/MapRowExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.io.Serializable;import java.util.HashMap;import java.util.Map;public class MapRowExtrator implements RowExtractor&lt;Map&lt;String,String&gt;&gt;,Serializable {    private static final long serialVersionUID = 1543027485077396235L;    private Map&lt;String,String&gt; row;    /* (non-Javadoc)     * @see com.bh.d406.bigdata.hbase.extractor.RowExtractor#extractRowData(org.apache.hadoop.hbase.client.Result, int)     */    @Override    public Map&lt;String, String&gt; extractRowData(Result result, int rowNum) throws IOException {        row = new HashMap&lt;String,String&gt;();        for(Cell cell :  result.listCells()) {            row.put(Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()),Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));        }        return row;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/MultiVersionRowExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import com.hsiehchou.hbase.entity.HBaseRow;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;public class MultiVersionRowExtrator implements RowExtractor&lt;HBaseRow&gt;{    private HBaseRow row;    public HBaseRow extractRowData(Result result, int rowNum) throws IOException {        row = new HBaseRow(Bytes.toString(result.getRow()));        String field = null;        String value = null;        long capTime = 0L;        for(Cell cell : result.listCells()){            field = Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength());            value = Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());            capTime = cell.getTimestamp();            row.addCell(field, value, capTime);        }        return  row ;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/OneColumnRowByteExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.client.Result;import java.io.IOException;import java.io.Serializable;public class OneColumnRowByteExtrator implements RowExtractor&lt;byte[]&gt; ,Serializable{    private static final long serialVersionUID = -3420092335124240222L;    private byte[] cf;    private byte[] cl;    public OneColumnRowByteExtrator( byte[] cf,byte[] cl ){        this.cf = cf;        this.cl = cl;    }    public byte[] extractRowData(Result result, int rowNum) throws IOException {        return result.getValue(cf, cl);    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/OneColumnRowStringExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.io.Serializable;public class OneColumnRowStringExtrator implements RowExtractor&lt;String&gt;  , Serializable{    private static final long serialVersionUID = -8585637277902568648L;    private byte[] cf ;    private byte[] cl ;    public OneColumnRowStringExtrator( byte[] cf , byte[] cl ){        this.cf = cf;        this.cl = cl;    }    /* (non-Javadoc)     * @see com.bh.d406.bigdata.hbase.extractor.RowExtractor#extractRowData(org.apache.hadoop.hbase.client.Result, int)     */    @Override    public String extractRowData(Result result, int rowNum) throws IOException {        byte[] value = result.getValue(cf, cl);        if( value == null ) return null;        return  Bytes.toString( value ) ;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/OnlyRowKeyExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.client.Result;import java.io.IOException;public class OnlyRowKeyExtrator implements RowExtractor&lt;byte[]&gt; {    @Override    public byte[] extractRowData(Result result, int rowNum) throws IOException {        // TODO Auto-generated method stub        return result.getRow();    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/OnlyRowKeyStringExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;public class OnlyRowKeyStringExtrator implements RowExtractor&lt;String&gt; {    public String extractRowData(Result result, int rowNum) throws IOException {        return Bytes.toString( result.getRow() );    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/RowExtractor.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.client.Result;import java.io.IOException;public interface RowExtractor&lt;T&gt;  {    /**      * description:      * @param result  result解析器      * @param rowNum        * @return      * @throws Exception      * T     */    T extractRowData(Result result, int rowNum) throws IOException;}</code></pre><p><strong>com/hsiehchou/hbase/extractor/SingleColumnMultiVersionRowExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.util.Set;public class SingleColumnMultiVersionRowExtrator implements RowExtractor&lt;Set&lt;String&gt;&gt;{    private Set&lt;String&gt; values;    private byte[] cf;    private byte[] cl;    /**     * 单列解析器  获取hbase 单列多版本数据     * @param cf     列簇     * @param cl     列     * @param values 返回值     */    public SingleColumnMultiVersionRowExtrator(byte[] cf, byte[] cl, Set&lt;String&gt; values){        this.cf = cf;        this.cl = cl;        this.values = values;    }    public Set&lt;String&gt; extractRowData(Result result, int rowNum) throws IOException {        for(Cell cell : result.getColumnCells(cf, cl)){            values.add(Bytes.toString(cell.getValueArray(),cell.getValueOffset(), cell.getValueLength()));        }        return values;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/StrToByteExtrator.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.io.Serializable;import java.util.HashMap;import java.util.Map;public class StrToByteExtrator implements RowExtractor&lt;Map&lt;String,byte[]&gt;&gt; ,Serializable {    private static final long serialVersionUID = 4633698173362569711L;    private Map&lt;String,byte[]&gt; row;    @Override    public Map&lt;String, byte[]&gt; extractRowData(Result result, int rowNum) throws IOException {        row = new HashMap&lt;String,byte[]&gt;();        for(Cell cell :  result.listCells()) {            row.put(Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()),                    Bytes.copy(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));        }        return row;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/ToRowList.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.CellUtil;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.util.HashMap;import java.util.LinkedList;import java.util.List;import java.util.Map;/** * Hbase数据库中数据提取接口实现： * 提取result的rowKey，和每个cell的值作为一行数据， * 一个cell=(row, family:qualifier:value, version) * * &lt;p&gt; * 每行数据的格式为：{rowKey column${separator}value column${separator}value ...} * 其中，不同的列之间用空格分隔，同样列元素的描述符与值之间用${separator}分隔 */public class ToRowList implements RowExtractor&lt;List&lt;String&gt;&gt; {    private Boolean currentVersion; //currentVersion为true:只取当前最新版本，false:取所有版本    private char separator; //不同元素之间拼接时的分隔符，默认为`#`    private ToRowList(Boolean currentVersion, char separator) {        this.separator = separator;        this.currentVersion = currentVersion;    }    public ToRowList(Boolean currentVersion) {        this(currentVersion, &#39;#&#39;);    }    public ToRowList() {        this(true, &#39;#&#39;);    }    /**      * 对{当前版本}存放在list[0] = {rowKey` `column`#`value` `column`#`value ...}      * 多版本的时候list({rowKey`#`version1` `column`#`value` `column`#`value ...},      * {rowKey`#`version2` `column`#`value` `column`#`value ...})      */    @Override    public List&lt;String&gt; extractRowData(Result result, int rowNum) throws IOException {        if(result == null || result.isEmpty()) return null;        final char SPACE = &#39; &#39;;        List&lt;String&gt; rows = new LinkedList&lt;&gt;();        //一个result是同一个rowKey的所有cells集合        String rowKey = Bytes.toString(result.getRow());        //build rowKey` `column`#`value` `column`#`value ...        StringBuilder row = new StringBuilder();        row.append(rowKey).append(SPACE);        //用于处理不同版本的映射        Map&lt;Long, String&gt; version2qualifiersAndValues = new HashMap&lt;&gt;();        List&lt;Cell&gt; cells = result.listCells();        for (Cell cell : cells) {            String value = Bytes.toString(cell.getValueArray(),                    cell.getValueOffset(), cell.getValueLength());            String qualifier = Bytes.toString(CellUtil.cloneQualifier(cell));            if (currentVersion) {                row.append(qualifier).append(separator).append(value).append(SPACE);            } else {                Long version = cell.getTimestamp();                String tmp = version2qualifiersAndValues.get(version);                version2qualifiersAndValues.put(version,                        StringUtils.isNotBlank(tmp) ? tmp + &quot; &quot; + qualifier + separator + value                                : rowKey + separator + version + &quot; &quot; + qualifier + separator + value);            }        }        if (currentVersion) {            rows.add(row.toString());        } else {            for (String v : version2qualifiersAndValues.values()) {                rows.add(v);            }        }        return rows;    }}</code></pre><p><strong>com/hsiehchou/hbase/extractor/ToRowMap.java</strong></p><pre><code>package com.hsiehchou.hbase.extractor;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.CellUtil;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.util.HashMap;import java.util.Map;/** * currentVersion 标识是否取多版本的数据，默认取当前版本 * 对当前版本，返回row`#`qualifier-&gt;value的映射 * 对多个版本，返回row`#`version`#`qualifier-&gt;value的映射 */public class ToRowMap implements RowExtractor&lt;Map&lt;String, String&gt;&gt; {    private Boolean currentVersion;    public ToRowMap() {        this(true);    }    private ToRowMap(Boolean currentVersion) {        this.currentVersion = currentVersion;    }    @Override    public Map&lt;String, String&gt; extractRowData(Result result, int rowNum)            throws IOException {        if(result == null || result.isEmpty()) return null;        final char HashTag = &#39;#&#39;;        HashMap&lt;String, String&gt; col2value = new HashMap&lt;&gt;();        String rowKey = Bytes.toString(result.getRow());        for (Cell cell : result.listCells()) {            String value = Bytes.toString(cell.getValueArray(),                    cell.getValueOffset(), cell.getValueLength());            String qualifier = Bytes.toString(CellUtil.cloneQualifier(cell));            if (currentVersion)                col2value.put(rowKey + HashTag + qualifier, value);            else {                long version = cell.getTimestamp();                col2value.put(rowKey + HashTag + version + HashTag + qualifier, value);            }        }        return col2value;    }}</code></pre><p><strong>com/hsiehchou/hbase/insert/HBaseInsertException.java</strong></p><pre><code>package com.hsiehchou.hbase.insert;import java.util.Iterator;public class HBaseInsertException extends Exception{    public HBaseInsertException(String message) {        super(message);    }    public final synchronized void addSuppresseds(Iterable&lt;Exception&gt; exceptions){        if(exceptions != null){            Iterator&lt;Exception&gt; iterator = exceptions.iterator();            while (iterator.hasNext()){                addSuppressed(iterator.next());            }        }    }}</code></pre><p><strong>com/hsiehchou/hbase/insert/HBaseInsertHelper.java</strong></p><pre><code>package com.hsiehchou.hbase.insert;import com.hsiehchou.hbase.config.HBaseTableUtil;import com.google.common.collect.Lists;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Table;import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;import java.io.Serializable;import java.util.ArrayList;import java.util.Collections;import java.util.List;import java.util.concurrent.CountDownLatch;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;/** * 添加HBASE 插入数据类 */public class HBaseInsertHelper implements Serializable{    private HBaseInsertHelper(){}    public static void put(String tableName, Put put) throws Exception {        put(tableName, Lists.newArrayList(put));    }    public static void put(String tableName, List&lt;Put&gt; puts) throws Exception {        if(!puts.isEmpty()){            Table table = HBaseTableUtil.getTable(tableName);            try {                table.put(puts);            }catch (Exception e){                e.printStackTrace();            }finally {                HBaseTableUtil.close(table);            }        }     }    public static void put(final String tableName, List&lt;Put&gt; puts, int perThreadPutSize) throws Exception {        int size = puts.size();        if(size &gt; perThreadPutSize){            int threadNum = (int)Math.ceil(size / (double)perThreadPutSize);            ExecutorService executorService = Executors.newFixedThreadPool(threadNum);            final CountDownLatch  cdl = new CountDownLatch(threadNum);            final List&lt;Exception&gt;  es = Collections.synchronizedList(new ArrayList&lt;Exception&gt;());            try {                for(int i = 0; i &lt; threadNum; i++){                    final List&lt;Put&gt; tmp;                    if(i == (threadNum - 1)){                        tmp = puts.subList(perThreadPutSize*i, size);                    }else{                        tmp = puts.subList(perThreadPutSize*i, perThreadPutSize*(i + 1));                    }                    executorService.execute(new Runnable() {                        public void run() {                            try {                                if(es.isEmpty()) put(tableName, tmp);                            } catch (Exception e) {                                es.add(e);                            }finally {                                cdl.countDown();                            }                        }                    });                }                cdl.await();            }finally {                executorService.shutdown();            }            if(es.size() &gt; 0){                HBaseInsertException insertException = new HBaseInsertException(String.format(&quot;put数据到表%s失败。&quot;));                insertException.addSuppresseds(es);                throw insertException;            }        }else {            put(tableName, puts);        }    }    public static void checkAndPut(String tableName, byte[] row, byte[] family, byte[] qualifier,                                   byte[] value, Put put) throws Exception {        checkAndPut(tableName, row, family, qualifier, null, value, put);    }    public static void checkAndPut(String tableName, byte[] row, byte[] family, byte[] qualifier,                                   CompareOp compareOp, byte[] value, Put put) throws Exception {        if(!put.isEmpty() ){            Table table = HBaseTableUtil.getTable(tableName);            try {                if(compareOp == null){                    table.checkAndPut(row, family, qualifier, value, put);                }else{                    table.checkAndPut(row, family, qualifier, compareOp, value, put);                }            }finally{                HBaseTableUtil.close(table);            }        }    }}</code></pre><p><strong>com/hsiehchou/hbase/search/HBaseSearchService.java</strong></p><pre><code>package com.hsiehchou.hbase.search;import com.hsiehchou.hbase.extractor.RowExtractor;import org.apache.hadoop.hbase.client.Get;import org.apache.hadoop.hbase.client.Scan;import java.io.IOException;import java.util.List;import java.util.Map;public interface HBaseSearchService {    /**      *  根据  用户 给定的解析类  解析  查询结果      * @param tableName      * @param scan      * @param extractor  用户自定义的 结果解析 类      * @return      * @throws IOException      * List&lt;T&gt;     */    &lt;T&gt; List&lt;T&gt; search(String tableName, Scan scan, RowExtractor&lt;T&gt; extractor) throws IOException;    /**      * 当存在多个  scan时  采用多线程查询      * @param tableName      * @param scans      * @param extractor  用户自定义的 结果解析 类      * @return      * @throws IOException      * List&lt;T&gt;     */    &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Scan&gt; scans, RowExtractor&lt;T&gt; extractor) throws IOException;    /**      * 采用多线程  同时查询多个表      * @param more      * @return      * @throws IOException      * List&lt;T&gt;     */    &lt;T&gt; Map&lt;String,List&lt;T&gt;&gt; searchMore(List&lt;SearchMoreTable&lt;T&gt;&gt; more) throws IOException;    /**      * 利用反射  自动封装实体类      * @param tableName      * @param scan          * @param cls   HBase表对应的实体类，属性只包含对应表的 列 ， 不区分大小写      * @return      * @throws IOException      * @throws InstantiationException      * @throws IllegalAccessException      * List&lt;T&gt;     */    &lt;T&gt; List&lt;T&gt; search(String tableName, Scan scan, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException;    /**      * 当存在多个 scan 时  采用多线程查询      * @param tableName      * @param scans      * @param cls   HBase表对应的实体类，属性只包含对应表的 列 ， 不区分大小写      * @return      * @throws IOException      * @throws InstantiationException      * @throws IllegalAccessException      * List&lt;T&gt;     */    &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Scan&gt; scans, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException;    /**      * 批量 get 查询  并按自定义的方式解析结果集      * @param tableName      * @param gets      * @param extractor  用户自定义的 结果解析 类      * @return      * @throws IOException      * List&lt;T&gt;     */    &lt;T&gt; List&lt;T&gt; search(String tableName, List&lt;Get&gt; gets, RowExtractor&lt;T&gt; extractor) throws IOException;    /**      * 多线程批量get, 并按自定义的方式解析结果集      * 建议 : perThreadExtractorGetNum &gt;= 100      * @param tableName      * @param gets      * @param perThreadExtractorGetNum    每个线程处理的 get的个数       * @param extractor  用户自定义的 结果解析 类      * @return      * @throws IOException      * List&lt;T&gt;     */    &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Get&gt; gets, int perThreadExtractorGetNum, RowExtractor&lt;T&gt; extractor) throws IOException;    /**      * 批量 get 查询  并利用反射 封装到指定的实体类中      * @param tableName      * @param gets      * @param  cls   HBase表对应的实体类，属性只包含对应表的 列 ， 不区分大小写      * @return            * @throws IOException      * @throws InstantiationException      * List&lt;T&gt;     */    &lt;T&gt; List&lt;T&gt; search(String tableName, List&lt;Get&gt; gets, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException;    /**      * 多线程批量 get 查询  并利用反射 封装到指定的实体类中      * 建议 : perThreadExtractorGetNum &gt;= 100      * @param tableName      * @param gets      * @param perThreadExtractorGetNum  每个线程处理的 get的个数       * @param  cls   HBase表对应的实体类，属性只包含对应表的 列 ， 不区分大小写      * @return      * @throws IOException      * @throws InstantiationException      * @throws IllegalAccessException      * List&lt;T&gt;     */    &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Get&gt; gets, int perThreadExtractorGetNum, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException;    /**      * get 查询  并按自定义的方式解析结果集      * @param tableName      * @param extractor   用户自定义的 结果解析 类      * @return     如果 查询不到  则 返回  null      * @throws IOException      * List&lt;T&gt;     */    &lt;T&gt; T search(String tableName, Get get, RowExtractor&lt;T&gt; extractor) throws IOException;    /**      * get 查询  并利用反射 封装到指定的实体类中      * @param tableName      * @param  cls   HBase表对应的实体类，属性只包含对应表的 列 ， 不区分大小写      * @return     如果 查询不到  则 返回  null      * @throws IOException      * @throws InstantiationException      * List&lt;T&gt;     */    &lt;T&gt; T search(String tableName, Get get, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException;}</code></pre><p><strong>com/hsiehchou/hbase/search/HBaseSearchServiceImpl.java</strong></p><pre><code>package com.hsiehchou.hbase.search;import com.hsiehchou.hbase.config.HBaseTableFactory;import com.hsiehchou.hbase.extractor.RowExtractor;import org.apache.hadoop.hbase.client.*;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.io.Serializable;import java.util.ArrayList;import java.util.Arrays;import java.util.List;import java.util.Map;public class HBaseSearchServiceImpl implements HBaseSearchService,Serializable{    private static final long serialVersionUID = -8657479861137115645L;    private static final Logger LOG = LoggerFactory.getLogger(HBaseSearchServiceImpl.class);    private HBaseTableFactory factory = new HBaseTableFactory();    private int poolCapacity = 6;    @Override    public &lt;T&gt; List&lt;T&gt; search(String tableName, Scan scan, RowExtractor&lt;T&gt; extractor) throws IOException {        return null;    }    @Override    public &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Scan&gt; scans, RowExtractor&lt;T&gt; extractor) throws IOException {        return null;    }    @Override    public &lt;T&gt; Map&lt;String, List&lt;T&gt;&gt; searchMore(List&lt;SearchMoreTable&lt;T&gt;&gt; more) throws IOException {        return null;    }    @Override    public &lt;T&gt; List&lt;T&gt; search(String tableName, Scan scan, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException {        return null;    }    @Override    public &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Scan&gt; scans, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException {        return null;    }    @Override    public &lt;T&gt; List&lt;T&gt; search(String tableName, List&lt;Get&gt; gets, RowExtractor&lt;T&gt; extractor) throws IOException {        List&lt;T&gt; data = new ArrayList&lt;T&gt;();        search(tableName, gets, extractor,data);        return data;    }    @Override    public &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Get&gt; gets, int perThreadExtractorGetNum, RowExtractor&lt;T&gt; extractor) throws IOException {        return null;    }    @Override    public &lt;T&gt; List&lt;T&gt; search(String tableName, List&lt;Get&gt; gets, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException {        return null;    }    @Override    public &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Get&gt; gets, int perThreadExtractorGetNum, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException {        return null;    }    @Override    public &lt;T&gt; T search(String tableName, Get get, RowExtractor&lt;T&gt; extractor) throws IOException {        T obj = null;        List&lt;T&gt; res = search(tableName,Arrays.asList(get),extractor);        if( !res.isEmpty()){            obj = res.get(0);        }        return obj;    }    @Override    public &lt;T&gt; T search(String tableName, Get get, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException {        return null;    }    private &lt;T&gt; void search(String tableName, List&lt;Get&gt; gets,                            RowExtractor&lt;T&gt; extractor , List&lt;T&gt; data ) throws IOException {        //根据table名获取表连接        Table table = factory.getHBaseTableInstance(tableName);        if(table != null ){            Result[] results = table.get(gets);            int n = 0;            T row = null;            for( Result result : results){                if( !result.isEmpty() ){                    row = extractor.extractRowData(result, n);                    if(row != null )data.add(row);                    n++;                }            }            close( table, null);        }else{            throw new IOException(&quot; table  &quot; + tableName + &quot; is not exists ..&quot;);        }    }    public static boolean  existsRowkey( Table table, String rowkey){        boolean exists =true;        try {            exists = table.exists(new Get(rowkey.getBytes()));        } catch (IOException e) {            LOG.error(&quot;失败。&quot;, e );        }        return exists;    }    public static void  close( Table table, ResultScanner scanner ){        try {            if( table != null ){                table.close();                table = null;            }            if( scanner != null ){                scanner.close();                scanner = null;            }        } catch (IOException e) {            LOG.error(&quot;关闭 HBase的表  &quot; + table.getName().toString() + &quot; 失败。&quot;, e );        }    }}</code></pre><p><strong>com/hsiehchou/hbase/search/SearchMoreTable.java</strong></p><pre><code>package com.hsiehchou.hbase.search;import com.hsiehchou.hbase.extractor.RowExtractor;import org.apache.hadoop.hbase.client.Scan;public class SearchMoreTable&lt;T&gt; {    private String tableName;    private Scan scan;    private RowExtractor&lt;T&gt; extractor;    public SearchMoreTable() {        super();    }    public SearchMoreTable(String tableName, Scan scan,            RowExtractor&lt;T&gt; extractor) {        super();        this.tableName = tableName;        this.scan = scan;        this.extractor = extractor;    }    public String getTableName() {        return tableName;    }    public void setTableName(String tableName) {        this.tableName = tableName;    }    public Scan getScan() {        return scan;    }    public void setScan(Scan scan) {        this.scan = scan;    }    public RowExtractor&lt;T&gt; getExtractor() {        return extractor;    }    public void setExtractor(RowExtractor&lt;T&gt; extractor) {        this.extractor = extractor;    }}</code></pre><p><strong>com/hsiehchou/hbase/spilt/SpiltRegionUtil.java</strong></p><pre><code>package com.hsiehchou.hbase.spilt;import org.apache.hadoop.hbase.util.Bytes;import java.util.Iterator;import java.util.TreeSet;/** * hbase 预分区 */public class SpiltRegionUtil {    /**     * 定义分区     * @return     */    public static byte[][] getSplitKeysBydinct() {        String[] keys = new String[]{&quot;1&quot;,&quot;2&quot;, &quot;3&quot;,&quot;4&quot;, &quot;5&quot;,&quot;6&quot;, &quot;7&quot;,&quot;8&quot;, &quot;9&quot;,&quot;a&quot;,&quot;b&quot;, &quot;c&quot;,&quot;d&quot;,&quot;e&quot;,&quot;f&quot;};        //String[] keys = new String[]{&quot;10|&quot;, &quot;20|&quot;, &quot;30|&quot;, &quot;40|&quot;, &quot;50|&quot;, &quot;60|&quot;, &quot;70|&quot;, &quot;80|&quot;, &quot;90|&quot;};        byte[][] splitKeys = new byte[keys.length][];        //通过treeset排序        TreeSet&lt;byte[]&gt; rows = new TreeSet&lt;byte[]&gt;(Bytes.BYTES_COMPARATOR);//升序排序        for (int i = 0; i &lt; keys.length; i++) {            rows.add(Bytes.toBytes(keys[i]));        }        Iterator&lt;byte[]&gt; rowKeyIter = rows.iterator();        int i = 0;        while (rowKeyIter.hasNext()) {            byte[] tempRow = rowKeyIter.next();            rowKeyIter.remove();            splitKeys[i] = tempRow;            i++;        }        return splitKeys;    }}</code></pre><h4 id="6、执行"><a href="#6、执行" class="headerlink" title="6、执行"></a>6、执行</h4><p>spark-submit <code>--</code>master local[1] <code>--</code>num-executors 1 <code>--</code>driver-memory 300m <code>--</code>executor-memory 500m <code>--</code>executor-cores 1 <code>--</code>jars $(echo /usr/chl/spark7/jars/*.jar | tr ‘ ‘ ‘,’) <code>--</code>class com.hsiehchou.spark.streaming.kafka.kafka2hbase.DataRelationStreaming /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</p><h4 id="7、执行截图"><a href="#7、执行截图" class="headerlink" title="7、执行截图"></a>7、执行截图</h4><p><img src="/medias/hbase_list.PNG" alt="hbase_list"></p><p><img src="/medias/hbase_scan.PNG" alt="hbase_scan"></p><p><img src="/medias/hbase%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE.PNG" alt="hbase写入数据"></p><h3 id="十二、SpringCloud-项目构建"><a href="#十二、SpringCloud-项目构建" class="headerlink" title="十二、SpringCloud 项目构建"></a>十二、SpringCloud 项目构建</h3><p><img src="/medias/SpringCloud%E5%BE%AE%E6%9C%8D%E5%8A%A1.PNG" alt="SpringCloud微服务"></p><p><img src="/medias/%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C.PNG" alt="服务注册"></p><p><strong>解决IntelliJ IDEA 创建Maven项目速度慢问题</strong><br>add Maven Property<br>Name:archetypeCatalog<br>Value:internal</p><h4 id="1、构建SpringCloud父项目"><a href="#1、构建SpringCloud父项目" class="headerlink" title="1、构建SpringCloud父项目"></a>1、构建SpringCloud父项目</h4><p>在原项目下新建 xz_bigdata_springcloud_dir目录</p><p><img src="/medias/%E6%96%B0%E5%BB%BA%20xz_bigdata_springcloud_dir%E7%9B%AE%E5%BD%95.PNG" alt="新建 xz_bigdata_springcloud_dir目录"></p><h4 id="2、在此目录下新建-xz-bigdata-springclod-root项目"><a href="#2、在此目录下新建-xz-bigdata-springclod-root项目" class="headerlink" title="2、在此目录下新建 xz_bigdata_springclod_root项目"></a>2、在此目录下新建 xz_bigdata_springclod_root项目</h4><p><img src="/medias/%E6%96%B0%E5%BB%BA%20xz_bigdata_springcloud_root%E9%A1%B9%E7%9B%AE.PNG" alt="新建 xz_bigdata_springcloud_root项目"></p><h4 id="3、-引入SpringCloud依赖"><a href="#3、-引入SpringCloud依赖" class="headerlink" title="3、    引入SpringCloud依赖"></a>3、    引入SpringCloud依赖</h4><p><strong>父pom.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;  &lt;modules&gt;    &lt;module&gt;xz_bigdata_springcloud_common&lt;/module&gt;    &lt;module&gt;xz_bigdata_springcloud_esquery&lt;/module&gt;    &lt;module&gt;xz_bigdata_springcloud_eureka&lt;/module&gt;    &lt;module&gt;xz_bigdata_springcloud_hbasequery&lt;/module&gt;  &lt;/modules&gt;  &lt;parent&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;    &lt;version&gt;2.0.9.RELEASE&lt;/version&gt;  &lt;/parent&gt;  &lt;groupId&gt;com.hsiehchou.springcloud&lt;/groupId&gt;  &lt;artifactId&gt;xz_bigdata_springcloud_root&lt;/artifactId&gt;  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;  &lt;packaging&gt;pom&lt;/packaging&gt;  &lt;name&gt;xz_bigdata_springcloud_root&lt;/name&gt;  &lt;properties&gt;    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;    &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;    &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;  &lt;/properties&gt;  &lt;!--CDH源--&gt;  &lt;repositories&gt;    &lt;repository&gt;      &lt;id&gt;cloudera&lt;/id&gt;      &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;    &lt;/repository&gt;  &lt;/repositories&gt;  &lt;!--依赖管理，用于管理spring-cloud的依赖--&gt;  &lt;dependencyManagement&gt;    &lt;dependencies&gt;      &lt;!--spring-cloud-dependencies--&gt;      &lt;dependency&gt;        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;        &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;        &lt;version&gt;Finchley.SR1&lt;/version&gt;        &lt;type&gt;pom&lt;/type&gt;        &lt;scope&gt;import&lt;/scope&gt;      &lt;/dependency&gt;    &lt;/dependencies&gt;  &lt;/dependencyManagement&gt;  &lt;!--打包插件--&gt;  &lt;build&gt;    &lt;plugins&gt;      &lt;plugin&gt;        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;        &lt;version&gt;3.1&lt;/version&gt;        &lt;configuration&gt;          &lt;source&gt;1.8&lt;/source&gt;          &lt;target&gt;1.8&lt;/target&gt;          &lt;encoding&gt;UTF-8&lt;/encoding&gt;        &lt;/configuration&gt;      &lt;/plugin&gt;    &lt;/plugins&gt;  &lt;/build&gt;&lt;/project&gt;</code></pre><p><strong>删除父项目src目录。因为这个项目主要是管理子项目不做任何逻辑业务</strong></p><h4 id="4、构建SpringCloud-Common子项目"><a href="#4、构建SpringCloud-Common子项目" class="headerlink" title="4、构建SpringCloud Common子项目"></a>4、构建SpringCloud Common子项目</h4><p><strong>新建子模块</strong><br>xz_bigdata_springcloud_common</p><p><strong>引入依赖</strong></p><p><strong>pom.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata_springcloud_root&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_springcloud_common&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_springcloud_common&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!--eureka-server--&gt;        &lt;!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-eureka-server --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;            &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt;                    &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;            &lt;version&gt;1.2.24&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><h4 id="5、构建Eureka服务注册中心"><a href="#5、构建Eureka服务注册中心" class="headerlink" title="5、构建Eureka服务注册中心"></a>5、构建Eureka服务注册中心</h4><p><strong>新建xz_bigdata_springcloud_eureka子模块</strong></p><p><img src="/medias/%E6%96%B0%E5%BB%BAxz_bigdata_springcloud_eureka%E5%AD%90%E6%A8%A1%E5%9D%97.PNG" alt="新建xz_bigdata_springcloud_eureka子模块"></p><p><strong>引入依赖</strong></p><p><strong>pom.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata_springcloud_root&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_springcloud_eureka&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_springcloud_eureka&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou.springcloud&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_springcloud_common&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;        &lt;/dependency&gt;        &lt;!--用户验证--&gt;  &lt;!--      &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;            &lt;version&gt;1.4.1.RELEASE&lt;/version&gt;        &lt;/dependency&gt;--&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;&lt;!--打包依赖的jar包--&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;outputDirectory&gt;${project.build.directory}/lib&lt;/outputDirectory&gt;                    &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt; &lt;!-- 表示是否不包含间接依赖的包 --&gt;                    &lt;stripVersion&gt;false&lt;/stripVersion&gt; &lt;!-- 去除版本信息 --&gt;                &lt;/configuration&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;copy-dependencies&lt;/id&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;!-- 拷贝项目依赖包到lib/目录下 --&gt;                            &lt;outputDirectory&gt;${project.build.directory}/jars&lt;/outputDirectory&gt;                            &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt;                            &lt;stripVersion&gt;false&lt;/stripVersion&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;!-- 打成jar包插件 --&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;                &lt;version&gt;2.5&lt;/version&gt;                &lt;configuration&gt;                    &lt;archive&gt;                        &lt;!--                        生成的jar中，不要包含pom.xml和pom.properties这两个文件                    --&gt;                        &lt;addMavenDescriptor&gt;false&lt;/addMavenDescriptor&gt;                        &lt;!-- 生成MANIFEST.MF的设置 --&gt;                        &lt;manifest&gt;                            &lt;!-- 为依赖包添加路径, 这些路径会写在MANIFEST文件的Class-Path下 --&gt;                            &lt;addClasspath&gt;true&lt;/addClasspath&gt;                            &lt;classpathPrefix&gt;jars/&lt;/classpathPrefix&gt;                            &lt;!-- jar启动入口类--&gt;                            &lt;mainClass&gt;com.cn.hbase.mr.HbaseMr&lt;/mainClass&gt;                        &lt;/manifest&gt;                        &lt;!--       &lt;manifestEntries&gt;                                   &amp;lt;!&amp;ndash; 在Class-Path下添加配置文件的路径 &amp;ndash;&amp;gt;                                   &lt;Class-Path&gt;&lt;/Class-Path&gt;                               &lt;/manifestEntries&gt;--&gt;                    &lt;/archive&gt;                    &lt;outputDirectory&gt;${project.build.directory}/&lt;/outputDirectory&gt;                    &lt;includes&gt;                        &lt;!-- 打jar包时，只打包class文件 --&gt;                        &lt;include&gt;**/*.class&lt;/include&gt;                        &lt;include&gt;**/*.properties&lt;/include&gt;                        &lt;include&gt;**/*.yml&lt;/include&gt;                    &lt;/includes&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><p>新建resources配置文件目录，添加application.yml配置文件或者 application.properties</p><p><strong>application.yml</strong></p><pre><code>server:  port: 8761eureka:  client:    register-with-eureka: false    fetch-registry: false    service-url:      defaultZone: http://root:root@hadoop3:8761/eureka/</code></pre><p><img src="/medias/xz_bigdata_springcloud_eureka%E7%BB%93%E6%9E%84.PNG" alt="xz_bigdata_springcloud_eureka结构"></p><p><strong>新建EurekaApplication 启动类</strong></p><p><strong>EurekaApplication.java</strong></p><pre><code>package com.hsiehchou.springcloud.eureka;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;/** * 注册中心 */@SpringBootApplication@EnableEurekaServerpublic class EurekaApplication{    public static void main( String[] args )    {        SpringApplication.run(EurekaApplication.class, args);    }}</code></pre><p><strong>执行EurekaApplication 启动</strong></p><p><strong>访问localhost:8761</strong></p><p><img src="/medias/%E8%AE%BF%E9%97%AEhadoop38761.PNG" alt="访问hadoop3:8761"></p><h4 id="6、构建HBase查询服务模块"><a href="#6、构建HBase查询服务模块" class="headerlink" title="6、构建HBase查询服务模块"></a>6、构建HBase查询服务模块</h4><p><strong>新建xz_bigdata_springcloud_root子模块</strong></p><p><img src="/medias/%E6%96%B0%E5%BB%BAxz_bigdata_springcloud_root%E5%AD%90%E6%A8%A1%E5%9D%97.PNG" alt="新建xz_bigdata_springcloud_root子模块"></p><p><strong>添加依赖</strong></p><p><strong>pom.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;artifactId&gt;xz_bigdata_springcloud_root&lt;/artifactId&gt;        &lt;groupId&gt;com.hsiehchou.springcloud&lt;/groupId&gt;        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;xz_bigdata_springcloud_hbasequery&lt;/artifactId&gt;    &lt;name&gt;xz_bigdata_springcloud_hbasequery&lt;/name&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!--spring common依赖--&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou.springcloud&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_springcloud_common&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt;                    &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;!--基础服务hbase依赖--&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;            &lt;artifactId&gt;xz_bigdata_hbase&lt;/artifactId&gt;            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;artifactId&gt;fastjson&lt;/artifactId&gt;                    &lt;groupId&gt;com.alibaba&lt;/groupId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;&lt;!--打包依赖的jar包--&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;outputDirectory&gt;${project.build.directory}/lib&lt;/outputDirectory&gt;                    &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt; &lt;!-- 表示是否不包含间接依赖的包 --&gt;                    &lt;stripVersion&gt;false&lt;/stripVersion&gt; &lt;!-- 去除版本信息 --&gt;                &lt;/configuration&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;copy-dependencies&lt;/id&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;!-- 拷贝项目依赖包到lib/目录下 --&gt;                            &lt;outputDirectory&gt;${project.build.directory}/jars&lt;/outputDirectory&gt;                            &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt;                            &lt;stripVersion&gt;false&lt;/stripVersion&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;!-- 打成jar包插件 --&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;                &lt;version&gt;2.5&lt;/version&gt;                &lt;configuration&gt;                    &lt;archive&gt;                        &lt;!--                        生成的jar中，不要包含pom.xml和pom.properties这两个文件                    --&gt;                        &lt;addMavenDescriptor&gt;false&lt;/addMavenDescriptor&gt;                        &lt;!-- 生成MANIFEST.MF的设置 --&gt;                        &lt;manifest&gt;                            &lt;!-- 为依赖包添加路径, 这些路径会写在MANIFEST文件的Class-Path下 --&gt;                            &lt;addClasspath&gt;true&lt;/addClasspath&gt;                            &lt;classpathPrefix&gt;jars/&lt;/classpathPrefix&gt;                            &lt;!-- jar启动入口类--&gt;                            &lt;mainClass&gt;com.cn.hbase.mr.HbaseMr&lt;/mainClass&gt;                        &lt;/manifest&gt;                        &lt;!--       &lt;manifestEntries&gt;                                   &amp;lt;!&amp;ndash; 在Class-Path下添加配置文件的路径 &amp;ndash;&amp;gt;                                   &lt;Class-Path&gt;&lt;/Class-Path&gt;                               &lt;/manifestEntries&gt;--&gt;                    &lt;/archive&gt;                    &lt;outputDirectory&gt;${project.build.directory}/&lt;/outputDirectory&gt;                    &lt;includes&gt;                        &lt;!-- 打jar包时，只打包class文件 --&gt;                        &lt;include&gt;**/*.class&lt;/include&gt;                        &lt;include&gt;**/*.properties&lt;/include&gt;                        &lt;include&gt;**/*.yml&lt;/include&gt;                    &lt;/includes&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><p><strong>添加配置文件</strong></p><p><strong>新建 resources 目录</strong><br>添加 <strong>application.properties</strong> 文件</p><pre><code>server.port=8002logging.level.root=INFOlogging.level.org.hibernate=INFOlogging.level.org.hibernate.type.descriptor.sql.BasicBinder=TRACElogging.level.org.hibernate.type.descriptor.sql.BasicExtractor= TRACElogging.level.com.itmuch=DEBUGspring.http.encoding.charset=UTF-8spring.http.encoding.enable=truespring.http.encoding.force=trueeureka.client.serviceUrl.defaultZone=http://root:root@hadoop3:8761/eureka/spring.application.name=xz-bigdata-springcloud-hbasequeryeureka.instance.prefer-ip-address=true</code></pre><p><strong>构建启动类</strong></p><p>新建 <strong>com.hsiehchou.springcloud.hbase</strong>包<br>构建 <strong>HbaseApplication</strong> 启动类</p><pre><code>package com.hsiehchou.springcloud;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;@SpringBootApplication@EnableEurekaServerpublic class HbaseQueryApplication{    public static void main( String[] args )    {        SpringApplication.run(HbaseQueryApplication.class, args);    }}</code></pre><p><img src="/medias/%E6%B3%A8%E5%86%8C%E6%88%90%E5%8A%9F.PNG" alt="注册成功"><br>说明注册成功</p><p><strong>构建服务</strong></p><p><img src="/medias/%E6%9E%84%E5%BB%BAHbase%E6%9C%8D%E5%8A%A1.PNG" alt="构建Hbase服务"></p><p>构建 <strong>com.hsiehchou.springcloud.hbase.controller</strong></p><p>创建 <strong>HbaseBaseController</strong></p><p><strong>HbaseBaseController.java</strong></p><pre><code>package com.hsiehchou.springcloud.hbase.controller;import com.hsiehchou.hbase.extractor.SingleColumnMultiVersionRowExtrator;import com.hsiehchou.hbase.search.HBaseSearchService;import com.hsiehchou.hbase.search.HBaseSearchServiceImpl;import com.hsiehchou.springcloud.hbase.service.HbaseBaseService;import org.apache.hadoop.hbase.client.Get;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.*;import javax.annotation.Resource;import java.io.IOException;import java.util.HashSet;import java.util.List;import java.util.Map;import java.util.Set;@Controller@RequestMapping(value=&quot;/hbase&quot;)public class HbaseBaseController {    private static Logger LOG = LoggerFactory.getLogger(HbaseBaseController.class);    //注入 通过这个注解可以直接拿到HbaseBaseService这个的实例    @Resource    private HbaseBaseService hbaseBaseService;    @ResponseBody    @RequestMapping(value=&quot;/search/{table}/{rowkey}&quot;, method={RequestMethod.GET,RequestMethod.POST})    public Set&lt;String&gt; search(@PathVariable(value = &quot;table&quot;) String table,                              @PathVariable(value = &quot;rowkey&quot;) String rowkey){        return hbaseBaseService.getSingleColumn(table,rowkey);    }    @ResponseBody    @RequestMapping(value=&quot;/search1&quot;, method={RequestMethod.GET,RequestMethod.POST})    public Set&lt;String&gt; search1( @RequestParam(name = &quot;table&quot;) String table,                                @RequestParam(name = &quot;rowkey&quot;) String rowkey){        //通过二级索引去找主关联表的rowkey 这个rowkey就是MAC        return hbaseBaseService.getSingleColumn(table,rowkey);    }    @ResponseBody    @RequestMapping(value = &quot;/getHbase&quot;,method = {RequestMethod.GET,RequestMethod.POST})    public Set&lt;String&gt; getHbase(@RequestParam(name=&quot;table&quot;) String table,                                @RequestParam(name=&quot;rowkey&quot;) String rowkey){        return hbaseBaseService.getSingleColumn(table, rowkey);    }    @ResponseBody    @RequestMapping(value = &quot;/getRelation&quot;,method = {RequestMethod.GET,RequestMethod.POST})    public Map&lt;String,List&lt;String&gt;&gt; getRelation(@RequestParam(name = &quot;field&quot;) String field,                                                @RequestParam(name = &quot;fieldValue&quot;) String fieldValue){        return hbaseBaseService.getRealtion(field,fieldValue);    }    public static void main(String[] args) {        HbaseBaseController hbaseBaseController = new HbaseBaseController();        hbaseBaseController.getHbase(&quot;send_mail&quot;, &quot;65497873@qq.com&quot;);    }}</code></pre><p>构建 <strong>com.hsiehchou.springcloud.hbase.service</strong></p><p>创建 <strong>HbaseBaseService</strong></p><p><strong>HbaseBaseService.java</strong></p><pre><code>package com.hsiehchou.springcloud.hbase.service;import com.hsiehchou.hbase.entity.HBaseCell;import com.hsiehchou.hbase.entity.HBaseRow;import com.hsiehchou.hbase.extractor.MultiVersionRowExtrator;import com.hsiehchou.hbase.extractor.SingleColumnMultiVersionRowExtrator;import com.hsiehchou.hbase.search.HBaseSearchService;import com.hsiehchou.hbase.search.HBaseSearchServiceImpl;import org.apache.hadoop.hbase.client.Get;import org.apache.hadoop.hbase.client.Put;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.stereotype.Service;import javax.annotation.Resource;import java.io.IOException;import java.util.*;@Servicepublic class HbaseBaseService {    private static Logger LOG = LoggerFactory.getLogger(HbaseBaseService.class);    @Resource    private HbaseBaseService hbaseBaseService;    /**     * 获取hbase单列数据的多版本信息     * @param field     * @param rowkey     * @return     */    public Set&lt;String&gt; getSingleColumn(String field,String rowkey){        //从索引表中获取总关联表的rowkey,获取phone对应的多版本MAC        Set&lt;String&gt; search = null;        HBaseSearchService hBaseSearchService = new HBaseSearchServiceImpl();        String table = &quot;test:&quot;+field;        Get get = new Get(rowkey.getBytes());        try {            get.setMaxVersions(100);        } catch (IOException e) {            e.printStackTrace();        }        Set set = new HashSet&lt;String&gt;();        SingleColumnMultiVersionRowExtrator singleColumnMultiVersionRowExtrator = new SingleColumnMultiVersionRowExtrator(&quot;cf&quot;.getBytes(), &quot;phone_mac&quot;.getBytes(), set);        try {            search = hBaseSearchService.search(table, get, singleColumnMultiVersionRowExtrator);            System.out.println(search.toString());        } catch (IOException e) {            e.printStackTrace();        }        return search;    }    /**     *  获取单列多版本     * @param table     * @param rowkey     * @param versions     * @return     */    public Set&lt;String&gt; getSingleColumn(String table,String rowkey,int versions){        Set&lt;String&gt; search = null;        try {            HBaseSearchService baseSearchService = new HBaseSearchServiceImpl();            Get get = new Get(rowkey.getBytes());            get.setMaxVersions(versions);            Set set = new HashSet&lt;String&gt;();            SingleColumnMultiVersionRowExtrator singleColumnMultiVersionRowExtrator = new SingleColumnMultiVersionRowExtrator(&quot;cf&quot;.getBytes(), &quot;phone_mac&quot;.getBytes(), set);            search = baseSearchService.search(table, get, singleColumnMultiVersionRowExtrator);        } catch (IOException e) {            LOG.error(null,e);        }        System.out.println(search);        return search;    }    /**     * 直接通过关联表字段值获取整条记录     * hbase 二级查找     * @param field     * @param fieldValue     * @return     */    public Map&lt;String,List&lt;String&gt;&gt; getRealtion(String field,String fieldValue){        //第一步 从二级索引表中找到多版本的rowkey        Map&lt;String,List&lt;String&gt;&gt; map = new HashMap&lt;&gt;();        //首先查找索引表        //查找的表名        String table = &quot;test:&quot; + field;        String indexRowkey = fieldValue;        HbaseBaseService hbaseBaseService = new HbaseBaseService();        Set&lt;String&gt; relationRowkeys = hbaseBaseService.getSingleColumn(table, indexRowkey, 100);        //第二步 拿到二级索引表中得到的 主关联表的rowkey        //对这些rowkey进行遍历 获取主关联表中rowkey对应的所有多版本数据        //遍历relationRowkeys，将其封装成List&lt;Get&gt;        List&lt;Get&gt; list = new ArrayList&lt;&gt;();        relationRowkeys.forEach(relationRowkey-&gt;{            //通过relationRowkey去找relation表中的所有信息            Get get = new Get(relationRowkey.getBytes());            try {                get.setMaxVersions(100);            } catch (IOException e) {                e.printStackTrace();            }            list.add(get);        });        MultiVersionRowExtrator multiVersionRowExtrator = new MultiVersionRowExtrator();        HBaseSearchService hBaseSearchService = new HBaseSearchServiceImpl();        try {            //&lt;T&gt; List&lt;T&gt; search(String tableName, List&lt;Get&gt; gets, RowExtractor&lt;T&gt; extractor) throws IOException;            List&lt;HBaseRow&gt; search = hBaseSearchService.search(&quot;test:relation&quot;, list, multiVersionRowExtrator);            search.forEach(hbaseRow-&gt;{                Map&lt;String, Collection&lt;HBaseCell&gt;&gt; cellMap = hbaseRow.getCell();                cellMap.forEach((key,value)-&gt;{                    //把Map&lt;String,Collection&lt;HBaseCell&gt;&gt;转为Map&lt;String,List&lt;String&gt;&gt;                    List&lt;String&gt; listValue = new ArrayList&lt;&gt;();                    value.forEach(x-&gt;{                        listValue.add(x.toString());                    });                    map.put(key,listValue);                });            });        } catch (IOException e) {            e.printStackTrace();        }        System.out.println(map.toString());     return map;    }    public static void main(String[] args) {        HbaseBaseService hbaseBaseService = new HbaseBaseService();//        hbaseBaseService.getRealtion(&quot;send_mail&quot;,&quot;65494533@qq.com&quot;);        hbaseBaseService.getSingleColumn(&quot;phone&quot;,&quot;18609765012&quot;);    }}</code></pre><h4 id="7、构建ES查询服务"><a href="#7、构建ES查询服务" class="headerlink" title="7、构建ES查询服务"></a>7、构建ES查询服务</h4><p>使用jest API 是走的 <strong>HTTP 请求</strong>  <strong>9200端口</strong><br>依赖如下:</p><pre><code>&lt;dependency&gt;    &lt;groupId&gt;io.searchbox&lt;/groupId&gt;    &lt;artifactId&gt;jest&lt;/artifactId&gt;    &lt;version&gt;6.3.1&lt;/version&gt;&lt;/dependency&gt;</code></pre><p>9200作为Http协议，<strong>主要用于外部通讯</strong></p><p>9300作为Tcp协议，jar之间就是通过 <strong>tcp协议通讯</strong></p><p><strong>ES集群之间是通过9300进行通讯</strong></p><p><strong>新建xz_bigdata_springcloud_esquery</strong></p><p><strong>新建xz_bigdata_springcloud_esquery子项目</strong></p><p><strong>准备</strong></p><p>新建 <strong>resources</strong> 配置文件目录</p><p><strong>增加配置文件</strong></p><p><strong>application.properties</strong></p><pre><code>server.port=8003logging.level.root=INFOlogging.level.org.hibernate=INFOlogging.level.org.hibernate.type.descriptor.sql.BasicBinder=TRACElogging.level.org.hibernate.type.descriptor.sql.BasicExtractor= TRACElogging.level.com.itmuch=DEBUGspring.http.encoding.charset=UTF-8spring.http.encoding.enable=truespring.http.encoding.force=trueeureka.client.serviceUrl.defaultZone=http://root:root@hadoop3:8761/eureka/spring.application.name=xz-bigdata-springcloud-esqueryeureka.instance.prefer-ip-address=true#关闭EDES检测management.health.elasticsearch.enabled=falsespring.elasticsearch.jest.uris=[&quot;http://192.168.116.201:9200&quot;]#全部索引esIndexs=wechat,mail,qq</code></pre><p><strong>新建ES微服务启动类</strong></p><p><strong>ESqueryApplication.java</strong></p><pre><code>package com.hsiehchou.springcloud.es;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;import org.springframework.cloud.openfeign.EnableFeignClients;@SpringBootApplication@EnableEurekaServer@EnableDiscoveryClient@EnableFeignClientspublic class ESqueryApplication {    public static void main(String[] args) {        SpringApplication.run(ESqueryApplication.class,args);    }}</code></pre><p><strong>启动 Eureka  ES 微服务</strong></p><p><img src="/medias/%E6%B3%A8%E5%86%8C%E6%88%90%E5%8A%9F.PNG" alt="注册成功"><br>说明注册成功</p><p><img src="/medias/ES%E8%B0%83%E7%94%A8Hbase.PNG" alt="ES调用Hbase"></p><p>构建 <strong>com.hsiehchou.springcloud.es.controller</strong></p><p>创建 <strong>EsBaseController</strong></p><pre><code>package com.hsiehchou.springcloud.es.controller;import com.hsiehchou.springcloud.es.feign.HbaseFeign;import com.hsiehchou.springcloud.es.service.EsBaseService;import org.springframework.beans.factory.annotation.Value;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.ResponseBody;import javax.annotation.Resource;import java.util.List;import java.util.Map;import java.util.Set;@Controller@RequestMapping(value = &quot;/es&quot;)public class EsBaseController {    @Value(&quot;${esIndexs}&quot;)    private String esIndexs;    @Resource    private EsBaseService esBaseService;    @Resource    private HbaseFeign hbaseFeign;    /**     * 基础查询     * @param indexName     * @param typeName     * @param sortField     * @param sortValue     * @param pageNumber     * @param pageSize     * @return     */    @ResponseBody    @RequestMapping(value = &quot;/getBaseInfo&quot;, method = {RequestMethod.GET, RequestMethod.POST})    public List&lt;Map&lt;String, Object&gt;&gt; getBaseInfo(@RequestParam(name = &quot;indexName&quot;) String indexName,                                                 @RequestParam(name = &quot;typeName&quot;) String typeName,                                                 @RequestParam(name = &quot;sortField&quot;) String sortField,                                                 @RequestParam(name = &quot;sortValue&quot;) String sortValue,                                                 @RequestParam(name = &quot;pageNumber&quot;) int pageNumber,                                                 @RequestParam(name = &quot;pageSize&quot;) int pageSize) {        // 根据数据类型, 排序，分页        // indexName typeName        // sortField sortValue        // pageNumber  pageSize        return  esBaseService.getBaseInfo(indexName,typeName,sortField,sortValue,pageNumber,pageSize);    }    /**     * 根据任意条件查找轨迹数据     * @param field     * @param fieldValue     * @return     */    @ResponseBody    @RequestMapping(value = &quot;/getLocus&quot;, method = {RequestMethod.GET, RequestMethod.POST})    public List&lt;Map&lt;String, Object&gt;&gt; getLocus(@RequestParam(name = &quot;field&quot;) String field,                                                 @RequestParam(name = &quot;fieldValue&quot;) String fieldValue) {        Set&lt;String&gt; macs = hbaseFeign.search1(field, fieldValue);        System.out.println(macs.toString());        // 根据数据类型, 排序，分页        // indexName typeName        // sortField sortValue        // pageNumber  pageSize        String mac = macs.iterator().next();        return  esBaseService.getLocus(mac);    }    /**     * 所有表数据总量     * @return     */    @ResponseBody    @RequestMapping(value=&quot;/getAllCount&quot;, method={RequestMethod.GET,RequestMethod.POST})    public Map&lt;String,Long&gt; getAllCount(){        Map&lt;String, Long&gt; allCount = esBaseService.getAllCount(esIndexs);        System.out.println(allCount);        return allCount;    }    @ResponseBody    @RequestMapping(value=&quot;/group&quot;, method={RequestMethod.GET,RequestMethod.POST})    public Map&lt;String,Long&gt; group(@RequestParam(name = &quot;indexName&quot;) String indexName,                                  @RequestParam(name = &quot;typeName&quot;) String typeName,                                  @RequestParam(name = &quot;field&quot;) String field){        return esBaseService.aggregation(indexName,typeName,field);    }    public static void main(String[] args){        EsBaseController esBaseController = new EsBaseController();        esBaseController.getLocus(&quot;phone&quot;,&quot;18609765432&quot;);    }}</code></pre><p>构建 <strong>com.hsiehchou.springcloud.es.service</strong></p><p>创建 <strong>EsBaseService</strong></p><pre><code>package com.hsiehchou.springcloud.es.service;import com.hsiehchou.es.jest.service.JestService;import com.hsiehchou.es.jest.service.ResultParse;import io.searchbox.client.JestClient;import io.searchbox.core.SearchResult;import org.springframework.stereotype.Service;import java.util.HashMap;import java.util.List;import java.util.Map;@Servicepublic class EsBaseService {    // 根据数据类型, 排序，分页    // indexName typeName    // sortField sortValue    // pageNumber  pageSize    public List&lt;Map&lt;String, Object&gt;&gt; getBaseInfo(String indexName,                                                 String typeName,                                                 String sortField,                                                 String sortValue,                                                 int pageNumber,                                                 int pageSize) {        //实现查询        JestClient jestClient = null;        List&lt;Map&lt;String, Object&gt;&gt; maps = null;        try {            jestClient = JestService.getJestClient();            SearchResult search = JestService.search(jestClient,                    indexName,                    typeName,                    &quot;&quot;,                    &quot;&quot;,                    sortField,                    sortValue,                    pageNumber,                    pageSize);            maps = ResultParse.parseSearchResultOnly(search);        } catch (Exception e) {            e.printStackTrace();        } finally {            JestService.closeJestClient(jestClient);        }        return maps;    }    // 传时间范围   比如你要查3天之内的轨迹    // es中text的类型的可以直接查询，而keyword类型的必须带.keyword，例如，phone_mac.keyword    public List&lt;Map&lt;String, Object&gt;&gt; getLocus(String mac){        //实现查询        JestClient jestClient = null;        List&lt;Map&lt;String, Object&gt;&gt; maps = null;        String[] includes = new String[]{&quot;latitude&quot;,&quot;longitude&quot;,&quot;collect_time&quot;};        try {            jestClient = JestService.getJestClient();            SearchResult search = JestService.search(jestClient,                    &quot;&quot;,                    &quot;&quot;,                    &quot;phone_mac.keyword&quot;,                    mac,                    &quot;collect_time&quot;,                    &quot;asc&quot;,                    1,                    2000,                    includes);            maps = ResultParse.parseSearchResultOnly(search);        } catch (Exception e) {            e.printStackTrace();        } finally {            JestService.closeJestClient(jestClient);        }        return maps;    }     public Map&lt;String,Long&gt; getAllCount(String esIndexs){        Map&lt;String,Long&gt; countMap = new HashMap&lt;&gt;();        JestClient jestClient = null;        try {            jestClient = JestService.getJestClient();            String[] split = esIndexs.split(&quot;,&quot;);            for (int i = 0; i &lt; split.length; i++) {                String index = split[i];                Long count = JestService.count(jestClient, index, index);                countMap.put(index,count);            }        } catch (Exception e) {            e.printStackTrace();        }finally {            JestService.closeJestClient(jestClient);        }        return countMap;    }    public Map&lt;String,Long&gt; aggregation(String indexName,String typeName,String field){        JestClient jestClient = null;        Map&lt;String, Long&gt; stringLongMap = null;        try {            jestClient = JestService.getJestClient();            SearchResult aggregation = JestService.aggregation(jestClient, indexName, typeName, field);            stringLongMap = ResultParse.parseAggregation(aggregation);        } catch (Exception e) {            e.printStackTrace();        }finally {            JestService.closeJestClient(jestClient);        }        return stringLongMap;    }}</code></pre><p><strong>这里用到了ES的大数据基础服务</strong></p><p><strong>轨迹查询</strong></p><p>用到了 <strong>HBase</strong> 的服务，使用 <strong>Fegin</strong><br><strong>SpringCloud Feign</strong></p><p><strong>Feign</strong> 是一个声明式的伪Http客户端，它使得写Http客户端变得更简单。使用 <strong>Feign</strong> ，只需要创建一个接口并用注解的方式来配置它，即可完成对服务提供方的接口绑定服务调用客户端的开发量。</p><p>构建 <strong>com.hsiehchou.springcloud.es.fegin</strong></p><p>创建 <strong>HbaseFeign</strong></p><pre><code>package com.hsiehchou.springcloud.es.feign;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.ResponseBody;import java.util.Set;@FeignClient(name = &quot;xz-bigdata-springcloud-hbasequery&quot;)public interface HbaseFeign {    @ResponseBody    @RequestMapping(value=&quot;/hbase/search1&quot;, method=RequestMethod.GET)    public Set&lt;String&gt; search1(@RequestParam(name = &quot;table&quot;) String table,                               @RequestParam(name = &quot;rowkey&quot;) String rowkey);}</code></pre><h4 id="8、微服务手动部署"><a href="#8、微服务手动部署" class="headerlink" title="8、微服务手动部署"></a>8、微服务手动部署</h4><p><strong>Maven添加打包插件</strong></p><pre><code> &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;&lt;!--打包依赖的jar包--&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;outputDirectory&gt;${project.build.directory}/lib&lt;/outputDirectory&gt;                    &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt; &lt;!-- 表示是否不包含间接依赖的包 --&gt;                    &lt;stripVersion&gt;false&lt;/stripVersion&gt; &lt;!-- 去除版本信息 --&gt;                &lt;/configuration&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;copy-dependencies&lt;/id&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;!-- 拷贝项目依赖包到lib/目录下 --&gt;                            &lt;outputDirectory&gt;${project.build.directory}/jars&lt;/outputDirectory&gt;                            &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt;                            &lt;stripVersion&gt;false&lt;/stripVersion&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;!-- 打成jar包插件 --&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;                &lt;version&gt;2.4&lt;/version&gt;                &lt;configuration&gt;                    &lt;archive&gt;                        &lt;!--                        生成的jar中，不要包含pom.xml和pom.properties这两个文件                    --&gt;                        &lt;addMavenDescriptor&gt;false&lt;/addMavenDescriptor&gt;                        &lt;!-- 生成MANIFEST.MF的设置 --&gt;                        &lt;manifest&gt;                            &lt;!-- 为依赖包添加路径, 这些路径会写在MANIFEST文件的Class-Path下 --&gt;                            &lt;addClasspath&gt;true&lt;/addClasspath&gt;                            &lt;classpathPrefix&gt;jars/&lt;/classpathPrefix&gt;                            &lt;!-- jar启动入口类--&gt;                            &lt;mainClass&gt;com.cn.hbase.mr.HbaseMr&lt;/mainClass&gt;                        &lt;/manifest&gt;                        &lt;!--       &lt;manifestEntries&gt;                                   &amp;lt;!&amp;ndash; 在Class-Path下添加配置文件的路径 &amp;ndash;&amp;gt;                                   &lt;Class-Path&gt;&lt;/Class-Path&gt;                               &lt;/manifestEntries&gt;--&gt;                    &lt;/archive&gt;                    &lt;outputDirectory&gt;${project.build.directory}/&lt;/outputDirectory&gt;                    &lt;includes&gt;                        &lt;!-- 打jar包时，只打包class文件 --&gt;                        &lt;include&gt;**/*.class&lt;/include&gt;                        &lt;include&gt;**/*.properties&lt;/include&gt;                        &lt;include&gt;**/*.yml&lt;/include&gt;                    &lt;/includes&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;</code></pre><p>因为微服务<strong>依赖 xz_bigdata2</strong> 所以<strong>先打包xz_bigdata2</strong> </p><p><strong>修改配置文件</strong></p><pre><code>defaultZone: http://root:root@hadoop3:8761/eureka/</code></pre><p>将注册中心 IP 改为部署服务器的IP<br>微服务同理</p><p>上面给出的配置文件已经修改好了</p><p><strong>部署</strong></p><ol><li><strong>先部署Erueka服务中心</strong><br>新建<strong>/usr/chl/springcloud/eureka</strong></li></ol><p><img src="/medias/%E9%83%A8%E7%BD%B2%E5%9C%B0%E6%96%B9.PNG" alt="部署地方"></p><p>上传jars 和jar</p><p><img src="/medias/eureka.PNG" alt="eureka"></p><ol start="2"><li><strong>启动服务中心</strong><br>eureka服务注册中心启动</li></ol><pre><code>nohup java -cp /usr/chl/springcloud/eureka/xz_bigdata_springcloud_eureka-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.eureka.EurekaApplication &amp;</code></pre><p>查看日志</p><pre><code>tail -f nohup.out</code></pre><ol start="3"><li><strong>部署esquery</strong><br>esquery微服务启动</li></ol><pre><code>nohup java -cp /usr/chl/springcloud/esquery/xz_bigdata_springcloud_esquery-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.es.ESqueryApplication &amp;</code></pre><ol start="4"><li><strong>部署hbasequery</strong><br>hbasequery微服务启动</li></ol><pre><code>nohup java -cp /usr/chl/springcloud/hbasequery/xz_bigdata_springcloud_hbasequery-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.HbaseQueryApplication &amp;</code></pre><h4 id="9、执行-1"><a href="#9、执行-1" class="headerlink" title="9、执行"></a>9、执行</h4><ol><li><p>hadoop3:8002/hbase/getRelation?field=phone&amp;fieldValue=18609765012<br><img src="/medias/10.PNG" alt="1"></p></li><li><p>hadoop3:8002/hbase/search1?table=phone&amp;rowkey=18609765012<br><img src="/medias/20.PNG" alt="2"></p></li><li><p>hadoop3:8002/hbase/getHbase?table=send_mail&amp;rowkey=65497873@qq.com<br><img src="/medias/30.PNG" alt="3"></p></li><li><p>hadoop3:8002/hbase/getHbase?table=phone&amp;rowkey=18609765012<br><img src="/medias/40.PNG" alt="4"></p></li><li><p>hadoop3:8002/hbase/search/phone/18609765012<br><img src="/medias/5.PNG" alt="5"></p></li><li><p>hadoop3:8003/es/getAllCount<br><img src="/medias/6.PNG" alt="6"></p></li><li><p>hadoop3:8003/es/getBaseInfo<br><img src="/medias/7.PNG" alt="7"></p></li><li><p>hadoop3:8003/es/getLocus<br><img src="/medias/8.PNG" alt="8"></p></li><li><p>hadoop3:8003/es/group<br><img src="/medias/9.PNG" alt="9"></p></li></ol><h3 id="十三、附录"><a href="#十三、附录" class="headerlink" title="十三、附录"></a>十三、附录</h3><h4 id="1、测试数据"><a href="#1、测试数据" class="headerlink" title="1、测试数据"></a>1、测试数据</h4><p><strong>mail_source1_1111101.txt</strong></p><pre><code>000000000000011    000000000000011    23.000011    24.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300088    65497873@qq.com    1789090763    11111111@qq.com    1789097863    今天出去打球吗    send000000000000011    000000000000011    24.000011    25.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300085    65497873@qq.com    1789090764    22222222@qq.com    1789097864    今天出去打球吗    send000000000000011    000000000000011    23.000011    24.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300088    65497873@qq.com    1789090763    33333333@qq.com    1789097863    今天出去打球吗    send000000000000011    000000000000011    24.000011    25.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300085    65497873@qq.com    1789090764    44444444@qq.com    1789097864    今天出去打球吗    send000000000000000    000000000000000    23.000001    24.000001    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305988    1323243@qq.com    1789098763    43432543@qq.com    1789098863    今天出去打球吗    send000000000000000    000000000000000    24.000001    25.000001    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    1323243@qq.com    1789098764    43432543@qq.com    1789098864    今天出去打球吗    send000000000000000    000000000000000    23.000001    24.000001    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305988    1323243@qq.com    1789098763    43432543@qq.com    1789098863    今天出去打球吗    send000000000000000    000000000000000    24.000001    25.000001    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    1323243@qq.com    1789098764    43432543@qq.com    1789098864    今天出去打球吗    send</code></pre><p><strong>qq_source1_1111101.txt</strong></p><pre><code>000000000000000    000000000000000    23.000000    24.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305988    andiy    18609765432    judy            1789098762000000000000000    000000000000000    24.000000    25.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    andiy    18609765432    judy            1789098763000000000000000    000000000000000    23.000000    24.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305988    andiy    18609765432    judy            1789098762000000000000000    000000000000000    24.000000    25.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    andiy    18609765432    judy            1789098763000000000000011    000000000000011    23.000011    24.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300388    xz    18609765012    ls            1789000653000000000000011    000000000000011    24.000011    25.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300545    xz    18609765012    ls            1789000343000000000000011    000000000000011    23.000011    24.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300658    xz    18609765012    ls            1789000542000000000000011    000000000000011    24.000011    25.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300835    xz    18609765012    ls            1789000263000000000000011    000000000000011    23.000021    24.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557300388    xz    18609765016    ls            1789001653000000000000011    000000000000011    24.000021    25.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557302235    xz    18609765016    ls            1789001343000000000000011    000000000000011    23.000021    24.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557303658    xz    18609765016    ls            1789001542000000000000011    000000000000011    24.000021    25.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557303835    xz    18609765016    ls            1789001263000000000000011    000000000000011    23.000031    24.000041    4c-6f-c7-3d-a4-3d    9g-gd-3h-3k-ld-3f    32109246    1557300001    xz    18609765014    ls            1789050653000000000000011    000000000000011    24.000031    25.000051    7c-8e-d4-a6-3d-5c    54-hg-gi-yx-ef-ge    32109246    1557300005    xz    18609765015    ls            1789070343000000000000011    000000000000011    23.000031    24.000061    8c-g1-ed-7b-5f-1b    47-fy-vv-hs-ue-fd    32109246    1557300008    xz    18609765017    ls            1789080542000000000000011    000000000000011    24.000031    25.000071    0c-76-2a-b1-3c-1a    f5-nw-hf-ud-ht-ea    32109246    1557300115    xz    18609765010    ls            1789082263</code></pre><p><strong>wechat_source1_1111101.txt</strong></p><pre><code>000000000000000    000000000000000    23.000000    24.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305988    andiy    18609765432    judy            1789098762000000000000000    000000000000000    24.000000    25.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    andiy    18609765432    judy            1789098763000000000000000    000000000000000    23.000000    24.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305988    andiy    18609765432    judy            1789098762000000000000000    000000000000000    24.000000    25.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    andiy    18609765432    judy            1789098763000000000000011    000000000000011    23.000011    24.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300388    xz    18609765012    ls            1789000653000000000000011    000000000000011    24.000011    25.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300545    xz    18609765012    ls            1789000343000000000000011    000000000000011    23.000011    24.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300658    xz    18609765012    ls            1789000542000000000000011    000000000000011    24.000011    25.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300835    xz    18609765012    ls            1789000263000000000000011    000000000000011    23.000021    24.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557300388    xz    18609765016    ls            1789001653000000000000011    000000000000011    24.000021    25.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557302235    xz    18609765016    ls            1789001343000000000000011    000000000000011    23.000021    24.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557303658    xz    18609765016    ls            1789001542000000000000011    000000000000011    24.000021    25.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557303835    xz    18609765016    ls            1789001263000000000000011    000000000000011    23.000031    24.000041    4c-6f-c7-3d-a4-3d    9g-gd-3h-3k-ld-3f    32109246    1557300001    xz    18609765014    ls            1789050653000000000000011    000000000000011    24.000031    25.000051    7c-8e-d4-a6-3d-5c    54-hg-gi-yx-ef-ge    32109246    1557300005    xz    18609765015    ls            1789070343000000000000011    000000000000011    23.000031    24.000061    8c-g1-ed-7b-5f-1b    47-fy-vv-hs-ue-fd    32109246    1557300008    xz    18609765017    ls            1789080542000000000000011    000000000000011    24.000031    25.000071    0c-76-2a-b1-3c-1a    f5-nw-hf-ud-ht-ea    32109246    1557300115    xz    18609765010    ls            1789082263</code></pre><h4 id="2、Kafka"><a href="#2、Kafka" class="headerlink" title="2、Kafka"></a>2、Kafka</h4><p>创建topic，1个副本3个分区<br>kafka-topics –zookeeper hadoop1:2181 –topic chl_test7 –create –replication-factor 1 –partitions 3</p><p><strong>删除topic</strong><br>kafka-topics –zookeeper hadoop1:2181 –delete –topic chl_test7</p><p><strong>列出所有的topic</strong><br>kafka-topics –zookeeper hadoop1:2181 –list</p><p><strong>消费</strong><br>kafka-console-consumer –bootstrap-server hadoop1:9092 –topic chl_test7 –from-beginning</p><h4 id="3、kafka2es"><a href="#3、kafka2es" class="headerlink" title="3、kafka2es"></a>3、kafka2es</h4><p><strong>启动sparkstreaming任务</strong></p><pre><code>spark-submit --master yarn-cluster --num-executors 1 --driver-memory 500m --executor-memory 1g --executor-cores 1 --jars $(echo /usr/chl/spark7/jars/*.jar | tr &#39; &#39; &#39;,&#39;) --class com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar chl_test7 chl_test7</code></pre><pre><code>spark-submit --master yarn-cluster    //集群启动--num-executors 1        //分配多少个进程--driver-memory 500m  //driver内存--executor-memory 1g //进程内存--executor-cores 1       //开多少个核，线程--jars $(echo /usr/chl/spark8/jars/*.jar | tr &#39; &#39; &#39;,&#39;) //加载jar--class com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming //执行类 /usr/chl/spark8/xz_bigdata_spark-1.0-SNAPSHOT.jar //包的位置</code></pre><h4 id="4、Yarn"><a href="#4、Yarn" class="headerlink" title="4、Yarn"></a>4、Yarn</h4><p><strong>将yarn的执行日志输出</strong><br>yarn logs -applicationId application_1561627166793_0002 &gt; log.log</p><p><strong>查看日志</strong><br>more log.log</p><p>cat log.log</p><h4 id="5、CDH的7180打不开"><a href="#5、CDH的7180打不开" class="headerlink" title="5、CDH的7180打不开"></a>5、CDH的7180打不开</h4><p><strong>查看cloudera-scm-server状态</strong><br>service cloudera-scm-server status</p><p><strong>查看cloudera-scm-server 日志</strong><br>cat /var/log/cloudera-scm-server/cloudera-scm-server.log</p><p><strong>重启cloudera-scm-server</strong><br>service cloudera-scm-server restart</p><h4 id="6、CDH的jdk设置—重要"><a href="#6、CDH的jdk设置—重要" class="headerlink" title="6、CDH的jdk设置—重要"></a>6、CDH的jdk设置—重要</h4><p><strong>/usr/local/jdk1.8</strong></p><h4 id="7、预警"><a href="#7、预警" class="headerlink" title="7、预警"></a>7、预警</h4><pre><code>spark-submit --master local[1] --num-executors 1 --driver-memory 300m --executor-memory 500m --executor-cores 1 --jars $(echo /usr/chl/spark7/jars/*.jar | tr &#39; &#39; &#39;,&#39;) --class com.hsiehchou.spark.streaming.kafka.warn.WarningStreamingTask /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</code></pre><h4 id="8、Kibana的DEV-Tools"><a href="#8、Kibana的DEV-Tools" class="headerlink" title="8、Kibana的DEV Tools"></a>8、Kibana的DEV Tools</h4><pre><code>GET _search{  &quot;query&quot;: {    &quot;match_all&quot;: {}  }}GET  _cat/indicesDELETE tanslator_test1111DELETE qqDELETE wechatDELETE mailGET wechatGET mailGET _searchGET mail/_searchGET mail/_mappingPUT mailPUT mail/mail/_mapping{  &quot;_source&quot;: {    &quot;enabled&quot;: true  },  &quot;properties&quot;: {    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;imsi&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;longitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;latitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;phone_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_number&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;collect_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;send_mail&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;send_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;accept_mail&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;accept_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;mail_content&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;mail_type&quot;:{&quot;type&quot;: &quot;keyword&quot;},     &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;table&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;absolute_filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}  }}GET qq/_searchGET qq/_mappingPUT qqPUT qq/qq/_mapping{  &quot;_source&quot;: {    &quot;enabled&quot;: true  },  &quot;properties&quot;: {    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;imsi&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;longitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;latitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;phone_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_number&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;collect_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;phone&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;object_username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;send_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;accept_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;message_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;table&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;absolute_filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}  }}GET wechat/_searchGET wechat/_mappingPUT wechatPUT wechat/wechat/_mapping{  &quot;_source&quot;: {    &quot;enabled&quot;: true  },  &quot;properties&quot;: {    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;imsi&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;longitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;latitude&quot;:{&quot;type&quot;: &quot;double&quot;},    &quot;phone_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;device_number&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;collect_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;phone&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;object_username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;send_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;accept_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;message_time&quot;:{&quot;type&quot;: &quot;long&quot;},    &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;table&quot;:{&quot;type&quot;: &quot;keyword&quot;},    &quot;filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},    &quot;absolute_filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}  }}</code></pre><h4 id="9、Hive"><a href="#9、Hive" class="headerlink" title="9、Hive"></a>9、Hive</h4><p><strong>kafka写入hive</strong></p><pre><code>spark-submit --master local[1] --num-executors 1 --driver-memory 300m --executor-memory 500m --executor-cores 1 --jars $(echo /usr/chl/spark7/jars/*.jar | tr &#39; &#39; &#39;,&#39;) --class com.hsiehchou.spark.streaming.kafka.kafka2hdfs.Kafka2HiveTest /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</code></pre><pre><code>show tables;hdfs dfs -ls /apps/hive/warehouse/externalhdfs dfs -rm -r /apps/hive/warehouse/external/maildrop table mail;desc qq;select * from qq limit 1;注意了：cdh的hive版本跟其对应的spark版本不一致的话此处执行不了select count(*) from qq;</code></pre><p><strong>合并小文件</strong></p><pre><code>crontab -e0 1 * * * spark-submit --master local[1] --num-executors 1 --driver-memory 300m --executor-memory 500m --executor-cores 1 --jars $(echo /usr/chl/spark7/jars/*.jar | tr &#39; &#39; &#39;,&#39;) --class com.hsiehchou.spark.streaming.kafka.kafka2hdfs.CombineHdfs /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</code></pre><p><img src="/medias/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1crontab.PNG" alt="定时任务crontab"></p><h4 id="10、Zookeeper"><a href="#10、Zookeeper" class="headerlink" title="10、Zookeeper"></a>10、Zookeeper</h4><p><strong>启动zookeeper客户端</strong><br>zookeeper-client</p><p><strong>清除消费者</strong><br>rmr /consumers/WarningStreamingTask2/offsets</p><p>rmr /consumers/Kafka2HiveTest/offsets</p><p>rmr /consumers/DataRelationStreaming1/offsets</p><h4 id="11、Hbase"><a href="#11、Hbase" class="headerlink" title="11、Hbase"></a>11、Hbase</h4><pre><code>spark-submit --master local[1] --num-executors 1 --driver-memory 500m --executor-memory 1g --executor-cores 1 --jars $(echo /usr/chl/spark7/jars/*.jar | tr &#39; &#39; &#39;,&#39;) --class com.hsiehchou.spark.streaming.kafka.kafka2hbase.DataRelationStreaming /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</code></pre><pre><code>hbase shelllistcreate &#39;t1&#39;,&#39;cf&#39;desc &#39;t1&#39;put &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,&#39;cf:qq&#39;,&#39;66666666&#39;put &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,&#39;cf:weixin&#39;,&#39;weixin1&#39;put &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,&#39;cf:mail&#39;,&#39;66666@qq.com&#39;scan &#39;t1&#39;将表变成多版本alter &#39;t1&#39;,{NAME=&gt;&#39;cf&#39;,VERSIONS=&gt;50}put &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,&#39;cf:qq&#39;,&#39;77777777&#39;get &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,{COLUMN=&gt;&#39;cf&#39;,VERSIONS=&gt;10}put &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,&#39;cf:qq&#39;,&#39;55555555&#39;put &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,&#39;cf:qq&#39;,&#39;88888888&#39;,1290300544执行DataRelationStreamingscan &#39;test:relation&#39;get &#39;test:username&#39;,&#39;andiy&#39;scan &#39;test:relation&#39;mail 改mac 邮箱get  &#39;test:relation&#39;,&#39;&#39;,{COLUMN=&gt;&#39;cf&#39;,VERSIONS=&gt;10}disable &#39;test:imei&#39;drop &#39;test:imei&#39;disable &#39;test:imsi&#39;drop &#39;test:imsi&#39;disable &#39;test:phone&#39;drop &#39;test:phone&#39;disable &#39;test:phone_mac&#39;drop &#39;test:phone_mac&#39;disable &#39;test:relation&#39;drop &#39;test:relation&#39;disable &#39;test:send_mail&#39;drop &#39;test:send_mail&#39;disable &#39;test:username&#39;drop &#39;test:username&#39;</code></pre><h4 id="12、SpringCloud"><a href="#12、SpringCloud" class="headerlink" title="12、SpringCloud"></a>12、SpringCloud</h4><p><strong>eureka服务注册中心启动</strong></p><pre><code>nohup java -cp /usr/chl/springcloud/eureka/xz_bigdata_springcloud_eureka-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.eureka.EurekaApplication &amp;</code></pre><p><strong>查看日志</strong></p><pre><code>tail -f nohup.out</code></pre><p><strong>esquery微服务启动</strong></p><pre><code>nohup java -cp /usr/chl/springcloud/esquery/xz_bigdata_springcloud_esquery-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.es.ESqueryApplication &amp;</code></pre><p><strong>hbasequery微服务启动</strong></p><pre><code>nohup java -cp /usr/chl/springcloud/hbasequery/xz_bigdata_springcloud_hbasequery-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.HbaseQueryApplication &amp;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据项目 </tag>
            
            <tag> 网络日志分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Eureka服务注册中心启动</title>
      <link href="/2019/07/25/eureka-fu-wu-zhu-ce-zhong-xin-qi-dong/"/>
      <url>/2019/07/25/eureka-fu-wu-zhu-ce-zhong-xin-qi-dong/</url>
      
        <content type="html"><![CDATA[<p><strong>nohup启动（nohup不间断运行）</strong></p><pre><code>nohup java -cp /usr/chl/springcloud/eureka/xz_bigdata_springcloud_eureka-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.eureka.EurekaApplication &amp;</code></pre><p><strong>查看日志</strong></p><pre><code>tail -f nohup.out</code></pre>]]></content>
      
      
      <categories>
          
          <category> SpringCloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringCloud </tag>
            
            <tag> Eureka </tag>
            
            <tag> nohup </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ZooKeeper清除已有的消费的offsets</title>
      <link href="/2019/07/23/zookeeper-qing-chu-yi-you-de-xiao-fei-de-offsets/"/>
      <url>/2019/07/23/zookeeper-qing-chu-yi-you-de-xiao-fei-de-offsets/</url>
      
        <content type="html"><![CDATA[<p><strong>CDH集群中的ZooKeeper</strong></p><p><strong>启动ZooKeeper客户端</strong><br>zookeeper-client</p><p><strong>清除消费者</strong></p><p><strong>例如</strong><br>rmr /consumers/WarningStreamingTask/offsets</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> ZooKeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkStreaming+Kafka的两种模式Receiver模式和Direct模式</title>
      <link href="/2019/07/19/sparkstreaming-kafka-de-liang-chong-mo-shi-receiver-mo-shi-he-direct-mo-shi/"/>
      <url>/2019/07/19/sparkstreaming-kafka-de-liang-chong-mo-shi-receiver-mo-shi-he-direct-mo-shi/</url>
      
        <content type="html"><![CDATA[<h3 id="SparkStreming-Kafka-Receiver模式理解"><a href="#SparkStreming-Kafka-Receiver模式理解" class="headerlink" title="SparkStreming + Kafka Receiver模式理解"></a>SparkStreming + Kafka Receiver模式理解</h3><p><img src="/medias/kafka%E7%9A%84receiver%E6%A8%A1%E5%BC%8F.PNG" alt="Kafka的Receiver模式"></p><h4 id="Receiver模式理解"><a href="#Receiver模式理解" class="headerlink" title="Receiver模式理解"></a>Receiver模式理解</h4><p>在SparkStreaming程序运行起来后，Executor中会有Receiver Tasks接收Kafka推送过来的数据。数据会被持久化，默认级别为MEMORY_AND_DISK_SER_2,这个级别也可以修改。Receiver Task对接收过来的数据进行存储和备份，这个过程会有节点之间的数据传输。备份完成后去ZooKeeper中更新消费偏移量，然后向Driver中的Receiver Tracker汇报数据的位置。最后Driver根据数据本地化将Task分发到不同节点上执行。</p><h4 id="Receiver模式中存在的问题"><a href="#Receiver模式中存在的问题" class="headerlink" title="Receiver模式中存在的问题"></a>Receiver模式中存在的问题</h4><p>当Driver进程挂掉后，Driver下的Executor都会被杀掉，当更新完ZooKeeper消费偏移量的时候，Driver如果挂掉了，就会存在找不到数据的问题，相当于丢失数据。</p><h4 id="如何解决这个问题？"><a href="#如何解决这个问题？" class="headerlink" title="如何解决这个问题？"></a>如何解决这个问题？</h4><p>开启WAL(write ahead log)预写日志机制,在接受过来数据备份到其他节点的时候，同时备份到HDFS上一份（我们需要将接收来的数据的持久化级别降级到MEMORY_AND_DISK），这样就能保证数据的安全性。不过，因为写HDFS比较消耗性能，要在备份完数据之后才能进行更新ZooKeeper以及汇报位置等，这样会增加job的执行时间，这样对于任务的执行提高了延迟度。</p><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><ol><li>开启WAL之后，接受数据级别要降级，有效率问题</li><li>开启WAL要checkpoint</li><li>开启WAL(write ahead log),往HDFS中备份一份数据</li></ol><h3 id="SparkStreming-Kafka-Receiver模式理解-1"><a href="#SparkStreming-Kafka-Receiver模式理解-1" class="headerlink" title="SparkStreming + Kafka Receiver模式理解"></a>SparkStreming + Kafka Receiver模式理解</h3><p><img src="/medias/kafka%E7%9A%84direct%E6%A8%A1%E5%BC%8F.PNG" alt="Kafka的Direct模式"></p><ol><li>简化数据处理流程</li><li>自己定义offset存储，保证数据0丢失，但是会存在重复消费问题。（解决消费等幂问题）</li><li>不用接收数据，自己去Kafka中拉取</li></ol>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Kafka </tag>
            
            <tag> SparkStreaming </tag>
            
            <tag> Receiver </tag>
            
            <tag> Direct </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch的9200端口和9300端口的区别</title>
      <link href="/2019/07/18/elasticsearch-de-9200-duan-kou-he-9300-duan-kou-de-qu-bie/"/>
      <url>/2019/07/18/elasticsearch-de-9200-duan-kou-he-9300-duan-kou-de-qu-bie/</url>
      
        <content type="html"><![CDATA[<p>9200端口作为HTTP协议，<strong>主要用于外部通讯</strong></p><p>9300端口作为TCP协议，jar之间就是通过 <strong>TCP协议通讯</strong></p><p><strong>ES集群之间是通过9300端口进行通讯</strong></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> ElasticSearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch集群配置</title>
      <link href="/2019/07/06/elasticsearch-ji-qun-an-zhuang-pei-zhi/"/>
      <url>/2019/07/06/elasticsearch-ji-qun-an-zhuang-pei-zhi/</url>
      
        <content type="html"><![CDATA[<p><strong>安装Elasticsearch</strong><br>mkdir /opt/software/elasticsearch/data/</p><p>mkdir /opt/software/elasticsearch/logs/</p><p>chmod 777 /opt/software/elasticsearch/data/</p><p>useradd elasticsearch<br>passwd elasticsearch</p><p>chown -R elasticsearch elasticsearch/</p><p><strong>vim /etc/security/limits.conf</strong><br>添加如下内容:<br><code>*</code> <strong>soft nofile 65536</strong><br><code>*</code> <strong>hard nofile 131072</strong><br><code>*</code> <strong>soft nproc 2048</strong><br><code>*</code> <strong>hard nproc 4096</strong></p><p>进入limits.d目录下修改配置文件<br><strong>vim /etc/security/limits.d/90-nproc.conf</strong></p><p>修改如下内容：<br><strong>soft nproc 4096（修改为此参数，6版本的默认就是4096）</strong></p><p>修改配置sysctl.conf<br><strong>vim /etc/sysctl.conf</strong></p><p>添加下面配置：<br><strong>vm.max_map_count=655360</strong></p><p>并执行命令：<br><strong>sysctl -p</strong></p><p><strong>hadoop1的conf配置</strong><br><strong>elasticsearch.yml</strong></p><pre><code>cluster.name: xz_esnode.name: node-1node.master: truenode.data: truepath.data: /opt/software/elasticsearch/datapath.logs: /opt/software/elasticsearch/logsbootstrap.memory_lock: falsebootstrap.system_call_filter: falsenetwork.host: 192.168.116.201discovery.zen.ping.unicast.hosts: [&quot;hadoop1&quot;, &quot;hadoop2&quot;, &quot;hadoop3&quot;]</code></pre><p><strong>jvm.options</strong><br>修改下<br>-Xms64m<br>-Xmx64m</p><p><strong>hadoop2的conf配置</strong><br><strong>elasticsearch.yml</strong></p><pre><code>cluster.name: xz_esnode.name: node-2node.master: falsenode.data: truepath.data: /opt/software/elasticsearch/datapath.logs: /opt/software/elasticsearch/logsbootstrap.memory_lock: falsebootstrap.system_call_filter: falsenetwork.host: 192.168.116.202discovery.zen.ping.unicast.hosts: [&quot;hadoop1&quot;, &quot;hadoop2&quot;, &quot;hadoop3&quot;]</code></pre><p><strong>jvm.options</strong><br>修改下<br>-Xms64m<br>-Xmx64m</p><p><strong>hadoop3的conf配置</strong><br><strong>elasticsearch.yml</strong></p><pre><code>cluster.name: xz_esnode.name: node-3node.master: falsenode.data: truepath.data: /opt/software/elasticsearch/datapath.logs: /opt/software/elasticsearch/logsbootstrap.memory_lock: falsebootstrap.system_call_filter: falsenetwork.host: 192.168.116.203discovery.zen.ping.unicast.hosts: [&quot;hadoop1&quot;, &quot;hadoop2&quot;, &quot;hadoop3&quot;]</code></pre><p><strong>jvm.options</strong><br>修改下<br>-Xms64m<br>-Xmx64m</p><p><strong>Kibana的conf配置</strong></p><p><strong>kibana.yml</strong></p><pre><code>server.port: 5601server.host: &quot;192.168.116.202&quot;elasticsearch.url: &quot;http://192.168.116.201:9200&quot;</code></pre><p><strong>运行Elasticsearch</strong><br>cd /opt/software/elasticsearch<br>su elasticsearch<br>bin/elasticsearch &amp;</p><p><strong>运行Kibana</strong><br>cd /opt/software/kibana/<br>bin/kibana &amp;</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> ElasticSearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一万小时天才理论（阅读笔记）</title>
      <link href="/2019/06/19/yi-wan-xiao-shi-tian-cai-li-lun/"/>
      <url>/2019/06/19/yi-wan-xiao-shi-tian-cai-li-lun/</url>
      
        <content type="html"><![CDATA[<p><strong>一万小时天才理论（阅读笔记）</strong></p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a><strong>前言</strong></h3><p>上大一时，我就听人讲过一个人只要在一个领域专攻1万小时，那么这个人便会成为这个领域的专家，把时间分配到每天3小时，需要10年，每天8小时（当然这很不现实，不可能工作的所有时间都花在学习上，还有周末休息啥的），原则上4年就够了。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>1、中国目前发展飞速，想要在这个大的环境中生存，必须靠自己，靠别人是永远靠不住的，因为你永远不知道别人办事的进度，可能什么都没做，也可能做的差不多了。总之，相信自己，依靠自己，才能生存。</p><p>2、遗传因素对我们对影响不大，所以，还是需要靠我们后天的学习和发展才能壮大自己。</p><p>3、犯错不用怕，怕的是犯错了不去找解决的方法，这样永远不会进步的。我们可以从犯错中总结有效的解决方法，发现问题，解决问题。</p><p>4、学会去爱做一件事或者很多事，当自己爱上了做这些事，便会发现用的这些时间并不是浪费，花得值得，养成一种习惯，成为这个领域的专家。6分钟的时间可以学到很多，只要自己坚持去做，自己会在不知不觉中受益。</p><h3 id="第一部分-精深"><a href="#第一部分-精深" class="headerlink" title="第一部分 精深"></a><strong>第一部分 精深</strong></h3><h3 id="01-冒牌哈佛"><a href="#01-冒牌哈佛" class="headerlink" title="01 冒牌哈佛"></a><strong>01 冒牌哈佛</strong></h3><p><strong>谁也不能随随便便成功，它来自彻底的自我管理和毅力</strong> 。 ——哈佛图书馆训言</p><p><strong>犯错让你更聪明</strong> 。                                                               ——德国寓言</p><p>我们需要去完成我们和别人都认为不可能完成的任务，突破自己的极限。</p><p>在不断精深练习中培养自己的技能，让这种动作成为自己的习惯，不需要通过大脑的深度思考，当遇到这种情况，我们不需要考虑就能自动的进行相应的操作。这跟人类的潜意识的养成相似，也可以说一样的（我是这么认为的）。</p><p>需要 <strong>不断地进行正确的精深练习</strong> ，学习到独特的体会和技巧，让其成为自己的一种能力，潜意识就可以办到。</p><h3 id="02-才能细胞"><a href="#02-才能细胞" class="headerlink" title="02 才能细胞"></a><strong>02 才能细胞</strong></h3><p>髓鞘质是在我们不断地进行精深练习后不断地增加厚度、变厚。改善自己的神经回路，优化自己的神经回路。</p><p>需要 <strong>有明确的目标</strong> 、 <strong>重视错误的练习</strong> ，这样我们便可以 <strong>不断地去发现问题</strong> 、 <strong>处理问题</strong> 。在这个过程中我们需要 <strong>保持激情和坚持</strong> ，只有这样才能 <strong>让自己的髓鞘质进化到巅峰水平</strong> 。</p><p>人的各种行动都是通过大脑发出的指令信号来进行的，进行各种预判，对可能发生的情况进行预估，做出对应的对策。当自己的经验越多，自己会不知不觉的完成很多的事情，自己都没有察觉自己做了什么。髓鞘质对于大脑的神经信息处理非常的关键，它能把握住时间点， <strong>该出手时就出手</strong> ，控制大脑传输速度，让各种信号准时到达。</p><p>髓鞘质无法逆转，一旦技能回路包裹上了绝缘体，那将无法去除。髓鞘质只在乎我们自己做了什么，它把这些技能更好的发扬光大，让我们充分利用。</p><p>任何领域的任何专家都要经过10000小时专心致志地学习。</p><p>我们通过不断地精深练习，达到1万小时后，便能成为这个领域的专家，人才的培养最好从小时候就开始，这个幼时的不断地练习会让孩子一生受益（我们人人都可以这样，但必须投入足够的时间去做）。</p><h3 id="03-天降人才"><a href="#03-天降人才" class="headerlink" title="03 天降人才"></a><strong>03 天降人才</strong></h3><p>我们做任何事情都需要坚持到最后，不能快到终点了自己就放弃了，只要熬过最后的这一分钟，我们便能做成这件事，实现自己的梦想。</p><p>一个人的成功离不开他 <strong>自身的努力</strong> ，我们往往看到的是那些成功人士眼前的辉煌，但是没法看到他们背后付出的辛酸和泪水。 <strong>一个人的成功离不开自己默默地付出</strong> ，需要 <strong>自己一个人慢慢地前行</strong> ，探索未知的领域，当自己慢慢摸透了这个未知的领域，自己便可以翱翔其中。</p><p> 我们的髓鞘质不在乎我们是谁，只关心我们做了什么，把这些做的东西经过我们的日积月累慢慢地形成我们的技能。</p><p><strong>文艺复兴时期伟大的艺术家的特点：</strong></p><p>每个人都把大部分的青春岁月投入在精深练习上，锤炼和优化技能回路，纠错，竞争，然后进步。每个人都参与了这副任何人都能创作的最伟大的艺术作品：构建自己的才能。</p><p><strong>造成青少年做出错误的决定的原因：</strong></p><p>冲动行为发生时，神经回路不会马上去阻止这个冲动行为，尽管它是可以的，青少年需要自己花时间去体会。</p><p>我们需要不断地学习，学习经验，因为髓鞘质也是会消亡的，我们需要生成新的髓鞘质来对已经消亡的髓鞘质做个补充。</p><p>读了本章，深刻体会到了髓鞘质对我们人类潜力的巨大作用，它在更新，也在消亡，但是我们需要不断地学习、不断地强化我们的髓鞘质（就是不断地进行精深练习），那我们的能力便会飞快的提升，从而成为一个领域的专家。</p><h3 id="04-三大秘技"><a href="#04-三大秘技" class="headerlink" title="04 三大秘技"></a><strong>04 三大秘技</strong></h3><p><strong>屡败屡战。屡战屡败。败了更好</strong> 。   ——塞缪尔·贝克特(Samuel Beckett)</p><p><strong>哇塞效应</strong></p><p><strong>组块化：</strong></p><p>整体吸收：花时间观察或者倾听自己想学习的技能，将技能具体化。</p><p>学会最高效地去模仿别人，把握住技巧，将学习的东西拆分成一个个的小模块，慢慢练习，找到自己的感觉，渐渐地就变成了自己的技能了。</p><p><strong>重复练习：</strong></p><p>我们不管干什么事，只要长时间不练习，我们便会忘记，这是我们人类的记忆规律。所以，想要掌握某一技能，就必须不间断地练习，每天练习2小时也好，只要不长时间地隔1个月不练习就行了。</p><p><strong>尝试体会：</strong></p><p>我们需要 <strong>自己切身去体会做</strong> ，不能看着别人感觉自己完成不了，我们需要去挑战自己，自己去体会，可能别人做不了的任务，而你却可以。有了这种尝试，我们就不再害怕，便对这些事感觉很平常。</p><p>除了亲自体会外，我们还需要自己去不断地练习，找到那种感觉，就是将那种要完成的任务牢牢地掌握在自己手心里。</p><p>注意力、连接、建立、完整的、警觉、关注、错误、重复、疲劳、边缘、唤醒</p><h3 id="第二部分-激情"><a href="#第二部分-激情" class="headerlink" title="第二部分 激情"></a><strong>第二部分 激情</strong></h3><h3 id="05-信号"><a href="#05-信号" class="headerlink" title="05 信号"></a><strong>05 信号</strong></h3><p>1、技能学习需要精深练习。</p><p>2、精深练习需要精力、激情和投入。</p><pre><code> 激情的存在就像为我们提供动力火箭的原料，维持我们不停地重新开始，锻炼技巧，不断进步。</code></pre><p>3、一次突破性的胜利，接着就会涌现出大规模的&quot;人才井喷&quot;。</p><p>4、总之，精深练习需要时间（1万小时的练习）。</p><p>我们意识到&quot;我就想成为那样的人&quot;的时刻，这就是激情工作的原理。只有激情才能刺激自己不断地突破极限。</p><p><strong>小小念头，影响深远</strong></p><p>短期承诺、中期承诺、长期承诺</p><p>研究表明，长期承诺的孩子，哪怕每周只花20分钟进行练习，也比那些花1个半小时的孩子进步神速。长期承诺的孩子充分的练习，技能已经出神入化了。</p><p>我想成为那样的人，可能就因为这个小小的念头，让我们改变一生，让我们走向成功。</p><p><strong>启动信号</strong></p><p>我们如果能够放弃眼前的舒适，去熬那些艰难的时光的话，就是让我们变成一个自己想要变成的人，自己为之不断地奋斗。</p><p>每个强烈的刺激人大脑神经的能让人立马行动信号指引人去做超级有意义的事情。这针对未来的归属感。</p><p>书上讲缺乏安全感会激发和引导自己的大脑去解决危险，处理生命中的可能性。</p><p>生活的不再安全让那些人开启了古老的自我保护的进化开关，从而让他们倾入时间和精力去耕耘事业，渐渐地，他们完成了一万小时的精深练习，养成了各自的才能。</p><p><strong>统治所有人的普遍原则：</strong></p><p>1、才能需要精深练习</p><p>2、精深练习需要充分的能量</p><p>3、某些信号会触发巨大能量的迸发</p><p>研究表明，往往家中老四是成就最高的。造成这种状况的原因是，家中老四，生得较晚，因为有兄长在前面作为榜样，往往会不断地去追赶自己的兄长们，长时间的练习形成了一种习惯，从而让自己最终走向人生的高处。</p><p><strong>好运临门</strong></p><p>稀缺感、归属感、安全感，通过激发人类自己的欲望，不断地针对这个去刺激自己。激情需要长久， <strong>长期的激情</strong> ， <strong>不断地去向自己地目标奋斗</strong> 。</p><h3 id="06-疯狂地海岛"><a href="#06-疯狂地海岛" class="headerlink" title="06 疯狂地海岛"></a><strong>06 疯狂地海岛</strong></h3><p><strong>打破保守，走出去，让激情之火不灭，永远保持</strong> ，这在开始的时候是非常困难的，但是一旦自己习惯了这种激情，便永远浇不灭了。</p><p>&quot;嘿！你也可以&quot;</p><p>相信自己，不停地去练习，一定会有很大收获的。</p><p><strong>激情的语言</strong></p><p>那些 <strong>鼓励的语言是我们前进的最大的动力</strong> 。（番外，这里让我想起了我高二的那个英语老师，我们英语差的从来不鼓励，破骂一顿，好像我们欠了她一个亿，那些英语好的基础好，表扬得不得了，醉了，反正我周围的同学都讨厌她；反之，我高二的物理老师，不断地鼓励我们，我高中物理总拿A（高三还进入了省级的物理竞赛，当然了省考特别难，什么都不会，写了点字也拿了个江苏省物理竞赛三等奖），而且当时我们班物理在整个理科都是佼佼者，有时候平均分超其他班20-30分，是不是有点过，这样的好的情况到了高三换了老师就没了，而且我们当时物理竞赛去省赛的5个人，其中4个都是我们高二物理老师教过的），我们这位物理老师，只要考好了，就有电影看，极大地刺激了我们的学习的动力。</p><p>激励性的语言是鼓励人们争取不断进步的语言，让我们向希望、梦想不断的前进。</p><p>精深练习需要深入认真的工作以及热情的劲头，集中我们的精力，然后慢慢进步。</p><p>赞人们勤奋的语句之所以有效，是因为它直达学习的核心，而想要点燃激情，没有比这更强大的了。</p><h3 id="07-点燃明灯"><a href="#07-点燃明灯" class="headerlink" title="07 点燃明灯"></a><strong>07 点燃明灯</strong></h3><p><strong>教育不是在填坑，而是点燃照明之火。</strong> ——叶芝</p><p><strong>X一代的荒谬念头</strong></p><p>KIPP这所学校的诞生也是原来两个创始人的一个奇特的想法，不再与教育系统抗衡，而是打算开办一所自己的学校。刚开始的一两年学校办的整体效果并不怎么好，但是，突然间，学生都变得非常优秀，我个人认为这是哲学上的量变向质变的转化，学生长时间的培养好的习惯，培养他们的思维，让他们能有自己的独立的想法，能安心做任何事情，学生成绩的提高就是KIPP学校使用这种模式的见证。</p><p>努力培养学生的整体素质，而不单纯是为了提高学生的学习成绩。学生做的任何一件事都会与其它事情有关联，这便让学生养成随时思考的好习惯，当学生独立生活时，他们便日子过得很滋润。</p><p>培养学生，一旦学生懂了，基本不需要老师去教了，自己会去学习的，所以KIPP就是这样的，为的是让学生终生受益，而不是为的只是学习成绩的提高。</p><h3 id="第三部分-伯乐"><a href="#第三部分-伯乐" class="headerlink" title="第三部分 伯乐"></a><strong>第三部分 伯乐</strong></h3><h3 id="08-伯乐的武器"><a href="#08-伯乐的武器" class="headerlink" title="08 伯乐的武器"></a><strong>08 伯乐的武器</strong></h3><p>伯乐，自古以来就流传很久，古传是伯乐相马，对马的研究非常出色，优质的马在他眼中是逃不掉的。古代就已经将伯乐相马比喻善于识别人才，爱惜人才。</p><p>一个好的伯乐是能找出自己要看的人的任何优点包括缺点，一眼就能看出来。长处、短处都能识别，优秀的人才在这里能够展现雄姿。</p><p>该怎么做，怎么做，何时强化。这样，不是那样。</p><p>上面那句红色的是著名的约翰·伍顿教练教他的学员的原则，先示范正确的动作，后示范错误的动作，最后再示范正确的动作。教学员该做什么，哪怕穿袜子这种简单的他都教学员，让其不容易起水泡。事实证明，他的教得非常得成功。</p><p>那些KEEP项目的人将其用在了阅读上，效果非常明显，还获得了格威文美尔奖。</p><p><strong>点燃热爱的火花</strong></p><p>一个人爱上做一件事是很难的，如果让一个人爱上一个事业是更难的，在打拼的过程中会遇到各种的磨难，克服它们是对我们的一种考验。</p><p>约翰·伍顿使用了人才理论中精深练习的部分，提供知识，纠正错误，加强技能回路。而玛丽对付的是激情部分，利用情绪开关，在邮箱中加满爱和动机。他们都取得了成功，因为髓鞘质回路不仅需要精深练习，也需要激情；他们的成功真实反应了天才理论的本身。</p><h3 id="09-伯乐的一万小时"><a href="#09-伯乐的一万小时" class="headerlink" title="09 伯乐的一万小时"></a><strong>09 伯乐的一万小时</strong></h3><p><strong>&quot;我可不是为了钱，我只是在做自己喜欢的事情。当我还是个孩子的时候我就梦想成为奥运冠军。&quot;</strong>                                               ——迈克尔·菲尔普斯</p><p><strong>教师的影响是永恒的；他永远无法知道自己的影响有多深远。</strong>      ——亨利·亚当斯</p><p><strong>教师的四大优势</strong></p><p>一个好的老师是关心学生的一言一行的，而且通过这种关心，可以利用他们老师自己对该课题已有的深刻的理解，捕捉到学生在技能学习道路上碰到的障碍，以及摸索过程中难以形容的状态，然后按照已经设定的目标与学生进行沟通。</p><p>老师是学生的学习导师，也是学生的人生导师，对学生前进的路起着向导的作用。这便要求导师有非常优秀的洞察力，能够找出学生目前的困难的解决方案的突破口，给出正确的信号，帮助学生达到真正的目标，并反复这个过程。</p><p><strong>优势一：知识矩阵 —— 伯乐的杀手锏</strong></p><p>知识矩阵是教练老师多年的技术上的知识、策略、经验等，是他们多年来知识等的总结，他们对自己的领域是非常的熟悉的，因为他们几乎全部遇到过，肯定都找到了解决方案了。他们对学生所遇到的几乎所有问题都是非常清楚的，可以不时的点拨下自己的学生。教练跟老师的知识矩阵并不是与生俱来的，而是通过激情和精深练习逐渐掌握的。</p><p>作为一个人，不可能做任何事情都很符合标准，多听听别人的建议，对自己进行相应的改进，成为一个优秀的人。不断地去尝试，不要放弃，增加自己的经验，然后通过精深练习，不断地掌握自己这个领域。</p><p><strong>优势二：洞察力 —— 鹰的视力</strong></p><p>教练跟老师都需要有敏锐的洞察力，对自己的学生要时刻进行观察，从观察中可以发现问题，从而解决问题，提高学生的整体素养。</p><p><strong>优势三：简明的指示 —— 神奇的教鞭</strong></p><p>作为一名导师，需要对他的学生进行指导，优秀的指导是直接指出错误，并给出一些专业的建议，话不需要多，言简意赅。导师不用说太多的废话，因为废话，学生听了没多大用，有用的信息不多，所以只要让学生知道自己的哪里有错并去改正就行了。</p><p><strong>优势四：气质与诚信 —— 不可阻挡的魅力</strong></p><p>导师需要有自己气质，有自己的个性。道德跟诚信是导师最需要的，因为作为一名导师，不能一直给自己的学生讲他已经知道的东西，这样学生不会成长，需要指出一个方向，按照导师自己规划的方向便可以走向自己能够达到的高度。</p><p><strong>足球训练与小提琴练习</strong></p><p>首先，足球训练与小提琴练习是大不相同的，这两样老师培养的性质是不一样的。</p><p>一个需要亲身体会技巧，自己琢磨技巧，老师是不能亲身感受到的，而另一个没有正确的技法便不会有好的结果，所以培养学生需要因行业而异。</p><p>实战中感悟技法是比老师传授的更加的好，因为这自己的实际情况是一致的，自己判断问题，解决问题。</p><h3 id="10-伯乐的赌注"><a href="#10-伯乐的赌注" class="headerlink" title="10 伯乐的赌注"></a><strong>10 伯乐的赌注</strong></h3><p>*<em>教师就是为了逐步淡出。 *</em>           ——托马斯·卡拉瑟斯 (Thomas Carruthers)</p><p>一名导师教了很多学生，培养人才，当学生远航的时候，自己留在原地仰望自己的学生，学生的成功自己便满足了。</p><p>导师对学生的教导也是赌注，可能失败也可能成功，毕竟不可能一种方法适合各种人，所以需要导师来对不同的学生选择不同的教导方法，这就考验导师的眼力了。</p><h3 id="后记"><a href="#后记" class="headerlink" title="后记"></a><strong>后记</strong></h3><p>如果出现问题，问5次为什么。先自己去看看能不能解决问题，去查找资料，再解决不了的话才去请教别人。</p><p>我们干什么事都 <strong>不要害羞</strong> ， <strong>克服自己的害怕的心理</strong> ，对自己害怕的领域不断地进行磨练，让自己变得娴熟，不再害怕。</p><p><strong>人脑越用越灵活</strong> 。</p>]]></content>
      
      
      <categories>
          
          <category> 读书 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书 </tag>
            
            <tag> 个人感悟 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>系统定时任务</title>
      <link href="/2019/06/04/xi-tong-ding-shi-ren-wu/"/>
      <url>/2019/06/04/xi-tong-ding-shi-ren-wu/</url>
      
        <content type="html"><![CDATA[<h3 id="crond系统定时任务"><a href="#crond系统定时任务" class="headerlink" title="crond系统定时任务"></a>crond系统定时任务</h3><h4 id="1、crond服务管理"><a href="#1、crond服务管理" class="headerlink" title="1、crond服务管理"></a>1、crond服务管理</h4><p>service crond restart             （重新启动服务）</p><h4 id="2、crontab定时任务设置"><a href="#2、crontab定时任务设置" class="headerlink" title="2、crontab定时任务设置"></a>2、crontab定时任务设置</h4><p><strong>1）基本语法</strong><br>crontab [选项]<br>选项：<br>  -e：  编辑crontab定时任务<br>  -l：  查询crontab任务<br>  -r：  删除当前用户所有的crontab任务</p><p><strong>2）参数说明</strong><br>crontab -e<br>（1）进入crontab编辑界面。会打开vim编辑你的工作<br><code>* * * * *</code> 执行的任务</p><table><thead><tr><th align="center">项目</th><th align="center">含义</th><th align="center">范围</th></tr></thead><tbody><tr><td align="center">第一个“*”</td><td align="center">一小时当中的第几分钟（分）</td><td align="center">0-59</td></tr><tr><td align="center">第二个“*”</td><td align="center">一天当中的第几小时（时）</td><td align="center">0-23</td></tr><tr><td align="center">第三个“*”</td><td align="center">一个月当中的第几天（天）</td><td align="center">1-31</td></tr><tr><td align="center">第四个“*”</td><td align="center">一年当中的第几月（月）</td><td align="center">1-12</td></tr><tr><td align="center">第五个“*”</td><td align="center">一周当中的星期几（周）</td><td align="center">0-7（0和7都代表星期日）</td></tr></tbody></table><p>（2）特殊符号</p><table><thead><tr><th align="center">特殊符号</th><th align="center">含义</th></tr></thead><tbody><tr><td align="center"><code>*</code></td><td align="center">代表任何时间。比如第一个“*”就代表一小时中每分钟都执行一次的意思</td></tr><tr><td align="center"><code>，</code></td><td align="center">代表不连续的时间。比如“0 8,12,16 * * * 命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令</td></tr><tr><td align="center"><code>-</code></td><td align="center">代表连续的时间范围。比如“0 5  *  *  1-6命令”，代表在周一到周六的凌晨5点0分执行命令</td></tr><tr><td align="center"><code>*/n</code></td><td align="center">代表每隔多久执行一次。比如“*/10  *  *  *  *  命令”，代表每隔10分钟就执行一遍命令</td></tr></tbody></table><p>（3）特定时间执行命令</p><table><thead><tr><th align="center">时间</th><th align="center">含义</th></tr></thead><tbody><tr><td align="center">45 22 * * *  命令</td><td align="center">在22点45分执行命令</td></tr><tr><td align="center">0  17 * * 1  命令</td><td align="center">每周1 的17点0分执行命令</td></tr><tr><td align="center">0 5 1,15 * * 命令</td><td align="center">每月1号和15号的凌晨5点0分执行命令</td></tr><tr><td align="center">40 4 * * 1-5 命令</td><td align="center">每周一到周五的凌晨4点40分执行命令</td></tr><tr><td align="center"><code>*/10</code> 4 * * * 命令</td><td align="center">每天的凌晨4点，每隔10分钟执行一次命令</td></tr><tr><td align="center">0 0 1,15 * 1 命令</td><td align="center">每月1号和15号，每周1的0点0分都会执行命令</td></tr></tbody></table><p>*<em>注意: *</em>星期几和几号最好不要同时出现，因为他们定义的都是天，非常容易让管理员混乱</p><p><strong>3）案例</strong><br><code>*/5</code> * * * * /bin/echo ”11” &gt;&gt; /tmp/test</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 定时脚本 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>电信大数据</title>
      <link href="/2019/05/19/dian-xin-da-shu-ju/"/>
      <url>/2019/05/19/dian-xin-da-shu-ju/</url>
      
        <content type="html"><![CDATA[<h3 id="一、项目背景"><a href="#一、项目背景" class="headerlink" title="一、项目背景"></a>一、项目背景</h3><p>通信运营商每时每刻会产生大量的通信数据，例如通话记录，短信记录，彩信记录，第三方服务资费等等繁多信息。数据量如此巨大，除了要满足用户的实时查询和展示之外，还需要定时定期的对已有数据进行离线的分析处理。例如，当日话单，月度话单，季度话单，年度话单，通话详情，通话记录等等+。我们以此为背景，寻找一个切入点，学习其中的方法论</p><h3 id="二、项目架构"><a href="#二、项目架构" class="headerlink" title="二、项目架构"></a>二、项目架构</h3><p><img src="/medias/%E7%94%B5%E4%BF%A1%E9%A1%B9%E7%9B%AE%E6%9E%B6%E6%9E%84.PNG" alt="电信项目架构"> </p><h3 id="三、项目实现"><a href="#三、项目实现" class="headerlink" title="三、项目实现"></a>三、项目实现</h3><p><strong>系统环境</strong></p><table><thead><tr><th align="center">系统</th><th align="center">版本</th></tr></thead><tbody><tr><td align="center">windows</td><td align="center">10 专业版</td></tr><tr><td align="center">linux</td><td align="center">CentOS7.2 1611内核</td></tr></tbody></table><p><strong>开发工具</strong></p><table><thead><tr><th align="center">工具</th><th align="center">版本</th></tr></thead><tbody><tr><td align="center">idea</td><td align="center">2018.2.5旗舰版</td></tr><tr><td align="center">maven</td><td align="center">3.3.9</td></tr><tr><td align="center">JDK</td><td align="center">1.8+</td></tr></tbody></table><p><strong>尖叫提示</strong>：idea2018.2.5必须使用Maven3.3.9，不要使用Maven3.5，有部分兼容性问题</p><h3 id="四、数据生产"><a href="#四、数据生产" class="headerlink" title="四、数据生产"></a>四、数据生产</h3><p>此情此景，对于该模块的业务，即数据生产过程，一般并不会让你来进行操作，数据生产是一套完整且严密的体系，这样可以保证数据的鲁棒性。但是如果涉及到项目的一体化方案的设计（数据的产生、存储、分析、展示），则必须清楚每一个环节是如何处理的，包括其中每个环境可能隐藏的问题；数据结构，数据内容可能出现的问题</p><h4 id="1、数据结构"><a href="#1、数据结构" class="headerlink" title="1、数据结构"></a>1、数据结构</h4><p>我们将在HBase中存储两个电话号码，以及通话建立的时间和通话持续时间，最后再加上一个flag作为判断第一个电话号码是否为主叫。姓名字段的存储我们可以放置于另外一张表做关联查询，当然也可以插入到当前表中</p><table><thead><tr><th align="center">列名</th><th align="center">解释</th><th align="center">举例</th></tr></thead><tbody><tr><td align="center">caller</td><td align="center">第一个手机号码</td><td align="center">15369468720</td></tr><tr><td align="center">callerName</td><td align="center">第一个手机号码人姓名(非必须)</td><td align="center">李雁</td></tr><tr><td align="center">callee</td><td align="center">第二个手机号码</td><td align="center">19920860202</td></tr><tr><td align="center">calleename</td><td align="center">第二个手机号码人姓名(非必须)</td><td align="center">卫艺</td></tr><tr><td align="center">dateTime</td><td align="center">建立通话的时间</td><td align="center">20181126091236</td></tr><tr><td align="center">date_time_ts</td><td align="center">建立通话的时间（时间戳形式）</td><td align="center"></td></tr><tr><td align="center">duration</td><td align="center">通话持续时间（秒）</td><td align="center">0820</td></tr><tr><td align="center">flag</td><td align="center">用于标记本次通话第一个字段(caller)是主叫还是被叫</td><td align="center">1为主叫，0为被叫</td></tr></tbody></table><h4 id="2、编写代码"><a href="#2、编写代码" class="headerlink" title="2、编写代码"></a>2、编写代码</h4><p><strong>思路</strong><br>a）创建Java集合类存放模拟的电话号码和联系人；<br>b） 随机选取两个手机号码当做“主叫”与“被叫”（注意判断两个手机号不能重复），产出<strong>caller</strong>与<strong>call2</strong>字段数据；<br>c） 创建随机生成通话建立时间的方法，可指定随机范围，最后生成通话建立时间，产出<strong>date_time</strong>字段数据；<br>d）随机一个通话时长，单位：秒，产出<strong>duration</strong>字段数据；<br>e）将产出的一条数据拼接封装到一个字符串中；<br>f）使用IO操作将产出的一条通话数据写入到本地文件中;</p><p>新建module项目：<strong>ct_producer</strong></p><p><strong>父pom.xml文件配置</strong></p><pre><code>&lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!-- https://mvnrepository.com/artifact/junit/junit --&gt;        &lt;dependency&gt;            &lt;groupId&gt;junit&lt;/groupId&gt;            &lt;artifactId&gt;junit&lt;/artifactId&gt;            &lt;version&gt;4.12&lt;/version&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;                &lt;version&gt;2.12.4&lt;/version&gt;                &lt;configuration&gt;                    &lt;skipTests&gt;true&lt;/skipTests&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;                &lt;version&gt;3.8.1&lt;/version&gt;                &lt;configuration&gt;                    &lt;source&gt;1.8&lt;/source&gt;                    &lt;target&gt;1.8&lt;/target&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;</code></pre><p>（1）随机输入一些手机号码以及联系人，保存于Java的集合中<br>新建类：ProductLog</p><pre><code>package producer;import java.io.FileOutputStream;import java.io.OutputStreamWriter;import java.text.DecimalFormat;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.*;public class ProductLog {    String startTime = &quot;2018-01-01&quot;;    String endTime = &quot;2018-12-31&quot;;    //存放tel的List集合    private List&lt;String&gt;  phoneList = new ArrayList&lt;String&gt;();    //存放tel和Name的Map集合    private Map&lt;String, String&gt; phoneNameMap = new HashMap&lt;&gt;();    /**     * 初始化数据     */    public void initPhone(){        phoneList.add(&quot;17078388295&quot;);        phoneList.add(&quot;13980337439&quot;);        phoneList.add(&quot;14575535933&quot;);        phoneList.add(&quot;18902496992&quot;);        phoneList.add(&quot;18549641558&quot;);        phoneList.add(&quot;17005930322&quot;);        phoneList.add(&quot;18468618874&quot;);        phoneList.add(&quot;18576581848&quot;);        phoneList.add(&quot;15978226424&quot;);        phoneList.add(&quot;15542823911&quot;);        phoneList.add(&quot;17526304161&quot;);        phoneList.add(&quot;15422018558&quot;);        phoneList.add(&quot;17269452013&quot;);        phoneList.add(&quot;17764278604&quot;);        phoneList.add(&quot;15711910344&quot;);        phoneList.add(&quot;15714728273&quot;);        phoneList.add(&quot;16061028454&quot;);        phoneList.add(&quot;16264433631&quot;);        phoneList.add(&quot;17601615878&quot;);        phoneList.add(&quot;15897468949&quot;);        phoneNameMap.put(&quot;17078388295&quot;, &quot;李为&quot;);        phoneNameMap.put(&quot;13980337439&quot;, &quot;王军&quot;);        phoneNameMap.put(&quot;14575535933&quot;, &quot;时俊&quot;);        phoneNameMap.put(&quot;18902496992&quot;, &quot;天机&quot;);        phoneNameMap.put(&quot;18549641558&quot;, &quot;蔡铭&quot;);        phoneNameMap.put(&quot;17005930322&quot;, &quot;陶尚&quot;);        phoneNameMap.put(&quot;18468618874&quot;, &quot;魏山帅&quot;);        phoneNameMap.put(&quot;18576581848&quot;, &quot;华倩&quot;);        phoneNameMap.put(&quot;15978226424&quot;, &quot;焦君山&quot;);        phoneNameMap.put(&quot;15542823911&quot;, &quot;钟尾田&quot;);        phoneNameMap.put(&quot;17526304161&quot;, &quot;司可可&quot;);        phoneNameMap.put(&quot;15422018558&quot;, &quot;官渡&quot;);        phoneNameMap.put(&quot;17269452013&quot;, &quot;上贵坡&quot;);        phoneNameMap.put(&quot;17764278604&quot;, &quot;时光机&quot;);        phoneNameMap.put(&quot;15711910344&quot;, &quot;李发&quot;);        phoneNameMap.put(&quot;15714728273&quot;, &quot;蒂冈&quot;);        phoneNameMap.put(&quot;16061028454&quot;, &quot;范德&quot;);        phoneNameMap.put(&quot;16264433631&quot;, &quot;周朝王&quot;);        phoneNameMap.put(&quot;17601615878&quot;, &quot;谢都都&quot;);        phoneNameMap.put(&quot;15897468949&quot;, &quot;刘何思&quot;);    }</code></pre><p>（2）创建随机生成通话时间的方法：randomDate<br>该时间生成后的格式为yyyy-MM-dd HH:mm:ss，并使之可以根据传入的起始时间和结束时间来随机生成</p><pre><code>   /**     * 注：传入时间要在时间[startTime, endTime]     * 公式：起始时间 + （结束时间 - 起始时间）* Math.random()     * @param startTime     * @param endTime     */    private String randomBuildTime(String startTime, String endTime) {        try {            SimpleDateFormat sdf1 = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);            Date startDate = sdf1.parse(startTime);            Date endDate = sdf1.parse(endTime);            if(endDate.getTime() &lt;= startDate.getTime()){                return null;            }            //公式：起始时间 + （结束时间 - 起始时间）* Math.random()            long randomTs = startDate.getTime() + (long) ((endDate.getTime() - startDate.getTime()) * Math.random());            Date resultDate = new Date(randomTs);            SimpleDateFormat sdf2 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);            String resultTimeString = sdf2.format(resultDate);            return resultTimeString;        } catch (ParseException e) {            e.printStackTrace();        }        return null;    }</code></pre><p>（3）创建生产日志一条日志的方法：productLog<br>随机抽取两个电话号码，随机产生通话建立时间，随机通话时长，将这几个字段拼接成一个字符串，然后return，便可以产生一条通话的记录。需要注意的是，如果随机出的两个电话号码一样，需要重新随机（随机过程可优化，但并非此次重点）。通话时长的随机为20分钟以内，即：60秒 * 30，并格式化为4位数字，例如：0600(10分钟)</p><pre><code>   /**     * 产生数据     * 格式： caller,callee,buildTime,duration     * @return     */    public String product(){        //ctrl + d 复制此行 ， ctrl + x 剪切此行 ，ctrl + y 删除此行        //主叫        String caller = null;        String callerName = null;        //被叫        String callee = null;        String calleeName = null;        //ctrl + alt + v 推导出前面的对象类型  Home前  End后        int callerIndex = (int) (Math.random() * phoneList.size());        caller = phoneList.get(callerIndex);        callerName = phoneNameMap.get(caller);        while(true) {            //ctrl + shift + 下  ：下移这行            int calleeIndex = (int) (Math.random() * phoneList.size());            callee = phoneList.get(calleeIndex);            calleeName = phoneNameMap.get(callee);            if(!caller.equals(callee)) break;        }        //第三个字段        String buildTime = randomBuildTime(startTime, endTime);        //第四个字段，最多时长        DecimalFormat df = new DecimalFormat(&quot;0000&quot;);        String duration = df.format((int) 30 * 60 * Math.random());        StringBuilder sb = new StringBuilder();        sb.append(caller + &quot;,&quot;).append(callee + &quot;,&quot;).append(buildTime + &quot;,&quot;).append( duration);        return sb.toString();    }</code></pre><p>（4）创建写入日志方法：writeLog<br>productLog每产生一条日志，便将日志写入到本地文件中，所以建立一个专门用于日志写入的方法，需要涉及到IO操作，需要注意的是，输出流每次写一条日之后需要flush，不然可能导致积攒多条数据才输出一次。最后需要将productLog方法放置于while死循环中执行</p><pre><code>/** * 把数据写到文件当中 * @param filePath */public void writeLog(String filePath){    try {        OutputStreamWriter osw = new OutputStreamWriter(new FileOutputStream(filePath, true), &quot;UTF-8&quot;);        while(true){            Thread.sleep(200);            String log = product();            System.out.println(log);             //一定要手动flush才可以确保每条数据都写入到文件一次            osw.write(log + &quot;\n&quot;);            osw.flush();        }    } catch (Exception e) {        e.printStackTrace();    }}</code></pre><p>（5）在主函数中初始化以上逻辑，并测试：</p><pre><code> public static void main(String[] args) {        //args = new String[]{&quot;E:\\CT Project file\\calllog.csv&quot;};        if(args == null || args.length &lt;= 0){            System.out.println(&quot;没写路径&quot;);            return ;        }        ProductLog productLog = new ProductLog();        productLog.initPhone();        productLog.writeLog(args[0]);    }</code></pre><h4 id="3、打包测试"><a href="#3、打包测试" class="headerlink" title="3、打包测试"></a>3、打包测试</h4><p>1）Maven打包方式<br>分别在Windows上和Linux中进行测试：<br>java -cp jar包的绝对路径 全类名 输出路径</p><p>2）将此包放在在/opt/jar下面，并写如下脚本</p><p><strong>product.sh</strong> </p><pre><code>#!bin.bashjava -cp /opt/jars/CT_producer-1.0-SNAPSHOT.jar producer.ProductLog /opt/jars/calllog.csv</code></pre><p>3）运行<br>sh product.sh<br>产生calllog.csv文件</p><h3 id="五、数据采集-消费-存储"><a href="#五、数据采集-消费-存储" class="headerlink" title="五、数据采集/消费(存储)"></a>五、数据采集/消费(存储)</h3><p>欢迎来到数据采集模块（消费），在企业中你要清楚流式数据采集框架Flume和Kafka的定位是什么。我们在此需要将实时数据通过Flume采集到Kafka然后供给给HBase消费</p><p>Flume：Cloudera公司研发<br>适合下游数据消费者不多的情况；<br>适合数据安全性要求不高的操作；<br>适合与Hadoop生态圈对接的操作</p><p>Kafka：linkedin公司研发<br>适合数据下游消费众多的情况；<br>适合数据安全性要求较高的操作（支持replication）；</p><p><strong>因此我们常用的一种模型是</strong>：<br>线上数据 –&gt; Flume –&gt; Kafka –&gt; Flume(根据情景增删该流程) –&gt; HDFS</p><p><strong>消费存储模块流程图</strong>：</p><p><img src="/medias/%E6%B6%88%E8%B4%B9%E5%AD%98%E5%82%A8%E6%A8%A1%E5%9D%97%E6%B5%81%E7%A8%8B%E5%9B%BE.PNG" alt="消费存储模块流程图"> </p><h4 id="1、数据采集：采集实时产生的数据到kafka集群"><a href="#1、数据采集：采集实时产生的数据到kafka集群" class="headerlink" title="1、数据采集：采集实时产生的数据到kafka集群"></a>1、数据采集：采集实时产生的数据到kafka集群</h4><p>0）基础配置</p><ul><li>配置Kafka 略</li><li>配置Flume(flume2kafka.conf)</li></ul><pre><code># definea1.sources = r1a1.sinks = k1a1.channels = c1# sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F -c +0 /opt/jars/calllog.csva1.sources.r1.shell = /bin/bash -c# sinka1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.brokerList = hsiehchou121:9092,hsiehchou122:9092,hsiehchou123:9092a1.sinks.k1.topic = callloga1.sinks.k1.batchSize = 20a1.sinks.k1.requiredAcks = 1# channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# binda1.sources.r1.channels = c1a1.sinks.k1.channel = c1</code></pre><p>1）进入Flume根目录下，启动flume<br>/opt/module/flume-1.8.0/bin/flume-ng agent –conf /opt/module/flume-1.8.0/conf/ –name a1 –conf-file /opt/jars/flume2kafka.conf</p><p>2）运行生产日志的任务脚本，观察kafka控制台消费者是否成功显示产生的数据<br>$ sh productlog.sh</p><h4 id="2、编写代码：数据消费（HBase）"><a href="#2、编写代码：数据消费（HBase）" class="headerlink" title="2、编写代码：数据消费（HBase）"></a>2、编写代码：数据消费（HBase）</h4><p>如果以上操作均成功，则开始编写操作HBase的代码，用于消费数据，将产生的数据实时存储在HBase中</p><p><strong>思路</strong>：<br>a） 编写Kafka消费者，读取kafka集群中缓存的消息，并打印到控制台以观察是否成功；</p><p>b）既然能够读取到kafka中的数据了，就可以将读取出来的数据写入到HBase中，所以编写调用HBaseAPI相关方法，将从Kafka中读取出来的数据写入到HBase；</p><p>c） 以上两步已经足够完成消费数据，存储数据的任务，但是涉及到解耦，所以过程中需要将一些属性文件外部化，HBase通用性方法封装到某一个类中</p><p>创建新的module项目：<strong>ct_consumer</strong></p><p><strong>pom.xml文件配置</strong></p><pre><code>&lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;junit&lt;/groupId&gt;            &lt;artifactId&gt;junit&lt;/artifactId&gt;            &lt;version&gt;4.12&lt;/version&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;            &lt;version&gt;0.11.0.2&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-client --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;            &lt;version&gt;1.3.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-server --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;            &lt;artifactId&gt;hbase-server&lt;/artifactId&gt;            &lt;version&gt;1.3.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --&gt;        &lt;dependency&gt;            &lt;groupId&gt;mysql&lt;/groupId&gt;            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;            &lt;version&gt;8.0.13&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;                &lt;version&gt;2.12.4&lt;/version&gt;                &lt;configuration&gt;                    &lt;skipTests&gt;true&lt;/skipTests&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;                &lt;version&gt;3.8.1&lt;/version&gt;                &lt;configuration&gt;                    &lt;source&gt;1.8&lt;/source&gt;                    &lt;target&gt;1.8&lt;/target&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;</code></pre><p>1）新建类：<strong>HBaseConsumer</strong>（kafka的package）<br>该类主要用于读取kafka中缓存的数据，然后调用HBaseAPI，持久化数据</p><pre><code>package kafka;import hbase.HBaseDao;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import utils.PropertiesUtil;import java.util.Arrays;public class HBaseConsumer {    public static void main(String[] args) {        //消费者API        KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;&gt;(PropertiesUtil.properties);        //kafka Topic        kafkaConsumer.subscribe(Arrays.asList(PropertiesUtil.getProperty(&quot;kafka.topics&quot;)));        //创建写入HBase的对象        HBaseDao hd = new HBaseDao();        while(true) {            //消费拉取数据            ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(100);            //遍历打印数据            for(ConsumerRecord&lt;String, String&gt; cr : records){                String value = cr.value();                //13980337439,16264433631,2018-02-08 10:27:32,1740                System.out.println(value);                //把数据写入到HBase中                hd.put(value);            }        }    }}</code></pre><p>2) 新建类：<strong>PropertiesUtil</strong>（utils的package）<br>该类主要用于将常用的项目所需的参数外部化，解耦，方便配置</p><pre><code>package utils;import java.io.IOException;import java.io.InputStream;import java.util.Properties;public class PropertiesUtil {    public static Properties properties = null;    static {        //ctrl + alt + v        InputStream is = ClassLoader.getSystemResourceAsStream(&quot;hbase_consumer.properties&quot;);        properties = new Properties();        try {            properties.load(is);        } catch (IOException e) {            e.printStackTrace();        }    }    public static String getProperty(String key){        return properties.getProperty(key);    }}</code></pre><p>3） 创建kafka.properties文件，并放置于resources目录下</p><pre><code># 设置kafka的brokerlistbootstrap.servers=hsiehchou121:9092,hsiehchou122:9092,hsiehchou123:9092# 设置消费者所属的消费组group.id=hbase_consumer_group# 设置是否自动确认offsetenable.auto.commit=true# 自动确认offset的时间间隔auto.commit.interval.ms=30000# 设置key，value的反序列化类的全名key.deserializer=org.apache.kafka.common.serialization.StringDeserializervalue.deserializer=org.apache.kafka.common.serialization.StringDeserializer# 以下为自定义属性设置# 设置本次消费的主题kafka.topics=calllog# 设置HBase的一些变量hbase.calllog.regions=6hbase.calllog.namespace=ns_cthbase.calllog.tablename=ns_ct:calllog</code></pre><p>4）将hdfs-site.xml、core-site.xml、hbase-site.xml、log4j.properties放置于resources目录</p><p>5）新建类：HBaseUtil（utils的package）<br>该类主要用于封装一些HBase的常用操作，比如创建命名空间，创建表等等</p><pre><code>package utils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HColumnDescriptor;import org.apache.hadoop.hbase.HTableDescriptor;import org.apache.hadoop.hbase.NamespaceDescriptor;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.Admin;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;import java.text.DecimalFormat;import java.util.Iterator;import java.util.TreeSet;/** * 1、NameSpace ====&gt;  命名空间 * 2、createTable ===&gt; 表 * 3、isTable   ====&gt;  判断表是否存在 * 4、Region、RowKey、分区键 */public class HBaseUtil {    /**     * 初始化命名空间     *     * @param conf  配置对象     * @param namespace 命名空间的名字     */    public static void initNameSpace(Configuration conf, String namespace) throws IOException {        //获取链接connection        Connection connection = ConnectionFactory.createConnection(conf);        //获取admin对象        Admin admin = connection.getAdmin();        //创建命名空间，命名空间描述器        NamespaceDescriptor nd = NamespaceDescriptor                .create(namespace)                //add配置信息不强制加                .addConfiguration(&quot;create_time&quot;, String.valueOf(System.currentTimeMillis()))                .build();        //通过admin对象创建namespace        admin.createNamespace(nd);        close(admin,connection);    }    /**     * 初始化表     *     * @param conf     * @param tableName     * @param regions     * @param columnFamily     */    public static void createTable(Configuration conf, String tableName, int regions, String... columnFamily) throws IOException {        //获取链接connection        Connection connection = ConnectionFactory.createConnection(conf);        //获取admin对象        Admin admin = connection.getAdmin();        //如果表已存在，就返回        if (isExistTable(conf, tableName)){            return ;        }        //创建表对象        HTableDescriptor htd = new HTableDescriptor(TableName.valueOf(tableName));        for (String cf : columnFamily){            htd.addFamily(new HColumnDescriptor(cf));        }        //添加协处理器的全类名        htd.addCoprocessor(&quot;hbase.CalleeWriteObserver&quot;);        //通过admin创建表（htd（列族），分裂的regions）        admin.createTable(htd, getSplitKeys(regions));        //关闭        close(admin, connection);    }    /**     * 分区     *     * @param regions     * @return     */    private static byte[][] getSplitKeys(int regions) {        //第一步：定义分区键数组        String[] keys = new String[regions];        //分区位数格式化        DecimalFormat df = new DecimalFormat(&quot;00&quot;);        //  00|01|02|03|04|05        for (int i = 0; i &lt; regions; i++){            keys[i] = df.format(i) + &quot;|&quot;;        }        //第二步        byte[][] splitsKeys = new byte[regions][];        //分区间有序        TreeSet&lt;byte[]&gt; treeSet = new TreeSet&lt;&gt;(Bytes.BYTES_COMPARATOR);        for (int i = 0; i &lt; regions; i++){            treeSet.add(Bytes.toBytes(keys[i]));        }        //第三步        Iterator&lt;byte[]&gt; splitKeysIterator = treeSet.iterator();        int index = 0;        while (splitKeysIterator.hasNext()){            byte[] next = splitKeysIterator.next();            splitsKeys[index++] = next;        }        return splitsKeys;    }    /**     * 判断表是否存在     *     * @param conf     * @param tableName     */    public static boolean isExistTable(Configuration conf, String tableName) throws IOException {        //获取链接connection        Connection connection = ConnectionFactory.createConnection(conf);        //获取admin对象        Admin admin = connection.getAdmin();        //判断表API        boolean b = admin.tableExists(TableName.valueOf(tableName));        //关闭        close(admin, connection);        return b;    }    /**     * 关闭     *     * @param admin     * @param connection     */    public static void close(Admin admin, Connection connection) throws IOException {        if (admin != null) {            admin.close();        }        if (connection != null) {            connection.close();        }    }    /**     * regionCode, caller, buildTime, callee, flag, duration     * regionCode（rowkey前的离散串）     * duration（通话建立时间）     * 主叫（flag:1）：13980337439,16264433631,2018-02-08 10:27:32,1740   ==&gt;f1列族     * 被叫（flag:0）：16264433631,13980337439,2018-02-08 10:27:32,1740   ==&gt;f2列族     *     * 面试常问rowkey相关的问题：你们公司如何设计的RowKey？怎么设计RowKey才能避免热点问题（频繁访问某个区）?     *     * @param regionCode 散列的键     * @param caller     叫     * @param buildTime  建立时间     * @param callee     被叫     * @param flag       标明是主叫还是被叫     * @param duration   通话持续时间     * @return     */    public static String getRowKey(String regionCode, String caller, String buildTime, String callee, String flag, String duration){        StringBuilder sb = new StringBuilder();        sb.append(regionCode + &quot;_&quot;)                .append(caller + &quot;_&quot;)                .append(buildTime + &quot;_&quot;)                .append(callee + &quot;_&quot;)                .append(flag + &quot;_&quot;)                .append(duration);        return sb.toString();    }    /**     * 当数据进入HBase的Region的时候是足够的离散     *     * @param caller 主叫     * @param buildTime 通话建立时间     * @param regions region个数     * @return 返回分区号     */    public static String getRegionCode(String caller, String buildTime, int regions){        //取出主叫的后四位,lastPhone caller最后的后四位        String lastPhone = caller.substring(caller.length() - 4);        //取出年月   2018-02-08 10:27:32,1740 中取出年月        String yearMonth = buildTime                .replaceAll(&quot;-&quot;, &quot;&quot;)                .replaceAll(&quot;:&quot;, &quot;&quot;)                .replaceAll(&quot; &quot;, &quot;&quot;)                .substring(0, 6);        //离散操作1：做异或处理 ^        Integer x = Integer.valueOf(lastPhone) ^ Integer.valueOf(yearMonth);        //离散操作2：把离散1的值再做hashcode        int y = x.hashCode();        //最终想要的分区号        int regionCode = y % regions;        DecimalFormat df = new DecimalFormat(&quot;00&quot;);        return df.format(regionCode);    }}</code></pre><p>6）新建类：ConnectionInstance（utils的package）</p><pre><code>package utils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import java.io.IOException;public class ConnectionInstance {    private static Connection conn;    public static synchronized Connection getConnection(Configuration configuration) {        try {            if (conn == null || conn.isClosed()) {                conn = ConnectionFactory.createConnection(configuration);            }        } catch (IOException e) {            e.printStackTrace();        }        return conn;    }}</code></pre><p>7）新建类：HBaseDAO（完成以下内容后，考虑数据put的效率如何优化）（hbase的package）<br>该类主要用于执行具体的保存数据的操作，rowkey的生成规则等等</p><pre><code>package hbase;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.HTable;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.util.Bytes;import utils.ConnectionInstance;import utils.HBaseUtil;import utils.PropertiesUtil;import java.io.IOException;import java.text.SimpleDateFormat;import java.util.ArrayList;import java.util.List;public class HBaseDao {    public static final Configuration CONF;    private String namespace;    private int regions;    private String tableName;    private HTable table;    private Connection connection;    private SimpleDateFormat sdf1 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);    private SimpleDateFormat sdf2 = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);    //用来存放一小堆数据（30行），用于优化    private List&lt;Put&gt; cacheList = new ArrayList&lt;&gt;();    static {        CONF = HBaseConfiguration.create();    }    //Alt + Insert  Constructor    /**     * 用于构造命名空间和表     */    public HBaseDao() {        try {            namespace = PropertiesUtil.getProperty(&quot;hbase.calllog.namespace&quot;);            tableName = PropertiesUtil.getProperty(&quot;hbase.calllog.tablename&quot;);            regions = Integer.valueOf(PropertiesUtil.getProperty(&quot;hbase.calllog.regions&quot;));            if (!HBaseUtil.isExistTable(CONF, tableName)){                HBaseUtil.initNameSpace(CONF, namespace);                HBaseUtil.createTable(CONF, tableName, regions, &quot;f1&quot;, &quot;f2&quot;);            }        } catch (IOException e) {            e.printStackTrace();        }    }    /**     *     * @param value 13980337439,16264433631,2018-02-08 10:27:32,1740     */    public void put(String value) {        try {            if(cacheList.size() == 0){                connection = ConnectionInstance.getConnection(CONF);                table = (HTable) connection.getTable(TableName.valueOf(tableName));                table.setAutoFlushTo(false);                table.setWriteBufferSize(2 * 1024 * 1024);            }            //如果出现下标越界异常            String[] splitValue = value.split(&quot;,&quot;);            String caller = splitValue[0];            String callee = splitValue[1];            String buildTime = splitValue[2];            String duration = splitValue[3];            //散列得分区号            String regionCode = HBaseUtil.getRegionCode(caller, buildTime, regions);            //这个变量用于插入到HBase的列中            String buildTimeReplace = sdf2.format(sdf1.parse(buildTime));            //作为rowkey所需的参数            String buildTimeTs = String.valueOf(sdf1.parse(buildTime).getTime());            String rowkey = HBaseUtil.getRowKey(regionCode, caller, buildTimeReplace, callee, &quot;1&quot;, duration);            Put put = new Put(Bytes.toBytes(rowkey));            //通过put对象添加rowkey和列值，参数说明：(列族：f1)，(列名：caller)，（列值：caller）            //快捷键：ctrl + d            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;caller&quot;), Bytes.toBytes(caller));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;callee&quot;), Bytes.toBytes(callee));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;build_time&quot;), Bytes.toBytes(buildTimeReplace));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;build_time_ts&quot;), Bytes.toBytes(buildTimeTs));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;flag&quot;), Bytes.toBytes(&quot;1&quot;));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;duration&quot;), Bytes.toBytes(duration));            //把rowkey，列族，列名，列值放到cacheList的对象中            cacheList.add(put);            if(cacheList.size() &gt;= 30) {                table.put(cacheList);                table.flushCommits();                table.close();                cacheList.clear();            }        } catch (Exception e) {            e.printStackTrace();        }    }}</code></pre><p>8）新建类：HBaseDao（hbase的package）</p><pre><code>package hbase;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.HTable;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.util.Bytes;import utils.ConnectionInstance;import utils.HBaseUtil;import utils.PropertiesUtil;import java.io.IOException;import java.text.SimpleDateFormat;import java.util.ArrayList;import java.util.List;public class HBaseDao {    public static final Configuration CONF;    private String namespace;    private int regions;    private String tableName;    private HTable table;    private Connection connection;    private SimpleDateFormat sdf1 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);    private SimpleDateFormat sdf2 = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);    //用来存放一小堆数据（30行），用于优化    private List&lt;Put&gt; cacheList = new ArrayList&lt;&gt;();    static {        CONF = HBaseConfiguration.create();    }    //Alt + Insert  Constructor    /**     * 用于构造命名空间和表     */    public HBaseDao() {        try {            namespace = PropertiesUtil.getProperty(&quot;hbase.calllog.namespace&quot;);            tableName = PropertiesUtil.getProperty(&quot;hbase.calllog.tablename&quot;);            regions = Integer.valueOf(PropertiesUtil.getProperty(&quot;hbase.calllog.regions&quot;));            if (!HBaseUtil.isExistTable(CONF, tableName)){                HBaseUtil.initNameSpace(CONF, namespace);                HBaseUtil.createTable(CONF, tableName, regions, &quot;f1&quot;, &quot;f2&quot;);            }        } catch (IOException e) {            e.printStackTrace();        }    }    /**     *     * @param value 13980337439,16264433631,2018-02-08 10:27:32,1740     */    public void put(String value) {        try {            if(cacheList.size() == 0){                connection = ConnectionInstance.getConnection(CONF);                table = (HTable) connection.getTable(TableName.valueOf(tableName));                table.setAutoFlushTo(false);                table.setWriteBufferSize(2 * 1024 * 1024);            }            //如果出现下标越界异常            String[] splitValue = value.split(&quot;,&quot;);            String caller = splitValue[0];            String callee = splitValue[1];            String buildTime = splitValue[2];            String duration = splitValue[3];            //散列得分区号            String regionCode = HBaseUtil.getRegionCode(caller, buildTime, regions);            //这个变量用于插入到HBase的列中            String buildTimeReplace = sdf2.format(sdf1.parse(buildTime));            //作为rowkey所需的参数            String buildTimeTs = String.valueOf(sdf1.parse(buildTime).getTime());            String rowkey = HBaseUtil.getRowKey(regionCode, caller, buildTimeReplace, callee, &quot;1&quot;, duration);            Put put = new Put(Bytes.toBytes(rowkey));            //通过put对象添加rowkey和列值，参数说明：(列族：f1)，(列名：caller)，（列值：caller）            //快捷键：ctrl + d            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;caller&quot;), Bytes.toBytes(caller));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;callee&quot;), Bytes.toBytes(callee));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;build_time&quot;), Bytes.toBytes(buildTimeReplace));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;build_time_ts&quot;), Bytes.toBytes(buildTimeTs));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;flag&quot;), Bytes.toBytes(&quot;1&quot;));            put.addColumn(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;duration&quot;), Bytes.toBytes(duration));            //把rowkey，列族，列名，列值放到cacheList的对象中            cacheList.add(put);            if(cacheList.size() &gt;= 30) {                table.put(cacheList);                table.flushCommits();                table.close();                cacheList.clear();            }        } catch (Exception e) {            e.printStackTrace();        }    }}</code></pre><h4 id="3、运行测试：HBase消费数据"><a href="#3、运行测试：HBase消费数据" class="headerlink" title="3、运行测试：HBase消费数据"></a>3、运行测试：HBase消费数据</h4><p>尖叫提示：请将Linux允许打开的文件个数和进程数进行优化，优化RegionServer与Zookeeper会话的超时时间。（参考HBase文档中优化章节）<br>项目成功后，则将项目打包后在linux中运行测试</p><p><strong>打包HBase消费者代码</strong><br>a）  在windows中，进入工程的pom.xml所在目录下（建议将该工程的pom.xml文件拷贝到其他临时目录中，例如我把pom.xml文件拷贝到了F:\maven-lib\目录下），然后使用mvn命令下载工程所有依赖的jar包<br>mvn -DoutputDirectory=./lib -DgroupId=com.hsiehchou -DartifactId=ct_consumer -Dversion=r-1.0-SNAPSHOT dependency:copy-dependencies<br>b）  使用maven打包工程<br>c） 测试执行该jar包</p><p>方案一：推荐，使用通配符，将所有依赖加入到classpath中，不可使用<em>.jar的方式<br>注意：如果是在Linux中实行，注意文件夹之间的分隔符。自己的工程要单独在cp中指定，不要直接放在maven-lib/lib目录下<br>java -cp F:\maven-lib\CT_consumerr-1.0-SNAPSHOT.jar;F:\maven-lib\lib\</em> com.hsiehchou.CT_kafka.HBaseConsumer</p><p>方案二：最最推荐，使用java.ext.dirs参数将所有依赖的目录添加进classpath中<br>注意：-Djava.ext.dirs=属性后边的路径不能为”~”<br>java -Djava.ext.dirs=F:\maven-lib\lib\ -cp F:\maven-lib\CT_consumerr-1.0-SNAPSHOT.jar com.hsiehchou.CT_consumer.kafka.HBaseConsumer</p><h4 id="4、编写代码：优化数据存储方案"><a href="#4、编写代码：优化数据存储方案" class="headerlink" title="4、编写代码：优化数据存储方案"></a>4、编写代码：优化数据存储方案</h4><p>现在我们要使用HBase查找数据时，尽可能的使用rowKey去精准的定位数据位置，而非使用ColumnValueFilter或者SingleColumnValueFilter，按照单元格Cell中的Value过滤数据，这样做在数据量巨大的情况下，效率是极低的——如果要涉及到全表扫描。所以尽量不要做这样可怕的事情。注意，这并非ColumnValueFilter就无用武之地。现在，我们将使用协处理器，将数据一分为二</p><p><strong>思路</strong></p><p>a）编写协处理器类，用于协助处理HBase的相关操作（增删改查）<br>b）在协处理器中，一条主叫日志成功插入后，将该日志切换为被叫视角再次插入一次，放入到与主叫日志不同的列族中<br>c）重新创建hbase表，并设置为该表设置协处理器<br>d）编译项目，发布协处理器的jar包到hbase的lib目录下，并群发该jar包<br>e）修改hbase-site.xml文件，设置协处理器，并群发该hbase-site.xml文件</p><p><strong>编码</strong></p><p>1） 新建协处理器类：CalleeWriteObserver，并覆写postPut方法，该方法会在数据成功插入之后被回调（hbase的package）</p><pre><code>package hbase;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.Durability;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Table;import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver;import org.apache.hadoop.hbase.coprocessor.ObserverContext;import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;import org.apache.hadoop.hbase.regionserver.wal.WALEdit;import org.apache.hadoop.hbase.util.Bytes;import utils.HBaseUtil;import utils.PropertiesUtil;import java.io.IOException;import java.text.ParseException;import java.text.SimpleDateFormat;public class CalleeWriteObserver extends BaseRegionObserver {    SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);    //ctrl + 0    /**     * 插入主叫数据后，随即插入被叫数据     * @param e     * @param put     * @param edit     * @param durability     * @throws IOException     */    @Override    public void postPut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e,                        Put put,                        WALEdit edit,                        Durability durability) throws IOException {        //注意：一定到删除super.postPut(e, put, edit, durability);        //操作的目标表        String targetTableName = PropertiesUtil.getProperty(&quot;hbase.calllog.tablename&quot;);        //当前操作put后的表        String currentTableName = e.getEnvironment().getRegionInfo().getTable().getNameAsString();        //不是同一个表返回        if (!targetTableName.equals(currentTableName)){            return;        }        //05_18902496992_20180720182543_14575535933_1_0076        String oriRowKey = Bytes.toString(put.getRow());        System.out.println(oriRowKey);        String[] splitOriRowKey = oriRowKey.split(&quot;_&quot;);        String caller = splitOriRowKey[1];        String callee = splitOriRowKey[3];        String buildTime = splitOriRowKey[2];        String duration = splitOriRowKey[5];        //如果当前插入的是被叫数据，则直接返回(因为默认提供的数据全部为主叫数据)        String flag = splitOriRowKey[4];        String calleeflag = &quot;0&quot;;        if (flag.equals(calleeflag) ){            return;        }        flag = calleeflag;        Integer regions = Integer.valueOf(PropertiesUtil.getProperty(&quot;hbase.calllog.regions&quot;));        String regionCode = HBaseUtil.getRegionCode(callee, buildTime, regions);        String calleeRowKey = HBaseUtil.getRowKey(regionCode, callee, buildTime, caller, flag, duration);        String buildTimeTs = &quot;&quot;;        try {            buildTimeTs = String.valueOf(sdf.parse(buildTime).getTime());        } catch (ParseException e1) {            e1.printStackTrace();        }        Put calleePut = new Put(Bytes.toBytes(calleeRowKey));        calleePut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;callee&quot;), Bytes.toBytes(caller));        calleePut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;caller&quot;), Bytes.toBytes(callee));        calleePut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;build_time&quot;), Bytes.toBytes(buildTime));        calleePut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;build_time_ts&quot;), Bytes.toBytes(buildTimeTs));        calleePut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;flag&quot;), Bytes.toBytes(flag));        calleePut.addColumn(Bytes.toBytes(&quot;f2&quot;), Bytes.toBytes(&quot;duration&quot;), Bytes.toBytes(duration));        Bytes.toBytes(100L);        Table table = e.getEnvironment().getTable(TableName.valueOf(targetTableName));        table.put(calleePut);        table.close();    }}</code></pre><p>2）重新创建HBase表，并设置为该表设置协处理器。在“表描述器”中调用addCoprocessor方法进行协处理器的设置，大概是这样的：（你需要找到你的建表的那部分代码，添加如下逻辑）<br>tableDescriptor.addCoprocessor(“hbase.CalleeWriteObserver”);</p><h4 id="5、运行测试：协处理器"><a href="#5、运行测试：协处理器" class="headerlink" title="5、运行测试：协处理器"></a>5、运行测试：协处理器</h4><p>重新编译项目，发布jar包到hbase的lib目录下（注意需群发）：<br>$ scp -r CT_consumer-1.0-SNAPSHOT.jar root@hsiehchou121:<code>pwd</code></p><p>重新修改<strong>hbase-site.xml</strong>：</p><pre><code>    &lt;property&gt;        &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;        &lt;value&gt;hbase.CalleeWriteObserver&lt;/value&gt;    &lt;/property&gt;</code></pre><p>完成以上步骤后，重新消费数据进行测试</p><h4 id="6、编写测试单元：范围查找数据"><a href="#6、编写测试单元：范围查找数据" class="headerlink" title="6、编写测试单元：范围查找数据"></a>6、编写测试单元：范围查找数据</h4><p><strong>思路</strong><br>a）已知要查询的手机号码以及起始时间节点和结束时间节点，查询该节点范围内的该手机号码的通话记录</p><p>b）拼装startRowKey和stopRowKey，即扫描范围，要想拼接出扫描范围，首先需要了解rowkey组成结构，我们再来复习一下，举个大栗子<br>rowkey：<br>分区号_手机号码1_通话建立时间_手机号码2_主(被)叫标记_通话持续时间<br>01_15837312345_20180725071833_1_0180</p><p>c）比如按月查询通话记录，则startRowKey举例：<br>regionHash_158373123456_20180805010000<br>stopRowKey举例：<br>regionHash_158373123456_20180805010000</p><p><strong>注意</strong>：startRowKey和stopRowKey设计时，后面的部分已经被去掉</p><p><strong>尖叫提示</strong>：rowKey的扫描范围为前闭后开</p><p><strong>尖叫提示</strong>：rowKey默认是有序的，排序规则为字符的按位比较<br>d）如果查找所有的，需要多次scan表，每次scan设置为下一个时间窗口即可，该操作可放置于for循环中</p><p><strong>编码</strong>：<br>e）<strong>运行测试</strong><br>观察是否已经按照时间范围查询出对应的数据</p><h4 id="7、将数据从本地读取到HBase"><a href="#7、将数据从本地读取到HBase" class="headerlink" title="7、将数据从本地读取到HBase"></a>7、将数据从本地读取到HBase</h4><p>1）<strong>启动ZooKeeper</strong>（配置了全局环境变量）</p><pre><code>zkServer.sh start</code></pre><p>2）<strong>启动Kafka</strong>（配置了全局环境变量）</p><pre><code>kafka-server-start.sh /root/hd/kafka/config/server.properties &amp;  </code></pre><p><strong>创建主题</strong></p><pre><code>bin/kafka-topics.sh --zookeeper hsiehchou121:2181 --topic calllog --create --replication-factor 1 --partitions 3</code></pre><p><strong>列出所有主题</strong></p><pre><code>bin/kafka-topics.sh --zookeeper hsiehchou121:2181 --list</code></pre><p><strong>启动 Kafka消费者</strong></p><pre><code>bin/kafka-console-consumer.sh --bootstrap-server hsiehchou121:9092 --topic calllog --from-beginning </code></pre><p>3）<strong>启动Hadoop</strong>（配置了全局环境变量）</p><pre><code>start-all.sh</code></pre><p>4）<strong>启动HBase</strong>（配置了全局环境变量）<br><strong>start-hbase.sh</strong></p><p>5）<strong>启动Flume</strong>（没有配置全局环境变量，去flume目录下）</p><pre><code>bin/flume-ng agent --conf conf/ --name a1 --conf-file myagent/flume2kafka.conf</code></pre><p>6）<strong>IDEA打包CT_consumer.jar</strong><br>此jar包要放入hbase的lib下面，不然HBase写不进数据</p><p>7）在IDEA里面<strong>运行HBaseConsumer.java</strong></p><h4 id="8、总结"><a href="#8、总结" class="headerlink" title="8、总结"></a>8、总结</h4><p>数据（本地）-&gt;Flume采集数据-&gt;Kafka消费数据-&gt;HBase</p><h3 id="六、数据分析"><a href="#六、数据分析" class="headerlink" title="六、数据分析"></a>六、数据分析</h3><p>我们的数据已经完整的采集到了HBase集群中，这次我们需要对采集到的数据进行分析，统计出我们想要的结果。注意，在分析的过程中，我们不一定会采取一个业务指标对应一个MapReduce-Job的方式，如果情景允许，我们会采取一个MapReduce分析多个业务指标的方式来进行任务</p><p><strong>数据分析模块流程图</strong></p><p><img src="/medias/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%A8%A1%E5%9D%97%E6%B5%81%E7%A8%8B%E5%9B%BE.PNG" alt="数据分析模块流程图"></p><p><strong>业务指标</strong><br>a）用户每天主叫通话个数统计，通话时间统计<br>b）用户每月通话记录统计，通话时间统计<br>c）用户之间亲密关系统计。（通话次数与通话时间体现用户亲密关系）</p><h4 id="1、MySQL表结构设计"><a href="#1、MySQL表结构设计" class="headerlink" title="1、MySQL表结构设计"></a>1、MySQL表结构设计</h4><p>我们将分析的结果数据保存到Mysql中，以方便Web端进行查询展示<br>1）表：<strong>db_telecom.tb_contacts</strong></p><p>用于存放用户手机号码与联系人姓名</p><table><thead><tr><th align="center">列</th><th align="center">备注</th><th align="center">类型</th></tr></thead><tbody><tr><td align="center">id</td><td align="center">自增主键</td><td align="center">int(11) NOT NULL</td></tr><tr><td align="center">telephone</td><td align="center">手机号码</td><td align="center">varchar(255) NOT NULL</td></tr><tr><td align="center">name</td><td align="center">联系人姓名</td><td align="center">varchar(255) NOT NULL</td></tr></tbody></table><p>2）表：<strong>db_telecom.tb_call</strong></p><p>用于存放某个时间维度下通话次数与通话时长的总和</p><table><thead><tr><th align="center">列</th><th align="center">备注</th><th align="center">类型</th></tr></thead><tbody><tr><td align="center">id_date_contact</td><td align="center">复合主键（联系人维度id，时间维度id）</td><td align="center">varchar(255) NOT NULL</td></tr><tr><td align="center">id_date_dimension</td><td align="center">时间维度id</td><td align="center">int(11) NOT NULL</td></tr><tr><td align="center">id_contact</td><td align="center">查询人的电话号码</td><td align="center">int(11) NOT NULL</td></tr><tr><td align="center">call_sum</td><td align="center">通话次数总和</td><td align="center">int(11) NOT NULL DEFAULT 0</td></tr><tr><td align="center">call_duration_sum</td><td align="center">通话时长总和</td><td align="center">int(11) NOT NULL DEFAULT 0</td></tr></tbody></table><p>3）表：<strong>db_telecom.tb_dimension_date</strong></p><p>用于存放时间维度的相关数据</p><table><thead><tr><th align="center">列</th><th align="center">备注</th><th align="center">类型</th></tr></thead><tbody><tr><td align="center">id</td><td align="center">自增主键</td><td align="center">int(11) NOT NULL</td></tr><tr><td align="center">year</td><td align="center">年，当前通话信息所在年</td><td align="center">int(11) NOT NULL</td></tr><tr><td align="center">month</td><td align="center">月，当前通话信息所在月，如果按照年来统计信息，则month为-1</td><td align="center">int(11) NOT NULL</td></tr><tr><td align="center">day</td><td align="center">日，当前通话信息所在日，如果是按照月来统计信息，则day为-1</td><td align="center">int(11) NOT NULL</td></tr></tbody></table><p>MySQL的建表语句：</p><p>CREATE TABLE <code>tb_call</code> (<br>  <code>id_date_contact</code> varchar(255) NOT NULL,<br>  <code>id_date_dimension</code> int(11) NOT NULL,<br>  <code>id_contact</code> int(11) NOT NULL,<br>  <code>call_sum</code> int(11) NOT NULL,<br>  <code>call_duration_sum</code> int(11) NOT NULL,<br>  PRIMARY KEY (<code>id_date_contact</code>)<br>) ENGINE=InnoDB DEFAULT CHARSET=utf8;</p><p>CREATE TABLE <code>tb_contacts</code> (<br>  <code>id</code> int(11) NOT NULL AUTO_INCREMENT,<br>  <code>telephone</code> varchar(255) NOT NULL,<br>  <code>name</code> varchar(255) NOT NULL,<br>  PRIMARY KEY (<code>id</code>)<br>) ENGINE=InnoDB AUTO_INCREMENT=21 DEFAULT CHARSET=utf8;</p><p>CREATE TABLE <code>tb_dimension_date</code> (<br>  <code>id</code> int(11) NOT NULL AUTO_INCREMENT,<br>  <code>year</code> int(11) NOT NULL,<br>  <code>month</code> int(11) NOT NULL,<br>  <code>day</code> int(11) NOT NULL,<br>  PRIMARY KEY (<code>id</code>)<br>) ENGINE=InnoDB AUTO_INCREMENT=263 DEFAULT CHARSET=utf8;</p><h4 id="2、需求：按照不同的维度统计通话"><a href="#2、需求：按照不同的维度统计通话" class="headerlink" title="2、需求：按照不同的维度统计通话"></a>2、需求：按照不同的维度统计通话</h4><p>根据需求目标，设计出如上表结构。我们需要按照时间范围（年月日），结合MapReduce统计出所属时间范围内所有手机号码的通话次数总和以及通话时长总和。<br>思路：<br>a）维度，即某个角度，某个视角，按照时间维度来统计通话，比如我想统计2018年所有月份所有日子的通话记录，那这个维度我们大概可以表述为2018年<code>*</code>月<code>*</code>日<br>b）通过Mapper将数据按照不同维度聚合给Reducer<br>c）通过Reducer拿到按照各个维度聚合过来的数据，进行汇总，输出。<br>d）根据业务需求，将Reducer的输出通过Outputformat把数据<br>数据输入：HBase<br>数据输出：MySQL<br>HBase中数据源结构：</p><table><thead><tr><th align="center">标签</th><th align="center">举例&amp;说明</th></tr></thead><tbody><tr><td align="center">rowkey</td><td align="center">hashregion_caller_datetime_callee_flag_duration;  01_15837312345_20180527081033_13766889900_1_0180</td></tr><tr><td align="center">family</td><td align="center">f1列族：存放主叫信息； f2列族：存放被叫信息</td></tr><tr><td align="center">caller</td><td align="center">第一个手机号码</td></tr><tr><td align="center">callee</td><td align="center">第二个手机号码</td></tr><tr><td align="center">date_time</td><td align="center">通话建立的时间，例如：20181017081520</td></tr><tr><td align="center">date_time_ts</td><td align="center">date_time对应的时间戳形式</td></tr><tr><td align="center">duration</td><td align="center">通话时长(单位：秒)</td></tr><tr><td align="center">flag</td><td align="center">标记caller是主叫还是被叫（caller的身份与call2的身份互斥）</td></tr></tbody></table><p>a）已知目标，那么需要结合目标思考已有数据是否能够支撑目标实现；<br>b） 根据目标数据结构，构建MySQL表结构，建表；<br>c）思考代码需要涉及到哪些功能模块，建立不同功能模块对应的包结构。<br>d）描述数据，一定是基于某个维度（视角）的，所以构建维度类。比如按照“年”与“手机号码”的组合作为key聚合所有的数据，便可以统计这个手机号码，这一年的相关结果<br>e）自定义OutputFormat用于对接MySQL，使数据输出<br>f）创建相关工具类</p><h4 id="3、环境准备"><a href="#3、环境准备" class="headerlink" title="3、环境准备"></a>3、环境准备</h4><p>1） <strong>新建module：ct_analysis</strong><br><strong>pom文件配置</strong></p><pre><code>  &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;junit&lt;/groupId&gt;            &lt;artifactId&gt;junit&lt;/artifactId&gt;            &lt;version&gt;4.12&lt;/version&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;mysql&lt;/groupId&gt;            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;            &lt;version&gt;8.0.13&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;            &lt;version&gt;1.3.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;            &lt;artifactId&gt;hbase-server&lt;/artifactId&gt;            &lt;version&gt;1.3.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;            &lt;artifactId&gt;hbase-common&lt;/artifactId&gt;            &lt;version&gt;1.3.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.aspectj&lt;/groupId&gt;            &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt;            &lt;version&gt;1.8.10&lt;/version&gt;        &lt;/dependency&gt;        &lt;!--简化javabean--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;version&gt;1.16.18&lt;/version&gt;            &lt;scope&gt;provided&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;                &lt;version&gt;2.12.4&lt;/version&gt;                &lt;configuration&gt;                    &lt;skipTests&gt;true&lt;/skipTests&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;                &lt;version&gt;3.8.1&lt;/version&gt;                &lt;configuration&gt;                    &lt;source&gt;1.8&lt;/source&gt;                    &lt;target&gt;1.8&lt;/target&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;</code></pre><p>2）<strong>创建包结构</strong>，根包：<strong>com.hsiehchou</strong>(不同颜色代表不同层级的递进)</p><p><img src="/medias/CT_XZ.PNG" alt="CT_XZ"></p><p>直接看CT_analysis</p><p>3） <strong>类表</strong><br>|  类名    |    备注    |<br>| :——–: | :——–:|<br>| DimensionConverter | 负责实际的维度转id功能接口|<br>| DimensionConverterImpl  |  DimensionConverter  实现类，负责实际的维度转id功能 |<br>| BaseDimension  |  维度（key）基类   |<br>| BaseValue    | 值（value）基类    |<br>| ComDimension  | 时间维度+联系人维度的组合维度  |<br>| ContactDimension | 联系人维度  |<br>| DateDimension    |  时间维度   |<br>| CountDurationValue  |  通话次数与通话时长的封装 |<br>| CountDurationMapper |  数据分析的Mapper类，继承自TableMapper   |<br>| MysqlOutputFormat  | 自定义Outputformat，对接Mysql  |<br>| CountDurationReducer  | 数据分析的Reducer类，继承自Reduccer  |<br>| CountDurationRunner | 数据分析的驱动类，组装Job |<br>| JDBCInstance    |  获取连接实例   |<br>| JDBCUtils  |  连接Mysql的工具类   |<br>| LRUCache   |  用于缓存已知的维度id，减少对mysql的操作次数，提高效率   |  </p><h4 id="4、编写代码：数据分析"><a href="#4、编写代码：数据分析" class="headerlink" title="4、编写代码：数据分析"></a>4、编写代码：数据分析</h4><p>1）创建类：<strong>CountDurationMapper</strong></p><pre><code>package mapper;import kv.key.ComDimension;import kv.key.ContactDimension;import kv.key.DateDimension;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableMapper;import org.apache.hadoop.hbase.util.Bytes;import org.apache.hadoop.io.Text;import java.io.IOException;import java.util.HashMap;import java.util.Map;public class CountDurationMapper extends TableMapper&lt;Comparable, Text&gt; {    private ComDimension comDimension = new ComDimension();    private Text durationText = new Text();    private Map&lt;String, String&gt; phoneMap;    @Override    protected void setup(Context context) throws IOException, InterruptedException {        phoneMap = new HashMap&lt;&gt;(20);        //批量修改名字Ctrl + Alt + Shift + J        phoneMap.put(&quot;17078388295&quot;, &quot;李为&quot;);        phoneMap.put(&quot;13980337439&quot;, &quot;王军&quot;);        phoneMap.put(&quot;14575535933&quot;, &quot;时俊&quot;);        phoneMap.put(&quot;18902496992&quot;, &quot;天机&quot;);        phoneMap.put(&quot;18549641558&quot;, &quot;蔡铭&quot;);        phoneMap.put(&quot;17005930322&quot;, &quot;陶尚&quot;);        phoneMap.put(&quot;18468618874&quot;, &quot;魏山帅&quot;);        phoneMap.put(&quot;18576581848&quot;, &quot;华倩&quot;);        phoneMap.put(&quot;15978226424&quot;, &quot;焦君山&quot;);        phoneMap.put(&quot;15542823911&quot;, &quot;钟尾田&quot;);        phoneMap.put(&quot;17526304161&quot;, &quot;司可可&quot;);        phoneMap.put(&quot;15422018558&quot;, &quot;官渡&quot;);        phoneMap.put(&quot;17269452013&quot;, &quot;上贵坡&quot;);        phoneMap.put(&quot;17764278604&quot;, &quot;时光机&quot;);        phoneMap.put(&quot;15711910344&quot;, &quot;李发&quot;);        phoneMap.put(&quot;15714728273&quot;, &quot;蒂冈&quot;);        phoneMap.put(&quot;16061028454&quot;, &quot;范德&quot;);        phoneMap.put(&quot;16264433631&quot;, &quot;周朝王&quot;);        phoneMap.put(&quot;17601615878&quot;, &quot;谢都都&quot;);        phoneMap.put(&quot;15897468949&quot;, &quot;刘何思&quot;);    }    @Override    protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException {        //05_18902496992_1525454104000_15711910344_1_1705        String rowkey = Bytes.toString(key.get());        String[] splits = rowkey.split(&quot;_&quot;);        if (&quot;0&quot;.equals(splits[4])){            return;        }        //聚合的是主叫数据        String caller = splits[1];        String callee = splits[3];        String buildTime = splits[2];        String duration = splits[5];        durationText.set(duration);        String year = buildTime.substring(0,4);        String month = buildTime.substring(4,6);        String day = buildTime.substring(6,8);        //年、月、日整数        DateDimension yearDimension = new DateDimension(year, &quot;-1&quot;, &quot;-1&quot;);        DateDimension monthDimension = new DateDimension(year, month, &quot;-1&quot;);        DateDimension dayDimension = new DateDimension(year, month, day);        //主叫callerContactDimension        ContactDimension callerContactDimension = new ContactDimension(caller, phoneMap.get(caller));        comDimension.setContactDimension(callerContactDimension);        //年        comDimension.setDateDimension(yearDimension);        context.write(comDimension, durationText);        //月        comDimension.setDateDimension(monthDimension);        context.write(comDimension, durationText);        //日        comDimension.setDateDimension(dayDimension);        context.write(comDimension, durationText);        //被叫callerContactDimension        ContactDimension calleeContactDimension = new ContactDimension(callee, phoneMap.get(callee));        comDimension.setContactDimension(calleeContactDimension);        //年        comDimension.setDateDimension(yearDimension);        context.write(comDimension, durationText);        //月        comDimension.setDateDimension(monthDimension);        context.write(comDimension, durationText);        //日        comDimension.setDateDimension(dayDimension);        context.write(comDimension, durationText);    }}</code></pre><p>2）创建类：<strong>CountDurationReducer</strong></p><pre><code>package reducer;import kv.key.ComDimension;import kv.value.CountDurationValue;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;import java.io.IOException;public class CountDurationReducer extends Reducer&lt;ComDimension, Text, ComDimension, CountDurationValue&gt; {    private CountDurationValue countDurationValue = new CountDurationValue();    @Override    protected void reduce(ComDimension key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException {        int callSum = 0;        int callDuration = 0;        for (Text t : values){            callSum++;            callDuration += Integer.valueOf(t.toString());        }        countDurationValue.setCallSum(String.valueOf(callSum));        countDurationValue.setCallDurationSum(String.valueOf(callDuration));        context.write(key, countDurationValue);    }}</code></pre><p>3）创建类：<strong>CountDurationRunner</strong></p><pre><code>package runner;import kv.key.ComDimension;import kv.value.CountDurationValue;import mapper.CountDurationMapper;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.Admin;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import org.apache.hadoop.hbase.client.Scan;import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;import outputformat.MysqlOutputFormat;import reducer.CountDurationReducer;import java.io.IOException;public class CountDurationRunner implements Tool {    private Configuration conf = null;    @Override    public void setConf(Configuration conf) {        this.conf = HBaseConfiguration.create(conf);    }    @Override    public Configuration getConf() {        return this.conf;    }    @Override    public int run(String[] strings) throws Exception {        //得到conf        Configuration conf = this.getConf();        //实例化Job        Job job = Job.getInstance(conf);        job.setJarByClass(CountDurationRunner.class);        //组装Mapper InputFormat        initHBaseInputConfig(job);        //组装Reducer OutputFormay        initReducerOutputConfig(job);        return job.waitForCompletion(true) ? 0:1;    }    private void initHBaseInputConfig(Job job) {        Connection connection = null;        Admin admin = null;        try {            String tableName = &quot;ns_ct:calllog&quot;;            connection = ConnectionFactory.createConnection(job.getConfiguration());            admin = connection.getAdmin();            if (!admin.tableExists(TableName.valueOf(tableName))){                throw new RuntimeException(&quot;无法找到目标表&quot;);            }            Scan scan = new Scan();            //可以优化            TableMapReduceUtil.initTableMapperJob(                    tableName,                    scan,                    CountDurationMapper.class,                    ComDimension.class,                    Text.class,                    job,                    true);        } catch (IOException e) {            e.printStackTrace();        }finally {            try {                if (admin != null){                    admin.close();                }                if (connection != null &amp;&amp; !connection.isClosed()){                    connection.close();                }            } catch (IOException e) {                e.printStackTrace();            }        }    }    private void initReducerOutputConfig(Job job){        job.setReducerClass(CountDurationReducer.class);        job.setOutputKeyClass(ComDimension.class);        job.setOutputKeyClass(CountDurationValue.class);        job.setOutputFormatClass(MysqlOutputFormat.class);    }    public static void main(String[] args) {        try {            int status = ToolRunner.run(new CountDurationRunner(), args);            System.exit(status);        } catch (Exception e) {            e.printStackTrace();        }    }}</code></pre><p>4）创建类：<strong>MysqlOutputFormat</strong></p><pre><code>package outputformat;import converter.DimensionConverterImpl;import kv.key.ComDimension;import kv.value.CountDurationValue;import org.apache.hadoop.fs.Path;import org.apache.hadoop.mapreduce.*;import org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import utils.JDBCInstance;import utils.JDBCUtils;import java.io.IOException;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.SQLException;public class MysqlOutputFormat extends OutputFormat&lt;ComDimension, CountDurationValue&gt; {    private OutputCommitter committer = null;    @Override    public RecordWriter&lt;ComDimension, CountDurationValue&gt; getRecordWriter(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {        //初始化JDBC连接对象        Connection conn = null;        conn = JDBCInstance.getInstance();        try {            //出问题的点之一，报空指针            conn.setAutoCommit(false);        } catch (SQLException e) {            throw new RuntimeException(e.getMessage());        }        return new MysqlRecordWriter(conn);    }    //输出校验    @Override    public void checkOutputSpecs(JobContext jobContext) throws InterruptedException {    }    @Override    public OutputCommitter getOutputCommitter(TaskAttemptContext context) throws IOException {        //此方法点击OutputFormat（按Ctrl + H的源码，复制getOutputCommitter，在FileOutputFormat）        if (committer == null){            String name = context.getConfiguration().get(FileOutputFormat.OUTDIR);            Path outputPath = name==null ? null:new Path(name);            committer = new FileOutputCommitter(outputPath, context);        }        return committer;    }    private static class MysqlRecordWriter extends RecordWriter&lt;ComDimension, CountDurationValue&gt;{        private DimensionConverterImpl dci = new DimensionConverterImpl();        private Connection conn = null;        private PreparedStatement preparedStatement = null;        private String insertSQL =null;        private int count = 0;        private final int BATCH_SIZE = 500;//批次大小        public MysqlRecordWriter(Connection conn){            this.conn = conn;        }        /**         * 输出到mysql         * @param key         * @param value         * @throws IOException         * @throws InterruptedException         */        @Override        public void write(ComDimension key, CountDurationValue value) throws IOException, InterruptedException {            try{                //tb_call                //id_date_contact, id_date_dimension, id_cantact, call_sum, call_duration_sum                //year month day                int idDateDimension = dci.getDimensionID(key.getDateDimension());                //telephone name                int idContactDimension = dci.getDimensionID(key.getContactDimension());                String idDateContact = idDateDimension + &quot;_&quot; + idContactDimension;                int callSum = Integer.valueOf(value.getCallSum());                int callDurationSum = Integer.valueOf(value.getCallDurationSum());                if (insertSQL == null){                    insertSQL = &quot;INSERT INTO `tb_call` (`id_date_contact`, `id_date_dimension`, `id_contact`,  `call_sum`, `call_duration_sum`) values (?,?,?,?,?) ON DUPLICATE KEY UPDATE `id_date_contact` = ?;&quot;;                }                if (preparedStatement == null){                    preparedStatement = conn.prepareStatement(insertSQL);                }                //本次SQL                int i = 0;                preparedStatement.setString(++i, idDateContact);                preparedStatement.setInt(++i, idDateDimension);                preparedStatement.setInt(++i, idContactDimension);                preparedStatement.setInt(++i, callSum);                preparedStatement.setInt(++i, callDurationSum);                //无则插入，有则更新的判断依据，增加批次                preparedStatement.setString(++i, idDateContact);                preparedStatement.addBatch();                //当前缓存了多少个sql语句等待批量执行，计数器                count++;                if (count &gt;= BATCH_SIZE){                    preparedStatement.executeBatch();//执行批处理命令                    conn.commit();                    count = 0;                    preparedStatement.clearBatch();//清除批处理命令                }            }catch (Exception e){                e.printStackTrace();            }        }        @Override        public void close(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {            try {                if (preparedStatement != null){                    preparedStatement.executeBatch();                        this.conn.commit();                }            } catch (SQLException e) {                e.printStackTrace();            }finally {                JDBCUtils.close(conn, preparedStatement, null);            }        }    }}</code></pre><p>5）创建类：<strong>BaseDimension</strong></p><pre><code>package kv.base;import org.apache.hadoop.io.WritableComparable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;public abstract class BaseDimension implements WritableComparable&lt;BaseDimension&gt; {    public abstract int compareTo(BaseDimension o);    //将字节写入二进制流    public abstract void write(DataOutput out) throws IOException;    //从二进制流读取字节    public abstract void readFields(DataInput in) throws IOException;}</code></pre><p>6）创建类：<strong>BaseValue</strong></p><pre><code>package kv.base;import org.apache.hadoop.io.Writable;public abstract class BaseValue implements Writable {}</code></pre><p>7）创建类：<strong>ComDimension</strong></p><pre><code>package kv.key;import kv.base.BaseDimension;import lombok.AllArgsConstructor;import lombok.Getter;import lombok.NoArgsConstructor;import lombok.Setter;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;@Getter@Setter@NoArgsConstructor@AllArgsConstructorpublic class ComDimension extends BaseDimension {    //联系人维度    private ContactDimension contactDimension = new ContactDimension();    //时间维度    private DateDimension dateDimension = new DateDimension();    @Override    public int compareTo(BaseDimension o) {        ComDimension o1 = (ComDimension) o;        int result = this.dateDimension.compareTo(o1.dateDimension);        if (result != 0) {            return result;        }        result = this.contactDimension.compareTo(o1.contactDimension);        return result;    }    @Override    public void write(DataOutput out) throws IOException {        contactDimension.write(out);        dateDimension.write(out);    }    @Override    public void readFields(DataInput in) throws IOException {        this.contactDimension.readFields(in);        this.dateDimension.readFields(in);    }}</code></pre><p>8）创建类：<strong>ContactDimension</strong></p><pre><code>package kv.key;import kv.base.BaseDimension;import lombok.AllArgsConstructor;import lombok.Getter;import lombok.NoArgsConstructor;import lombok.Setter;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;/** * 联系人维度类 */@Getter@Setter@NoArgsConstructor@AllArgsConstructorpublic class ContactDimension extends BaseDimension {    //手机号码    private String telephone;    //姓名    private String name;    @Override    public int compareTo(BaseDimension o) {        ContactDimension o1 = (ContactDimension) o;        int result = this.name.compareTo(o1.name);        if (result != 0){            return result;        }        result = this.telephone.compareTo(o1.telephone);        return result;    }    //将字节写入二进制流    @Override    public void write(DataOutput out) throws IOException {        out.writeUTF(this.telephone);        out.writeUTF(this.name);    }    //从二进制流读取字节    // Alt + Enter    @Override    public void readFields(DataInput in) throws IOException {        this.telephone = in.readUTF();        this.name = in.readUTF();    }}</code></pre><p>9）创建类：<strong>DateDimension</strong></p><pre><code>package kv.key;import kv.base.BaseDimension;import lombok.AllArgsConstructor;import lombok.Getter;import lombok.NoArgsConstructor;import lombok.Setter;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;/** * 时间维度类 */@Getter@Setter@NoArgsConstructor@AllArgsConstructorpublic class DateDimension extends BaseDimension {    //时间维度：当前通话信息所在年    private String year;    //时间维度：当前通话信息所在月,如果按照年来统计信息，则month为-1    private String month;    //时间维度：当前通话信息所在日,如果按照年来统计信息，则day为-1。    private String day;    @Override    public int compareTo(BaseDimension o) {        DateDimension o1 = (DateDimension) o;        int result = this.year.compareTo(o1.year);        if (result != 0){            return result;        }        result = this.month.compareTo(o1.month);        if (result != 0){           return result;        }        result = this.day.compareTo(o1.day);        return result;    }    @Override    public void write(DataOutput out) throws IOException {        out.writeUTF(this.year);        out.writeUTF(this.month);        out.writeUTF(this.day);    }    @Override    public void readFields(DataInput in) throws IOException {        this.year = in.readUTF();        this.month = in.readUTF();        this.day = in.readUTF();    }}</code></pre><p>10）创建类：<strong>CountDurationValue</strong></p><pre><code>package kv.value;import kv.base.BaseValue;import lombok.AllArgsConstructor;import lombok.Getter;import lombok.NoArgsConstructor;import lombok.Setter;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;@Setter@Getter@AllArgsConstructor@NoArgsConstructorpublic class CountDurationValue extends BaseValue {    //某个维度通话次数总和    private String callSum;    //某个维度通话时间总和    private String callDurationSum;    @Override    public void write(DataOutput dataOutput) throws IOException {        dataOutput.writeUTF(this.callSum);        dataOutput.writeUTF(this.callDurationSum);    }    @Override    public void readFields(DataInput dataInput) throws IOException {        this.callSum = dataInput.readUTF();        this.callDurationSum = dataInput.readUTF();    }}</code></pre><p>11） 创建类：<strong>JDBCUtil</strong></p><pre><code>package utils;import java.sql.*;public class JDBCUtils {    private static final String MYSQL_DRIVER_CLASS = &quot;com.mysql.cj.jdbc.Driver&quot;;    private static final String MYSQL_URL = &quot;jdbc:mysql://hsiehchou121:3306/db_telecom?useUnicode=true&amp;characterEncoding=UTF-8&quot;;    private static final String MYSQL_USERNAME = &quot;root&quot;;    private static final String MYSQL_PASSWORD = &quot;root&quot;;    /**     * 实例化JDBC连接器     * @return     */    public static Connection getConnection(){        try {            Class.forName(MYSQL_DRIVER_CLASS);            return DriverManager.getConnection(MYSQL_URL, MYSQL_USERNAME, MYSQL_PASSWORD);        } catch (ClassNotFoundException e) {            e.printStackTrace();        } catch (SQLException e) {            e.printStackTrace();        }        return null;    }    public static void close(Connection connection, Statement statement, ResultSet resultSet){        try {            if (resultSet != null &amp;&amp; !resultSet.isClosed()){                resultSet.close();            }            if (statement != null &amp;&amp; !statement.isClosed()){                statement.close();            }            if (connection != null &amp;&amp; !connection.isClosed()){                connection.close();            }        } catch (SQLException e) {            e.printStackTrace();        }    }}</code></pre><p>12）创建类：<strong>JDBCInstance</strong></p><pre><code>package utils;import java.sql.Connection;import java.sql.SQLException;/** * 获取链接实例 */public class JDBCInstance {    private static Connection connection = null;    public JDBCInstance() {    }    public static Connection getInstance(){        try {            if (connection == null || connection.isClosed() || !connection.isValid(3)){                connection = JDBCUtils.getConnection();            }        } catch (SQLException e) {            e.printStackTrace();        }        return connection;    }}</code></pre><p>13） 创建接口：<strong>DimensionConverter</strong></p><pre><code>package converter;import kv.base.BaseDimension;public interface DimensionConverter {    int getDimensionID(BaseDimension dimension);}</code></pre><p>14）创建类：<strong>DimensionConverterImpl</strong></p><pre><code>package converter;import kv.base.BaseDimension;import kv.key.ContactDimension;import kv.key.DateDimension;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import utils.JDBCInstance;import utils.JDBCUtils;import utils.LRUCache;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.ResultSet;import java.sql.SQLException;/** * 1、根据传入的维度数据，得到该数据对应的在表中的主键id * ** 做内存缓存，LRUCache * 分支 * -- 缓存中有数据 -&gt; 直接返回id * -- 缓存中无数据 -&gt; * ** 查询Mysql * 分支： * -- Mysql中有该条数据 -&gt; 直接返回id -&gt; 将本次读取到的id缓存到内存中 * -- Mysql中没有该数据  -&gt; 插入该条数据 -&gt; 再次反查该数据，得到id并返回 -&gt; 缓存到内存中 */public class DimensionConverterImpl implements DimensionConverter {    // Logger 打印该类的日志，取代resources里的log4j.properties    private static final Logger logger = LoggerFactory.getLogger(DimensionConverterImpl.class);    //对象线程化，用于每个线程管理自己的JDBC连接器    private ThreadLocal&lt;Connection&gt; threadLocalConnection = new ThreadLocal&lt;&gt;();    //构建内存缓存对象    private LRUCache lruCache = new LRUCache(3000);    public DimensionConverterImpl() {        //jvm关闭时，释放资源        Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; JDBCUtils.close(threadLocalConnection.get(), null, null)));    }    @Override    public int getDimensionID(BaseDimension dimension) {        //1、根据传入的维度对象获取对应的主键id，先从LRUCache中获取        //时间维度：date_dimension_year_month_day, 10        //联系人维度：contact_dimension_telephone_name(到了电话号码就不会重复了), 12        String cacheKey = getCacheKey(dimension);        //尝试获取缓存的id        if (lruCache.containsKey(cacheKey)) {            return lruCache.get(cacheKey);        }        //没有得到缓存id，需要执行select操作        //sqls包含了1组sql语句：查询和插入        String[] sqls = null;        if (dimension instanceof DateDimension) {            sqls = getDateDimensionSQL();        } else if (dimension instanceof ContactDimension) {            sqls = getContactDimensionSQL();        } else {            throw new RuntimeException(&quot;没有匹配到对应维度信息.&quot;);        }        //准备对Mysql表进行操作，先查询，有可能再插入        Connection conn = this.getConnection();        int id = -1;        synchronized (this) {            id = execSQL(conn, sqls, dimension);        }        //将刚查询的id加入到缓存中        lruCache.put(cacheKey, id);        return id;    }    /**     * 得到当前线程维护的Connection对象     *     * @return     */    public Connection getConnection() {        Connection conn = null;        try {            conn = threadLocalConnection.get();            if (conn == null || conn.isClosed()) {                conn = JDBCInstance.getInstance();                threadLocalConnection.set(conn);            }        } catch (SQLException e) {            e.printStackTrace();        }        return conn;    }    /**     * @param conn      JDBC连接器     * @param sqls      长度为2，第一个位置为查询语句，第二个位置为插入语句     * @param dimension 对应维度所保存的数据     * @return     */    private int execSQL(Connection conn, String[] sqls, BaseDimension dimension) {        PreparedStatement preparedStatement = null;        ResultSet resultSet = null;        try {            //1            //查询的preparedStatement            preparedStatement = conn.prepareStatement(sqls[0]);            //根据不同的维度，封装不同的SQL语句            setArguments(preparedStatement, dimension);            //执行查询            resultSet = preparedStatement.executeQuery();            if (resultSet.next()) {                int result = resultSet.getInt(1);                //释放资源                JDBCUtils.close(null, preparedStatement, resultSet);                return result;            }            //释放资源            JDBCUtils.close(null, preparedStatement, resultSet);            //2            //执行插入，封装插入的sql语句            preparedStatement = conn.prepareStatement(sqls[1]);            setArguments(preparedStatement, dimension);            //执行插入            preparedStatement.executeUpdate();            //释放资源            JDBCUtils.close(null, preparedStatement, null);            //3            //查询的preparedStatement            preparedStatement = conn.prepareStatement(sqls[0]);            //根据不同的维度，封装不同的SQL语句            setArguments(preparedStatement, dimension);            //执行查询            resultSet = preparedStatement.executeQuery();            if (resultSet.next()) {                return resultSet.getInt(1);            }        } catch (SQLException e) {            e.printStackTrace();        } finally {            //释放资源            JDBCUtils.close(null, preparedStatement, resultSet);        }        return -1;    }    /**     * 设置SQL语句的具体参数     *     * @param preparedStatement     * @param dimension     */    private void setArguments(PreparedStatement preparedStatement, BaseDimension dimension) {        int i = 0;        try {            if (dimension instanceof DateDimension) {                //可以优化                DateDimension dateDimension = (DateDimension) dimension;                preparedStatement.setString(++i, dateDimension.getYear());                preparedStatement.setString(++i, dateDimension.getMonth());                preparedStatement.setString(++i, dateDimension.getDay());            } else if (dimension instanceof ContactDimension) {                ContactDimension contactDimension = (ContactDimension) dimension;                preparedStatement.setString(++i, contactDimension.getTelephone());                preparedStatement.setString(++i, contactDimension.getName());            }        } catch (SQLException e) {            e.printStackTrace();        }    }    /**     * 返回联系人表的查询和插入语句     *     * @return     */    private String[] getContactDimensionSQL() {        String query = &quot;SELECT `id` FROM `tb_contacts` WHERE `telephone` = ? AND `name` = ? ORDER BY `id`;&quot;;        String insert = &quot;INSERT INTO `tb_contacts`(`telephone`, `name`) VALUES(?, ?);&quot;;        return new String[]{query, insert};    }    /**     * 返回时间表的查询和插入语句     *     * @return     */    private String[] getDateDimensionSQL() {        String query = &quot;SELECT `id` FROM `tb_dimension_date` WHERE `year` = ? AND `month` = ? AND `day` = ? ORDER BY `id`;&quot;;        String insert = &quot;INSERT INTO `tb_dimension_date`(`year`,`month`,`day`) VALUES(?, ?, ?);&quot;;        return new String[]{query, insert};    }    /**     * 根据维度信息得到维度对应的缓存键     *     * @param dimension     * @return     */    private String getCacheKey(BaseDimension dimension) {        StringBuilder sb = new StringBuilder();        if (dimension instanceof DateDimension) {            DateDimension dateDimension = (DateDimension) dimension;            sb.append(&quot;date_dimension&quot;)                    .append(dateDimension.getYear())                    .append(dateDimension.getMonth())                    .append(dateDimension.getDay());        } else if (dimension instanceof ContactDimension) {            ContactDimension contactDimension = (ContactDimension) dimension;            sb.append(&quot;contact_dimension&quot;)                    .append(contactDimension.getTelephone());        }        return sb.toString();    }}</code></pre><p>15） 创建类：<strong>LRUCache</strong></p><pre><code>package utils;import java.util.LinkedHashMap;import java.util.Map;public class LRUCache extends LinkedHashMap&lt;String, Integer&gt; {    private static  final long serialVersionUID = 1L;    protected int maxElements;    public LRUCache(int maxSize){        super(maxSize, 0.75F, true);        this.maxElements = maxSize;    }    @Override    protected boolean removeEldestEntry(Map.Entry&lt;String, Integer&gt; eldest) {        return (size() &gt; this.maxElements);    }}</code></pre><h4 id="5、运行测试"><a href="#5、运行测试" class="headerlink" title="5、运行测试"></a>5、运行测试</h4><p>1）将mysql驱动包放入到<strong>/opt/jars的lib目录下</strong><br><strong>mysql-connector-java-8.0.13.jar</strong> </p><p>2）<strong>提交任务</strong></p><pre><code>$ /root/hd/hadoop-2.8.4/bin/yarn jar /opt/jars/CT_analysis-1.0-SNAPSHOT.jar runner.CountDurationRunner -libjars /opt/jars/lib/mysql-connector-java-8.0.13.jar</code></pre><p>观察Mysql中的结果</p><h3 id="七、数据展示"><a href="#七、数据展示" class="headerlink" title="七、数据展示"></a>七、数据展示</h3><p>令人兴奋的时刻马上到了，接下来我们需要将某人按照不同维度查询出来的结果，展示到web页面上<br><strong>数据展示模块流程图</strong></p><p><img src="/medias/%E6%95%B0%E6%8D%AE%E5%B1%95%E7%A4%BA%E6%A8%A1%E5%9D%97%E6%B5%81%E7%A8%8B%E5%9B%BE.PNG" alt="数据展示模块流程图"></p><h4 id="1、环境准备"><a href="#1、环境准备" class="headerlink" title="1、环境准备"></a>1、环境准备</h4><p>1）新建module或项目：<strong>CT_web</strong><br>pom.xml配置文件：</p><pre><code>&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;  &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;  &lt;artifactId&gt;CT_web&lt;/artifactId&gt;  &lt;packaging&gt;war&lt;/packaging&gt;  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;  &lt;name&gt;ct_web Maven Webapp&lt;/name&gt;  &lt;url&gt;http://maven.apache.org&lt;/url&gt;  &lt;dependencies&gt;    &lt;!-- https://mvnrepository.com/artifact/junit/junit --&gt;    &lt;dependency&gt;      &lt;groupId&gt;junit&lt;/groupId&gt;      &lt;artifactId&gt;junit&lt;/artifactId&gt;      &lt;version&gt;4.12&lt;/version&gt;      &lt;scope&gt;test&lt;/scope&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;mysql&lt;/groupId&gt;      &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;      &lt;version&gt;8.0.13&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;c3p0&lt;/groupId&gt;      &lt;artifactId&gt;c3p0&lt;/artifactId&gt;      &lt;version&gt;0.9.1.2&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;org.mybatis&lt;/groupId&gt;      &lt;artifactId&gt;mybatis&lt;/artifactId&gt;      &lt;version&gt;3.2.1&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;org.springframework&lt;/groupId&gt;      &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt;      &lt;version&gt;4.3.3.RELEASE&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;org.springframework&lt;/groupId&gt;      &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt;      &lt;version&gt;4.3.3.RELEASE&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;org.springframework&lt;/groupId&gt;      &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt;      &lt;version&gt;4.3.3.RELEASE&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;org.mybatis&lt;/groupId&gt;      &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt;      &lt;version&gt;1.3.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;org.aspectj&lt;/groupId&gt;      &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt;      &lt;version&gt;1.8.10&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;javax.servlet&lt;/groupId&gt;      &lt;artifactId&gt;servlet-api&lt;/artifactId&gt;      &lt;version&gt;2.5&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;javax.servlet&lt;/groupId&gt;      &lt;artifactId&gt;jstl&lt;/artifactId&gt;      &lt;version&gt;1.2&lt;/version&gt;    &lt;/dependency&gt;      &lt;dependency&gt;          &lt;groupId&gt;org.springframework&lt;/groupId&gt;          &lt;artifactId&gt;spring-beans&lt;/artifactId&gt;          &lt;version&gt;4.3.3.RELEASE&lt;/version&gt;      &lt;/dependency&gt;  &lt;/dependencies&gt;  &lt;build&gt;    &lt;finalName&gt;ct_web&lt;/finalName&gt;    &lt;plugins&gt;      &lt;plugin&gt;        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;        &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;        &lt;version&gt;2.12.4&lt;/version&gt;        &lt;configuration&gt;          &lt;skipTests&gt;true&lt;/skipTests&gt;        &lt;/configuration&gt;      &lt;/plugin&gt;      &lt;plugin&gt;        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;        &lt;version&gt;3.8.1&lt;/version&gt;        &lt;configuration&gt;          &lt;source&gt;1.8&lt;/source&gt;          &lt;target&gt;1.8&lt;/target&gt;        &lt;/configuration&gt;      &lt;/plugin&gt;    &lt;/plugins&gt;  &lt;/build&gt;&lt;/project&gt;</code></pre><p>2）创建包结构，根包：com.hsiehchou<br>bean<br>controller<br>dao<br>3）类表</p><table><thead><tr><th align="center">类名</th><th align="center">备注</th></tr></thead><tbody><tr><td align="center">CallLog</td><td align="center">用于封装数据分析结果的JavaBean</td></tr><tr><td align="center">QueryInfo</td><td align="center">用于封装向服务器发来的请求参数</td></tr><tr><td align="center">CallLogHandler</td><td align="center">用于处理请求的Controller</td></tr><tr><td align="center">CallLogDAO</td><td align="center">查询某人某个维度通话记录的DAO</td></tr></tbody></table><p>4）web目录结构，web部分的根目录：webapp</p><table><thead><tr><th align="center">文件夹名</th><th align="center">备注</th></tr></thead><tbody><tr><td align="center">css</td><td align="center">存放css静态资源的文件夹</td></tr><tr><td align="center">html</td><td align="center">存放html静态资源的文件夹</td></tr><tr><td align="center">images</td><td align="center">存放图片静态资源文件夹</td></tr><tr><td align="center">js</td><td align="center">存放js静态资源的文件夹</td></tr><tr><td align="center">jsp</td><td align="center">存放jsp页面的文件夹</td></tr><tr><td align="center">WEB-INF</td><td align="center">存放web相关配置的文件夹</td></tr></tbody></table><p>5） resources目录下创建spring相关配置文件</p><p><strong>dbconfig.properties</strong>：用于存放数据库连接配置</p><pre><code>jdbc.user=rootjdbc.password=rootjdbc.jdbcUrl=jdbc:mysql://hsiehchou121:3306/db_telecom?useUnicode=true&amp;characterEncoding=UTF-8jdbc.driverClass=com.mysql.cj.jdbc.Driver</code></pre><p><strong>applicationContext.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;       xmlns:p=&quot;http://www.springframework.org/schema/p&quot;       xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot;       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd        http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd        http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd        http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd        http://www.springframework.org/schema/jee http://www.springframework.org/schema/jee/spring-jee.xsd        http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd&quot;&gt;    &lt;context:property-placeholder location=&quot;classpath:dbconfig.properties&quot;/&gt;    &lt;bean id=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt;        &lt;property name=&quot;user&quot; value=&quot;${jdbc.user}&quot;/&gt;        &lt;property name=&quot;password&quot; value=&quot;${jdbc.password}&quot;/&gt;        &lt;property name=&quot;driverClass&quot; value=&quot;${jdbc.driverClass}&quot;/&gt;        &lt;property name=&quot;jdbcUrl&quot; value=&quot;${jdbc.jdbcUrl}&quot;/&gt;    &lt;/bean&gt;    &lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt;        &lt;constructor-arg name=&quot;dataSource&quot; value=&quot;#{dataSource}&quot;&gt;&lt;/constructor-arg&gt;    &lt;/bean&gt;    &lt;bean id=&quot;namedParameterJdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate&quot;&gt;        &lt;constructor-arg name=&quot;dataSource&quot; value=&quot;#{dataSource}&quot;&gt;&lt;/constructor-arg&gt;    &lt;/bean&gt;    &lt;!-- 包扫描 --&gt;    &lt;context:component-scan base-package=&quot;controller&quot;&gt;&lt;/context:component-scan&gt;    &lt;context:component-scan base-package=&quot;dao&quot;&gt;&lt;/context:component-scan&gt;    &lt;!-- 配置视图解析器--&gt;    &lt;bean id=&quot;viewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt;        &lt;property name=&quot;prefix&quot; value=&quot;/&quot;/&gt;        &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt;    &lt;/bean&gt;    &lt;!--&lt;mvc:annotation-driven /&gt;--&gt;    &lt;!--&lt;mvc:default-servlet-handler /&gt;--&gt;    &lt;!--&lt;mvc:resources location=&quot;/images/&quot; mapping=&quot;/images/**&quot;/&gt;--&gt;    &lt;!--&lt;mvc:resources location=&quot;/js/&quot; mapping=&quot;/js/**&quot;/&gt;--&gt;    &lt;!--&lt;mvc:resources location=&quot;/css/&quot; mapping=&quot;/css/**&quot;/&gt;--&gt;&lt;/beans&gt;</code></pre><p>6） webapp的WEB-INF目录下创建web相关配置<br><strong>web.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd&quot;         version=&quot;3.1&quot;&gt;    &lt;display-name&gt;SpringMVC_CRUD&lt;/display-name&gt;    &lt;!-- spring拦截器 --&gt;    &lt;servlet&gt;        &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt;        &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt;        &lt;init-param&gt;            &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;            &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt;        &lt;/init-param&gt;        &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;    &lt;/servlet&gt;    &lt;servlet-mapping&gt;        &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt;        &lt;url-pattern&gt;/&lt;/url-pattern&gt;    &lt;/servlet-mapping&gt;    &lt;welcome-file-list&gt;        &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt;    &lt;/welcome-file-list&gt;    &lt;filter&gt;        &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt;        &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt;        &lt;init-param&gt;            &lt;param-name&gt;encoding&lt;/param-name&gt;            &lt;param-value&gt;utf-8&lt;/param-value&gt;        &lt;/init-param&gt;        &lt;init-param&gt;            &lt;param-name&gt;forceEncoding&lt;/param-name&gt;            &lt;param-value&gt;true&lt;/param-value&gt;        &lt;/init-param&gt;    &lt;/filter&gt;    &lt;filter-mapping&gt;        &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt;        &lt;url-pattern&gt;/*&lt;/url-pattern&gt;    &lt;/filter-mapping&gt;    &lt;servlet-mapping&gt;        &lt;servlet-name&gt;default&lt;/servlet-name&gt;        &lt;url-pattern&gt;*.jpg&lt;/url-pattern&gt;    &lt;/servlet-mapping&gt;    &lt;servlet-mapping&gt;        &lt;servlet-name&gt;default&lt;/servlet-name&gt;        &lt;url-pattern&gt;*.js&lt;/url-pattern&gt;    &lt;/servlet-mapping&gt;    &lt;servlet-mapping&gt;        &lt;servlet-name&gt;default&lt;/servlet-name&gt;        &lt;url-pattern&gt;*.css&lt;/url-pattern&gt;    &lt;/servlet-mapping&gt;&lt;/web-app&gt;</code></pre><p>7）拷贝js框架到webapp的js目录下<br>框架名称：<br>echarts.min.js</p><h4 id="2、编写代码-1"><a href="#2、编写代码-1" class="headerlink" title="2、编写代码"></a>2、编写代码</h4><p>思路：<br>a）首先测试数据通顺以及完整性，写一个联系人的测试用例。<br>b）测试通过后，通过输入手机号码以及时间参数，查询指定维度的数据，并以图表展示。<br>代码：<br>1）新建类： <strong>CallLog</strong></p><pre><code>package bean;/** * 用于存放返回给用户的数据 */public class CallLog {    private String id_date_contact;    private String id_date_dimension;    private String id_contact;    private String call_sum;    private String call_duration_sum;    private String telephone;    private String name;    private String year;    private String month;    private String day;    public String getId_date_contact() {        return id_date_contact;    }    public void setId_date_contact(String id_date_contact) {        this.id_date_contact = id_date_contact;    }    public String getId_date_dimension() {        return id_date_dimension;    }    public void setId_date_dimension(String id_date_dimension) {        this.id_date_dimension = id_date_dimension;    }    public String getId_contact() {        return id_contact;    }    public void setId_contact(String id_contact) {        this.id_contact = id_contact;    }    public String getCall_sum() {        return call_sum;    }    public void setCall_sum(String call_sum) {        this.call_sum = call_sum;    }    public String getCall_duration_sum() {        return call_duration_sum;    }    public void setCall_duration_sum(String call_duration_sum) {        this.call_duration_sum = call_duration_sum;    }    public String getTelephone() {        return telephone;    }    public void setTelephone(String telephone) {        this.telephone = telephone;    }    public String getName() {        return name;    }    public void setName(String name) {        this.name = name;    }    public String getYear() {        return year;    }    public void setYear(String year) {        this.year = year;    }    public String getMonth() {        return month;    }    public void setMonth(String month) {        this.month = month;    }    public String getDay() {        return day;    }    public void setDay(String day) {        this.day = day;    }    @Override    public String toString() {        return &quot;CallLog{&quot; +                &quot;call_sum=&#39;&quot; + call_sum + &#39;\&#39;&#39; +                &quot;, call_duration_sum=&#39;&quot; + call_duration_sum + &#39;\&#39;&#39; +                &quot;, telephone=&#39;&quot; + telephone + &#39;\&#39;&#39; +                &quot;, name=&#39;&quot; + name + &#39;\&#39;&#39; +                &quot;, year=&#39;&quot; + year + &#39;\&#39;&#39; +                &quot;, month=&#39;&quot; + month + &#39;\&#39;&#39; +                &quot;, day=&#39;&quot; + day + &#39;\&#39;&#39; +                &#39;}&#39;;    }}</code></pre><p>2）新建类：<strong>QueryInfo</strong></p><pre><code>package bean;/** * 该类用于存放用户请求的数据 */public class QueryInfo {    private String telephone;    private String year;    private String month;    private String day;    public QueryInfo() {        super();    }    public QueryInfo(String telephone, String year, String month, String day) {        super();        this.telephone = telephone;        this.year = year;        this.month = month;        this.day = day;    }    public String getTelephone() {        return telephone;    }    public void setTelephone(String telephone) {        this.telephone = telephone;    }    public String getYear() {        return year;    }    public void setYear(String year) {        this.year = year;    }    public String getMonth() {        return month;    }    public void setMonth(String month) {        this.month = month;    }    public String getDay() {        return day;    }    public void setDay(String day) {        this.day = day;    }}</code></pre><p>3）新建类： <strong>CallLogDAO</strong></p><pre><code>package dao;import bean.CallLog;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.jdbc.core.BeanPropertyRowMapper;import org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate;import org.springframework.stereotype.Repository;import java.util.HashMap;import java.util.List;@Repositorypublic class CallLogDAO {    @Autowired    private NamedParameterJdbcTemplate namedParameterJdbcTemplate;    public List&lt;CallLog&gt; getCallLogList(HashMap&lt;String, String&gt; paramsMap) {        //按照年统计：统计某个用户，1年12个月的所有的数据（不精确到day）        String sql = &quot;SELECT `call_sum`, `call_duration_sum`, `telephone`, `name`, `year` , `month`, `day` FROM tb_dimension_date t4 INNER JOIN ( SELECT `id_date_dimension`, `call_sum`, `call_duration_sum`, `telephone`, `name` FROM tb_call t2 INNER JOIN ( SELECT `id`, `telephone`, `name` FROM tb_contacts WHERE telephone = :telephone ) t1 ON t2.id_contact = t1.id ) t3 ON t4.id = t3.id_date_dimension WHERE `year` = :year AND `month` != :month AND `day` = :day ORDER BY `year`, `month`;&quot;;        BeanPropertyRowMapper&lt;CallLog&gt; beanPropertyRowMapper = new BeanPropertyRowMapper&lt;&gt;(CallLog.class);        List&lt;CallLog&gt; list = namedParameterJdbcTemplate.query(sql, paramsMap, beanPropertyRowMapper);        return list;    }}</code></pre><p>4）新建类：<strong>CallLogHandler</strong></p><pre><code>package controller;import bean.CallLog;import bean.QueryInfo;import dao.CallLogDAO;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.RequestMapping;import java.util.HashMap;import java.util.List;@Controllerpublic class CallLogHandler {    @RequestMapping(&quot;/queryCallLogList&quot;)    public String queryCallLog(Model model, QueryInfo queryInfo){        ApplicationContext applicationContext = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;);        CallLogDAO callLogDAO = applicationContext.getBean(CallLogDAO.class);        HashMap&lt;String, String&gt; hashMap = new HashMap&lt;&gt;();        hashMap.put(&quot;telephone&quot;, queryInfo.getTelephone());        hashMap.put(&quot;year&quot;, queryInfo.getYear());        hashMap.put(&quot;month&quot;, queryInfo.getMonth());        hashMap.put(&quot;day&quot;, queryInfo.getDay());        List&lt;CallLog&gt; list = callLogDAO.getCallLogList(hashMap);        StringBuilder dateSB = new StringBuilder();        StringBuilder callSumSB = new StringBuilder();        StringBuilder callDurationSumSB = new StringBuilder();        for(int i = 0; i &lt; list.size(); i++){            CallLog callLog = list.get(i);            //1月, 2月, ....12月,            dateSB.append(callLog.getMonth() + &quot;月,&quot;);            callSumSB.append(callLog.getCall_sum() + &quot;,&quot;);            callDurationSumSB.append(callLog.getCall_duration_sum() + &quot;,&quot;);        }        dateSB.deleteCharAt(dateSB.length() - 1);        callSumSB.deleteCharAt(callSumSB.length() - 1);        callDurationSumSB.deleteCharAt(callDurationSumSB.length() - 1);        //通过model返回数据        model.addAttribute(&quot;telephone&quot;, list.get(0).getTelephone());        model.addAttribute(&quot;name&quot;, list.get(0).getName());        model.addAttribute(&quot;date&quot;, dateSB.toString());        model.addAttribute(&quot;count&quot;, callSumSB.toString());        model.addAttribute(&quot;duration&quot;, callDurationSumSB.toString());        return &quot;jsp/CallLogListEchart&quot;;    }}</code></pre><p>5）新建：<strong>index.jsp</strong></p><pre><code>&lt;%@ taglib prefix=&quot;c&quot; uri=&quot;http://java.sun.com/jsp/jstl/core&quot; %&gt;&lt;%@ page language=&quot;java&quot; contentType=&quot;text/html; charset=utf-8&quot;         pageEncoding=&quot;utf-8&quot; %&gt;&lt;%    String path = request.getContextPath();%&gt;&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.w3.org/TR/html4/loose.dtd&quot;&gt;&lt;html&gt;    &lt;head&gt;        &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;        &lt;title&gt;电信查询系统&lt;/title&gt;        &lt;link href=&quot;//cdn.bootcss.com/bootstrap/3.3.6/css/bootstrap.min.css&quot; rel=&quot;stylesheet&quot;&gt;        &lt;script src=&quot;//cdn.bootcss.com/jquery/2.1.1/jquery.min.js&quot;&gt;&lt;/script&gt;        &lt;script src=&quot;//cdn.bootcss.com/bootstrap/3.3.6/js/bootstrap.min.js&quot;&gt;&lt;/script&gt;    &lt;/head&gt;    &lt;body&gt;        &lt;div style=&quot;width:410px;margin: 0 auto;&quot;&gt;            &lt;h3&gt;电信查询用户通话次数和通话时长系统&lt;/h3&gt;            &lt;br /&gt;            &lt;form role=&quot;form&quot; action=&quot;/queryCallLogList&quot; method=&quot;post&quot;&gt;                &lt;div class=&quot;form-group&quot; style=&quot;margin-bottom: 0;&quot;&gt;                    &lt;label for=&quot;name&quot;&gt;手机号码&lt;/label&gt;                    &lt;input type=&quot;text&quot; name=&quot;telephone&quot; class=&quot;form-control&quot; id=&quot;name&quot; placeholder=&quot;请输入手机号码&quot;&gt;                &lt;/div&gt;                &lt;div class=&quot;form-group&quot; style=&quot;margin-bottom: 0;&quot;&gt;                    &lt;label for=&quot;name&quot;&gt;年&lt;/label&gt;                    &lt;input type=&quot;text&quot; name=&quot;year&quot; class=&quot;form-control&quot; id=&quot;name&quot; placeholder=&quot;请输入年&quot;&gt;                &lt;/div&gt;                &lt;div class=&quot;form-group&quot; style=&quot;margin-bottom: 0;&quot;&gt;                    &lt;label for=&quot;name&quot;&gt;月&lt;/label&gt;                    &lt;input type=&quot;text&quot; name=&quot;month&quot; class=&quot;form-control&quot; id=&quot;name&quot; placeholder=&quot;请输入月&quot;&gt;                &lt;/div&gt;                &lt;div class=&quot;form-group&quot; style=&quot;margin-bottom: 0;&quot;&gt;                    &lt;label for=&quot;name&quot;&gt;日&lt;/label&gt;                    &lt;input type=&quot;text&quot; name=&quot;day&quot; class=&quot;form-control&quot; id=&quot;name&quot; placeholder=&quot;请输入日&quot;&gt;                &lt;/div&gt;                &lt;button type=&quot;submit&quot; class=&quot;btn btn-default&quot;&gt;查询&lt;/button&gt;            &lt;/form&gt;        &lt;/div&gt;        &lt;br /&gt;        &lt;div style=&quot;width: 1000px;margin: 0 auto;&quot;&gt;            &lt;table class=&quot;table&quot;&gt;                &lt;h4&gt;数据库电话号码表&lt;/h4&gt;                &lt;thead&gt;                    &lt;tr&gt;                        &lt;th&gt;姓名&lt;/th&gt;                        &lt;th&gt;手机号&lt;/th&gt;                        &lt;th&gt;&lt;/th&gt;                        &lt;th&gt;姓名&lt;/th&gt;                        &lt;th&gt;手机号&lt;/th&gt;                        &lt;th&gt;&lt;/th&gt;                        &lt;th&gt;姓名&lt;/th&gt;                        &lt;th&gt;手机号&lt;/th&gt;                        &lt;th&gt;&lt;/th&gt;                        &lt;th&gt;姓名&lt;/th&gt;                        &lt;th&gt;手机号&lt;/th&gt;                        &lt;th&gt;&lt;/th&gt;                        &lt;th&gt;姓名&lt;/th&gt;                        &lt;th&gt;手机号&lt;/th&gt;                    &lt;/tr&gt;                &lt;/thead&gt;                &lt;tbody&gt;                    &lt;tr class=&quot;active&quot;&gt;                        &lt;td&gt;李为&lt;/td&gt;                        &lt;td&gt;17078388295&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;王军&lt;/td&gt;                        &lt;td&gt;13980337439&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;时俊&lt;/td&gt;                        &lt;td&gt;14575535933&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;天机&lt;/td&gt;                        &lt;td&gt;18902496992&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;蔡铭&lt;/td&gt;                        &lt;td&gt;18549641558&lt;/td&gt;                    &lt;/tr&gt;                    &lt;tr class=&quot;success&quot;&gt;                        &lt;td&gt;陶尚&lt;/td&gt;                        &lt;td&gt;17005930322&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;魏山帅&lt;/td&gt;                        &lt;td&gt;18468618874&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;华倩&lt;/td&gt;                        &lt;td&gt;18576581848&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;焦君山&lt;/td&gt;                        &lt;td&gt;15978226424&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;钟尾田&lt;/td&gt;                        &lt;td&gt;15542823911&lt;/td&gt;                    &lt;/tr&gt;                    &lt;tr  class=&quot;warning&quot;&gt;                        &lt;td&gt;司可可&lt;/td&gt;                        &lt;td&gt;17526304161&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;官渡&lt;/td&gt;                        &lt;td&gt;15422018558&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;上贵坡&lt;/td&gt;                        &lt;td&gt;17269452013&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;时光机&lt;/td&gt;                        &lt;td&gt;17764278604&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;李发&lt;/td&gt;                        &lt;td&gt;15711910344&lt;/td&gt;                    &lt;/tr&gt;                    &lt;tr  class=&quot;danger&quot;&gt;                        &lt;td&gt;蒂冈&lt;/td&gt;                        &lt;td&gt;15714728273&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;范德&lt;/td&gt;                        &lt;td&gt;16061028454&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;周朝王&lt;/td&gt;                        &lt;td&gt;16264433631&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;谢都都&lt;/td&gt;                        &lt;td&gt;17601615878&lt;/td&gt;                        &lt;td&gt;&lt;/td&gt;                        &lt;td&gt;刘何思&lt;/td&gt;                        &lt;td&gt;15897468949&lt;/td&gt;                    &lt;/tr&gt;                &lt;/tbody&gt;            &lt;/table&gt;        &lt;/div&gt;    &lt;/body&gt;&lt;/html&gt;</code></pre><p>6）新建：<strong>CallLogListEchart.jsp</strong></p><pre><code>&lt;%--  Created by IntelliJ IDEA.  User: Z  Date: 2017/10/28  Time: 14:36  To change this template use File | Settings | File Templates.--%&gt;&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; isELIgnored=&quot;false&quot; %&gt;&lt;html&gt;&lt;head&gt;    &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;    &lt;title&gt;显示通话记录&lt;/title&gt;    &lt;script type=&quot;text/javascript&quot; src=&quot;../js/echarts.min.js&quot;&gt;&lt;/script&gt;    &lt;%--&lt;script type=&quot;text/javascript&quot; src=&quot;${pageContext.request.contextPath}/js/echarts.min.js&quot;&gt;&lt;/script&gt;--%&gt;    &lt;%--&lt;script type=&quot;text/javascript&quot; src=&quot;${pageContext.request.contextPath}/jquery-3.2.0.min.js&quot;&gt;&lt;/script&gt;--%&gt;    &lt;%--&lt;script type=&quot;text/javascript&quot; src=&quot;http://echarts.baidu.com/gallery/vendors/echarts/echarts-all-3.js&quot;&gt;&lt;/script&gt;--%&gt;&lt;/head&gt;&lt;body style=&quot;height: 100%; margin: 0; background-color: #3C3F41&quot;&gt;&lt;style type=&quot;text/css&quot;&gt;    h3 {        font-size: 12px;        color: #ffffff;        display: inline    }&lt;/style&gt;&lt;h4 style=&quot;color: #ffffff;text-align:center&quot;&gt;通话月单查询：${requestScope.name}&lt;/h4&gt;&lt;%--&lt;h3 style=&quot;margin-left: 70%&quot;&gt;通话次数&lt;/h3&gt;--%&gt;&lt;%--&lt;h3 style=&quot;margin-left: 20%&quot;&gt;通话时长&lt;/h3&gt;--%&gt;&lt;div id=&quot;container1&quot; style=&quot;height: 80%; width: 50%; float:left&quot;&gt;&lt;/div&gt;&lt;div id=&quot;container2&quot; style=&quot;height: 80%; width: 50%; float:right&quot;&gt;&lt;/div&gt;&lt;script type=&quot;text/javascript&quot;&gt;    var telephone = &quot;${requestScope.telephone}&quot;    var name = &quot;${requestScope.name}&quot;    var date = &quot;${requestScope.date}&quot;//1月,2月,3月,xxxxx    var count = &quot;${requestScope.count}&quot;    var duration = &quot;${requestScope.duration}&quot;    var pieData = converterFun(duration.split(&quot;,&quot;), date.split(&quot;,&quot;))    callog1();    callog2();    function converterFun(duration, date) {        var array = [];        for (var i = 0; i &lt; duration.length; i++) {            var map = {};            map[&#39;value&#39;] = parseFloat(duration[i]);            map[&#39;name&#39;] = date[i];            array.push(map);        }        return array;    }    function callog1() {        var dom = document.getElementById(&quot;container1&quot;);        var myChart = echarts.init(dom);        myChart.showLoading();        var option = {            title: {                text: &#39;通话次数&#39;,                textStyle: {                    //文字颜色                    color: &#39;#ffffff&#39;,                    //字体风格,&#39;normal&#39;,&#39;italic&#39;,&#39;oblique&#39;                    fontStyle: &#39;normal&#39;,                    //字体粗细 &#39;normal&#39;,&#39;bold&#39;,&#39;bolder&#39;,&#39;lighter&#39;,100 | 200 | 300 | 400...                    fontWeight: &#39;bold&#39;,                    //字体系列                    fontFamily: &#39;sans-serif&#39;,                    //字体大小                    fontSize: 13                },                itemGap: 12,            },            grid: {                x: 80,                y: 60,                x2: 80,                y2: 60,                backgroundColor: &#39;rgba(0,0,0,0)&#39;,                borderWidth: 1,                borderColor: &#39;#ffffff&#39;            },            tooltip: {                trigger: &#39;axis&#39;            },            legend: {                borderColor: &#39;#ffffff&#39;,                itemGap: 10,                data: [&#39;通话次数&#39;],                textStyle: {                    color: &#39;#ffffff&#39;// 图例文字颜色                }            },            toolbox: {                show: false,                feature: {                    dataZoom: {                        yAxisIndex: &#39;none&#39;                    },                    dataView: {readOnly: false},                    magicType: {type: [&#39;line&#39;, &#39;bar&#39;]},                    restore: {},                    saveAsImage: {}                }            },            xAxis: {                data: date.split(&quot;,&quot;),                axisLine: {                    lineStyle: {                        color: &#39;#ffffff&#39;,                        width: 2                    }                }            },            yAxis: {                axisLine: {                    lineStyle: {                        color: &#39;#ffffff&#39;,                        width: 2                    }                }            },            series: [                {                    type: &#39;line&#39;,                    data: count.split(&quot;,&quot;),                    itemStyle: {                        normal: {                            color: &#39;#ffca29&#39;,                            lineStyle: {                                color: &#39;#ffd80d&#39;,                                width: 2                            }                        }                    },                    markPoint: {                        data: [                            {type: &#39;max&#39;, name: &#39;最大值&#39;},                            {type: &#39;min&#39;, name: &#39;最小值&#39;}                        ]                    },                    markLine: {                        data: [                            {type: &#39;average&#39;, name: &#39;平均值&#39;}                        ]                    }                }            ]        };        if (option &amp;&amp; typeof option === &quot;object&quot;) {            myChart.setOption(option, true);        }        myChart.hideLoading()    }    function callog2() {        var dom = document.getElementById(&quot;container2&quot;);        var myChart = echarts.init(dom);        myChart.showLoading();        var option = {            title: {                text: &#39;通话时长&#39;,                textStyle: {                    //文字颜色                    color: &#39;#ffffff&#39;,                    //字体风格,&#39;normal&#39;,&#39;italic&#39;,&#39;oblique&#39;                    fontStyle: &#39;normal&#39;,                    //字体粗细 &#39;normal&#39;,&#39;bold&#39;,&#39;bolder&#39;,&#39;lighter&#39;,100 | 200 | 300 | 400...                    fontWeight: &#39;bold&#39;,                    //字体系列                    fontFamily: &#39;sans-serif&#39;,                    //字体大小                    fontSize: 13                },                itemGap: 12,            },            tooltip: {                trigger: &#39;item&#39;,                formatter: &quot;{a} &lt;br/&gt;{b} : {c} ({d}%)&quot;            },            visualMap: {                show: false,                min: Math.min.apply(null, duration.split(&quot;,&quot;)),                max: Math.max.apply(null, duration.split(&quot;,&quot;)),                inRange: {                    colorLightness: [0, 0.5]                }            },            series: [                {                    name: &#39;通话时长&#39;,                    type: &#39;pie&#39;,                    radius: &#39;55%&#39;,                    center: [&#39;50%&#39;, &#39;50%&#39;],                    data: pieData.sort(function (a, b) {                        return a.value - b.value;                    }),                    roseType: &#39;radius&#39;,                    label: {                        normal: {                            textStyle: {                                color: &#39;rgba(255, 255, 255, 0.3)&#39;                            }                        }                    },                    labelLine: {                        normal: {                            lineStyle: {                                color: &#39;rgba(255, 255, 255, 0.3)&#39;                            },                            smooth: 0.2,                            length: 10,                            length2: 20                        }                    },                    itemStyle: {                        normal: {                            color: &#39;#01c1c2&#39;,                            shadowBlur: 200,                            shadowColor: &#39;rgba(0, 0, 0, 0.5)&#39;                        }                    },                    animationType: &#39;scale&#39;,                    animationEasing: &#39;elasticOut&#39;,                    animationDelay: function (idx) {                        return Math.random() * 200;                    }                }            ]        };        if (option &amp;&amp; typeof option === &quot;object&quot;) {            myChart.setOption(option, true);        }        myChart.hideLoading()    }&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;</code></pre><h4 id="3、最终预览"><a href="#3、最终预览" class="headerlink" title="3、最终预览"></a>3、最终预览</h4><p>查询人通话时长与通话次数统计大概如下所示：</p><p><strong>首页</strong></p><p><img src="/medias/index.PNG" alt="index"></p><p><strong>统一展示</strong></p><p><img src="/medias/%E5%9B%BE%E5%BD%A2%E5%8C%96%E5%B1%95%E7%A4%BA.PNG" alt="图形化展示"></p><h3 id="八、定时任务"><a href="#八、定时任务" class="headerlink" title="八、定时任务"></a>八、定时任务</h3><p>新的数据每天都会产生，所以我们每天都需要更新离线的分析结果，所以此时我们可以用各种各样的定时任务调度工具来完成此操作。此例我们使用crontab来执行该操作。<br>1）编写任务脚本：<strong>analysis.sh</strong></p><pre><code>#!/bin/bash/root/hd/hadoop-2.8.4/bin/yarn jar /opt/jars/CT_analysis-1.0-SNAPSHOT.jar runner.CountDurationRunner -libjars /root/hd/hadoop-2.8.4/lib/*</code></pre><p>2） 制定crontab任务</p><pre><code># .------------------------------------------minute(0~59)# | .----------------------------------------hours(0~23)# | | .--------------------------------------day of month(1~31)# | | | .------------------------------------month(1~12)# | | | | .----------------------------------day of week(0~6)# | | | | | .--------------------------------command# | | | | | |# | | | | | |0 0 * * * /opt/jars/analysis.sh</code></pre><p>3）考虑数据处理手段是否安全<br>a、定时任务统计结果是否会重复<br>b、定时任务处理的数据是否全面</p><h3 id="九、项目总结"><a href="#九、项目总结" class="headerlink" title="九、项目总结"></a>九、项目总结</h3><p>重新总结梳理整个项目流程和方法论<br>1、实现月查询（某个月每一天的数据展示）<br>2、用户亲密度展示<br>3、考虑Hive实现<br>4、用户按照时间区间，查找所有的通话数据<br>5、给读者建议—–按代码来—–》成功运行——》掌握此项目</p><h3 id="十、附录"><a href="#十、附录" class="headerlink" title="十、附录"></a>十、附录</h3><h4 id="1、-flume-myagent-flume2kafka-conf"><a href="#1、-flume-myagent-flume2kafka-conf" class="headerlink" title="1、/flume/myagent/flume2kafka.conf"></a>1、/flume/myagent/flume2kafka.conf</h4><pre><code># definea1.sources = r1a1.sinks = k1a1.channels = c1# sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F -c +0 /opt/jars/calllog.csva1.sources.r1.shell = /bin/bash -c# sinka1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.brokerList = hsiehchou121:9092,hsiehchou122:9092,hsiehchou123:9092a1.sinks.k1.topic = callloga1.sinks.k1.batchSize = 20a1.sinks.k1.requiredAcks = 1# channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# binda1.sources.r1.channels = c1a1.sinks.k1.channel = c1</code></pre><h4 id="2、core-site-xml"><a href="#2、core-site-xml" class="headerlink" title="2、core-site.xml"></a>2、core-site.xml</h4><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;    &lt;!-- 指定hdfs的nameservice为mycluster --&gt;    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://mycluster&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定hadoop临时目录 --&gt;    &lt;property&gt;        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;        &lt;value&gt;/root/hd/hadoop-2.8.4/tmp&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定zookeeper地址 --&gt;    &lt;property&gt;        &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;        &lt;value&gt;hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181&lt;/value&gt;    &lt;/property&gt;    &lt;!--&lt;property&gt;        &lt;name&gt;ipc.client.connect.max.retries&lt;/name&gt;        &lt;value&gt;30&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;ipc.client.connect.retry.interval&lt;/name&gt;        &lt;value&gt;1000&lt;/value&gt;    &lt;/property&gt; --&gt;&lt;/configuration&gt;</code></pre><h4 id="3、hdfs-site-xml"><a href="#3、hdfs-site-xml" class="headerlink" title="3、hdfs-site.xml"></a>3、hdfs-site.xml</h4><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;     &lt;!--指定hdfs的nameservice为mycluster，需要和core-site.xml中的保持一致 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.nameservices&lt;/name&gt;        &lt;value&gt;mycluster&lt;/value&gt;    &lt;/property&gt;    &lt;!-- mycluster下面有两个NameNode，分别是nn1，nn2 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;        &lt;value&gt;nn1,nn2&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn1的RPC通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;        &lt;value&gt;hsiehchou121:9000&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn1的http通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;        &lt;value&gt;hsiehchou121:50070&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn2的RPC通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;        &lt;value&gt;hsiehchou122:9000&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn2的http通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;        &lt;value&gt;hsiehchou122:50070&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定NameNode的日志在JournalNode上的存放位置 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;        &lt;value&gt;qjournal://hsiehchou121:8485;hsiehchou122:8485;/mycluster&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;        &lt;value&gt;/root/hd/hadoop-2.8.4/journal&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 开启NameNode失败自动切换 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 配置失败自动切换实现方式 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;        &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;        &lt;value&gt;            sshfence            shell(/bin/true)        &lt;/value&gt;    &lt;/property&gt;    &lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;        &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 配置sshfence隔离机制超时时间 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;        &lt;value&gt;30000&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="4、hbase-site-xml"><a href="#4、hbase-site-xml" class="headerlink" title="4、hbase-site.xml"></a>4、hbase-site.xml</h4><pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!--/** * * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements.  See the NOTICE file * distributed with this work for additional information * regarding copyright ownership.  The ASF licenses this file * to you under the Apache License, Version 2.0 (the * &quot;License&quot;); you may not use this file except in compliance * with the License.  You may obtain a copy of the License at * *     http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an &quot;AS IS&quot; BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */--&gt;&lt;configuration&gt;     &lt;property&gt;         &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;         &lt;value&gt;true&lt;/value&gt;     &lt;/property&gt;     &lt;property&gt;         &lt;name&gt;hbase.rootdir&lt;/name&gt;         &lt;value&gt;hdfs://mycluster/hbase&lt;/value&gt;     &lt;/property&gt;     &lt;property&gt;         &lt;name&gt;hbase.master.port&lt;/name&gt;         &lt;value&gt;16000&lt;/value&gt;     &lt;/property&gt;     &lt;property&gt;         &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;         &lt;value&gt;hsiehchou121,hsiehchou122,hsiehchou123&lt;/value&gt;     &lt;/property&gt;     &lt;!-- hbase的元数据信息存储在zookeeper的位置 --&gt;     &lt;property&gt;         &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;         &lt;value&gt;/root/hd/zookeeper-3.4.10/zkData&lt;/value&gt;     &lt;/property&gt;     &lt;property&gt;         &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;         &lt;value&gt;2181&lt;/value&gt;     &lt;/property&gt;     &lt;property&gt;         &lt;name&gt;zookeeper.session.timeout&lt;/name&gt;         &lt;value&gt;120000&lt;/value&gt;     &lt;/property&gt;     &lt;property&gt;         &lt;name&gt;hbase.zookeeper.property.tickTime&lt;/name&gt;         &lt;value&gt;6000&lt;/value&gt;     &lt;/property&gt;     &lt;!-- 保证HBase之间时间同步 --&gt;    &lt;property&gt;        &lt;name&gt;hbase.master.maxclockskew&lt;/name&gt;        &lt;value&gt;180000&lt;/value&gt;        &lt;description&gt;Time difference of regionserver from master&lt;/description&gt;    &lt;/property&gt;    &lt;!-- 使用HBase Coprocessor协处理器 --&gt;    &lt;property&gt;        &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;        &lt;value&gt;hbase.CalleeWriteObserver&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="5、log4j-properties"><a href="#5、log4j-properties" class="headerlink" title="5、log4j.properties"></a>5、log4j.properties</h4><pre><code># Licensed to the Apache Software Foundation (ASF) under one# or more contributor license agreements.  See the NOTICE file# distributed with this work for additional information# regarding copyright ownership.  The ASF licenses this file# to you under the Apache License, Version 2.0 (the# &quot;License&quot;); you may not use this file except in compliance# with the License.  You may obtain a copy of the License at##     http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# Define some default values that can be overridden by system propertieshbase.root.logger=INFO,consolehbase.security.logger=INFO,consolehbase.log.dir=.hbase.log.file=hbase.log# Define the root logger to the system property &quot;hbase.root.logger&quot;.log4j.rootLogger=${hbase.root.logger}# Logging Thresholdlog4j.threshold=ALL## Daily Rolling File Appender#log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.DRFA.File=${hbase.log.dir}/${hbase.log.file}# Rollver at midnightlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd# 30-day backup#log4j.appender.DRFA.MaxBackupIndex=30log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout# Pattern format: Date LogLevel LoggerName LogMessagelog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n# Rolling File Appender propertieshbase.log.maxfilesize=256MBhbase.log.maxbackupindex=20# Rolling File Appenderlog4j.appender.RFA=org.apache.log4j.RollingFileAppenderlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}log4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}log4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}log4j.appender.RFA.layout=org.apache.log4j.PatternLayoutlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n## Security audit appender#hbase.security.log.file=SecurityAuth.audithbase.security.log.maxfilesize=256MBhbase.security.log.maxbackupindex=20log4j.appender.RFAS=org.apache.log4j.RollingFileAppenderlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}log4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}log4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}log4j.appender.RFAS.layout=org.apache.log4j.PatternLayoutlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%nlog4j.category.SecurityLogger=${hbase.security.logger}log4j.additivity.SecurityLogger=false#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.visibility.VisibilityController=TRACE## Null Appender#log4j.appender.NullAppender=org.apache.log4j.varia.NullAppender## console# Add &quot;console&quot; to rootlogger above if you want to use this#log4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.target=System.errlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%nlog4j.appender.asyncconsole=org.apache.hadoop.hbase.AsyncConsoleAppenderlog4j.appender.asyncconsole.target=System.err# Custom Logging levelslog4j.logger.org.apache.zookeeper=INFO#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUGlog4j.logger.org.apache.hadoop.hbase=INFO# Make these two classes INFO-level. Make them DEBUG to see more zk debug.log4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFOlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO#log4j.logger.org.apache.hadoop.dfs=DEBUG# Set this class to log INFO only otherwise its OTT# Enable this to get detailed connection error/retry logging.# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG# Uncomment the below if you want to remove logging of client region caching&#39;# and scan of hbase:meta messages# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO# Prevent metrics subsystem start/stop messages (HBASE-17722)log4j.logger.org.apache.hadoop.metrics2.impl.MetricsConfig=WARNlog4j.logger.org.apache.hadoop.metrics2.impl.MetricsSinkAdapter=WARNlog4j.logger.org.apache.hadoop.metrics2.impl.MetricsSystemImpl=WARN</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据项目 </tag>
            
            <tag> 电信 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink练习</title>
      <link href="/2019/05/18/flink-lian-xi/"/>
      <url>/2019/05/18/flink-lian-xi/</url>
      
        <content type="html"><![CDATA[<h3 id="一、Flink开发IDEA环境搭建与测试"><a href="#一、Flink开发IDEA环境搭建与测试" class="headerlink" title="一、Flink开发IDEA环境搭建与测试"></a>一、Flink开发IDEA环境搭建与测试</h3><h4 id="1、IDEA开发环境"><a href="#1、IDEA开发环境" class="headerlink" title="1、IDEA开发环境"></a>1、IDEA开发环境</h4><p>先虚拟机联网，然后执行yum -y install nc<br>nc是用来打开端口的工具<br>然后nc -l 9000  </p><p><strong>1.pom文件设置</strong></p><pre><code>&lt;properties&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;        &lt;encoding&gt;UTF-8&lt;/encoding&gt;        &lt;scala.version&gt;2.11.12&lt;/scala.version&gt;        &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt;        &lt;hadoop.version&gt;2.8.4&lt;/hadoop.version&gt;        &lt;flink.version&gt;1.6.1&lt;/flink.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;            &lt;artifactId&gt;scala-library&lt;/artifactId&gt;            &lt;version&gt;${scala.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;            &lt;artifactId&gt;flink-java&lt;/artifactId&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;            &lt;artifactId&gt;flink-streaming-java_${scala.binary.version}&lt;/artifactId&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;            &lt;artifactId&gt;flink-scala_${scala.binary.version}&lt;/artifactId&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;            &lt;artifactId&gt;flink-streaming-scala_${scala.binary.version}&lt;/artifactId&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;            &lt;artifactId&gt;flink-table_${scala.binary.version}&lt;/artifactId&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;            &lt;artifactId&gt;flink-clients_${scala.binary.version}&lt;/artifactId&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;            &lt;artifactId&gt;flink-connector-kafka-0.10_${scala.binary.version}&lt;/artifactId&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;            &lt;version&gt;${hadoop.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;mysql&lt;/groupId&gt;            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;            &lt;version&gt;5.1.38&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;            &lt;version&gt;1.2.22&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt;        &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;                &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;                &lt;version&gt;3.2.0&lt;/version&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;goals&gt;                            &lt;goal&gt;compile&lt;/goal&gt;                            &lt;goal&gt;testCompile&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;args&gt;                                &lt;!-- &lt;arg&gt;-make:transitive&lt;/arg&gt; --&gt;                                &lt;arg&gt;-dependencyfile&lt;/arg&gt;                                &lt;arg&gt;${project.build.directory}/.scala_dependencies&lt;/arg&gt;                            &lt;/args&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;                &lt;version&gt;2.18.1&lt;/version&gt;                &lt;configuration&gt;                    &lt;useFile&gt;false&lt;/useFile&gt;                    &lt;disableXmlReport&gt;true&lt;/disableXmlReport&gt;                    &lt;includes&gt;                        &lt;include&gt;**/*Test.*&lt;/include&gt;                        &lt;include&gt;**/*Suite.*&lt;/include&gt;                    &lt;/includes&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;                &lt;version&gt;3.0.0&lt;/version&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;shade&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;filters&gt;                                &lt;filter&gt;                                    &lt;artifact&gt;*:*&lt;/artifact&gt;                                    &lt;excludes&gt;                                        &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;                                        &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;                                        &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;                                    &lt;/excludes&gt;                                &lt;/filter&gt;                            &lt;/filters&gt;                            &lt;transformers&gt;                                &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;                                    &lt;mainClass&gt;org.apache.spark.WordCount&lt;/mainClass&gt;                                &lt;/transformer&gt;                            &lt;/transformers&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;</code></pre><h4 id="2、Flink开发流程"><a href="#2、Flink开发流程" class="headerlink" title="2、Flink开发流程"></a>2、Flink开发流程</h4><p>Flink具有特殊类DataSet并DataStream在程序中表示数据。您可以将它们视为可以包含重复项的不可变数据集合。在DataSet数据有限的情况下，对于一个DataStream元素的数量可以是无界的</p><p>这些集合在某些关键方面与常规Java集合不同。首先，它们是不可变的，这意味着一旦创建它们就无法添加或删除元素。你也不能简单地检查里面的元素</p><p>集合最初通过在弗林克程序添加源创建和新的集合从这些通过将它们使用API方法如衍生map，filter等等</p><p>Flink程序看起来像是转换数据集合的常规程序。每个程序包含相同的基本部分：<br>1）获取execution environment,<br>final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</p><p>2）加载/创建初始化数据<br>DataStream<code>&lt;String&gt;</code> text = env.readTextFile(“file:///path/to/file”);</p><p>3）指定此数据的转换<br>val mapped = input.map { x =&gt; x.toInt }</p><p>4）指定放置计算结果的位置<br>writeAsText(String path)<br>print()</p><p>5）触发程序执行<br>在local模式下执行程序<br>execute()<br>将程序达成jar运行在线上<br>./bin/flink run <br>-m hsiehchou121:8081 <br>./examples/batch/WordCount.jar <br>–input  hdfs:///user/root/input/wc.txt <br>–output  hdfs:///user/root/output2  \</p><p>####3、WordCount案例</p><p>1）<strong>Scala代码</strong></p><pre><code>import org.apache.flink.api.java.utils.ParameterToolimport org.apache.flink.streaming.api.scala.StreamExecutionEnvironmentimport org.apache.flink.streaming.api.windowing.time.Timeobject SocketWindowWordCountScala {  def main(args: Array[String]) : Unit = {    // 定义一个数据类型保存单词出现的次数    case class WordWithCount(word: String, count: Long)    // port 表示需要连接的端口    val port: Int = try {      ParameterTool.fromArgs(args).getInt(&quot;port&quot;)    } catch {      case e: Exception =&gt; {        System.err.println(&quot;No port specified. Please run &#39;SocketWindowWordCount --port &lt;port&gt;&#39;&quot;)        return      }    }    // 获取运行环境    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment    // 连接此socket获取输入数据    val text = env.socketTextStream(&quot;hsiehchou121&quot;, port, &#39;\n&#39;)    //需要加上这一行隐式转换 否则在调用flatmap方法的时候会报错    import org.apache.flink.api.scala._    // 解析数据, 分组, 窗口化, 并且聚合求SUM    val windowCounts = text      .flatMap { w =&gt; w.split(&quot;\\s&quot;) }      .map { w =&gt; WordWithCount(w, 1) }      .keyBy(&quot;word&quot;)      .timeWindow(Time.seconds(5), Time.seconds(1))      .sum(&quot;count&quot;)    // 打印输出并设置使用一个并行度    windowCounts.print().setParallelism(1)    env.execute(&quot;Socket Window WordCount&quot;)  }}</code></pre><p>2）<strong>Java代码</strong></p><pre><code>import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.util.Collector;public class WordCount {    //先在虚拟机上打开你的端口号 nc -l 9000    public static void main(String[] args) throws Exception {        //定义socket的端口号        int port;        try{            ParameterTool parameterTool = ParameterTool.fromArgs(args);            port = parameterTool.getInt(&quot;port&quot;);        }catch (Exception e){            System.err.println(&quot;没有指定port参数，使用默认值9000&quot;);            port = 9000;        }        //获取运行环境        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        //连接socket获取输入的数据        DataStreamSource&lt;String&gt; text = env.socketTextStream(&quot;192.168.1.52&quot;, port, &quot;\n&quot;);        //计算数据        DataStream&lt;WordWithCount&gt; windowCount = text.flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() {            public void flatMap(String value, Collector&lt;WordWithCount&gt; out) throws Exception {                String[] splits = value.split(&quot;\\s&quot;);                for (String word:splits) {                    out.collect(new WordWithCount(word,1L));                }            }        })//打平操作，把每行的单词转为&lt;word,count&gt;类型的数据                .keyBy(&quot;word&quot;)//针对相同的word数据进行分组                .timeWindow(Time.seconds(2),Time.seconds(1))//指定计算数据的窗口大小和滑动窗口大小                .sum(&quot;count&quot;);        //把数据打印到控制台        windowCount.print()                .setParallelism(1);//使用一个并行度        //注意：因为flink是懒加载的，所以必须调用execute方法，上面的代码才会执行        env.execute(&quot;streaming word count&quot;);    }    /**     * 主要为了存储单词以及单词出现的次数     */    public static class WordWithCount{        public String word;        public long count;        public WordWithCount(){}        public WordWithCount(String word, long count) {            this.word = word;            this.count = count;        }        @Override        public String toString() {            return &quot;WordWithCount{&quot; +                    &quot;word=&#39;&quot; + word + &#39;\&#39;&#39; +                    &quot;, count=&quot; + count +                    &#39;}&#39;;        }    }}</code></pre><p>3）<strong>运行测试</strong></p><p>首先，使用nc命令启动一个本地监听，命令是：<br>[root@hsiehchou121 ~]$ nc -l 9000</p><p>通过netstat命令观察9000端口。 netstat -anlp | grep 9000，启动监听如果报错：-bash: nc: command not found，请先安装nc，在线安装命令：yum -y install nc。<br>然后，IDEA上运行flink官方案例程序<br>hsiehchou121上输入<br>[root@hsiehchou121 ~] nc -l 9000<br>learn flink<br>hadoop storm flink<br>flink flink hsiehchou</p><p>4）<strong>集群测试</strong></p><p>这里单机测试官方案例<br>[root@hsiehchou121 flink-1.6.1]$ pwd<br>/opt/flink-1.6.1</p><p>[root@hsiehchou121 flink-1.6.1]$ ./bin/start-cluster.sh<br>Starting cluster.<br>Starting standalonesession daemon on host hsiehchou121.<br>Starting taskexecutor daemon on host hsiehchou121.</p><p>[root@hsiehchou121 flink-1.6.1]$ jps<br>StandaloneSessionClusterEntrypoint<br>TaskManagerRunner<br>Jps</p><p>[root@hsiehchou121 flink-1.6.1]$ ./bin/flink run examples/streaming/SocketWindowWordCount.jar –port 9000<br>单词在5秒的时间窗口（处理时间，翻滚窗口）中计算并打印到stdout。监视TaskManager的输出文件并写入一些文本nc（输入在点击后逐行发送到Flink）：</p><h4 id="4、使用IDEA开发离线程序"><a href="#4、使用IDEA开发离线程序" class="headerlink" title="4、使用IDEA开发离线程序"></a>4、使用IDEA开发离线程序</h4><p>Dataset是flink的常用程序，数据集通过source进行初始化，例如读取文件或者序列化集合，然后通过transformation（filtering、mapping、joining、grouping）将数据集转成，然后通过sink进行存储，既可以写入hdfs这种分布式文件系统，也可以打印控制台，flink可以有很多种运行方式，如local、flink集群、yarn等.<br>1）<strong>scala程序</strong></p><pre><code>import org.apache.flink.api.scala.ExecutionEnvironmentimport org.apache.flink.api.scala._object WordCountScala{  def main(args: Array[String]) {    //初始化环境    val env = ExecutionEnvironment.getExecutionEnvironment    //从字符串中加载数据    val text = env.fromElements(      &quot;Who&#39;s there?&quot;,      &quot;I think I hear them. Stand, ho! Who&#39;s there?&quot;)    //分割字符串、汇总tuple、按照key进行分组、统计分组后word个数    val counts = text.flatMap { _.toLowerCase.split(&quot;\\W+&quot;) filter { _.nonEmpty } }      .map { (_, 1) }      .groupBy(0)      .sum(1)    //打印    counts.print()  }}</code></pre><p>2） <strong>Java程序</strong></p><pre><code>import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.util.Collector;public class WordCountJava {    public static void main(String[] args) throws Exception {        //构建环境        final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();        //通过字符串构建数据集        DataSet&lt;String&gt; text = env.fromElements(                &quot;Who&#39;s there?&quot;,                &quot;I think I hear them. Stand, ho! Who&#39;s there?&quot;);        //分割字符串、按照key进行分组、统计相同的key个数        DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; wordCounts = text                .flatMap(new LineSplitter())                .groupBy(0)                .sum(1);        //打印        wordCounts.print();    }    //分割字符串的方法    public static class LineSplitter implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; {        @Override        public void flatMap(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) {            for (String word : line.split(&quot; &quot;)) {                out.collect(new Tuple2&lt;String, Integer&gt;(word, 1));            }        }    }}</code></pre><p>3）<strong>运行</strong></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink基础</title>
      <link href="/2019/05/16/flink-ji-chu/"/>
      <url>/2019/05/16/flink-ji-chu/</url>
      
        <content type="html"><![CDATA[<h3 id="一、Flink概述"><a href="#一、Flink概述" class="headerlink" title="一、Flink概述"></a>一、Flink概述</h3><p>官网：<a href="http://flink.apache.org/" target="_blank" rel="noopener">http://flink.apache.org/</a><br>MapReduce-&gt;MaxCompute<br>HBase-&gt;部门<br>QuickBI<br>DataV<br>Hive-&gt;高德地图<br>Storm-&gt;JStorm</p><p>2019年1月 阿里正式开源Flink-&gt;Blink<br>Apache Flink是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。Flink设计为在所有常见的集群环境中运行，以内存速度和任何规模执行计算。</p><p>大数据计算框架</p><h4 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h4><p>Flink核心是一个流式的数据流执行引擎，其针对数据流的分布式计算提供了数据分布、数据通信以及容错机制等功能。基于流执行引擎，Flink提供了诸多更高抽象层的API以便用户编写分布式任务：</p><p>DataSet API，对静态数据进行批处理操作，将静态数据抽象成分布式的数据集，用户可以方便地使用Flink提供的各种操作符对分布式数据集进行处理，支持Java、Scala和Python</p><p>DataStream API，对数据流进行流处理操作，将流式的数据抽象成分布式的数据流，用户可以方便地对分布式数据流进行各种操作，支持Java和Scala</p><p>Table API，对结构化数据进行查询操作，将结构化数据抽象成关系表，并通过类SQL的DSL对关系表进行各种查询操作，支持Java和Scala</p><p>此外，Flink还针对特定的应用领域提供了领域库，例如：<br>Flink ML，Flink的机器学习库，提供了机器学习Pipelines API并实现了多种机器学习算法<br>Gelly，Flink的图计算库，提供了图计算的相关API及多种图计算算法实现</p><h4 id="2、统一的批处理与流处理系统"><a href="#2、统一的批处理与流处理系统" class="headerlink" title="2、统一的批处理与流处理系统"></a>2、统一的批处理与流处理系统</h4><p>在大数据处理领域，批处理任务与流处理任务一般被认为是两种不同的任务，一个大数据项目一般会被设计为只能处理其中一种任务，例如Apache Storm、Apache Smaza只支持流处理任务，而Aapche MapReduce、Apache Tez、Apache Spark只支持批处理任务。Spark Streaming是Apache Spark之上支持流处理任务的子系统，看似一个特例，实则不然——Spark Streaming采用了一种micro-batch的架构，即把输入的数据流切分成细粒度的batch，并为每一个batch数据提交一个批处理的Spark任务，所以Spark Streaming本质上还是基于Spark批处理系统对流式数据进行处理，和Apache Storm、Apache Smaza等完全流式的数据处理方式完全不同。通过其灵活的执行引擎，Flink能够同时支持批处理任务与流处理任务</p><p>在执行引擎这一层，流处理系统与批处理系统最大不同在于节点间的数据传输方式</p><p>对于一个流处理系统，其节点间数据传输的标准模型是：当一条数据被处理完成后，序列化到缓存中，然后立刻通过网络传输到下一个节点，由下一个节点继续处理</p><p>而对于一个批处理系统，其节点间数据传输的标准模型是：当一条数据被处理完成后，序列化到缓存中，并不会立刻通过网络传输到下一个节点，当缓存写满，就持久化到本地硬盘上，当所有数据都被处理完成后，才开始将处理后的数据通过网络传输到下一个节点</p><p>这两种数据传输模式是两个极端，对应的是流处理系统对低延迟的要求和批处理系统对高吞吐量的要求。Flink的执行引擎采用了一种十分灵活的方式，同时支持了这两种数据传输模型。Flink以固定的缓存块为单位进行网络数据传输，用户可以通过缓存块超时值指定缓存块的传输时机。如果缓存块的超时值为0，则Flink的数据传输方式类似上文所提到流处理系统的标准模型，此时系统可以获得最低的处理延迟。如果缓存块的超时值为无限大，则Flink的数据传输方式类似上文所提到批处理系统的标准模型，此时系统可以获得最高的吞吐量</p><p>同时缓存块的超时值也可以设置为0到无限大之间的任意值</p><p>缓存块的超时阈值越小，则Flink流处理执行引擎的数据处理延迟越低，但吞吐量也会降低，<br>反之亦然。通过调整缓存块的超时阈值，用户可根据需求灵活地权衡系统延迟和吞吐量</p><h4 id="3、架构"><a href="#3、架构" class="headerlink" title="3、架构"></a>3、架构</h4><p>要了解一个系统，一般都是从架构开始。我们关心的问题是：系统部署成功后各个节点都启动了哪些服务，各个服务之间又是怎么交互和协调的。下方是 Flink 集群启动后架构图</p><p><img src="/medias/Flink%20%E9%9B%86%E7%BE%A4%E5%90%AF%E5%8A%A8%E5%90%8E%E6%9E%B6%E6%9E%84%E5%9B%BE.PNG" alt="Flink 集群启动后架构图"></p><p>当 Flink 集群启动后，首先会启动一个 JobManger 和一个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程</p><p>Client 为提交 Job 的客户端，可以是运行在任何机器上（与 JobManager 环境连通即可）。提交 Job 后，Client 可以结束进程（Streaming的任务），也可以不结束并等待结果返回</p><p>JobManager 主要负责调度 Job 并协调 Task 做 checkpoint，职责上很像 Storm 的 Nimbus。从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行</p><p>TaskManager 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自己的上游建立 Netty 连接，接收数据并处理</p><p>可以看到 Flink 的任务调度是多线程模型，并且不同Job/Task混合在一个 TaskManager 进程中。虽然这种方式可以有效提高 CPU 利用率，但是个人不太喜欢这种设计，因为不仅缺乏资源隔离机制，同时也不方便调试。类似 Storm 的进程模型，一个JVM 中只跑该 Job 的 Tasks 实际应用中更为合理</p><p>Flink编程模型</p><p><img src="/medias/Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B.PNG" alt="Flink编程模型"></p><h3 id="二、Flink特点"><a href="#二、Flink特点" class="headerlink" title="二、Flink特点"></a>二、Flink特点</h3><p>1）mapreduce<br>2）storm<br>3）spark</p><p>适用于所有企业，不同企业有不同的业务场景。处理数据量，模型都不一样</p><p><strong>Flink</strong></p><h4 id="1、随处部署应用"><a href="#1、随处部署应用" class="headerlink" title="1、随处部署应用"></a>1、随处部署应用</h4><p>与其它组件集成！<br>flink是分布式系统，需要计算资源才可执行程序。flink可以与常见的集群资源管理器进行集成(H<br>adoop Yarn,Apache Mesos..)</p><p>可以单独作为独立集群运行</p><p>通过不同部署模式实现</p><p>这些模式允许flink以其惯有的方式进行交互</p><p>当我们部署flink应用程序时，Flink会根据应用程序配置的并行性自动识别所需资源。从资源管理<br>器中请求它们</p><p>如果发生故障，flink会请求新的资源来替换发生故障的容器</p><p>提交或控制程序都通过REST调用进行，简化Flink在许多环境的集成。孵化…</p><h4 id="2、以任何比例应用程序（小集群、无限集群）"><a href="#2、以任何比例应用程序（小集群、无限集群）" class="headerlink" title="2、以任何比例应用程序（小集群、无限集群）"></a>2、以任何比例应用程序（小集群、无限集群）</h4><p>Flink旨在以任何规模运行有状态流应用程序。应用程序可以并行化在集群中分布和同时执行程<br>序</p><p>因此，我们的应用集群可以利用无限的cpu和磁盘与网络IO</p><p>Flink可以轻松的维护非常大的应用程序状态<br>用户可拓展性报告：</p><ul><li>应用程序每天可以处理万亿个事件</li><li>应用程序每天可以维护多个TB的状态</li><li>应用程序可以在数千个内核运行</li></ul><h4 id="3、利用内存中的性能"><a href="#3、利用内存中的性能" class="headerlink" title="3、利用内存中的性能"></a>3、利用内存中的性能</h4><p>有状态Flink应用程序针对于对本地状态访问进行了优化。任务状态始终的保留在内存中，或者如果<br>大小超过了可用内存，则保存在访问高效的磁盘数据结构中(SSD 机械/固态)</p><p>任务可以通过访问本地来执行所有计算。从来产生极小的延迟</p><p>Flink定期和异步检查本地状态持久存储来保持出现故障时一次状态的一致性</p><h3 id="三、有界无界"><a href="#三、有界无界" class="headerlink" title="三、有界无界"></a>三、有界无界</h3><h4 id="1、无界"><a href="#1、无界" class="headerlink" title="1、无界"></a>1、无界</h4><p>有开始，没有结束…<br>处理实时数据</p><h4 id="2、有界"><a href="#2、有界" class="headerlink" title="2、有界"></a>2、有界</h4><p>有开始，有结束…<br>处理批量数据</p><h3 id="四、无界数据集应用场景（实时计算）"><a href="#四、无界数据集应用场景（实时计算）" class="headerlink" title="四、无界数据集应用场景（实时计算）"></a>四、无界数据集应用场景（实时计算）</h3><p>1）源源不断的日志数据<br>2）web应用，指标分析<br>3）移动设备终端(分析app状况)<br>4）应用在任何数据源不断产生的项目中</p><h3 id="五、Flink运行模型"><a href="#五、Flink运行模型" class="headerlink" title="五、Flink运行模型"></a>五、Flink运行模型</h3><p>1）<strong>流计算</strong><br>数据源源不断产生，我们的需求是源源不断的处理</p><p>程序需要一直保持在计算的状态</p><p>2）<strong>批处理</strong><br>计算一段完整的数据集，计算成功后释放资源，那么此时工作结束</p><h3 id="六、Flink的使用"><a href="#六、Flink的使用" class="headerlink" title="六、Flink的使用"></a>六、Flink的使用</h3><h4 id="1、处理结果准确"><a href="#1、处理结果准确" class="headerlink" title="1、处理结果准确"></a>1、处理结果准确</h4><p>无论是有序数据还是延迟到达的数据</p><h4 id="2、容错机制"><a href="#2、容错机制" class="headerlink" title="2、容错机制"></a>2、容错机制</h4><p>有状态：保持每次的结果往下传递，实现累加。DAG（有向无环图）</p><h4 id="3、有很强大的吞吐量和低延迟"><a href="#3、有很强大的吞吐量和低延迟" class="headerlink" title="3、有很强大的吞吐量和低延迟"></a>3、有很强大的吞吐量和低延迟</h4><p>计算速度快，吞吐量处理的量级大</p><h4 id="4、精准的维护一次的应用状态"><a href="#4、精准的维护一次的应用状态" class="headerlink" title="4、精准的维护一次的应用状态"></a>4、精准的维护一次的应用状态</h4><p>storm:会发生要么多计算一次，要么漏计算</p><h4 id="5、支持大规模的计算"><a href="#5、支持大规模的计算" class="headerlink" title="5、支持大规模的计算"></a>5、支持大规模的计算</h4><p>可以运行在数千台节点上</p><h4 id="6、支持流处理和窗口化操作"><a href="#6、支持流处理和窗口化操作" class="headerlink" title="6、支持流处理和窗口化操作"></a>6、支持流处理和窗口化操作</h4><h4 id="7、版本化处理"><a href="#7、版本化处理" class="headerlink" title="7、版本化处理"></a>7、版本化处理</h4><h4 id="8、检查点机制实现精准的一次性计算保证"><a href="#8、检查点机制实现精准的一次性计算保证" class="headerlink" title="8、检查点机制实现精准的一次性计算保证"></a>8、检查点机制实现精准的一次性计算保证</h4><p>checkpoint</p><h4 id="9、支持yarn与mesos资源管理器"><a href="#9、支持yarn与mesos资源管理器" class="headerlink" title="9、支持yarn与mesos资源管理器"></a>9、支持yarn与mesos资源管理器</h4><h3 id="七、flink单节点安装部署"><a href="#七、flink单节点安装部署" class="headerlink" title="七、flink单节点安装部署"></a>七、flink单节点安装部署</h3><p>1）下载安装包<br>2）上传<br>3）解压<br>tar -zxvf .tar<br>4）启动<br>bin/start-cluster.sh<br>5）访问ui界面<br><a href="http://192.168.116.201:8081" target="_blank" rel="noopener">http://192.168.116.201:8081</a></p><h3 id="八、搭建Flink1-6-1分布式集群"><a href="#八、搭建Flink1-6-1分布式集群" class="headerlink" title="八、搭建Flink1.6.1分布式集群"></a>八、搭建Flink1.6.1分布式集群</h3><h4 id="1、Flink的下载"><a href="#1、Flink的下载" class="headerlink" title="1、Flink的下载"></a>1、Flink的下载</h4><p>安装包下载地址：<a href="http://flink.apache.org/downloads.html" target="_blank" rel="noopener">http://flink.apache.org/downloads.html</a>  ，选择对应Hadoop的Flink版本下载<br>[root@hsiehchou121 software]$ wget <a href="http://mirrors.tuna.tsinghua.edu.cn/apache/flink/flink-1.6.1/flink-1.6.1-bin-hadoop28-scala_2.11.tgz" target="_blank" rel="noopener">http://mirrors.tuna.tsinghua.edu.cn/apache/flink/flink-1.6.1/flink-1.6.1-bin-hadoop28-scala_2.11.tgz</a><br>[root@hsiehchou121 software]$ ll<br>-rw-rw-r– 1 root root 301867081 Sep 15 15:47 flink-1.6.1-bin-hadoop28-scala_2.11.tgz<br>Flink 有三种部署模式，分别是 Local、Standalone Cluster 和 Yarn Cluster</p><h4 id="2、Local模式"><a href="#2、Local模式" class="headerlink" title="2、Local模式"></a>2、Local模式</h4><p>对于 Local 模式来说，JobManager 和 TaskManager 会公用一个 JVM 来完成 Workload</p><p>如果要验证一个简单的应用，Local 模式是最方便的。实际应用中大多使用 Standalone 或者 Yarn Cluster，而local模式只是将安装包解压启动（./bin/start-cluster.sh）即可，在这里不在演示</p><h4 id="3、Standalone-模式"><a href="#3、Standalone-模式" class="headerlink" title="3、Standalone 模式"></a>3、Standalone 模式</h4><p>快速入门教程地址：<br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.6/quickstart/setup_quickstart.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.6/quickstart/setup_quickstart.html</a></p><p>1）  <strong>软件要求</strong><br>•    Java 1.8.x或更高版本<br>•    ssh（必须运行sshd才能使用管理远程组件的Flink脚本）</p><p><strong>集群部署规划</strong></p><table><thead><tr><th align="center">节点名称</th><th align="center">master</th><th align="center">worker</th><th align="center">zookeeper</th></tr></thead><tbody><tr><td align="center">hsiehchou121</td><td align="center">master</td><td align="center"></td><td align="center">zookeeper</td></tr><tr><td align="center">hsiehchou122</td><td align="center">master</td><td align="center">worker</td><td align="center">zookeeper</td></tr><tr><td align="center">hsiehchou123</td><td align="center"></td><td align="center">worker</td><td align="center">zookeeper</td></tr></tbody></table><p>2）<strong>解压</strong></p><p>[root@hsiehchou121 software]$ <code>tar -zxvf flink-1.6.1-bin-hadoop27-scala_2.11.tgz -C /opt/module/</code><br>[root@hsiehchou121 software]$ cd /opt/module/</p><p>[root@hsiehchou121 module]$ ll<br>drwxr-xr-x 8 root root 125 Sep 15 04:47 flink-1.6.1</p><p>3）<strong>修改配置文件</strong></p><p>[root@hsiehchou121 conf]$ ls<br>flink-conf.yaml       log4j-console.properties  log4j-yarn-session.properties  logback.xml       masters  sql-client-defaults.yaml<br>log4j-cli.properties  log4j.properties          logback-console.xml            logback-yarn.xml  slaves   zoo.cfg</p><p>修改flink/conf/masters，slaves，flink-conf.yaml</p><p>[root@hsiehchou121 conf]$ sudo vi masters<br>hsiehchou121:8081</p><p>[root@hsiehchou121 conf]$ sudo vi slaves<br>hsiehchou122<br>hsiehchou123</p><p>[root@hsiehchou121 conf]$ sudo vi flink-conf.yaml<br>taskmanager.numberOfTaskSlots：2   //52行 和storm slot类似<br>jobmanager.rpc.address: hsiehchou121  //33行</p><p>可选配置：<br>•    每个JobManager（jobmanager.heap.mb）的可用内存量<br>•    每个TaskManager（taskmanager.heap.mb）的可用内存量<br>•    每台机器的可用CPU数量（taskmanager.numberOfTaskSlots）<br>•    集群中的CPU总数（parallelism.default）和<br>•    临时目录（taskmanager.tmp.dirs）</p><p>4）<strong>拷贝安装包到各节点</strong></p><p>[root@hsiehchou121 module]$ scp -r flink-1.6.1/ root@hsiehchou122:<code>pwd</code><br>[root@hsiehchou121 module]$ scp -r flink-1.6.1/ root@hsiehchou123:<code>pwd</code></p><p>5） <strong>配置环境变量</strong></p><p>配置所有节点Flink的环境变量<br>[root@hsiehchou121 flink-1.6.1]$ sudo vi /etc/profile<br>export FLINK_HOME=/opt/module/flink-1.6.1<br>export PATH=<code>$PATH:$</code>FLINK_HOME/bin</p><p>[root@hsiehchou121 flink-1.6.1]$ source /etc/profile</p><p>6）<strong>启动Flink</strong></p><p>[root@hsiehchou121 flink-1.6.1]$ ./bin/start-cluster.sh<br>Starting cluster.<br>Starting standalonesession daemon on host hsiehchou121.<br>Starting taskexecutor daemon on host hsiehchou122.<br>Starting taskexecutor daemon on host hsiehchou123.</p><p>jps查看进程<br>hsiehchou121<br>2122 StandaloneSessionClusterEntrypoint<br>2172 Jps</p><p>hsiehchou122<br>1616 TaskManagerRunner<br>1658 Jps</p><p>hsiehchou123<br>1587 TaskManagerRunner<br>1627 Jps</p><p>7） <strong>WebUI查看</strong></p><p><a href="http://hsiehchou121:8081" target="_blank" rel="noopener">http://hsiehchou121:8081</a></p><p>8）<strong>Flink的HA</strong></p><p>首先，我们需要知道 Flink 有两种部署的模式，分别是 Standalone 以及 Yarn Cluster 模式。对于 Standalone 来说，Flink 必须依赖于 Zookeeper 来实现 JobManager 的 HA（Zookeeper 已经成为了大部分开源框架 HA 必不可少的模块）。在 Zookeeper 的帮助下，一个 Standalone 的 Flink 集群会同时有多个活着的 JobManager，其中只有一个处于工作状态，其他处于 Standby 状态。当工作中的 JobManager 失去连接后（如宕机或 Crash），Zookeeper 会从 Standby 中选举新的 JobManager 来接管 Flink 集群</p><p>对于 Yarn Cluaster 模式来说，Flink 就要依靠 Yarn 本身来对 JobManager 做 HA 了。其实这里完全是 Yarn 的机制。对于 Yarn Cluster 模式来说，JobManager 和 TaskManager 都是被 Yarn 启动在 Yarn 的 Container 中。此时的 JobManager，其实应该称之为 Flink Application Master。也就说它的故障恢复，就完全依靠着 Yarn 中的 ResourceManager（和 MapReduce 的 AppMaster 一样）。由于完全依赖了 Yarn，因此不同版本的 Yarn 可能会有细微的差异。这里不再做深究</p><p>（1） <strong>修改配置文件</strong></p><p>修改flink-conf.yaml，HA模式下，jobmanager不需要指定，在master file中配置，由zookeeper选出leader与standby。<br><strong>jobmanager.rpc.address: hsiehchou121</strong><br>high-availability:zookeeper   //73行</p><p><strong>指定高可用模式（必须） //88行</strong><br>high-availability.zookeeper.quorum:hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181</p><p><strong>ZooKeeper仲裁是ZooKeeper服务器的复制组，它提供分布式协调服务（必须） //82行</strong><br>high-availability.storageDir:hdfs:///flink/ha/       </p><p><strong>JobManager元数据保存在文件系统storageDir中，只有指向此状态的指针存储在ZooKeeper中（必须） //没有</strong><br>high-availability.zookeeper.path.root:/flink         </p><p><strong>根ZooKeeper节点，在该节点下放置所有集群节点（推荐） //没有</strong><br>high-availability.cluster-id:/flinkCluster           </p><p><strong>&amp;&amp;&amp;&amp;&amp;自定义集群（推荐）</strong><br>state.backend: filesystem<br>state.checkpoints.dir: hdfs:///flink/checkpoints<br>state.savepoints.dir: hdfs:///flink/checkpoints</p><p><strong>修改conf/zoo.cfg</strong><br>server.1=hsiehchou121:2888:3888<br>server.2=hsiehchou122:2888:3888<br>server.3=hsiehchou123:2888:3888</p><p><strong>修改conf/masters</strong><br>hsiehchou121:8081<br>hsiehchou122:8081</p><p><strong>修改slaves</strong><br>hsiehchou122<br>hsiehchou123<br>同步配置文件conf到各节点</p><p>（2） <strong>启动HA</strong></p><p>先启动zookeeper集群各节点（测试环境中也可以用Flink自带的start-zookeeper-quorum.sh），启动dfs ,再启动flink<br>[root@hsiehchou121 flink-1.6.1]$ start-cluster.sh </p><p>WebUI查看，这是会自动产生一个主Master，如下</p><p>（3） <strong>验证HA</strong></p><p>手动杀死hsiehchou122上的master，此时，hsiehchou121上的备用master转为主mater</p><p>（4）<strong>手动将JobManager / TaskManager实例添加到群集</strong></p><p>您可以使用bin/jobmanager.sh和bin/taskmanager.sh脚本将JobManager和TaskManager实例添加到正在运行的集群中</p><p>添加JobManager<br>bin/jobmanager.sh ((start|start-foreground) <code>[host] [webui-port]</code>)|stop|stop-all</p><p>添加TaskManager<br>bin/taskmanager.sh start|start-foreground|stop|stop-all</p><p>[root@hsiehchou122 flink-1.6.1]$ jobmanager.sh start hsiehchou122<br>新添加的为从master</p><p>9）<strong>运行测试任务</strong></p><p>[root@hsiehchou121 flink-1.6.1]$ flink run -m hsiehchou121:8081 ./examples/batch/WordCount.jar –input /opt/wcinput/wc.txt –output /opt/wcoutput/</p><p>[root@hsiehchou121 flink-1.6.1]$ bin/flink run -m hsiehchou121:8081 ./examples/batch/WordCount.jar –input hdfs:///emp.csv –output hdfs:///user/root/output2</p><h4 id="4、Yarn-Cluster模式"><a href="#4、Yarn-Cluster模式" class="headerlink" title="4、Yarn Cluster模式"></a>4、Yarn Cluster模式</h4><p>1）<strong>引入</strong><br>在一个企业中，为了最大化的利用集群资源，一般都会在一个集群中同时运行多种类型的 Workload。因此 Flink 也支持在 Yarn 上面运行。首先，让我们了解下 Yarn 和 Flink 的关系</p><p><img src="/medias/Yarn%20%E5%92%8C%20Flink%20%E7%9A%84%E5%85%B3%E7%B3%BB.PNG" alt="Yarn 和 Flink 的关系"></p><p>在图中可以看出，Flink 与 Yarn 的关系与 MapReduce 和 Yarn 的关系是一样的。Flink 通过 Yarn 的接口实现了自己的 App Master。当在 Yarn 中部署了 Flink，Yarn 就会用自己的 Container 来启动 Flink 的 JobManager（也就是 App Master）和 TaskManager</p><p>启动新的Flink YARN会话时，客户端首先检查所请求的资源（容器和内存）是否可用。之后，它将包含Flink和配置的jar上传到HDFS（步骤1）</p><p>客户端的下一步是请求（步骤2）YARN容器以启动ApplicationMaster（步骤3）。由于客户端将配置和jar文件注册为容器的资源，因此在该特定机器上运行的YARN的NodeManager将负责准备容器（例如，下载文件）。完成后，将启动ApplicationMaster（AM）</p><p>该JobManager和AM在同一容器中运行。一旦它们成功启动，AM就知道JobManager（它自己的主机）的地址。它正在为TaskManagers生成一个新的Flink配置文件（以便它们可以连接到JobManager）。该文件也上传到HDFS。此外，AM容器还提供Flink的Web界面。YARN代码分配的所有端口都是临时端口。这允许用户并行执行多个Flink YARN会话</p><p>之后，AM开始为Flink的TaskManagers分配容器，这将从HDFS下载jar文件和修改后的配置。完成这些步骤后，即可建立Flink并准备接受作业</p><p>2）<strong>修改环境变量</strong></p><p>export  HADOOP_CONF_DIR= /opt/module/hadoop-2.8.4/etc/hadoop</p><p>3）<strong>部署启动</strong> </p><p>[root@hsiehchou121 flink-1.6.1]$ yarn-session.sh -d -s 1 -tm 800 -n 2<br>-n : TaskManager的数量，相当于executor的数量</p><p>-s : 每个JobManager的core的数量，executor-cores。建议将slot的数量设置每台机器的处理器数量</p><p>-tm : 每个TaskManager的内存大小，executor-memory</p><p>-jm : JobManager的内存大小，driver-memory</p><p>上面的命令的意思是，同时向Yarn申请3个container，其中 2 个 Container 启动 TaskManager（-n 2），每个 TaskManager 拥有两个 Task Slot（-s 2），并且向每个 TaskManager 的 Container 申请 800M 的内存，以及一个ApplicationMaster（Job Manager）</p><p>Flink部署到Yarn Cluster后，会显示Job Manager的连接细节信息<br>Flink on Yarn会覆盖下面几个参数，如果不希望改变配置文件中的参数，可以动态的通过-D选项指定，如<br> -Dfs.overwrite-files=true -Dtaskmanager.network.numberOfBuffers=16368</p><p>jobmanager.rpc.address：因为JobManager会经常分配到不同的机器上</p><p>taskmanager.tmp.dirs：使用Yarn提供的tmp目录</p><p>parallelism.default：如果有指定slot个数的情况下</p><p>yarn-session.sh会挂起进程，所以可以通过在终端使用CTRL+C或输入stop停止yarn-session</p><p>如果不希望Flink Yarn client长期运行，Flink提供了一种detached YARN session，启动时候加上参数-d或—detached</p><p>在上面的命令成功后，我们就可以在 Yarn Application 页面看到 Flink 的纪录</p><p>如果在虚拟机中测试，可能会遇到错误。这里需要注意内存的大小，Flink 向 Yarn 会申请多个 Container，但是 Yarn 的配置可能限制了 Container 所能申请的内存大小，甚至 Yarn 本身所管理的内存就很小。这样很可能无法正常启动 TaskManager，尤其当指定多个 TaskManager 的时候。因此，在启动 Flink 之后，需要去 Flink 的页面中检查下 Flink 的状态。这里可以从 RM 的页面中，直接跳转（点击 Tracking UI）</p><p>yarn-session.sh启动命令参数如下：</p><p>[root@hsiehchou121 flink-1.6.1]$ yarn-session.sh –help<br>Usage:<br>   Required<br>     -n,–container <code>&lt;arg&gt;</code>   Number of YARN container to allocate (=Number of Task Managers)<br>   Optional<br>     -D &lt;property=value&gt;             use value for given property<br>     -d,–detached                   If present, runs the job in detached mode<br>     -h,–help                       Help for the Yarn session CLI.<br>     -id,–applicationId <code>&lt;arg&gt;</code>       Attach to running YARN session<br>     -j,–jar <code>&lt;arg&gt;</code>                  Path to Flink jar file<br>     -jm,–jobManagerMemory <code>&lt;arg&gt;</code>    Memory for JobManager Container with optional unit (default: MB)<br>     -m,–jobmanager <code>&lt;arg&gt;</code>           Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified i<br>n the configuration.     -n,–container <code>&lt;arg&gt;</code>            Number of YARN container to allocate (=Number of Task Managers)<br>     -nl,–nodeLabel <code>&lt;arg&gt;</code>           Specify YARN node label for the YARN application<br>     -nm,–name <code>&lt;arg&gt;</code>                Set a custom name for the application on YARN<br>     -q,–query                      Display available YARN resources (memory, cores)<br>     -qu,–queue <code>&lt;arg&gt;</code>               Specify YARN queue.<br>     -s,–slots <code>&lt;arg&gt;</code>                Number of slots per TaskManager<br>     -st,–streaming                 Start Flink in streaming mode<br>     -t,–ship <code>&lt;arg&gt;</code>                 Ship files in the specified directory (t for transfer)<br>     -tm,–taskManagerMemory <code>&lt;arg&gt;</code>   Memory per TaskManager Container with optional unit (default: MB)<br>     -yd,–yarndetached              If present, runs the job in detached mode (deprecated; use non-YARN specific option instead)<br>     -z,–zookeeperNamespace <code></code>   Namespace to create the Zookeeper sub-paths for high availability mode</p><p>4）<strong>提交任务</strong></p><p>之后，我们可以通过这种方式提交我们的任务<br>[root@hsiehchou121 flink-1.6.1]$ ./bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar –input /opt/wcinput/wc.txt –output /opt/wcoutput/<br>bin/flink run -m yarn-cluster -yn2 examples/batch/WordCount.jar –input /input/ –output /XZ</p><p>以上命令在参数前加上y前缀，-yn表示TaskManager个数</p><p>在这个模式下，同样可以使用-m yarn-cluster提交一个”运行后即焚”的detached yarn（-yd）作业到yarn cluster</p><p>5）<strong>停止yarn cluster</strong></p><p>yarn application -kill application_1539058959130_0001</p><p>6） <strong>Yarn模式的HA</strong></p><p>应用最大尝试次数（<strong>yarn-site.xml</strong>），您必须配置为尝试应用的最大数量的设置yarn-site.xml，当前YARN版本的默认值为2（表示允许单个JobManager失败）<br><code>&lt;property&gt;</code><br><code>&lt;name&gt;</code>yarn.resourcemanager.am.max-attempts<code>&lt;/name&gt;</code><br>  <code>&lt;value&gt;</code>4<code>&lt;/value&gt;</code><br> <code>&lt;description&gt;</code>The maximum number of application master execution attempts<code>&lt;/description&gt;</code><br><code>&lt;/property&gt;</code></p><p>申请尝试（<strong>flink-conf.yaml</strong>），您还必须配置最大尝试次数conf/flink-conf.yaml： yarn.application-attempts：10</p><p>示例：<strong>高度可用的YARN会话</strong><br>配置HA模式和zookeeper法定人数在conf/flink-conf.yaml：<br>high-availability: zookeeper<br>high-availability.zookeeper.quorum: hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181<br>high-availability.storageDir: hdfs:///flink/recovery<br>high-availability.zookeeper.path.root: /flink<br>yarn.application-attempts: 10</p><p>配置ZooKeeper的服务器中conf/zoo.cfg（目前它只是可以运行每台机器的单一的ZooKeeper服务器）：<br>server.1=hsiehchou121:2888:3888<br>server.2=hsiehchou122:2888:3888<br>server.3=hsiehchou123:2888:3888</p><p><strong>启动ZooKeeper仲裁</strong>：<br>$ bin / start-zookeeper-quorum.sh</p><p><strong>启动HA群集</strong>：<br>$ bin / yarn-session.sh -n 2</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Storm练习</title>
      <link href="/2019/05/14/storm-lian-xi/"/>
      <url>/2019/05/14/storm-lian-xi/</url>
      
        <content type="html"><![CDATA[<p><strong>Storm练习</strong></p><h3 id="一、需求"><a href="#一、需求" class="headerlink" title="一、需求"></a>一、需求</h3><p>需求：统计网站访问量(实时统计)</p><p>技术选型：特点（数据量大、做计算、实时）</p><p>实时计算框架：storm<br>1）spout<br>    数据源，接入数据<br>    本地文件</p><p>2）bolt<br>    业务逻辑处理<br>    切分数据<br>    查到网址</p><p>3）bolt<br>    累加次数求和</p><h3 id="二、代码编写"><a href="#二、代码编写" class="headerlink" title="二、代码编写"></a>二、代码编写</h3><ol><li>PvCountSpout.java</li></ol><pre><code>package com.hsiehchou.pvcount;import java.io.BufferedReader;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;import java.io.InputStreamReader;import java.util.Map;import org.apache.storm.spout.SpoutOutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.IRichSpout;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Values;public class PvCountSpout implements IRichSpout {    private SpoutOutputCollector collector;    private BufferedReader br;    private String line;    @Override    public void nextTuple() {        //发送读取数据的每一行        try {            while((line = br.readLine()) != null) {                //发送数据到splitbolt                collector.emit(new Values(line));                //设置延迟                Thread.sleep(500);            }        } catch (IOException | InterruptedException e) {            e.printStackTrace();        }    }    @Override    public void open(Map arg0, TopologyContext arg1, SpoutOutputCollector collector) {        this.collector = collector;        //读取文件        try {            br = new BufferedReader(new InputStreamReader(new FileInputStream(&quot;e:/weblog.log&quot;)));        } catch (FileNotFoundException e) {            e.printStackTrace();        }    }    @Override    public void declareOutputFields(OutputFieldsDeclarer declarer) {        //声明        declarer.declare(new Fields(&quot;logs&quot;));    }    //处理Tuple成功 回调的方法    @Override    public void ack(Object arg0) {    }    //如果spout在失效的模式中，调用此方法来激活    @Override    public void activate() {    }    //在spout程序关闭前执行，不能保证一定执行，kill -9是不执行  storm kill是不执行    @Override    public void close() {    }    //在spout失效期间，nextTuple不会被调用    @Override    public void deactivate() {    }    //处理Tuple失败回调的方法    @Override    public void fail(Object arg0) {    }    //配置    @Override    public Map&lt;String, Object&gt; getComponentConfiguration() {        return null;    }}</code></pre><ol start="2"><li>PvCountSplitBolt.java</li></ol><pre><code>package com.hsiehchou.pvcount;import java.util.Map;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.IRichBolt;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Tuple;import org.apache.storm.tuple.Values;public class PvCountSplitBolt implements IRichBolt {    private OutputCollector collector;    private int pvnum = 0;    //一个bolt即将关闭时调用，不能保证一定会被调用    @Override    public void cleanup() {    }    //业务逻辑-分布式-集群-并发度-线程（接收Tuple然后进行处理）资源清理    @Override    public void execute(Tuple input) {        //1.获取数据        String line = input.getStringByField(&quot;logs&quot;);        //2.切分数据        String[] fields = line.split(&quot;\t&quot;);        String session_id = fields[1];        //3.局部累加        if(session_id != null) {            //列累加            pvnum++;            //输出            collector.emit(new Values(Thread.currentThread().getId(),pvnum));        }    }    //初始化时调用    @Override    public void prepare(Map arg0, TopologyContext arg1, OutputCollector collector) {        this.collector = collector;    }    //声明    @Override    public void declareOutputFields(OutputFieldsDeclarer declarer) {        //声明输出字段        declarer.declare(new Fields(&quot;threadid&quot;,&quot;pvnum&quot;));    }    //配置    @Override    public Map&lt;String, Object&gt; getComponentConfiguration() {        return null;    }}</code></pre><ol start="3"><li>PvCountBolt.java</li></ol><pre><code>package com.hsiehchou.pvcount;import java.util.HashMap;import java.util.Iterator;import java.util.Map;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.IRichBolt;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.tuple.Tuple;public class PvCountBolt implements IRichBolt {    private HashMap&lt;Long, Integer&gt; hashmap = new HashMap&lt;&gt;();    @Override    public void cleanup() {    }    //全局累加求和 业务逻辑    @Override    public void execute(Tuple input) {        //1.获取数据        Long threadid = input.getLongByField(&quot;threadid&quot;);        Integer pvnum = input.getIntegerByField(&quot;pvnum&quot;);        //2.创建集合 存储(threadid,pvnum)        hashmap.put(threadid,pvnum);        //3.累加求和        Iterator&lt;Integer&gt; iterator = hashmap.values().iterator();        //4.清空之前的数据        int sumnum = 0;        while(iterator.hasNext()) {            sumnum += iterator.next();        }        System.out.println(Thread.currentThread().getName() + &quot;总访问量为：&quot; + sumnum);    }    @Override    public void prepare(Map arg0, TopologyContext arg1, OutputCollector arg2) {    }    @Override    public void declareOutputFields(OutputFieldsDeclarer arg0) {    }    @Override    public Map&lt;String, Object&gt; getComponentConfiguration() {        return null;    }}</code></pre><ol start="4"><li>PvCountDriver.java</li></ol><pre><code>package com.hsiehchou.pvcount;import org.apache.storm.Config;import org.apache.storm.LocalCluster;import org.apache.storm.topology.TopologyBuilder;import org.apache.storm.tuple.Fields;public class PvCountDriver {    public static void main(String[] args) {        //1.hadoop --&gt; Job Storm --&gt; Topology 创建拓扑        TopologyBuilder builder = new TopologyBuilder();        builder.setSpout(&quot;PvCountSpout&quot;, new PvCountSpout(), 1);        //builder.setBolt(&quot;PvCountSplitBolt&quot;, new PvCountSplitBolt(), 6).setNumTasks(4).shuffleGrouping(&quot;PvCountSpout&quot;);        builder.setBolt(&quot;PvCountSplitBolt&quot;, new PvCountSplitBolt(), 6).setNumTasks(4)            .fieldsGrouping(&quot;PvCountSpout&quot;, new Fields(&quot;logs&quot;));        //builder.setBolt(&quot;PvCountSumBolt&quot;, new PvCountBolt(), 1).shuffleGrouping(&quot;PvCountSplitBolt&quot;);        builder.setBolt(&quot;PvCountSumBolt&quot;, new PvCountBolt(), 1).fieldsGrouping(&quot;PvCountSplitBolt&quot;, new Fields(&quot;pvnum&quot;));        Config conf = new Config();        conf.setNumWorkers(1);        LocalCluster localCluster = new LocalCluster();        localCluster.submitTopology(&quot;pvcountsum&quot;, conf, builder.createTopology());    }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Storm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Storm集群和集成</title>
      <link href="/2019/05/12/storm-ji-qun-he-ji-cheng/"/>
      <url>/2019/05/12/storm-ji-qun-he-ji-cheng/</url>
      
        <content type="html"><![CDATA[<h3 id="一、Storm集群任务提交流程"><a href="#一、Storm集群任务提交流程" class="headerlink" title="一、Storm集群任务提交流程"></a>一、Storm集群任务提交流程</h3><p><img src="/medias/Storm%E9%9B%86%E7%BE%A4%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B.PNG" alt="Storm集群任务提交流程"></p><h3 id="二、Storm内部通信机制"><a href="#二、Storm内部通信机制" class="headerlink" title="二、Storm内部通信机制"></a>二、Storm内部通信机制</h3><p><img src="/medias/Worker%E8%BF%9B%E7%A8%8B1.PNG" alt="Worker进程"></p><p><img src="/medias/Worker.PNG" alt="Worker"></p><h3 id="三、集成Storm"><a href="#三、集成Storm" class="headerlink" title="三、集成Storm"></a>三、集成Storm</h3><h4 id="1、与JDBC集成"><a href="#1、与JDBC集成" class="headerlink" title="1、与JDBC集成"></a>1、与JDBC集成</h4><ul><li><p>将Storm Bolt处理的结果插入MySQL数据库中</p></li><li><p>需要依赖的jar包<br>    <code>$STORM_HOME</code>\external\sql\storm-sql-core*.jar<br>    <code>$STORM_HOME</code>\external\storm-jdbc\storm-jdbc-1.0.3.jar<br>    mysql的驱动<br>    commons-lang3-3.1.jar</p></li><li><p>与JDBC集成的代码实现<br>    修改主程序WordCountTopology，增加如下代码：</p><pre><code>//创建一个JDBCBolt将结果插入数据库中builder.setBolt(&quot;wordcount_jdbcBolt&quot;, createJDBCBolt()).shuffleGrouping(&quot;wordcount_count&quot;);</code></pre></li></ul><p>增加一个新方法创建JDBCBolt组件</p><pre><code>//创建JDBC Insert Bolt组件//需要事先在MySQL数据库中创建对应的表，resultprivate static IRichBolt createJDBCBolt(){    ConnectionProvider connectionProvider = new MyConnectionProvider();    JdbcMapper simpleJdbcMapper = new SimpleJdbcMapper(&quot;aaa&quot;, connectionProvider);    return new JdbcInsertBolt(connectionProvider, simpleJdbcMapper).withTableName(&quot;result&quot;).withQueryTimeoutSecs(30);}</code></pre><p>    实现ConnectionProvider接口</p><pre><code>class MyConnectionProvider implements ConnectionProvider{    private static String driver = &quot;com.mysql.cj.jdbc.Driver&quot;;    private static String url = &quot;jdbc:mysql://192.168.116.121:3306/demo&quot;;    private static String user = &quot;root&quot;;    private static String password = &quot;password&quot;;    //静态块    static{//注册驱动        try{            Class.forName(driver);        }catch(ClassNotFoundException e){            throw new ExceptionInInitializerError(e);        }    }    @Override    public Connection getConnection(){        try{            return DriverManager.getConnection(url, user, password);        }catch{            e.printStackTrace();        }            return null;        }    public void cleanup(){}    public void prepare(){}}</code></pre><p>    修改WordCountSplitBolt组件，将统计后的结果发送给下一个组件写入MySQL</p><pre><code>public class WordCountSplitBolt extends BaseRichBolt {    private Map&lt;String, Integer&gt; result = new HashMap&lt;String, Integer&gt;();    private OutputCollector collector;    @Override    public void execute(Tuple tuple){        String word = tuple.getStringByField(&quot;word&quot;);        int count = tuple.getIntegerByField(&quot;count&quot;);        if(result.containsKey(word)) {            int total = result.get(word);            result.put(word, total+count);        }else{            result.put(word, 1);        }        //直接输出到屏幕        //System.out.println(&quot;输出的结果是：&quot; + result);        //将统计结果发送下一个Bolt，插入数据        this.collector.emit(new Values*(word, result.get(word));    }    @Override    public void prepare(Map arg0, TopologyContext arg1, OutputCollector collector) {        this.collector = collector;    }    @Override    public void declareOutputFields(OutputFieldsDecler declare){        declare.declarer(new Fields(&quot;word&quot;, &quot;sum&quot;));    }}</code></pre><h4 id="2、与Redis集成"><a href="#2、与Redis集成" class="headerlink" title="2、与Redis集成"></a>2、与Redis集成</h4><p>Redis是一个key-value存储系统。和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set –有序集合)和hash（哈希类型）。与Memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步</p><p>Redis 是一个高性能的key-value数据库。Redis的出现，很大程度补偿了memcached这类key/value存储的不足，在部分场合可以对关系数据库起到很好的补充作用。它提供了Java，C/C++，C#，PHP，JavaScript，Perl，Object-C，Python，Ruby，Erlang等客户端，使用很方便。</p><p>Redis支持主从同步。数据可以从主服务器向任意数量的从服务器上同步，从服务器可以是关联其他从服务器的主服务器</p><p>    修改代码：WordCountTopology.java</p><pre><code>builder.setBolt(&quot;wordcount_redisBolt&quot;, createRedisBolt()).shuffleGrouping(&quot;wordcount_count&quot;);//创建Redis Bolt 组件private staticIRichBolt createRedisBolt(){    JedisPoolConfig.Builder builder = new JedisPoolConfig.Builder();    builder.setHost(&quot;192.168.116.121&quot;);    builder.setPort(6379);    JedisPoolConfig poolConfig = builder.build();    //RedisStoreMapper用于指定存入Redis中的数据格式    return new RedisStoreBolt(poolConfig, new RedisStoreMapper(){        @Override        public RedisDateTypeDescription getDataTypeDescription(){            return new RedisDateTypeDescription(RedisDateTypeDescription.RedisDateType.HASH, &quot;wordCount&quot;);        }        @Override        public String getValueFromTuple(){            return String.valueOf(tuple.getIntegerByField(&quot;total&quot;));        }        @Override        public String getKeyFromTuple(){            return tuple.getStringByField(&quot;word&quot;);            }    })}</code></pre><h4 id="3、与HDFS集成"><a href="#3、与HDFS集成" class="headerlink" title="3、与HDFS集成"></a>3、与HDFS集成</h4><ul><li><p>需要的jar包：<br>    <code>$STORM_HOME</code>\external\storm-hdfs\storm-hdfs-1.0.3.jar<br>    HDFS相关的jar包</p></li><li><p>开发新的bolt组件</p></li></ul><pre><code>//创建一个新的HDFS Bolt组件，把前一个bolt组件处理的结果存入HDFSprivate static IRichBolt createHDFSBolt(){    HdfsBolt bolt = new HdfsBolt();    //HDFS的位置    bolt.withFsUrl(&quot;hdfs://192.168.116.121:9000&quot;);    //数据保存在HDFS上的目录    bolt.withFileNameFormat(new DefaultFileNameFormat().withPath(&quot;/stormdata&quot;));    //写入HDFS的数据的分隔符 | 结果：Beijing|10    bolt.withRecordFormat(new DelimitedRecordFormat().withFieldDelimiter(&quot;|&quot;));    //每5M的数据生成一个文件    bolt.withRotationPolicy(new FileSizeRotationPolicy(5.0f, Units.MB));    //Bolt输出tuple,当tuple达到一定的大小（没1K），与HDFS进行同步    bolt.withSyncPolicy(new CountSyncPolicy(1000));    return bolt;}</code></pre><h4 id="4、与HBase集成"><a href="#4、与HBase集成" class="headerlink" title="4、与HBase集成"></a>4、与HBase集成</h4><ul><li><p>需要的jar包：HBase的相关包</p></li><li><p>开发新的bolt组件（WordCountBoltHBase.java）</p><pre><code>/*** 在HBase中创建表，保存数据* create &#39;result&#39;,&#39;info&#39;*/public class WordCountBoltHBase extends BaseRichBolt {public void execute(Tuple tuple){   //如何处理？将上一个bolt组件发送过来的结果，存入HBase   //取出上一个组件发送过来的数据   String word = tuple.getStringByField(&quot;word&quot;);   int total = tuple.getIntegerByField(&quot;total&quot;);   //构造一个Put对象   Put put = new Put(Bytes.toBytes(word));   put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;word&quot;), Bytes.toBytes(word));   put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;total&quot;), Bytes.toBytes(String.valueOf(total)));   //把数据插入HBase   try{       table.put(put);   }catch(Exception e){       e.printStackTrace();   }}public void prepare(Map arg0, TopologyContext arg1, OutputCollector arg2){   //初始化   //指定ZK的地址   Configuration conf = new Configuration();   conf.set(&quot;hbase.zookeeper.quorum&quot;,&quot;192.168.116.121&quot;);   //创建table的客户端   try{       table = new HTable(conf, &quot;result&quot;);   }catch(Exception ex){       ex.printStackTrace();   }}}</code></pre></li></ul><h4 id="5、与Apache-Kafka集成"><a href="#5、与Apache-Kafka集成" class="headerlink" title="5、与Apache Kafka集成"></a>5、与Apache Kafka集成</h4><ul><li>注意：需要把slf4j-log4j12-1.6.4.jar包去掉，有冲突（有两个）</li></ul><pre><code>private static IRichSpout creatKafkaSpout(){    //定义ZK地址    BrokerHosts hosts = new ZkHosts(&quot;hadoop121:2181,hadoop122:2181,hadoop123:2181&quot;);    //指定Topic的信息    SpoutConfig spoutConf = new SpoutConfig(hosts, &quot;mydemo2&quot;, &quot;/mydemo2&quot;, UUID.randomUUID().toString());    //定义收到消息的Schema格式    spoutConf.scheme = new SchemeAsMultiScheme(new Scheme(){        @Override        public Fields getOutputFields(){            return new Fields(&quot;sentence&quot;);        }        @Override        public List&lt;Object&gt; deserialize(ByteBuffer buffer){            try{                String msg = (Charset.forName(&quot;UTF-8&quot;).newDecoder()).decode(buffer).asReadOnlyBuffer().toString();                System.out.println(&quot;**********收到的数据是msg&quot; + msg);                return new Values(msg);            }catch(Exception e){                e.printStackTrace();            }            return null;        }    });    return new KafkaSpout(spoutConf);}</code></pre><h4 id="6、与Hive集成"><a href="#6、与Hive集成" class="headerlink" title="6、与Hive集成"></a>6、与Hive集成</h4><ul><li>由于集成Storm和Hive依赖的jar较多，并且冲突的jar包很多，强烈建议使用Maven来搭建新的工程</li></ul><pre><code>&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;        &lt;artifactId&gt;storm-core&lt;/artifactId&gt;        &lt;version&gt;1.0.3&lt;/version&gt;        &lt;scope&gt;provided&lt;/scope&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;        &lt;artifactId&gt;storm-hive&lt;/artifactId&gt;        &lt;version&gt;1.0.3&lt;/version&gt;        &lt;type&gt;jar&lt;/type&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;</code></pre><ul><li>需要对Hive做一定的配置（在hive-site.xml文件中）：</li></ul><pre><code>&lt;property&gt;  &lt;name&gt;hive.in.test&lt;/name&gt;  &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;</code></pre><ul><li>需要使用下面的语句在hive中创建表：</li></ul><pre><code>create table wordcount(word string,total int)clustered by (word) into 10 bucketsstored as orc TBLPROPERTIES(&#39;transactional&#39;=&#39;true&#39;);</code></pre><ul><li>启动metastore服务：hive –service metastore</li><li>开发新的bolt组件，用于将前一个bolt处理的结果写入Hive</li></ul><pre><code>private static IRichBolt createHiveBolt(){    //设置环境变量，能找到winutils.exe    System.setProperty(&quot;hadoop.home.dir&quot;, &quot;D:\\tools\\hadoop-2.8.4&quot;);    //作用：将bolt组件处理后的结果tuple，如何存入hive表    DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper().withColumnFields(new Fields(&quot;word&quot;, &quot;total&quot;));    //配置Hive的参数信息    HiveOptions options = new HiveOptions(&quot;thrift://hadoop121:9083&quot;,//hive的metastore                                          &quot;default&quot;,//hive数据库的名字                                          &quot;wordcount&quot;,//保存数据的表                            mapper)                            .withTxnsPerBatch(10)                            .withBatchSize(1000)                            .withIdleTimeout(10);    //创建一个Hive的bolt组件，将单词计数后的结果存入hive    HiveBolt bolt = new HiveBolt(options);    return bolt;}</code></pre><ul><li>为了测试的方便，我们依然采用之前随机产生字符串的Spout组件产生数据</li></ul><h4 id="7、与JMS集成"><a href="#7、与JMS集成" class="headerlink" title="7、与JMS集成"></a>7、与JMS集成</h4><p>JMS即Java消息服务（Java Message Service）应用程序接口，是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。Java消息服务是一个与具体平台无关的API，绝大多数MOM提供商都对JMS提供支持</p><p>JMS的两种消息类型：Queue和Topic<br>基于Weblogic的JMS架构 ：</p><p><img src="/medias/%E5%9F%BA%E4%BA%8EWeblogic%E7%9A%84JMS%E6%9E%B6%E6%9E%84.PNG" alt="基于Weblogic的JMS架构"></p><pre><code>private static IRichBolt createJMSBolt(){    //创建一个JMS Bolt，将前一个bolt发送过来的数据 写入JMS    JmsBolt bolt = new JmsBolt();    //指定JMSBolt的provider    bolt.setJmsProvider(new MyJMSProvider());    //指定bolt如何解析信息    bolt.setJmsMessageProducer(new JmsMessageProducer(){        @Override        public Message toMessage(Session session, ITuple tuple) throws JMSException {            //取出上一个组件发送过来的数据            String word = tuple.getStringByField(&quot;word&quot;);            int total = tuple.getIntegerByField(&quot;total&quot;);            return session.createTextMessage(word + &quot;   &quot; + total);        }    });    return bolt;}</code></pre><ul><li>需要的weblogic的jar包</li></ul><p>wljmsclient.jar<br>wlclient.jar</p><ul><li>permission javax.management.MBeanTrustPermission “register”;</li></ul>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Storm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>私有网络软件仓库搭建和挂载网络系统镜像</title>
      <link href="/2019/05/10/si-you-wang-luo-ruan-jian-cang-ku-da-jian-he-gua-zai-wang-luo-xi-tong-jing-xiang/"/>
      <url>/2019/05/10/si-you-wang-luo-ruan-jian-cang-ku-da-jian-he-gua-zai-wang-luo-xi-tong-jing-xiang/</url>
      
        <content type="html"><![CDATA[<h3 id="一、私有网络软件仓库"><a href="#一、私有网络软件仓库" class="headerlink" title="一、私有网络软件仓库"></a>一、私有网络软件仓库</h3><p>在集群安装的过程中，要求每个节点都必须挂载光驱， 而对于每台节点都手动的去挂载光驱太麻烦，也不方便。这里使用每个节点都指向同一个私有网络镜像来解决这个问题</p><p>我们的集群采用的是全离线安装，也不可能逐个节点的安装，同样是也使用指向同一个私有的网络软件包来解决</p><p>因此选择在hadoop-4上搭建一个私有的网络软件仓库，以下是搭建的全过程</p><h4 id="1、上传镜像"><a href="#1、上传镜像" class="headerlink" title="1、上传镜像"></a>1、上传镜像</h4><p>搭建私有网络镜像使用的镜像文件最好就使用安装系统的镜像，这里是选择了CentOS6.7x64的镜像，将其上传到hadoop1的/disk下（需新建/disk目录）<br>mkdir disk<br>上传CentOS-6.7-x86_64-bin-DVD1.iso</p><h4 id="2、挂载镜像"><a href="#2、挂载镜像" class="headerlink" title="2、挂载镜像"></a>2、挂载镜像</h4><p>首先创建文件夹 /media/CentOS ：<br>mkdir -p /media/CentOS</p><p>挂载镜像 ：<br>mount -o loop /disk/CentOS-6.7-x86_64-bin-DVD1.iso /media/CentOS/</p><p>进入目录/etc/yum.repos.d ：<br>cd /etc/yum.repos.d</p><p>修改CentOS-Base.repo的名称 ：<br>mv CentOS-Base.repo CentOS-Base.repo.bak</p><p>修改CentOS-Media.repo文件 ：<br>vim CentOS-Media.repo</p><p>修改如下 ：<br>将enable=0改成enable=1</p><p>清除yum的缓存 ：<br>yum clean metadata<br>yum clean dbcache</p><p>查看是否挂载成功 ：<br>yum list | wc -l </p><p>这是统计镜像中有多少个软件包的命令，CentOS6.7x64位的系统的软件包个数一般在3000以上</p><h4 id="3、安装http-如果已经安装可以省略，但是需要启动，一b般最小化安装不会安装此服务"><a href="#3、安装http-如果已经安装可以省略，但是需要启动，一b般最小化安装不会安装此服务" class="headerlink" title="3、安装http(如果已经安装可以省略，但是需要启动，一b般最小化安装不会安装此服务)"></a>3、安装http(如果已经安装可以省略，但是需要启动，一b般最小化安装不会安装此服务)</h4><p>检查是否安装<br>service httpd status</p><p>网络镜像需要通过http请求访问，因此需要安装http:<br>yum –y install http</p><p>启动http服务，并让其开机自启 ：<br>service httpd start<br>chkconfig httpd on</p><p>由于http的默认端口为80，通过浏览器访问 ：<br>192.168.116.201:80</p><p>创建网络软件仓库目录 ：<br>mkdir –p /var/www/html</p><p>http默认将上面的目录作为软件仓库的目录</p><h4 id="4、安装createrepo-如果已经安装省略"><a href="#4、安装createrepo-如果已经安装省略" class="headerlink" title="4、安装createrepo(如果已经安装省略)"></a>4、安装createrepo(如果已经安装省略)</h4><p>该软件使用来生成http镜像的网络识别路径的：<br>yum -y install createrepo</p><p>到此 私有的网络软件仓库搭建完成</p><h3 id="二、挂载网络系统镜像"><a href="#二、挂载网络系统镜像" class="headerlink" title="二、挂载网络系统镜像"></a>二、挂载网络系统镜像</h3><h4 id="1、创建网络系统镜像"><a href="#1、创建网络系统镜像" class="headerlink" title="1、创建网络系统镜像"></a>1、创建网络系统镜像</h4><p>将从镜像中挂载的文件拷贝到软件仓库的目录中<br>cp -r /media/CentOS /var/www/html/</p><p>删除目录repodata<br>cd /var/www/html/CentOS<br>rm -rf ./repodata</p><p>生成新的软件路径目录repodata<br>createrepo .</p><p>也可以通过网络访问查看（192.168.116.201/CentOS/）：</p><p>到此网络镜像创建成功</p><h4 id="2、使用网络系统镜像"><a href="#2、使用网络系统镜像" class="headerlink" title="2、使用网络系统镜像"></a>2、使用网络系统镜像</h4><p>解除对镜像文件的挂载 ：<br>umount /media/CentOS</p><p>如下， 目录下无文件则说明解除挂载成功<br>  [root@hadoop1 yum.repos.d]# ll  /media/CentOS</p><p>如果出现如下说明有进程在占用挂载点<br> [root@hadoop1 yum.repos.d]# fuser -m /media/CentOS/<br> /media/CentOS 3157</p><p>出现这种情况，表示还有进程在使用/medis/CentOS挂载点，那么此时可以借助fuser命令找出占用目录/medis/CentOS的所有进程，然后kill掉，此时就可以umount 了<br>fuser -m /media/CentOS/</p><p>修改文件CentOS-Media.repo让其指向刚才创建的网络镜像<br>vim /etc/yum.repos.d/CentOS-Media.repo</p><p>修改如下：（修改前的配置参考3.1.2）<br> baseurl=<a href="https://192.168.116.201/CentOS/" target="_blank" rel="noopener">https://192.168.116.201/CentOS/</a></p><p>清楚yum的缓存， 并查看软件包个数<br>yum clean metadata<br>yum clean dbcache<br>yum list | wc -l </p>]]></content>
      
      
      <categories>
          
          <category> CentOS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CentOS </tag>
            
            <tag> 私有网络软件仓库 </tag>
            
            <tag> 挂载网络系统镜像 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据图片汇总</title>
      <link href="/2019/05/06/da-shu-ju-tu-pian-hui-zong/"/>
      <url>/2019/05/06/da-shu-ju-tu-pian-hui-zong/</url>
      
        <content type="html"><![CDATA[<h3 id="1、大数据课程概述与大数据背景知识"><a href="#1、大数据课程概述与大数据背景知识" class="headerlink" title="1、大数据课程概述与大数据背景知识"></a>1、大数据课程概述与大数据背景知识</h3><h4 id="（1）数据仓库与大数据"><a href="#（1）数据仓库与大数据" class="headerlink" title="（1）数据仓库与大数据"></a>（1）数据仓库与大数据</h4><p><img src="/medias/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E5%A4%A7%E6%95%B0%E6%8D%AE.PNG" alt="数据仓库与大数据"></p><h4 id="（2）PageRank"><a href="#（2）PageRank" class="headerlink" title="（2）PageRank"></a>（2）PageRank</h4><p><img src="/medias/PageRank.PNG" alt="PageRank"></p><h4 id="（3）MR基本原理"><a href="#（3）MR基本原理" class="headerlink" title="（3）MR基本原理"></a>（3）MR基本原理</h4><p><img src="/medias/MR%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86.PNG" alt="MR基本原理"></p><h4 id="（4）HDFS原理"><a href="#（4）HDFS原理" class="headerlink" title="（4）HDFS原理"></a>（4）HDFS原理</h4><p><img src="/medias/HDFS%E5%8E%9F%E7%90%86.PNG" alt="HDFS原理"></p><h4 id="（5）bigtable与Habase"><a href="#（5）bigtable与Habase" class="headerlink" title="（5）bigtable与Habase"></a>（5）bigtable与Habase</h4><p><img src="/medias/bigtable%E4%B8%8EHabase.PNG" alt="bigtable与Habase"></p><h3 id="2、搭建Hadoop的环境"><a href="#2、搭建Hadoop的环境" class="headerlink" title="2、搭建Hadoop的环境"></a>2、搭建Hadoop的环境</h3><h4 id="（1）1-PNG"><a href="#（1）1-PNG" class="headerlink" title="（1）1.PNG"></a>（1）1.PNG</h4><p><img src="/medias/1.PNG" alt="1"></p><h4 id="（2）2-PNG"><a href="#（2）2-PNG" class="headerlink" title="（2）2.PNG"></a>（2）2.PNG</h4><p><img src="/medias/2.PNG" alt="2"></p><h4 id="（3）3-PNG"><a href="#（3）3-PNG" class="headerlink" title="（3）3.PNG"></a>（3）3.PNG</h4><p><img src="/medias/3.PNG" alt="3"></p><h4 id="（4）4-PNG"><a href="#（4）4-PNG" class="headerlink" title="（4）4.PNG"></a>（4）4.PNG</h4><p><img src="/medias/4.PNG" alt="4"></p><h3 id="3、HDFS基础与操作"><a href="#3、HDFS基础与操作" class="headerlink" title="3、HDFS基础与操作"></a>3、HDFS基础与操作</h3><h4 id="（1）startup"><a href="#（1）startup" class="headerlink" title="（1）startup"></a>（1）startup</h4><p><img src="/medias/startup.PNG" alt="startup"></p><h3 id="4、HDFS上传与下载的原理"><a href="#4、HDFS上传与下载的原理" class="headerlink" title="4、HDFS上传与下载的原理"></a>4、HDFS上传与下载的原理</h3><h4 id="（1）HDFS-Upload"><a href="#（1）HDFS-Upload" class="headerlink" title="（1）HDFS_Upload"></a>（1）HDFS_Upload</h4><p><img src="/medias/HDFS_Upload.PNG" alt="HDFS_Upload"></p><h4 id="（2）HDFS-DownLoad"><a href="#（2）HDFS-DownLoad" class="headerlink" title="（2）HDFS_DownLoad"></a>（2）HDFS_DownLoad</h4><p><img src="/medias/HDFS_DownLoad.PNG" alt="HDFS_DownLoad"></p><h3 id="5、HDFS-工作机制"><a href="#5、HDFS-工作机制" class="headerlink" title="5、HDFS-工作机制"></a>5、HDFS-工作机制</h3><h4 id="（1）namenode工作机制"><a href="#（1）namenode工作机制" class="headerlink" title="（1）namenode工作机制"></a>（1）namenode工作机制</h4><p><img src="/medias/namenode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.PNG" alt="namenode工作机制"></p><h4 id="（2）datanode工作机制"><a href="#（2）datanode工作机制" class="headerlink" title="（2）datanode工作机制"></a>（2）datanode工作机制</h4><p><img src="/medias/datanode%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6.PNG" alt="datanode工作机制"></p><h3 id="6、MapReduce基础"><a href="#6、MapReduce基础" class="headerlink" title="6、MapReduce基础"></a>6、MapReduce基础</h3><h4 id="（1）mapreduce思想"><a href="#（1）mapreduce思想" class="headerlink" title="（1）mapreduce思想"></a>（1）mapreduce思想</h4><p><img src="/medias/mapreduce%E6%80%9D%E6%83%B3.PNG" alt="mapreduce思想"></p><h3 id="7、MapReduce分布式编程模型"><a href="#7、MapReduce分布式编程模型" class="headerlink" title="7、MapReduce分布式编程模型"></a>7、MapReduce分布式编程模型</h3><h4 id="（1）maptask决定机制"><a href="#（1）maptask决定机制" class="headerlink" title="（1）maptask决定机制"></a>（1）maptask决定机制</h4><p><img src="/medias/maptask%E5%86%B3%E5%AE%9A%E6%9C%BA%E5%88%B6.PNG" alt="maptask决定机制"></p><h3 id="8、MapReduce案例分析"><a href="#8、MapReduce案例分析" class="headerlink" title="8、MapReduce案例分析"></a>8、MapReduce案例分析</h3><h4 id="（1）yarn工作流程"><a href="#（1）yarn工作流程" class="headerlink" title="（1）yarn工作流程"></a>（1）yarn工作流程</h4><p><img src="/medias/yarn%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.PNG" alt="yarn工作流程"></p><h4 id="（2）日志格式"><a href="#（2）日志格式" class="headerlink" title="（2）日志格式"></a>（2）日志格式</h4><p><img src="/medias/%E6%97%A5%E5%BF%97%E6%A0%BC%E5%BC%8F.PNG" alt="日志格式"></p><h3 id="9、分区排序"><a href="#9、分区排序" class="headerlink" title="9、分区排序"></a>9、分区排序</h3><h4 id="（1）mapreduce流程"><a href="#（1）mapreduce流程" class="headerlink" title="（1）mapreduce流程"></a>（1）mapreduce流程</h4><p><img src="/medias/mapreduce%E6%B5%81%E7%A8%8B.PNG" alt="mapreduce流程"></p><h3 id="10、Shuffle机制"><a href="#10、Shuffle机制" class="headerlink" title="10、Shuffle机制"></a>10、Shuffle机制</h3><h4 id="（1）shuffle机制"><a href="#（1）shuffle机制" class="headerlink" title="（1）shuffle机制"></a>（1）shuffle机制</h4><p><img src="/medias/shuffle%E6%9C%BA%E5%88%B6.PNG" alt="shuffle机制"></p><h3 id="11、mapjoin与reducejoin"><a href="#11、mapjoin与reducejoin" class="headerlink" title="11、mapjoin与reducejoin"></a>11、mapjoin与reducejoin</h3><h4 id="（1）yarn架构介绍"><a href="#（1）yarn架构介绍" class="headerlink" title="（1）yarn架构介绍"></a>（1）yarn架构介绍</h4><p><img src="/medias/yarn%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D.PNG" alt="yarn架构介绍"></p><h3 id="12、Zookeeper介绍"><a href="#12、Zookeeper介绍" class="headerlink" title="12、Zookeeper介绍"></a>12、Zookeeper介绍</h3><h4 id="（1）zookeeper功能"><a href="#（1）zookeeper功能" class="headerlink" title="（1）zookeeper功能"></a>（1）zookeeper功能</h4><p><img src="/medias/zookeeper%E5%8A%9F%E8%83%BD.PNG" alt="zookeeper功能"></p><h4 id="（2）选举机制"><a href="#（2）选举机制" class="headerlink" title="（2）选举机制"></a>（2）选举机制</h4><p><img src="/medias/%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6.PNG" alt="选举机制"></p><h3 id="13、Hive"><a href="#13、Hive" class="headerlink" title="13、Hive"></a>13、Hive</h3><h4 id="（1）Hive架构原理"><a href="#（1）Hive架构原理" class="headerlink" title="（1）Hive架构原理"></a>（1）Hive架构原理</h4><p><img src="/medias/Hive%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86.PNG" alt="Hive架构原理"></p><h3 id="14、Flume"><a href="#14、Flume" class="headerlink" title="14、Flume"></a>14、Flume</h3><h4 id="（1）多channel多sink流程"><a href="#（1）多channel多sink流程" class="headerlink" title="（1）多channel多sink流程"></a>（1）多channel多sink流程</h4><p><img src="/medias/%E5%A4%9Achannel%E5%A4%9Asink%E6%B5%81%E7%A8%8B.PNG" alt="多channel多sink流程"></p><h3 id="15、HBase"><a href="#15、HBase" class="headerlink" title="15、HBase"></a>15、HBase</h3><h4 id="（1）HBase架构图"><a href="#（1）HBase架构图" class="headerlink" title="（1）HBase架构图"></a>（1）HBase架构图</h4><p><img src="/medias/HBase%E6%9E%B6%E6%9E%84%E5%9B%BE.PNG" alt="HBase架构图"></p><h4 id="（2）HBase数据读取流程"><a href="#（2）HBase数据读取流程" class="headerlink" title="（2）HBase数据读取流程"></a>（2）HBase数据读取流程</h4><p><img src="/medias/HBase%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E6%B5%81%E7%A8%8B.PNG" alt="HBase数据读取流程"></p><h4 id="（3）HBase读取数据的详细流程"><a href="#（3）HBase读取数据的详细流程" class="headerlink" title="（3）HBase读取数据的详细流程"></a>（3）HBase读取数据的详细流程</h4><p><img src="/medias/HBase%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E7%9A%84%E8%AF%A6%E7%BB%86%E6%B5%81%E7%A8%8B.PNG" alt="HBase读取数据的详细流程"></p><h4 id="（4）HBase写入数据的详细流程"><a href="#（4）HBase写入数据的详细流程" class="headerlink" title="（4）HBase写入数据的详细流程"></a>（4）HBase写入数据的详细流程</h4><p><img src="/medias/HBase%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE%E7%9A%84%E8%AF%A6%E7%BB%86%E6%B5%81%E7%A8%8B.PNG" alt="HBase写入数据的详细流程"></p><h3 id="16、Scala"><a href="#16、Scala" class="headerlink" title="16、Scala"></a>16、Scala</h3><h4 id="（1）Scala高阶函数"><a href="#（1）Scala高阶函数" class="headerlink" title="（1）Scala高阶函数"></a>（1）Scala高阶函数</h4><p><img src="/medias/Scala%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0.PNG" alt="Scala高阶函数"></p><h4 id="（2）Actor01"><a href="#（2）Actor01" class="headerlink" title="（2）Actor01"></a>（2）Actor01</h4><p><img src="/medias/Actor01.PNG" alt="Actor01"></p><h4 id="（3）NewAkkaSystem"><a href="#（3）NewAkkaSystem" class="headerlink" title="（3）NewAkkaSystem"></a>（3）NewAkkaSystem</h4><p><img src="/medias/NewAkkaSystem.PNG" alt="NewAkkaSystem"></p><h4 id="（4）PingPongExample"><a href="#（4）PingPongExample" class="headerlink" title="（4）PingPongExample"></a>（4）PingPongExample</h4><p><img src="/medias/PingPongExample.PNG" alt="PingPongExample"></p><h3 id="17、Spark"><a href="#17、Spark" class="headerlink" title="17、Spark"></a>17、Spark</h3><h4 id="（1）Spark体系架构"><a href="#（1）Spark体系架构" class="headerlink" title="（1）Spark体系架构"></a>（1）Spark体系架构</h4><p><img src="/medias/Spark%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84.PNG" alt="Spark体系架构"></p><h4 id="（2）大数据HA"><a href="#（2）大数据HA" class="headerlink" title="（2）大数据HA"></a>（2）大数据HA</h4><p><img src="/medias/%E5%A4%A7%E6%95%B0%E6%8D%AEHA.PNG" alt="大数据HA"></p><h4 id="（3）蒙特卡洛求PI"><a href="#（3）蒙特卡洛求PI" class="headerlink" title="（3）蒙特卡洛求PI"></a>（3）蒙特卡洛求PI</h4><p><img src="/medias/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%B1%82PI.PNG" alt="蒙特卡洛求PI"></p><h4 id="（4）spark的调用任务过程"><a href="#（4）spark的调用任务过程" class="headerlink" title="（4）spark的调用任务过程"></a>（4）spark的调用任务过程</h4><p><img src="/medias/spark%E7%9A%84%E8%B0%83%E7%94%A8%E4%BB%BB%E5%8A%A1%E8%BF%87%E7%A8%8B.PNG" alt="spark的调用任务过程"></p><h4 id="（5）RDD"><a href="#（5）RDD" class="headerlink" title="（5）RDD"></a>（5）RDD</h4><p><img src="/medias/RDD.PNG" alt="RDD"></p><h4 id="（6）WordCount程序分析"><a href="#（6）WordCount程序分析" class="headerlink" title="（6）WordCount程序分析"></a>（6）WordCount程序分析</h4><p><img src="/medias/WordCount%E7%A8%8B%E5%BA%8F%E5%88%86%E6%9E%90.PNG" alt="WordCount程序分析"></p><h4 id="（7）aggregate聚合操作"><a href="#（7）aggregate聚合操作" class="headerlink" title="（7）aggregate聚合操作"></a>（7）aggregate聚合操作</h4><p><img src="/medias/aggregate%E8%81%9A%E5%90%88%E6%93%8D%E4%BD%9C.PNG" alt="aggregate聚合操作"></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> 图片 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Storm基础</title>
      <link href="/2019/05/04/storm-ji-chu/"/>
      <url>/2019/05/04/storm-ji-chu/</url>
      
        <content type="html"><![CDATA[<p><strong>流式计算专题</strong><br>批量计算、实时计算、离线计算、流式计算</p><p>共同点：<br>数据源  –&gt;   采集数据   –&gt;   task worker –&gt;   task worker  –&gt;  sink 输出</p><p><strong>批量计算和流式计算</strong><br>区别：<br>处理数据粒度不一样</p><p>批量计算每次处理一定大小的数据块。流式计算，每次处理一条记录</p><p>流式计算可以提供类似批量计算的功能，为什么我们还要批量计算系统？</p><p>1、流式系统的吞吐量不如批量系统</p><p>2、流式系统无法提供精准的计算</p><ol><li>任务类型不一样</li><li>流式计算会一直运行</li><li>数据源的区别<br>对于批量计算而言，数据是有限数据<br>而对于流式计算，是无限数据</li></ol><h3 id="一、Storm—-是最早流式计算框架"><a href="#一、Storm—-是最早流式计算框架" class="headerlink" title="一、Storm—-是最早流式计算框架"></a>一、Storm—-是最早流式计算框架</h3><h4 id="1、Storm概述"><a href="#1、Storm概述" class="headerlink" title="1、Storm概述"></a>1、Storm概述</h4><p><strong>1）什么是Storm</strong><br>网址：<a href="http://storm.apache.org/" target="_blank" rel="noopener">http://storm.apache.org/</a><br>Apache Storm是一个<strong>免费的开源分布式实时计算系统</strong>。Storm可以<strong>轻松可靠地处理无限数据流</strong>，<strong>实现Hadoop对批处理所做的实时处理</strong>。Storm非常<strong>简单</strong>，可以<strong>与任何编程语言一起使用</strong>，并且使用起来很有趣！</p><p>Storm为<strong>分布式实时计算</strong>提供了一组通用原语，可被用于“<strong>流处理</strong>”之中，<strong>实时处理消息并更新数据库</strong>。这是管理队列及工作者集群的另一种方式。 Storm也可被用于“<strong>连续计算</strong>”（continuous computation），对数据流做连续查询，在计算时就将结果以流的形式输出给用户。它还可被用于“<strong>分布式RPC</strong>”，以并行的方式运行昂贵的运算</p><p>Storm可以<strong>方便地在</strong>一个计算机<strong>集群中编写与扩展复杂</strong>的<strong>实时计算</strong>，Storm用于<strong>实时处理</strong>，就好比 Hadoop 用于批处理。Storm<strong>保证每个消息都会得到处理</strong>，而且它<strong>很快</strong>——<strong>在一个小集群中</strong>，<strong>每秒</strong>可以<strong>处理数以百万计的消息</strong>。更棒的是你<strong>可以使用任意编程语言来</strong>做<strong>开发</strong></p><p>Storm有许多用例：<strong>实时分析</strong>，<strong>在线机器学习</strong>，<strong>连续计算</strong>，<strong>分布式RPC</strong>，<strong>ETL</strong>等。风暴很快：一个基准测试表示每个节点<strong>每秒处理超过一百万个元组。</strong>它具有<strong>可扩展性</strong>，<strong>容错性</strong>，可<strong>确保</strong>您的<strong>数据得到处理</strong>，并且<strong>易于设置和操作</strong></p><p>Storm<strong>集成</strong>了您<strong>已经使用的排队和数据库技术</strong>。<strong>Storm拓扑消耗数据流</strong>并<strong>以任意复杂的方式处理这些流</strong>，然后<strong>在计算的每个阶段之间重新划分流</strong></p><h4 id="2、离线计算和流式计算"><a href="#2、离线计算和流式计算" class="headerlink" title="2、离线计算和流式计算"></a>2、离线计算和流式计算</h4><p>①　<strong>离线计算</strong></p><ul><li>离线计算：批量获取数据、批量传输数据、周期性批量计算数据、数据展示</li><li>代表技术：Sqoop批量导入数据、HDFS批量存储数据、MapReduce批量计算、Hive、Flume批量获取数据、Sqoop批量传输、HDFS/Hive/HBase批量存储、MR/Hive计算数据、BI</li></ul><p>②　<strong>流式计算</strong></p><ul><li>流式计算：数据实时产生、数据实时传输、数据实时计算、实时展示</li><li>代表技术：Flume实时获取数据、Kafka/metaq实时数据存储、Storm/JStorm实时数据计算、Redis实时结果缓存、持久化存储(mysql)、阿里实时展示(DataV/QuickBI)</li></ul><p>一句话总结：将源源不断产生的数据实时收集并实时计算，尽可能快的得到计算结果</p><p>③　<strong>Storm与Hadoop的区别</strong></p><table><thead><tr><th align="center">Storm用于实时计算</th><th align="center">Hadoop用于离线计算</th></tr></thead><tbody><tr><td align="center">Storm处理的数据保存在内存中，源源不断中，一批一批</td><td align="center">Hadoop处理的数据保存在文件系统</td></tr><tr><td align="center">Storm的数据通过网络传输进来</td><td align="center">Hadoop的数据保存在磁盘中</td></tr><tr><td align="center">Storm与Hadoop的编程模型相似</td><td align="center"></td></tr></tbody></table><p><strong>Storm与Hadoop</strong><br><strong>角色</strong></p><table><thead><tr><th align="center">hadoop</th><th align="center">storm</th></tr></thead><tbody><tr><td align="center">JobTracker</td><td align="center">Nimbus</td></tr><tr><td align="center">TaskTracker</td><td align="center">Supervisor</td></tr><tr><td align="center">Child</td><td align="center">Worker</td></tr></tbody></table><p><strong>应用名称</strong></p><table><thead><tr><th align="center">hadoop</th><th align="center">storm</th></tr></thead><tbody><tr><td align="center">Job</td><td align="center">Topology</td></tr></tbody></table><p><strong>编程接口</strong></p><table><thead><tr><th align="center">hadoop</th><th align="center">storm</th></tr></thead><tbody><tr><td align="center">Mapper/Reducer</td><td align="center">Spout/Bolt</td></tr></tbody></table><h4 id="3、Storm的体系结构"><a href="#3、Storm的体系结构" class="headerlink" title="3、Storm的体系结构"></a>3、Storm的体系结构</h4><p><img src="/medias/nimbus.PNG" alt="nimbus"></p><p><img src="/medias/topology.PNG" alt="topology"></p><ul><li><p><strong>Nimbus</strong>：负责资源分配和任务调度</p></li><li><p><strong>Supervisor</strong>：负责接受Nimbus分配的任务，启动和停止属于自己管理的worker进程。通过配置文件设置当前Supervisor上启动多少个Worker</p></li><li><p><strong>Worker</strong>：运行具体处理组件逻辑的进程。Worker运行的任务类型只有两种，一种是Spout任务，一种是Bolt任务</p></li><li><p><strong>Executor</strong>：Storm 0.8之后，Executor为Worker进程中的具体的物理线程，同一个Spout/Bolt的Task可能会共享一个物理线程，一个Executor中只能运行隶属于同一个Spout/Bolt的Task</p></li><li><p><strong>Task</strong>：Worker中每一个Spout/Bolt的线程称为一个Task. 在Storm0.8之后，Task不再与物理线程对应，不同Spout/Bolt的Task可能会共享一个物理线程，该线程称为Executor</p></li></ul><p><img src="/medias/Worker%E8%BF%9B%E7%A8%8B.PNG" alt="Worker进程"></p><h4 id="4、Storm编程模型"><a href="#4、Storm编程模型" class="headerlink" title="4、Storm编程模型"></a>4、Storm编程模型</h4><p><img src="/medias/Storm%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B.PNG" alt="Storm编程模型"></p><p><strong>tuple</strong>：元组<br>是消息传输的基本单元</p><p><strong>Spout</strong>：水龙头<br>Storm的核心抽象。拓扑的流的来源。Spout通常从外部数据源读取数据。转换为t敺内部的源数据。<br>主要方法：<br>nextTuple() -》 发出一个新的元组到拓扑<br>ack()<br>fail()</p><p><strong>Bolt</strong>：转接头<br>Bolt是对流的处理节点。Bolt作用：过滤、业务、连接运算</p><p><strong>Topology</strong>：拓扑<br>是一个实时的应用程序<br>永远运行除非被杀死<br>Spout到Bolt是一个连接流</p><h4 id="5、Storm的运行机制"><a href="#5、Storm的运行机制" class="headerlink" title="5、Storm的运行机制"></a>5、Storm的运行机制</h4><p><img src="/medias/Nimbus-Supervisor.PNG" alt="Nimbus-Supervisor"></p><ul><li>整个处理流程的组织协调不用用户去关心，用户只需要去定义每一个步骤中的具体业务处理逻辑</li><li>具体执行任务的角色是Worker，Worker执行任务时具体的行为则有我们定义的业务逻辑决定</li></ul><p><img src="/medias/Storm%E7%89%A9%E7%90%86%E9%9B%86%E7%BE%A4%E7%BB%93%E6%9E%84.PNG" alt="Storm物理集群结构"></p><h4 id="6、Storm的集群安装配置"><a href="#6、Storm的集群安装配置" class="headerlink" title="6、Storm的集群安装配置"></a>6、Storm的集群安装配置</h4><p><strong>（1）Storm集群安装部署</strong><br>1）准备工作</p><table><thead><tr><th align="center">hsiehchou121</th><th align="center">hsiehchou122</th><th align="center">hsiehchou123</th></tr></thead><tbody><tr><td align="center">storm01</td><td align="center">storm02</td><td align="center">storm03</td></tr></tbody></table><p>2）下载安装包<br><a href="http://storm.apache.org/downloads.html" target="_blank" rel="noopener">http://storm.apache.org/downloads.html</a></p><p>3）上传</p><p>4）解压<br>tar -zxvf apache-storm-1.1.0.tar.gz<br>mv apache-storm-1.1.0 storm</p><p>5）<strong>设置环境变量</strong><br><strong>#STORM_HOME</strong><br>export STORM_HOME=/root/hd/storm<br>export PATH=<code>$STORM_HOME/bin:$PATH</code><br>source /etc/profile</p><p>6）<strong>修改配置文件</strong><br>$ <strong>vi storm.yaml</strong></p><pre><code>#设置Zookeeper的主机名称storm.zookeeper.servers:- &quot;hsiehchou121&quot;- &quot;hsiehchou122&quot;- &quot;hsiehchou123&quot;#设置主节点的主机名称nimbus.seeds: [&quot;hsiehchou121&quot;]#设置Storm的数据存储路径storm.local.dir: &quot;/root/hd/storm/data&quot;#设置Worker的端口号supervisor.slots.ports:- 6700- 6701- 6702- 6703</code></pre><p>++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br><strong>注意</strong>：如果要搭建<strong>Storm的HA</strong>，只需要在<strong>nimbus.seeds</strong>中<strong>设置多个nimbus</strong>即可<br>++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br>mkdir data</p><p>7）<strong>分发到其他机器</strong><br>[root@hsiehchou121 hd]# scp -r storm/ hsiehchou122:<code>$PWD</code><br>[root@hsiehchou121 hd]# scp -r storm/ hsiehchou123:<code>$PWD</code></p><h4 id="7、启动和查看Storm"><a href="#7、启动和查看Storm" class="headerlink" title="7、启动和查看Storm"></a>7、启动和查看Storm</h4><p>0）<strong>启动ZooKeeper</strong><br>zkServer.sh start</p><p>1）在nimbus.host所属的机器上启动 nimbus服务和logviewer服务</p><p><strong>启动nimbus</strong></p><ul><li>storm nimbus &amp;</li></ul><p>*<em>启动logviewer *</em></p><ul><li>storm logviewer &amp;</li></ul><p>2）在nimbus.host所属的机器上启动ui服务<br><strong>启动ui界面</strong></p><ul><li>storm ui &amp;</li></ul><p>3）在其它个节点上启动supervisor服务和logviewer服务<br><strong>启动supervisor</strong></p><ul><li>storm supervisor &amp;</li></ul><p><strong>启动logviewer</strong></p><ul><li>storm logviewer &amp;</li></ul><p>4）查看Storm集群：访问nimbus.host:/8080，即可看到storm的ui界面<br><a href="http://hsiehchou121:8080/index.html" target="_blank" rel="noopener">http://hsiehchou121:8080/index.html</a></p><h4 id="8、Storm的常用命令"><a href="#8、Storm的常用命令" class="headerlink" title="8、Storm的常用命令"></a>8、Storm的常用命令</h4><p>有许多简单且有用的命令可以用来管理拓扑，它们可以提交、杀死、禁用、再平衡拓扑</p><p><strong>1）查看命令帮助</strong><br>storm help</p><p><strong>2）查看版本</strong><br>storm version</p><p><strong>3）查看当前正在运行拓扑及其状态</strong><br>storm list</p><p><strong>4）提交任务命令格式</strong><br>storm jar 【jar路径】 【拓扑包名.拓扑类名】 【拓扑名称】<br>storm jar <code>[/路径/.jar][全类名]</code>[拓扑名称]</p><p><strong>5）杀死任务命令格式</strong><br>storm kill 【拓扑名称】 -w 10<br>（执行kill命令时可以通过-w [等待秒数]指定拓扑停用以后的等待时间）<br>storm kill topology-name -w 10</p><p><strong>6）停用任务命令格式</strong><br>storm deactivte  【拓扑名称】<br>storm deactivate topology-name</p><p><strong>7）启用任务命令格式</strong><br>storm activate【拓扑名称】<br>storm activate topology-name</p><p><strong>8）重新部署任务命令格式</strong><br>storm rebalance  【拓扑名称】<br>storm rebalance topology-name<br>再平衡使你重分配集群任务。这是个很强大的命令。比如，你向一个运行中的集群增加了节点。再平衡命令将会停用拓扑，然后在相应超时时间之后重分配工人，并重启拓扑</p><h3 id="二、Storm编程案例"><a href="#二、Storm编程案例" class="headerlink" title="二、Storm编程案例"></a>二、Storm编程案例</h3><h4 id="1、WordCount及流程分析"><a href="#1、WordCount及流程分析" class="headerlink" title="1、WordCount及流程分析"></a>1、WordCount及流程分析</h4><p>通过查看Storm UI上每个组件的events链接，可以查看Storm的每个组件（spout、blot）发送的消息。但Storm的event logger的功能默认是禁用的，需要在配置文件中设置：topology.eventlogger.executors: 1，具体说明如下：</p><ul><li>“topology.eventlogger.executors”: 0     默认，禁用</li><li>“topology.eventlogger.executors”: 1     一个topology分配一个Event Logger</li><li>“topology.eventlogger.executors”: nil     每个worker.分配一个Event Logger</li></ul><p><strong>WordCount的数据流程分析</strong></p><p><img src="/medias/WordCount%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90.PNG" alt="WordCount的数据流程分析"></p><h4 id="2、Storm编程案例：WordCount"><a href="#2、Storm编程案例：WordCount" class="headerlink" title="2、Storm编程案例：WordCount"></a>2、Storm编程案例：WordCount</h4><p>流式计算一般架构图：</p><p><img src="/medias/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97%E4%B8%80%E8%88%AC%E6%9E%B6%E6%9E%84%E5%9B%BE.PNG" alt="流式计算一般架构图"></p><ul><li>Flume用来获取数据</li><li>Kafka用来临时保存数据</li><li>Strom用来计算数据</li><li>Redis是个内存数据库，用来保存数据</li></ul><p>代码编写：</p><ol><li>创建Spout（WordCountSpout）组件采集数据，作为整个Topology的数据源</li></ol><p><strong>WordCountSpout类</strong></p><pre><code>package com.hsiehchou.wc;import java.util.Map;import org.apache.storm.spout.SpoutOutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichSpout;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Values;/** * 需求：单词计数  Hello ni hao! Hello China! *  * 实现接口：IRichSpout  IRichBolt * 继承抽象类：BaseRichSpout  BaseRichBolt * @author hsiehchou */public class WordCountSpout extends BaseRichSpout {    //定义收集器    private SpoutOutputCollector collector;    //发送数据    @Override    public void nextTuple() {        //1.发送数据        collector.emit(new Values(&quot;I am a boy!&quot;));        //2.设置延迟        try {            Thread.sleep(1000);        } catch (InterruptedException e) {            e.printStackTrace();        }    }    //创建收集器    @Override    public void open(Map arg0, TopologyContext arg1, SpoutOutputCollector collector) {        this.collector = collector;    }    //声明    @Override    public void declareOutputFields(OutputFieldsDeclarer declare) {        //起别名        declare.declare(new Fields(&quot;hsiehchou&quot;));    }}</code></pre><ol start="2"><li>创建Bolt（WordCountSplitBolt）组件进行分词操作</li></ol><p><strong>WordCountSplitBolt类</strong></p><pre><code>package com.hsiehchou.wc;import java.util.Map;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.tuple.Fields;import org.apache.storm.tuple.Tuple;import org.apache.storm.tuple.Values;public class WordCountSplitBolt extends BaseRichBolt {    //数据继续发送到下一个bolt    private OutputCollector collector;    //业务逻辑    @Override    public void execute(Tuple in) {        //1.获取数据        String line = in.getStringByField(&quot;hsiehchou&quot;);        //2.切分数据        String[] fields = line.split(&quot; &quot;);        //3.&lt;单词,1&gt;发送出去，下一个bolt（累加求和）        for(String w:fields) {            collector.emit(new Values(w,1));        }    }    //初始化    @Override    public void prepare(Map arg0, TopologyContext arg1, OutputCollector collector) {        this.collector = collector;    }    //声明描述    @Override    public void declareOutputFields(OutputFieldsDeclarer declare) {        declare.declare(new Fields(&quot;word&quot;,&quot;sum&quot;));    }}</code></pre><ol start="3"><li>创建Bolt（WordCountBoltCount）组件进行单词计数作</li></ol><p><strong>WordCountBoltCount类</strong></p><pre><code>package com.hsiehchou.wc;import java.util.HashMap;import java.util.Map;import org.apache.storm.task.OutputCollector;import org.apache.storm.task.TopologyContext;import org.apache.storm.topology.OutputFieldsDeclarer;import org.apache.storm.topology.base.BaseRichBolt;import org.apache.storm.tuple.Tuple;public class WordCountBoltCount extends BaseRichBolt {    private Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;();    //累加求和    @Override    public void execute(Tuple in) {        //1.获取数据        String word = in.getStringByField(&quot;word&quot;);        Integer sum = in.getIntegerByField(&quot;sum&quot;);        //2.业务处理        if(map.containsKey(word)) {            //之前出现的次数            Integer count = map.get(word);            //已有的            map.put(word, count + sum);        }else {            map.put(word, sum);        }        //3.打印控制台        System.err.println(Thread.currentThread().getId() + &quot;单位为：&quot; + word + &quot;\t 当前出现次数为：&quot; + map.get(word));        }    @Override    public void prepare(Map arg0, TopologyContext arg1, OutputCollector arg2) {    }    @Override    public void declareOutputFields(OutputFieldsDeclarer arg0) {    }}</code></pre><ol start="4"><li>也可以将主程序Topology（WordCountTopology）提交到Storm集群运行</li></ol><p><strong>WordCountTopology类</strong></p><pre><code>package com.hsiehchou.wc;import org.apache.storm.Config;import org.apache.storm.LocalCluster;import org.apache.storm.topology.TopologyBuilder;import org.apache.storm.tuple.Fields;public class WordCountTopology {    public static void main(String[] args) {        //1.hadoop --&gt; Job Storm --&gt; Topology 创建拓扑        TopologyBuilder builder = new TopologyBuilder();        //2.指定设置        builder.setSpout(&quot;WordCountSpout&quot;, new WordCountSpout(), 1);        builder.setBolt(&quot;WordCountSplitBolt&quot;, new WordCountSplitBolt(), 4).fieldsGrouping(&quot;WordCountSpout&quot;, new Fields(&quot;hsiehchou&quot;));        builder.setBolt(&quot;WordCountBoltCount&quot;, new WordCountBolt(), 2).fieldsGrouping(&quot;WordCountSplitBolt&quot;, new Fields(&quot;word&quot;));        //3.创建配置信息        Config conf = new Config();        //4.提交任务        LocalCluster localCluster = new LocalCluster();        localCluster.submitTopology(&quot;wordcounttopology&quot;, conf, builder.createTopology());    }}</code></pre><h4 id="3、集群部署"><a href="#3、集群部署" class="headerlink" title="3、集群部署"></a>3、集群部署</h4><p><strong>对WordCountDriver类进行修改，把本地模式修改为集群模式</strong></p><pre><code>public class WordCountDriver {        //集群模式运行        try {            StormSubmitter.submitTopology(args[0], conf, builder.createTopology());        } catch (AlreadyAliveException | InvalidTopologyException | AuthorizationException e) {            e.printStackTrace();        }        //4.提交任务        //LocalCluster localCluster = new LocalCluster();        //localCluster.submitTopology(&quot;wordcounttopology&quot;, conf, builder.createTopology());}</code></pre><p><strong>提交到集群</strong></p><p>storm jar StormWordCount.jar com.hsiehchou.wc.WordCountDriver wordcount01</p><h3 id="三、分组策略"><a href="#三、分组策略" class="headerlink" title="三、分组策略"></a>三、分组策略</h3><p>1）fields Grouping</p><p><strong>按照字段分组</strong><br>相同字段发送到一个task中<br>fieldsGrouping<br>builder.setBolt(“WordCountSplitBolt”, new WordCountSplitBolt(), 4).fieldsGrouping(“WordCountSpout”, new Fields(“hsiehchou”));</p><p>builder.setBolt(“WordCountBolt”, new WordCountBolt(), 2).fieldsGrouping(“WordCountSplitBolt”, new Fields(“word”));</p><p>2）shuffle Grouping</p><p><strong>随机分组</strong><br>轮询。平均分配。随机分发tuple，保证每个bolt中的tuple数量相同<br>shuffleGrouping<br>builder.setBolt(“WordCountSplitBolt”, new WordCountSplitBolt(), 4).shuffleGrouping(“WordCountSpout”);</p><p>builder.setBolt(“WordCountBolt”, new WordCountBolt(), 2).shuffleGrouping(“WordCountSplitBolt”);</p><p>3）None Grouping</p><p><strong>不分组</strong><br>采用这种策略每个bolt中接收的单词不同<br>noneGrouping<br>builder.setBolt(“WordCountSplitBolt”, new WordCountSplitBolt(), 4).noneGrouping(“WordCountSpout”);</p><p>builder.setBolt(“WordCountBolt”, new WordCountBolt(), 2).noneGrouping(“WordCountSplitBolt”);</p><p>4）All Grouping</p><p><strong>广播发送</strong><br>tuple分发给每一个bolt<br>allGrouping<br>builder.setBolt(“WordCountSplitBolt”, new WordCountSplitBolt(), 4).allGrouping(“WordCountSpout”);</p><p>builder.setBolt(“WordCountBolt”, new WordCountBolt(), 2).allGrouping(“WordCountSplitBolt”);</p><p>5）Global Grouping</p><p><strong>全局分组</strong><br>分配给task id值最小的<br>根据线程id判断，只分配给线程id最小的<br>builder.setBolt(“WordCountSplitBolt”, new WordCountSplitBolt(), 4).globalGrouping(“WordCountSpout”);</p><p>builder.setBolt(“WordCountBolt”, new WordCountBolt(), 2).globalGrouping(“WordCountSplitBolt”);</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Storm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop的HA高可用</title>
      <link href="/2019/04/29/hadoop-de-ha-gao-ke-yong-ke-xing/"/>
      <url>/2019/04/29/hadoop-de-ha-gao-ke-yong-ke-xing/</url>
      
        <content type="html"><![CDATA[<p><strong>Hadoop的HA高可用（可行）</strong></p><h3 id="一、集群的规划"><a href="#一、集群的规划" class="headerlink" title="一、集群的规划"></a>一、集群的规划</h3><p><strong>ZooKeeper集群</strong></p><table><thead><tr><th align="center">192.168.116.121</th><th align="center">192.168.116.122</th><th align="center">192.168.116.123</th></tr></thead><tbody><tr><td align="center">hsiehchou121</td><td align="center">hsiehchou122</td><td align="center">hsiehchou123</td></tr></tbody></table><p><strong>Hadoop集群</strong></p><table><thead><tr><th align="center">192.168.116.121</th><th align="center">192.168.116.122</th><th align="center">192.168.116.123</th><th align="center">192.168.116.124</th></tr></thead><tbody><tr><td align="center">hsiehchou121</td><td align="center">hsiehchou122</td><td align="center">hsiehchou123</td><td align="center">hsiehchou124</td></tr><tr><td align="center">NameNode1</td><td align="center">NameNode2</td><td align="center">DataNode1</td><td align="center">DataNode2</td></tr><tr><td align="center">ResourceManager1</td><td align="center">ResourceManager2</td><td align="center">NodeManager1</td><td align="center">NodeManager2</td></tr><tr><td align="center">Journalnode</td><td align="center">Journalnode</td><td align="center"></td><td align="center"></td></tr></tbody></table><h3 id="二、准备工作"><a href="#二、准备工作" class="headerlink" title="二、准备工作"></a>二、准备工作</h3><p>1、安装JDK<br>2、配置环境变量<br>3、配置免密码登录<br>4、配置主机名</p><h3 id="三、配置Zookeeper（在192-168-116-121安装）"><a href="#三、配置Zookeeper（在192-168-116-121安装）" class="headerlink" title="三、配置Zookeeper（在192.168.116.121安装）"></a>三、配置Zookeeper（在192.168.116.121安装）</h3><p>在主节点（hsiehchou121）上配置ZooKeeper</p><h4 id="1、配置-root-hd-zookeeper-3-4-10-conf-zoo-cfg文件"><a href="#1、配置-root-hd-zookeeper-3-4-10-conf-zoo-cfg文件" class="headerlink" title="1、配置/root/hd/zookeeper-3.4.10/conf/zoo.cfg文件"></a>1、配置/root/hd/zookeeper-3.4.10/conf/zoo.cfg文件</h4><pre><code>dataDir=/root/hd/zookeeper-3.4.10/zkData+++++++++++++++zkconfig+++++++++++++++++server.1=hsiehchou121:2888:3888server.2=hsiehchou122:2888:3888server.3=hsiehchou123:2888:3888</code></pre><h4 id="2、在-root-training-zookeeper-3-4-6-tmp目录下创建一个myid的空文件"><a href="#2、在-root-training-zookeeper-3-4-6-tmp目录下创建一个myid的空文件" class="headerlink" title="2、在/root/training/zookeeper-3.4.6/tmp目录下创建一个myid的空文件"></a>2、在/root/training/zookeeper-3.4.6/tmp目录下创建一个myid的空文件</h4><p>echo 1 &gt; /root/hd/zookeeper-3.4.10/tmp/myid</p><h4 id="3、将配置好的ZooKeeper拷贝到其他节点，同时修改各自的myid文件"><a href="#3、将配置好的ZooKeeper拷贝到其他节点，同时修改各自的myid文件" class="headerlink" title="3、将配置好的ZooKeeper拷贝到其他节点，同时修改各自的myid文件"></a>3、将配置好的ZooKeeper拷贝到其他节点，同时修改各自的myid文件</h4><p>scp -r /root/hd/zookeeper-3.4.10/ hsiehchou122:/root/hd<br>scp -r /root/hd/zookeeper-3.4.10/ hsiehchou123:/root/hd</p><h3 id="四、安装Hadoop集群（在hsiehchou121上安装）"><a href="#四、安装Hadoop集群（在hsiehchou121上安装）" class="headerlink" title="四、安装Hadoop集群（在hsiehchou121上安装）"></a>四、安装Hadoop集群（在hsiehchou121上安装）</h3><h4 id="1、修改hadoop-env-sh"><a href="#1、修改hadoop-env-sh" class="headerlink" title="1、修改hadoop-env.sh"></a>1、修改hadoop-env.sh</h4><pre><code>export JAVA_HOME=/root/hd/jdk1.8.0_192</code></pre><h4 id="2、修改core-site-xml"><a href="#2、修改core-site-xml" class="headerlink" title="2、修改core-site.xml"></a>2、修改core-site.xml</h4><pre><code>&lt;configuration&gt;    &lt;!-- 指定hdfs的nameservice为mycluster --&gt;    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://mycluster&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定hadoop临时目录 --&gt;    &lt;property&gt;        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;        &lt;value&gt;/root/hd/hadoop-2.8.4/tmp&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定zookeeper地址 --&gt;    &lt;property&gt;        &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;        &lt;value&gt;hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="3、修改hdfs-site-xml（配置这个nameservice中有几个NameNode）"><a href="#3、修改hdfs-site-xml（配置这个nameservice中有几个NameNode）" class="headerlink" title="3、修改hdfs-site.xml（配置这个nameservice中有几个NameNode）"></a>3、修改hdfs-site.xml（配置这个nameservice中有几个NameNode）</h4><pre><code>&lt;configuration&gt;     &lt;!--指定hdfs的nameservice为mycluster，需要和core-site.xml中的保持一致 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.nameservices&lt;/name&gt;        &lt;value&gt;mycluster&lt;/value&gt;    &lt;/property&gt;    &lt;!-- mycluster下面有两个NameNode，分别是nn1，nn2 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;        &lt;value&gt;nn1,nn2&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn1的RPC通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;        &lt;value&gt;hsiehchou121:9000&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn1的http通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;        &lt;value&gt;hsiehchou121:50070&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn2的RPC通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;        &lt;value&gt;hsiehchou122:9000&lt;/value&gt;    &lt;/property&gt;    &lt;!-- nn2的http通信地址 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;        &lt;value&gt;hsiehchou122:50070&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定NameNode的日志在JournalNode上的存放位置 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;        &lt;value&gt;qjournal://hsiehchou121:8485;hsiehchou122:8485;/mycluster&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;        &lt;value&gt;/root/hd/hadoop-2.8.4/journal&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 开启NameNode失败自动切换 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 配置失败自动切换实现方式 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;        &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;        &lt;value&gt;            sshfence            shell(/bin/true)        &lt;/value&gt;    &lt;/property&gt;    &lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;        &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 配置sshfence隔离机制超时时间 --&gt;    &lt;property&gt;        &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;        &lt;value&gt;30000&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="4、修改mapred-site-xml"><a href="#4、修改mapred-site-xml" class="headerlink" title="4、修改mapred-site.xml"></a>4、修改mapred-site.xml</h4><pre><code>&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;        &lt;value&gt;yarn&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="5、修改yarn-site-xml"><a href="#5、修改yarn-site-xml" class="headerlink" title="5、修改yarn-site.xml"></a>5、修改yarn-site.xml</h4><pre><code>&lt;configuration&gt;    &lt;!-- 开启RM高可靠 --&gt;    &lt;property&gt;       &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;       &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定RM的cluster id --&gt;    &lt;property&gt;       &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;       &lt;value&gt;yarncluster&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定RM的名字 --&gt;    &lt;property&gt;       &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;       &lt;value&gt;rm1,rm2&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 分别指定RM的地址 --&gt;    &lt;property&gt;       &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;       &lt;value&gt;hsiehchou121&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;       &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;       &lt;value&gt;hsiehchou122&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 指定zk集群地址 --&gt;    &lt;property&gt;       &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;       &lt;value&gt;hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;       &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;       &lt;value&gt;mapreduce_shuffle&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;         &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;         &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;          &lt;value&gt;32768&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;        &lt;value&gt;32768&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;        &lt;value&gt;4096&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;        &lt;value&gt;24&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;        &lt;value&gt;/tmp/yarn-logs&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="6、修改slaves"><a href="#6、修改slaves" class="headerlink" title="6、修改slaves"></a>6、修改slaves</h4><p>hsiehchou123<br>hsiehchou124</p><h4 id="7、将配置好的hadoop拷贝到其他节点"><a href="#7、将配置好的hadoop拷贝到其他节点" class="headerlink" title="7、将配置好的hadoop拷贝到其他节点"></a>7、将配置好的hadoop拷贝到其他节点</h4><p>scp -r /root/hd/hadoop-2.8.4/ root@hsiehchou122:/root/hd/<br>scp -r /root/hd/hadoop-2.8.4/ root@hsiehchou123:/root/hd/<br>scp -r /root/hd/hadoop-2.8.4/ root@hsiehchou124:/root/hd/</p><h3 id="五、启动Zookeeper集群"><a href="#五、启动Zookeeper集群" class="headerlink" title="五、启动Zookeeper集群"></a>五、启动Zookeeper集群</h3><h4 id="1、启动Zookeeper集群"><a href="#1、启动Zookeeper集群" class="headerlink" title="1、启动Zookeeper集群"></a>1、启动Zookeeper集群</h4><p>[root@hsiehchou121 hadoop-2.8.4]# <strong>zkServer.sh start</strong><br>[root@hsiehchou122 hadoop-2.8.4]# <strong>zkServer.sh start</strong><br>[root@hsiehchou123 hadoop-2.8.4]# <strong>zkServer.sh start</strong></p><h3 id="六、在hsiehchou121和hsiehchou122上启动journalnode"><a href="#六、在hsiehchou121和hsiehchou122上启动journalnode" class="headerlink" title="六、在hsiehchou121和hsiehchou122上启动journalnode"></a>六、在hsiehchou121和hsiehchou122上启动journalnode</h3><p><strong>hadoop-daemon.sh start journalnode</strong></p><h3 id="七、格式化HDFS（在hsiehchou121上执行）"><a href="#七、格式化HDFS（在hsiehchou121上执行）" class="headerlink" title="七、格式化HDFS（在hsiehchou121上执行）"></a>七、格式化HDFS（在hsiehchou121上执行）</h3><h4 id="1-格式化ZooKeeper"><a href="#1-格式化ZooKeeper" class="headerlink" title="1. 格式化ZooKeeper"></a>1. 格式化ZooKeeper</h4><p>[root@hsiehchou121 hadoop-2.8.4]# <strong>bin/hdfs zkfc -formatZK</strong></p><h4 id="2、启动HDFS"><a href="#2、启动HDFS" class="headerlink" title="2、启动HDFS"></a>2、启动HDFS</h4><p>1）在各个JournalNode节点上，输入以下命令启动journalnode服务<br>[root@hsiehchou121 hadoop-2.8.4]# <strong>sbin/hadoop-daemon.sh start journalnode</strong></p><p>2）在[nn1]上，对其进行格式化，并启动<br>[root@hsiehchou121 hadoop-2.8.4]# <strong>bin/hdfs namenode -format</strong><br>[root@hsiehchou121 hadoop-2.8.4]# <strong>sbin/hadoop-daemon.sh start namenode</strong></p><p>3）在[nn2]上，同步nn1的元数据信息<br>[root@hsiehchou122 hadoop-2.8.4]# <strong>bin/hdfs namenode -bootstrapStandby</strong></p><h3 id="八、在hsiehchou121上启动Hadoop集群"><a href="#八、在hsiehchou121上启动Hadoop集群" class="headerlink" title="八、在hsiehchou121上启动Hadoop集群"></a>八、在hsiehchou121上启动Hadoop集群</h3><p>[root@hsiehchou121 hadoop-2.8.4]#  <strong>start-all.sh</strong></p><p><strong>日志</strong><br>This script is Deprecated. Instead use start-dfs.sh and start-yar<br>Starting namenodes on [hsiehchou121 hsiehchou122]<br>hsiehchou121: starting namenode, logging to /root/hd/hadoop-2.8.4-hsiehchou121.out<br>hsiehchou122: starting namenode, logging to /root/hd/hadoop-2.8.4-hsiehchou122.out<br>hsiehchou124: starting datanode, logging to /root/hd/hadoop-2.8.4-hsiehchou124.out<br>hsiehchou123: starting datanode, logging to /root/hd/hadoop-2.8.4-hsiehchou123.out<br>Starting journal nodes [hsiehchou121 hsiehchou122 ]<br>hsiehchou121: starting journalnode, logging to /root/hd/hadoop-2.alnode-hsiehchou121.out<br>hsiehchou122: starting journalnode, logging to /root/hd/hadoop-2.alnode-hsiehchou122.out<br>Starting ZK Failover Controllers on NN hosts [hsiehchou121 hsiehc<br>hsiehchou121: starting zkfc, logging to /root/hd/hadoop-2.8.4/logou121.out<br>hsiehchou122: starting zkfc, logging to /root/hd/hadoop-2.8.4/logou122.out<br>starting yarn daemons<br>starting resourcemanager, logging to /root/hd/hadoop-2.8.4/logs/ysiehchou121.out<br>hsiehchou123: starting nodemanager, logging to /root/hd/hadoop-2.ager-hsiehchou123.out<br>hsiehchou124: starting nodemanager, logging to /root/hd/hadoop-2.ager-hsiehchou124.out</p><p>hsiehchou122上的ResourceManager需要单独启动<br><strong>命令</strong><br>[root@hsiehchou121 hadoop-2.8.4]# <strong>./sbin/yarn-daemon.sh start resourcemanager</strong></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hadoop HA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka</title>
      <link href="/2019/04/26/kafka/"/>
      <url>/2019/04/26/kafka/</url>
      
        <content type="html"><![CDATA[<p><strong>离线部分</strong><br>Hadoop-&gt;离线计算(hdfs / mapreduce) yarn<br>zookeeper-&gt;分布式协调（动物管理员）<br>hive-&gt;数据仓库（离线计算 / sql）easy coding<br>flume-&gt;数据采集<br>sqoop-&gt;数据迁移mysql-&gt;hdfs/hive hdfs/hive-&gt;mysql<br>Azkaban-&gt;任务调度工具<br>hbase-&gt;数据库（nosql）列式存储 读写速度</p><p><strong>实时</strong><br>kafka<br>storm</p><h3 id="一、Kafka是什么"><a href="#一、Kafka是什么" class="headerlink" title="一、Kafka是什么"></a>一、Kafka是什么</h3><p>kafka一般用来缓存数据</p><h4 id="1、开源消息系统"><a href="#1、开源消息系统" class="headerlink" title="1、开源消息系统"></a>1、开源消息系统</h4><h4 id="2、最初是LinkedIn公司开发，2011年开源"><a href="#2、最初是LinkedIn公司开发，2011年开源" class="headerlink" title="2、最初是LinkedIn公司开发，2011年开源"></a>2、最初是LinkedIn公司开发，2011年开源</h4><p>2012年10月从Apache Incubator毕业</p><p>项目目标是为处理实时数据，提供一个统一、高通量、低等待的平台</p><h4 id="3、Kafka是一个分布式消息队列"><a href="#3、Kafka是一个分布式消息队列" class="headerlink" title="3、Kafka是一个分布式消息队列"></a>3、Kafka是一个分布式消息队列</h4><p>消息根据Topic来归类，发送消息 Producer，接收 Consumer</p><p>kafka集群有多个kafka实例组成，每个实例成为broker</p><h4 id="4、依赖于-Zookeeper-集群"><a href="#4、依赖于-Zookeeper-集群" class="headerlink" title="4、依赖于 Zookeeper 集群"></a>4、依赖于 Zookeeper 集群</h4><p>无论是kafka集群，还是 Producer、Consumer 都依赖于 Zookeeper 集群保存元信息，来保证系统可用性</p><p><img src="/medias/kafka%E4%BB%8B%E7%BB%8D.PNG" alt="kafka介绍"></p><p><strong>官网</strong><br><a href="http://kafka.apache.org/" target="_blank" rel="noopener">http://kafka.apache.org/</a><br>ApacheKafka?是一个分布式流媒体平台<br>流媒体平台有三个关键功能：</p><ol><li>发布和订阅记录流，类似于消息队列或企业消息传递系统</li><li>以容错的持久方式存储记录流</li><li>记录发生时处理流</li></ol><p>Kafka通常用于两大类应用：<br>构建可在系统或应用程序之间可靠获取数据的实时流数据管道<br>构建转换或响应数据流的实时流应用程序</p><p>kafka在流计算中，kafka主要功能是用来缓存数据，storm可以通过消费kafka中的数据进行流计算，是一套开源的消息系统，由scala写成，支持javaAPI。</p><p>kafka最初由LinkedIn公司开发，2011年开源，2012年从Apache毕业，是一个分布式消息队列，kafka读消息保存采用Topic进行归类</p><h3 id="二、消息队列"><a href="#二、消息队列" class="headerlink" title="二、消息队列"></a>二、消息队列</h3><p>点对点<br>发布、订阅模式</p><p><strong>角色</strong><br>发送消息：Producer(生产者)<br>接收消息：Consumer(消费者)</p><h3 id="三、为什么需要消息队列"><a href="#三、为什么需要消息队列" class="headerlink" title="三、为什么需要消息队列"></a>三、为什么需要消息队列</h3><h4 id="1、解耦"><a href="#1、解耦" class="headerlink" title="1、解耦"></a>1、解耦</h4><p>为了避免出现问题</p><h4 id="2、冗余"><a href="#2、冗余" class="headerlink" title="2、冗余"></a>2、冗余</h4><p>消息队列把数据进行持久化，直到他们已经被完全处理</p><h4 id="3、扩展性（拓展性）"><a href="#3、扩展性（拓展性）" class="headerlink" title="3、扩展性（拓展性）"></a>3、扩展性（拓展性）</h4><p>可增加处理过程</p><h4 id="4、灵活性"><a href="#4、灵活性" class="headerlink" title="4、灵活性"></a>4、灵活性</h4><p>面对访问量剧增，不会因为超负荷请求而完全瘫痪</p><h4 id="5、可恢复性"><a href="#5、可恢复性" class="headerlink" title="5、可恢复性"></a>5、可恢复性</h4><p>一部分组件失效，不会影响整个系统。可以进行恢复</p><h4 id="6、顺序保证（相对）"><a href="#6、顺序保证（相对）" class="headerlink" title="6、顺序保证（相对）"></a>6、顺序保证（相对）</h4><p>kafka保证一个Partition内部的消息有序，对消息进行有序处理</p><h4 id="7、缓冲"><a href="#7、缓冲" class="headerlink" title="7、缓冲"></a>7、缓冲</h4><p>控制数据流经过系统的速度</p><h4 id="8、异步通信"><a href="#8、异步通信" class="headerlink" title="8、异步通信"></a>8、异步通信</h4><p>akka，消息队列提供了异步处理的机制</p><p>很多时候，用户不想也不需要立即处理消息，消息队列提供异步处理机制，允许用户把消息放入队列，但不立即处理</p><h3 id="四、Kafka架构"><a href="#四、Kafka架构" class="headerlink" title="四、Kafka架构"></a>四、Kafka架构</h3><p><img src="/medias/kafka%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84.PNG" alt="kafka系统架构"></p><h4 id="1、Producer：消息生产者"><a href="#1、Producer：消息生产者" class="headerlink" title="1、Producer：消息生产者"></a>1、Producer：消息生产者</h4><p>就是往kafka中发消息的客户端</p><h4 id="2、Consumer：消息消费者"><a href="#2、Consumer：消息消费者" class="headerlink" title="2、Consumer：消息消费者"></a>2、Consumer：消息消费者</h4><p>向kafka broker中取消息的客户端</p><h4 id="3、Topic-理解为队列"><a href="#3、Topic-理解为队列" class="headerlink" title="3、Topic 理解为队列"></a>3、Topic 理解为队列</h4><h4 id="4、Consumer-Group-消费者组"><a href="#4、Consumer-Group-消费者组" class="headerlink" title="4、Consumer Group 消费者组"></a>4、Consumer Group 消费者组</h4><p>组内有多个消费者实例，共享一个公共的ID，即groupID<br>组内所有消费者协调在一起，消费Topic<br>每个分区，只能有同一个消费组内的一个Consumer消费</p><h4 id="5、broker"><a href="#5、broker" class="headerlink" title="5、broker"></a>5、broker</h4><p>一台kafka服务器就是一个broker</p><h4 id="6、partition：一个topic分为多个partition"><a href="#6、partition：一个topic分为多个partition" class="headerlink" title="6、partition：一个topic分为多个partition"></a>6、partition：一个topic分为多个partition</h4><p>每个partition是一个有序队列<br>kafka保证按一个partition中的顺序将消息发送个consumer<br>不能保证topic整体有序</p><h4 id="7、offset：Kafka存储文件按照offset-kafka命名"><a href="#7、offset：Kafka存储文件按照offset-kafka命名" class="headerlink" title="7、offset：Kafka存储文件按照offset.kafka命名"></a>7、offset：Kafka存储文件按照offset.kafka命名</h4><p><img src="/medias/kafka%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B%E5%9B%BE.PNG" alt="kafka读写流程图"></p><h3 id="五、Kafka部署"><a href="#五、Kafka部署" class="headerlink" title="五、Kafka部署"></a>五、Kafka部署</h3><p>前提：Zookeeper</p><p>官网下载安装包<br><a href="http://kafka.apache.org/downloads" target="_blank" rel="noopener">http://kafka.apache.org/downloads</a></p><p>上传tar</p><p>解压<br>[root@hsiehchou121 hd]# tar -zxvf kafka_2.11-2.1.1.tgz </p><p>[root@hsiehchou121 hd]# mv kafka_2.11-2.1.1 kafka</p><p>在kafka目录中，创建一个logs文件夹<br>[root@hsiehchou121 kafka]# mkdir logs</p><p>如果不创建，默认放在 /tmp 目录下</p><p><strong>修改文件</strong><br><strong>config/server.properties</strong><br>21 broker.id=0<br>broker 的 全局唯一编号，不能重复</p><p>新增<br>22 delete.topic.enable=true<br>允许删除topic</p><p><code>#</code>The number of threads that the server uses for receiving requests from the network and sending responses to the network<br>42 num.network.threads=3<br>处理网络请求的线程数量</p><p><code>#</code>The number of threads that the server uses for processing requests, which may include disk I/O<br>46 num.io.threads=8<br>用来处理磁盘IO的线程数量</p><p><code>#</code>A comma separated list of directories under which to store log files<br>62 log.dirs=/root/hd/kafka/logs<br>kafka运行日志存放的路径</p><p><code>#</code> The default number of log partitions per topic. More partitions allow greater<br><code>#</code>parallelism for consumption, but this will also result in more files across<br><code>#</code> the brokers.<br>67 num.partitions=1<br>当前主题在broker上的分区个数</p><p><code>#</code>· The number of threads per data directory to be used for log recovery at start        up and flushing at shutdown.<br><code>#</code> This value is recommended to be increased for installations with data dirs lo        cated in RAID array.<br> 71 num.recovery.threads.per.data.dir=1<br>恢复和清理data下的线程数量</p><p>125 zookeeper.connect=hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181<br>zookeeper相关信息</p><p><code>#</code> Timeout in ms for connecting to zookeeper<br>128 zookeeper.connection.timeout.ms=6000<br>zookeeper连接超时的时间</p><p>添加全局环境变量，以便于在任何地方都可以启动<br>[root@hsiehchou121 config]# vi /etc/profile<br><code>#</code>kafka_home<br>export KAFKA_HOME=/root/hd/kafka<br>export PATH=<code>$KAFKA_HOME/bin:$PATH</code></p><p>[root@hsiehchou121 config]# source /etc/profile</p><p>分发安装包<br>注意：要修改配置文件中 borker.id的值<br>broker id 不得重复<br>21 broker.id=0 —- hsiehchou121<br>21 broker.id=1 —- hsiehchou122<br>21 broker.id=2 —- hsiehchou123</p><p><strong>启动kafka集群</strong><br>首先需要先启动zookeeper<br>zkServer.sh start<br>[root@hsiehchou121 kafka]# zkServer.sh start<br>[root@hsiehchou122 kafka]# zkServer.sh start<br>[root@hsiehchou123 kafka]# zkServer.sh start</p><p>（<strong>必须先启动！！！！！！！！！</strong>）<br>./bin/kafka-server-start.sh config/server.properties &amp;<br><strong>&amp;====后台运行</strong><br>[root@hsiehchou121 kafka]# ./bin/kafka-server-start.sh config/server.properties &amp;<br>[root@hsiehchou122 kafka]# ./bin/kafka-server-start.sh config/server.properties &amp;<br>[root@hsiehchou123 kafka]# ./bin/kafka-server-start.sh config/server.properties &amp;</p><p>jps命令可以看Kafka进程<br>[root@hsiehchou121 kafka]# jps<br>7104 Jps<br>6742 Kafka<br>6715 QuorumPeerMain</p><p>关闭命令：<br>./bin/kafka-server-stop.sh stop<br>或者./bin/kafka-server-stop.sh </p><h3 id="六、Kafka命令行操作"><a href="#六、Kafka命令行操作" class="headerlink" title="六、Kafka命令行操作"></a>六、Kafka命令行操作</h3><h4 id="1、查看当前服务器中所有的topic"><a href="#1、查看当前服务器中所有的topic" class="headerlink" title="1、查看当前服务器中所有的topic"></a>1、查看当前服务器中所有的topic</h4><p>[root@hsiehchou121 kafka]# ./bin/kafka-topics.sh <code>--zookeeper</code> hsiehchou121:2181 <code>--list</code></p><h4 id="2、创建topic"><a href="#2、创建topic" class="headerlink" title="2、创建topic"></a>2、创建topic</h4><p>[root@hsiehchou121 kafka]# ./bin/kafka-topics.sh <code>--zookeeper</code> hsiehchou121:2181 <code>--create</code> <code>--replication-factor</code> 3 <code>--partitions</code> 1 <code>--topic</code> second</p><p>[root@hsiehchou121 kafka]# ./bin/kafka-topics.sh <code>--zookeeper</code> hsiehchou121:2181 <code>--create</code> <code>--replication-factor</code> 3 <code>--partitions</code> 1 <code>--topic</code> xz</p><p><code>--zookeeper</code> ：连接zk集群<br><code>--create</code> ：创建<br><code>--replication-factor</code>： 副本<br><code>--partitions</code> ：分区<br><code>--topic</code> ：主题名</p><h4 id="3、删除topic"><a href="#3、删除topic" class="headerlink" title="3、删除topic"></a>3、删除topic</h4><p>[root@hsiehchou121 kafka]# ./bin/kafka-topics.sh <code>--zookeeper</code> hsiehchou121:2181 <code>--delete</code> <code>--topic</code> xz</p><h4 id="4、发送消息"><a href="#4、发送消息" class="headerlink" title="4、发送消息"></a>4、发送消息</h4><p>[root@hsiehchou121 kafka]# ./bin/kafka-console-producer.sh <code>--broker-list</code> hsiehchou121:9092 <code>--topic</code> second<br><code>&gt;</code> 输入消息</p><h4 id="5、消费消息"><a href="#5、消费消息" class="headerlink" title="5、消费消息"></a>5、消费消息</h4><p>[root@hsiehchou122 kafka]# ./bin/kafka-console-consumer.sh <code>--bootstrap-server</code> hsiehchou121:9092 <code>--from-beginning</code> <code>--topic</code> second<br>接收消息</p><p><strong>注意：</strong>这里的<code>--from-beginning</code>  是从头开始消费，不加则是消费当前正在发送到该topic的消息</p><h4 id="6、查看主题描述信息"><a href="#6、查看主题描述信息" class="headerlink" title="6、查看主题描述信息"></a>6、查看主题描述信息</h4><p>[root@hsiehchou121 kafka]# ./bin/kafka-topics.sh –zookeeper hsiehchou121:2181 –describe –topic second<br>Topic:second    PartitionCount:1    ReplicationFactor:3    Configs:<br>    Topic: second    Partition: 0    Leader: 2    Replicas: 2,1,0    Isr: 2,1,0</p><h4 id="7、消费者组消费"><a href="#7、消费者组消费" class="headerlink" title="7、消费者组消费"></a>7、消费者组消费</h4><p>修改 consumer.properties 配置文件<br>consumer group id<br>group.id=xz</p><h4 id="8、启动生产者"><a href="#8、启动生产者" class="headerlink" title="8、启动生产者"></a>8、启动生产者</h4><p>[root@hsiehchou121 kafka]# ./bin/kafka-console-producer.sh <code>--broker-list</code> hsiehchou121:9092 <code>--topic</code> second</p><h4 id="9、启动消费者"><a href="#9、启动消费者" class="headerlink" title="9、启动消费者"></a>9、启动消费者</h4><p>[root@hsiehchou122 kafka]# ./bin/kafka-console-consumer.sh <code>--bootstrap-server</code> hsiehchou121:9092 <code>--topic</code> second <code>--consumer.config</code> config/consumer.properties</p><h3 id="七、Kafka工作流程分析"><a href="#七、Kafka工作流程分析" class="headerlink" title="七、Kafka工作流程分析"></a>七、Kafka工作流程分析</h3><h4 id="1、Kafka生产过程分析"><a href="#1、Kafka生产过程分析" class="headerlink" title="1、Kafka生产过程分析"></a>1、Kafka生产过程分析</h4><p>1）<strong>写入方式</strong><br>producer采用推（push）模式将消息发布到broker，每条消息都被追加（append）到分区（patition）中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障kafka吞吐率）</p><p>2） <strong>分区</strong>（Partition）<br>    Kafka集群有多个消息代理服务器（broker-server）组成，发布到Kafka集群的每条消息都有一个类别，用主题（topic）来表示。通常，不同应用产生不同类型的数据，可以设置不同的主题。一个主题一般会有多个消息的订阅者，当生产者发布消息到某个主题时，订阅了这个主题的消费者都可以接收到生成者写入的新消息</p><p>Kafka集群为每个主题维护了分布式的分区（partition）日志文件，物理意义上可以把主题（topic）看作进行了分区的日志文件（partition log）。主题的每个分区都是一个有序的、不可变的记录序列，新的消息会不断追加到日志中。分区中的每条消息都会按照时间顺序分配到一个单调递增的顺序编号，叫做偏移量（offset），这个偏移量能够唯一地定位当前分区中的每一条消息</p><p>消息发送时都被发送到一个topic，其本质就是一个目录，而topic是由一些Partition Logs(分区日志)组成，其组织结构如下图所示：<br>下图中的topic有3个分区，每个分区的偏移量都从0开始，不同分区之间的偏移量都是独立的，不会相互影响</p><p><img src="/medias/partitions.PNG" alt="partitions"></p><p>我们可以看到，每个Partition中的消息都是有序的，生产的消息被不断追加到Partition log上，其中的每一个消息都被赋予了一个唯一的offset值</p><p>发布到Kafka主题的每条消息包括键值和时间戳。消息到达服务器端的指定分区后，都会分配到一个自增的偏移量。原始的消息内容和分配的偏移量以及其他一些元数据信息最后都会存储到分区日志文件中。消息的键也可以不用设置，这种情况下消息会均衡地分布到不同的分区</p><ol><li>分区的原因<br>（1）方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了。<br>（2）可以提高并发，因为可以以Partition为单位读写了。<br>传统消息系统在服务端保持消息的顺序，如果有多个消费者消费同一个消息队列，服务端会以消费存储的顺序依次发送给消费者。但由于消息是异步发送给消费者的，消息到达消费者的顺序可能是无序的，这就意味着在并行消费时，传统消息系统无法很好地保证消息被顺序处理。虽然我们可以设置一个专用的消费者只消费一个队列，以此来解决消息顺序的问题，但是这就使得消费处理无法真正执行。<br>Kafka比传统消息系统有更强的顺序性保证，它使用主题的分区作为消息处理的并行单元。Kafka以分区作为最小的粒度，将每个分区分配给消费者组中不同的而且是唯一的消费者，并确保一个分区只属于一个消费者，即这个消费者就是这个分区的唯一读取线程。那么，只要分区的消息是有序的，消费者处理的消息顺序就有保证。每个主题有多个分区，不同的消费者处理不同的分区，所以Kafka不仅保证了消息的有序性，也做到了消费者的负载均衡。</li><li>分区的原则<br>（1）指定了patition，则直接使用；<br>（2）未指定patition但指定key，通过对key的value进行hash出一个patition<br>（3）patition和key都未指定，使用轮询选出一个patition。</li></ol><p><strong>DefaultPartitioner类</strong></p><pre><code>public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {        List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);        int numPartitions = partitions.size();        if (keyBytes == null) {            int nextValue = nextValue(topic);            List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);            if (availablePartitions.size() &gt; 0) {                int part = Utils.toPositive(nextValue) % availablePartitions.size();                return availablePartitions.get(part).partition();            } else {                // no partitions are available, give a non-available partition                return Utils.toPositive(nextValue) % numPartitions;            }        } else {            // hash the keyBytes to choose a partition            return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;        }    }</code></pre><p>3）副本（Replication）<br>同一个partition可能会有多个replication（对应 server.properties 配置中的 default.replication.factor=N）。没有replication的情况下，一旦broker 宕机，其上所有 patition 的数据都不可被消费，同时producer也不能再将数据存于其上的patition。引入replication之后，同一个partition可能会有多个replication，而这时需要在这些replication之间选出一个leader，producer和consumer只与这个leader交互，其它replication作为follower从leader 中复制数据</p><p>4） 写入流程<br> producer写入消息流程如下：</p><ol><li>producer先从zookeeper的 “/brokers/…/state”节点找到该partition的leader</li><li>producer将消息发送给该leader</li><li>leader将消息写入本地log</li><li>followers从leader pull消息，写入本地log后向leader发送ACK</li><li>leader收到所有ISR中的replication的ACK后，增加HW（high watermark，最后commit 的offset）并向producer发送ACK</li></ol><h4 id="2、Broker-保存消息"><a href="#2、Broker-保存消息" class="headerlink" title="2、Broker 保存消息"></a>2、Broker 保存消息</h4><p>1）存储方式<br>物理上把topic分成一个或多个patition（对应 server.properties 中的num.partitions=3配置），每个patition物理上对应一个文件夹（该文件夹存储该patition的所有消息和索引文件），如下：</p><pre><code>[root @hsiehchou121 logs]$ lldrwxrwxr-x. 2 root root 4096 4月   6 14:37 first-0drwxrwxr-x. 2 root root 4096 4月   6 14:35 first-1drwxrwxr-x. 2 root root 4096 4月   6 14:37 first-2[root @hsiehchou121 logs]$ cd first-0[root @hsiehchou121 first-0]$ ll-rw-rw-r--. 1 root root 10485760 4月   6 14:33 00000000000000000000.index-rw-rw-r--. 1 root root     219 4月   6 15:07 00000000000000000000.log-rw-rw-r--. 1 root root 10485756 4月   6 14:33 00000000000000000000.timeindex-rw-rw-r--. 1 root root       8 4月   6 14:37 leader-epoch-checkpoint</code></pre><p>2）存储策略<br>无论消息是否被消费，kafka都会保留所有消息。有两种策略可以删除旧数据：</p><ol><li>基于时间：log.retention.hours=168</li><li>基于大小：log.retention.bytes=1073741824<br>需要注意的是，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。</li></ol><p>3）Zookeeper存储结构<br><img src="/medias/kafka%20zookeeper%E7%9A%84%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84.PNG" alt="kafka zookeeper的存储结构"><br>注意：producer不在zk中注册，消费者在zk中注册</p><h4 id="3、Kafka消费过程分析"><a href="#3、Kafka消费过程分析" class="headerlink" title="3、Kafka消费过程分析"></a>3、Kafka消费过程分析</h4><p>kafka提供了两套consumer API：高级Consumer API和低级API<br>1）消费模型<br>    消息由生产者发布到Kafka集群后，会被消费者消费。消息的消费模型有两种：推送模型（push）和拉取模型（pull）</p><p>基于推送模型（push）的消息系统，由消息代理记录消费者的消费状态。消息代理在将消息推送到消费者后，标记这条消息为已消费，但这种方式无法很好地保证消息被处理。比如，消息代理把消息发送出去后，当消费进程挂掉或者由于网络原因没有收到这条消息时，就有可能造成消息丢失（因为消息代理已经把这条消息标记为已消费了，但实际上这条消息并没有被实际处理）。如果要保证消息被处理，消息代理发送完消息后，要设置状态为“已发送”，只有收到消费者的确认请求后才更新为“已消费”，这就需要消息代理中记录所有的消费状态，这种做法显然是不可取的</p><p>Kafka采用拉取模型，由消费者自己记录消费状态，每个消费者互相独立地顺序读取每个分区的消息。如下图所示，有两个消费者（不同消费者组）拉取同一个主题的消息，消费者A的消费进度是3，消费者B的消费进度是6。消费者拉取的最大上限通过最高水位（watermark）控制，生产者最新写入的消息如果还没有达到备份数量，对消费者是不可见的。这种由消费者控制偏移量的优点是：消费者可以按照任意的顺序消费消息。比如，消费者可以重置到旧的偏移量，重新处理之前已经消费过的消息；或者直接跳到最近的位置，从当前的时刻开始消费</p><p>在一些消息系统中，消息代理会在消息被消费之后立即删除消息。如果有不同类型的消费者订阅同一个主题，消息代理可能需要冗余地存储同一消息；或者等所有消费者都消费完才删除，这就需要消息代理跟踪每个消费者的消费状态，这种设计很大程度上限制了消息系统的整体吞吐量和处理延迟。Kafka的做法是生产者发布的所有消息会一致保存在Kafka集群中，不管消息有没有被消费。用户可以通过设置保留时间来清理过期的数据，比如，设置保留策略为两天。那么，在消息发布之后，它可以被不同的消费者消费，在两天之后，过期的消息就会自动清理掉</p><p>2）高级API</p><ol><li>高级API优点<br>高级API 写起来简单<br>不需要自行去管理offset，系统通过zookeeper自行管理。<br>不需要管理分区，副本等情况，.系统自动管理。<br>消费者断线会自动根据上一次记录在zookeeper中的offset去接着获取数据（默认设置1分钟更新一下zookeeper中存的offset）<br>可以使用group来区分对同一个topic 的不同程序访问分离开来（不同的group记录不同的offset，这样不同程序读取同一个topic才不会因为offset互相影响）</li><li>高级API缺点<br>不能自行控制offset（对于某些特殊需求来说）<br>不能细化控制如分区、副本、zk等</li></ol><p>3）低级API</p><ol><li>低级 API 优点<br>能够让开发者自己控制offset，想从哪里读取就从哪里读取。<br>自行控制连接分区，对分区自定义进行负载均衡<br>对zookeeper的依赖性降低（如：offset不一定非要靠zk存储，自行存储offset即可，比如存在文件或者内存中）</li><li>低级API缺点<br>太过复杂，需要自行控制offset，连接哪个分区，找到分区leader 等。</li></ol><p>4）消费者组<br>消费者是以consumer group消费者组的方式工作，由一个或者多个消费者组成一个组，共同消费一个topic。每个分区在同一时间只能由group中的一个消费者读取，但是多个group可以同时消费这个partition。在图中，有一个由三个消费者组成的group，有一个消费者读取主题中的两个分区，另外两个分别读取一个分区。某个消费者读取某个分区，也可以叫做某个消费者是某个分区的拥有者</p><p>在这种情况下，消费者可以通过水平扩展的方式同时读取大量的消息。另外，如果一个消费者失败了，那么其他的group成员会自动负载均衡读取之前失败的消费者读取的分区</p><p>5）消费方式<br>consumer采用pull（拉）模式从broker中读取数据</p><p>push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息</p><p>对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率，同时consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义</p><p>pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直等待数据到达。为了避免这种情况，我们在我们的拉请求中有参数，允许消费者请求在等待数据到达的“长轮询”中进行阻塞（并且可选地等待到给定的字节数，以确保大的传输大小）</p><p>6） 消费者组案例</p><ol><li>需求：测试同一个消费者组中的消费者，同一时刻只能有一个消费者消费</li><li>案例实操<br> （1）在hsiehchou121、hsiehchou122上修改/root/hd/kafka/config/consumer.properties配置文件中的group.id属性为任意组名</li></ol><p>[root@hsiehchou122 config]$ vi consumer.properties<br>    group.id=root</p><p>规划：hsiehchou121生产者，hsiehchou122消费者，hsiehchou123消费者<br>（2）在hsiehchou122、hsiehchou123上分别启动消费者</p><pre><code>[root@hsiehchou122 kafka]$ ./bin/kafka-console-consumer.sh --bootstrap-server hsiehchou121:9092 --topic second --consumer.config config/consumer.properties[root@hsiehchou123 kafka]$ ./bin/kafka-console-consumer.sh --bootstrap-server hsiehchou121:9092 --topic second --consumer.config config/consumer.properties</code></pre><p>（3）在hsiehchou121上启动生产者</p><pre><code>[root@hsiehchou121 kafka]$ bin/kafka-console-producer.sh --broker-list hsiehchou121:9092 --topic first&gt;hello world</code></pre><p>（4）查看hsiehchou122和hsiehchou123的接收者<br>        同一时刻只有一个消费者接收到消息</p><h3 id="八、-Kafka-API实战"><a href="#八、-Kafka-API实战" class="headerlink" title="八、 Kafka API实战"></a>八、 Kafka API实战</h3><h4 id="1、-环境准备"><a href="#1、-环境准备" class="headerlink" title="1、 环境准备"></a>1、 环境准备</h4><p>1）在eclipse中创建一个java工程<br>2）在工程的根目录创建一个lib文件夹<br>3）解压kafka安装包，将安装包libs目录下的jar包拷贝到工程的lib目录下，并build path<br>4）启动zk和kafka集群，在kafka集群中打开一个消费者</p><pre><code>[root@hsiehchou121 kafka]$ bin/kafka-console-consumer.sh --zookeeper hsiehchou121:2181 --topic first</code></pre><h4 id="2、Kafka生产者Java-API"><a href="#2、Kafka生产者Java-API" class="headerlink" title="2、Kafka生产者Java API"></a>2、Kafka生产者Java API</h4><p>1）<strong>创建生产者</strong><br><strong>Producer1 类</strong></p><pre><code>package com.hsiehchou.kafka.kafka_producer;import java.util.Properties;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;/** * kafka * @author hsiehchou */public class Producer1 {    public static void main(String[] args) {        //1.配置生产者的属性        Properties prop = new Properties();        //2.参数配置        //kafka节点的地址，Kafka服务端的主机名和端口号        prop.put(&quot;bootstrap.servers&quot;, &quot;192.168.116.121:9092&quot;);        //发送消息是否等待应答，等待所有副本节点的应答        prop.put(&quot;acks&quot;, &quot;all&quot;);        //配置发送消息失败重试，消息发送最大尝试次数        prop.put(&quot;retries&quot;, &quot;0&quot;);        //配置批量处理消息的大小，一批消息处理大小        prop.put(&quot;batch.size&quot;, &quot;16384&quot;);        //配置批量处理数据的延迟，请求延时        prop.put(&quot;linger.ms&quot;, &quot;1&quot;);        //配置内存缓冲区的大小，发送缓存区内存大小        prop.put(&quot;buffer.memory&quot;, 33445552);        //消息在发送前必须要序列化，key序列化        prop.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);        prop.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);        //3.实例化producer        KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(prop);        //4.发送消息        for (int i = 0; i &lt; 50; i++) {            producer.send(new ProducerRecord&lt;String, String&gt;(&quot;first&quot;, Integer.toString(i), &quot;hello world-&quot; + i));        }        //5.关闭资源        producer.close();        }}</code></pre><p>[root@hsiehchou122 kafka]# ./bin/kafka-console-consumer.sh <code>--bootstrap-server</code> hsiehchou121:9092 <code>--topic</code> first <code>--consumer.config</code> config/consumer.properties<br>hello world-0<br>hello world-1<br>hello world-2<br>hello world-3<br><code>......</code><br>hello world-49</p><p>2）创建生产者带回调函数<br><strong>Producer2类</strong></p><pre><code>package com.hsiehchou.kafka.kafka_producer;import java.util.Properties;import org.apache.kafka.clients.producer.Callback;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;public class Producer2 {    public static void main(String[] args) {        //1.配置生产者的属性        Properties prop = new Properties();        //2.参数配置        //kafka节点的地址，Kafka服务端的主机名和端口号        prop.put(&quot;bootstrap.servers&quot;, &quot;192.168.116.121:9092&quot;);        //发送消息是否等待应答，等待所有副本节点的应答        prop.put(&quot;acks&quot;, &quot;all&quot;);        //配置发送消息失败重试，消息发送最大尝试次数        prop.put(&quot;retries&quot;, &quot;0&quot;);        //配置批量处理消息的大小，一批消息处理大小        prop.put(&quot;batch.size&quot;, &quot;16384&quot;);        //配置批量处理数据的延迟，请求延时        prop.put(&quot;linger.ms&quot;, &quot;1&quot;);        //配置内存缓冲区的大小，发送缓存区内存大小        prop.put(&quot;buffer.memory&quot;, 33445552);        //消息在发送前必须要序列化，key序列化        prop.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);        prop.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);        //3.实例化producer        KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(prop);        //4.发送消息        for (int i = 0; i &lt; 50; i++) {            producer.send(new ProducerRecord&lt;String, String&gt;(&quot;first&quot;, Integer.toString(i), &quot;hello world-&quot; + i), new Callback() {                public void onCompletion(RecordMetadata metadata, Exception exception) {                    //如果metadata不为null，拿到当前的数据偏移量与分区                    if(metadata != null) {                        System.out.println(metadata.topic() + &quot;----&quot; + metadata.offset() + &quot;----&quot; + metadata.partition());                    }                }            });        }        //5.关闭资源            producer.close();        }}</code></pre><p>[root@hsiehchou122 kafka]# ./bin/kafka-console-consumer.sh <code>--bootstrap-server</code> hsiehchou121:9092 <code>--topic</code> three <code>--consumer.config</code> config/consumer.properties</p><p>控制台输出：<br>first<code>----</code>00<code>----</code>0<br>first<code>----</code>01<code>----</code>0<br>first<code>----</code>02<code>----</code>0<br>first<code>----</code>03<code>----</code>0<br><code>......</code><br>first<code>----</code>48<code>----</code>0<br>first<code>----</code>49<code>----</code>0</p><p>3）自定义分区生产者<br><strong>Partition1 类</strong></p><pre><code>package com.hsiehchou.kafka.kafka_producer;import java.util.Map;import org.apache.kafka.clients.producer.Partitioner;import org.apache.kafka.common.Cluster;/** * 自定义分区 * @author hsiehchou */public class Partition1 implements Partitioner{    //设置    public void configure(Map&lt;String, ?&gt; configs) {    }    //分区逻辑，控制分区    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {        return 2;    }    //释放资源    public void close() {    }}</code></pre><p><strong>Producer2类中新增</strong></p><pre><code>prop.put(&quot;partitioner.class&quot;, &quot;com.hsiehchou.kafka.kafka_producer.Partition1&quot;);</code></pre><p>[root@hsiehchou122 kafka]# ./bin/kafka-console-consumer.sh <code>--bootstrap-server</code> hsiehchou121:9092 <code>--topic</code> three <code>--consumer.config</code> config/consumer.properties<br>first—–1—-2<br>first—–1—-2<br>first—–1—-2<br><code>......</code><br>first—–1—-2<br>first—–1—-2</p><h4 id="3、Kafka消费者Java-API"><a href="#3、Kafka消费者Java-API" class="headerlink" title="3、Kafka消费者Java API"></a>3、Kafka消费者Java API</h4><p><strong>Consumer1 类</strong></p><pre><code>package com.hsiehchou.kafka.kafka_consumer;import java.util.Arrays;import java.util.Properties;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;/** * 创建消费者 * @author hsiehchou */public class Consumer1 {    public static void main(String[] args) {        //配置消费者属性        Properties prop = new Properties();        //2.参数配置        //kafka节点的地址，Kafka服务端的主机名和端口号        prop.put(&quot;bootstrap.servers&quot;, &quot;192.168.116.122:9092&quot;);        //配置消费者组        prop.put(&quot;group.id&quot;, &quot;g1&quot;);        //配置是否自动确认offset        prop.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);        //序列化        prop.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);        prop.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);        //3.实例消费者，定义consumer        final KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(prop);        //释放资源        //5.释放资源，线程安全        Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() {            public void run() {                if(consumer != null) {                    consumer.close();                }            }        }));        //订阅消息        consumer.subscribe(Arrays.asList(&quot;first&quot;));        //4.拉消息 推push 拉poll        while(true) {            ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);            //遍历消息            for(ConsumerRecord&lt;String, String&gt; record:records) {                System.out.println(record.topic() + &quot;--------&quot; + record.value());            }        }    }}</code></pre><p><strong>Producer1 类</strong><br>跟之前的一样，此处省略</p><p><strong>strong text</strong>启动kafka集群<br>[root@hsiehchou121 kafka]# ./bin/kafka-server-start.sh config/server.properties &amp;<br>[root@hsiehchou122 kafka]# ./bin/kafka-server-start.sh config/server.properties &amp;<br>[root@hsiehchou123 kafka]# ./bin/kafka-server-start.sh config/server.properties &amp;<br>分别对Consumer1类和Producer1类依次进行Run  as  Java Application</p><p><strong>控制台输出</strong><br>first——–1556248624834-hello world-0<br>first——–1556248625017-hello world-1<br>first——–1556248625017-hello world-2<br>first——–1556248625017-hello world-3<br><code>......</code><br>first——–1556248625021-hello world-48<br>first——–1556248625021-hello world-49</p><h3 id="九、Kafka-producer拦截器-interceptor"><a href="#九、Kafka-producer拦截器-interceptor" class="headerlink" title="九、Kafka producer拦截器(interceptor)"></a>九、Kafka producer拦截器(interceptor)</h3><h4 id="1、拦截器原理"><a href="#1、拦截器原理" class="headerlink" title="1、拦截器原理"></a>1、拦截器原理</h4><p>Producer拦截器(interceptor)是在Kafka 0.10版本被引入的，主要用于实现clients端的定制化控制逻辑</p><p>对于producer而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如<strong>修改消息</strong>等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。Intercetpor的实现接口是<strong>org.apache.kafka.clients.producer.ProducerInterceptor</strong>，其定义的方法包括：<br>1）<strong>configure(configs)</strong><br>获取配置信息和初始化数据时调用</p><p>2）<strong>onSend(ProducerRecord)</strong><br>该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。<strong>用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区</strong>，否则会影响目标分区的计算</p><p>3）<strong>onAcknowledgement(RecordMetadata, Exception)</strong><br><strong>该方法会在消息被应答或消息发送失败时调用</strong>，并且通常都是在producer回调逻辑触发之前。onAcknowledgement运行在producer的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率</p><p>4）<strong>close</strong><br><strong>关闭interceptor，主要用于执行一些资源清理工作</strong><br>如前所述，interceptor可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外<strong>倘若指定了多个interceptor，则producer将按照指定顺序调用它们</strong>，并仅仅是捕获每个interceptor可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意</p><h4 id="2、拦截器案例"><a href="#2、拦截器案例" class="headerlink" title="2、拦截器案例"></a>2、拦截器案例</h4><p>1）需求：<br>实现一个简单的双interceptor组成的拦截链。第一个interceptor会在消息发送前将时间戳信息加到消息value的最前部；第二个interceptor会在消息发送后更新成功发送消息数或失败发送消息数。<br>2）案例实操<br>（1）增加时间戳拦截器<br><strong>TimeInterceptor 类</strong></p><pre><code>package com.hsiehchou.kafka.interceptor;import java.util.Map;import org.apache.kafka.clients.producer.ProducerInterceptor;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;public class TimeInterceptor implements ProducerInterceptor&lt;String, String&gt; {    //配置信息    public void configure(Map&lt;String, ?&gt; configs) {    }    //业务逻辑    public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) {        return new ProducerRecord&lt;String, String&gt;(                record.topic(),                 record.partition(),                record.timestamp(),                record.key(),                System.currentTimeMillis() + &quot;-&quot; + record.value()        );    }    //发送失败调用    public void onAcknowledgement(RecordMetadata metadata, Exception exception) {    }    //关闭资源    public void close() {    }}</code></pre><p>（2）统计发送消息成功和发送失败消息数，并在producer关闭时打印这两个计数器<br><strong>CounterInterceptor 类</strong></p><pre><code>package com.hsiehchou.kafka.interceptor;import java.util.Map;import org.apache.kafka.clients.producer.ProducerInterceptor;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;public class CounterInterceptor implements ProducerInterceptor&lt;String, String&gt; {    private int errorCounter = 0;    private int successCounter = 0;    public void configure(Map&lt;String, ?&gt; configs) {    }    public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) {        return record;    }    public void onAcknowledgement(RecordMetadata metadata, Exception exception) {        //统计成功和失败的次数        if(exception == null) {            successCounter++;        }else {            errorCounter++;        }    }    public void close() {        //保存结果        System.out.println(&quot;Successful sent:&quot; + successCounter);        System.out.println(&quot;Failed sent:&quot; + errorCounter);    }}</code></pre><p><strong>在Producer1类中增加</strong></p><pre><code>//拦截器        ArrayList&lt;String&gt; inList = new ArrayList&lt;String&gt;();         inList.add(&quot;com.hsiehchou.kafka.interceptor.TimeInterceptor&quot;);        inList.add(&quot;com.hsiehchou.kafka.interceptor.CounterInterceptor&quot;);        prop.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, inList);</code></pre><p>测试一：<br>[root@hsiehchou122 kafka]# ./bin/kafka-server-start.sh config/server.properties &amp;</p><p><code>--from beginning</code> 是从头开始消费，不加则是消费当前正在发送到该topic的消息<br>[root@hsiehchou123 kafka]#./bin/kafka-console-consumer.sh <code>--bootstrap-server</code> hsiehchou121:9092 <code>--topic</code> first <code>--from-beginning</code></p><p>测试二：<br>[root@hsiehchou122 kafka]# ./bin/kafka-server-start.sh config/server.properties &amp;</p><p>[root@hsiehchou122 kafka]#  ./bin/kafka-console-consumer.sh <code>--bootstrap-server</code> hsiehchou121:9092 <code>--topic</code> first <code>--consumer.config</code> config/consumer.properties<br>结果：<br>1556253447912-hello world-0<br>1556253448100-hello world-1<br>1556253448100-hello world-2<br>1556253448100-hello world-3<br>1556253448100-hello world-4<br><code>......</code><br>1556253448100-hello world-12<br>1556253448100-hello world-13<br>1556253448104-hello world-14<br>1556253448104-hello world-15<br>1556253448104-hello world-16<br><code>......</code><br>1556253448104-hello world-48<br>1556253448104-hello world-49</p><p>Successful sent:50<br>Failed sent:0</p><h3 id="十、Kafka-Streams"><a href="#十、Kafka-Streams" class="headerlink" title="十、Kafka Streams"></a>十、Kafka Streams</h3><h4 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h4><p>1）Kafka Streams<br>Kafka Streams。Apache Kafka开源项目的一个组成部分。是一个功能强大，易于使用的库。用于在Kafka上构建高可分布式、拓展性，容错的应用程序</p><p>2）Kafka Streams特点</p><ol><li>功能强大<br>高扩展性，弹性，容错 </li><li>轻量级<br>无需专门的集群<br>一个库，而不是框架</li><li>完全集成<br>100%的Kafka 0.10.0版本兼容<br>易于集成到现有的应用程序 </li><li>实时性<br>毫秒级延迟<br>并非微批处理<br>窗口允许乱序数据<br>允许迟到数据</li></ol><p>3）为什么要有Kafka Stream<br>当前已经有非常多的流式处理系统，最知名且应用最多的开源流式处理系统有Spark Streaming和Apache Storm。Apache Storm发展多年，应用广泛，提供记录级别的处理能力，当前也支持SQL on Stream。而Spark Streaming基于Apache Spark，可以非常方便与图计算，SQL处理等集成，功能强大，对于熟悉其它Spark应用开发的用户而言使用门槛低。另外，目前主流的Hadoop发行版，如Cloudera和Hortonworks，都集成了Apache Storm和Apache Spark，使得部署更容易</p><p>既然Apache Spark与Apache Storm拥用如此多的优势，那为何还需要Kafka Stream呢？主要有如下原因<br>第一，Spark和Storm都是流式处理框架，而Kafka Stream提供的是一个基于Kafka的流式处理类库。框架要求开发者按照特定的方式去开发逻辑部分，供框架调用。开发者很难了解框架的具体运行方式，从而使得调试成本高，并且使用受限。而Kafka Stream作为流式处理类库，直接提供具体的类给开发者调用，整个应用的运行方式主要由开发者控制，方便使用和调试</p><p>第二，虽然Cloudera与Hortonworks方便了Storm和Spark的部署，但是这些框架的部署仍然相对复杂。而Kafka Stream作为类库，可以非常方便的嵌入应用程序中，它对应用的打包和部署基本没有任何要求</p><p>第三，就流式处理系统而言，基本都支持Kafka作为数据源。例如Storm具有专门的kafka-spout，而Spark也提供专门的spark-streaming-kafka模块。事实上，Kafka基本上是主流的流式处理系统的标准数据源。换言之，大部分流式系统中都已部署了Kafka，此时使用Kafka Stream的成本非常低</p><p>第四，使用Storm或Spark Streaming时，需要为框架本身的进程预留资源，如Storm的supervisor和Spark on YARN的node manager。即使对于应用实例而言，框架本身也会占用部分资源，如Spark Streaming需要为shuffle和storage预留内存。但是Kafka作为类库不占用系统资源</p><p>第五，由于Kafka本身提供数据持久化，因此Kafka Stream提供滚动部署和滚动升级以及重新计算的能力</p><p>第六，由于Kafka Consumer Rebalance机制，Kafka Stream可以在线动态调整并行度</p><h4 id="2、Kafka-Stream数据清洗案例"><a href="#2、Kafka-Stream数据清洗案例" class="headerlink" title="2、Kafka Stream数据清洗案例"></a>2、Kafka Stream数据清洗案例</h4><p>1）需求：<br>    实时处理单词带有”&gt;&gt;&gt;”前缀的内容。例如输入”itstar&gt;&gt;&gt;ximenqing”，最终处理成“ximenqing”<br>2）需求分析：<br>3）案例实操</p><ol><li>创建一个工程，并添加jar包</li><li>创建主类</li></ol><p><strong>Application类</strong></p><pre><code>package com.hsiehchou.kafka.kafka_stream;import java.util.Properties;import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.Topology;import org.apache.kafka.streams.processor.Processor;import org.apache.kafka.streams.processor.ProcessorSupplier;/** *  * 需求：对数据进行清洗操作 * 思路：xie-hs    把-清洗掉 * @author hsiehchou */public class Application {    public static void main(String[] args) {        //1.定义主题 发送到 另外一个主题中 数据清洗        String oneTopic = &quot;t1&quot;;        String twoTopic = &quot;t1&quot;;        //2.设置参数        Properties prop = new Properties();        prop.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;logProcessor&quot;);        prop.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;192.168.116.121:9092,192.168.116.122:9092,192.168.116.123:9092&quot;);        //3.实例化对象        StreamsConfig config = new StreamsConfig(prop);        //4. 流计算 拓扑        Topology builder = new Topology();         //5.定义kafka组件数据源        builder.addSource(&quot;Source&quot;, oneTopic).addProcessor(&quot;Processor&quot;, new ProcessorSupplier&lt;byte[], byte[]&gt;() {            public Processor&lt;byte[], byte[]&gt; get() {                return new LogProcesser();            }            //从哪里来        }, &quot;Source&quot;)        //到哪里去        .addSink(&quot;Sink&quot;, twoTopic, &quot;Processor&quot;);        //6.实例化KafkaStream        KafkaStreams kafkaStreams = new KafkaStreams(builder, prop);        kafkaStreams.start();    }}</code></pre><p><strong>LogPRocessor类</strong></p><pre><code>package com.hsiehchou.kafka.kafka_stream;import org.apache.kafka.streams.processor.Processor;import org.apache.kafka.streams.processor.ProcessorContext;public class LogProcesser implements Processor&lt;byte[], byte[]&gt;{    private ProcessorContext context;    //初始化    public void init(ProcessorContext context) {        //传输        this.context = context;    }    //业务逻辑    public void process(byte[] key, byte[] value) {        //1.拿到消息数据        String message = new String(value);        //2.如果包含 -  去除        if(message.contains(&quot;-&quot;)) {            //3.把-去掉之后去掉左侧数据            message = message.split(&quot;-&quot;)[1];            //4.发送数据            context.forward(key, message.getBytes());        }    }    //释放资源    public void close() {    }}</code></pre><p>[root@hsiehchou121 kafka]#./bin/kafka-topics.sh <code>--zookeeper</code> hsiehchou121:2181 <code>--create</code> <code>--replication-factor</code> 3 <code>--partitions</code> 1 <code>--topic</code> t1</p><p>[root@hsiehchou122 kafka]# ./bin/kafka-console-consumer.sh <code>--bootstrap-server</code> hsiehchou122:9092 <code>--topic</code> t2 <code>--from-beginning</code> -<code>-consumer.config</code> config/consumer.properties</p><p>[root@hsiehchou121 kafka]# ./bin/kafka-console-producer.sh <code>--broker-list</code> hsiehchou121:9092 <code>--topic</code> t1</p><h3 id="十一、CDH搭建："><a href="#十一、CDH搭建：" class="headerlink" title="十一、CDH搭建："></a>十一、CDH搭建：</h3><p><a href="https://juejin.im/post/5a55814e518825734859d69a" target="_blank" rel="noopener">https://juejin.im/post/5a55814e518825734859d69a</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker入门操作</title>
      <link href="/2019/04/24/docker-ru-men-cao-zuo/"/>
      <url>/2019/04/24/docker-ru-men-cao-zuo/</url>
      
        <content type="html"><![CDATA[<p><strong>docker</strong><br>2013年发布</p><h3 id="一、环境配置难题"><a href="#一、环境配置难题" class="headerlink" title="一、环境配置难题"></a>一、环境配置难题</h3><p>开发环境运行没有问题，生产不能用，因为生产缺乏某些组件</p><p>换一台机器，需要重新配置一遍</p><p>能不能从根本上解决问题：安装的时候，把原始环境，一模一样地安装一遍</p><h3 id="二、虚拟机"><a href="#二、虚拟机" class="headerlink" title="二、虚拟机"></a>二、虚拟机</h3><p>带环境安装的一种解决方案</p><p>缺点：<br>占用资源多：虚拟机本身需要消耗资源，程序1MB，环境几百MB</p><p>冗余步骤多：虚拟机是完整的操作系统，一些系统级别的操作步骤，无法跳过，比如用户登录</p><p>启动慢：启动操作系统要多久，启动虚拟机就要多久</p><h3 id="三、Linux容器"><a href="#三、Linux容器" class="headerlink" title="三、Linux容器"></a>三、Linux容器</h3><p>针对虚拟机的缺点，Linux发展出另外的一种虚拟化技术：Linux容器</p><p>Linux容器不是模拟完整的操作系统，而是对进程进行隔离，即在正常进程的外面，套一个保护层，对于容器里面的进程来说，它接触到的资源都是虚拟的，实现与底层系统的隔离</p><p>优点：<br>启动快：容器里面的应用，直接就是底层系统中的一个进程，启动容器相当于启动本机的进程。而不是启动操作系统</p><p>占用资源少：容器只占用需要的资源，不占用没有用到的资源</p><p>体积小：只包含用到的组件，而虚拟机包含了整个操作系统。所以容器文件比虚拟机文件小的多</p><h3 id="四、Docker是什么"><a href="#四、Docker是什么" class="headerlink" title="四、Docker是什么"></a>四、Docker是什么</h3><p>Docker属于Linux容器的一种封装，提供了简单易用的容器使用接口</p><p>Docker将应用程序与该程序的依赖，打包到一个文件里面，运行这个文件，就会产生一个虚拟容器</p><p>程序在虚拟容器中运行，就好像运行在真正的物理机上一样</p><p>Docker提供版本管理、复制、分享、修改等功能，就像管理普通代码一样管理Docker容器</p><h3 id="五、Docker的用途"><a href="#五、Docker的用途" class="headerlink" title="五、Docker的用途"></a>五、Docker的用途</h3><p>Docker的主要用途，目前有三大类</p><h4 id="1、提供一次性的环境"><a href="#1、提供一次性的环境" class="headerlink" title="1、提供一次性的环境"></a>1、提供一次性的环境</h4><p>本地测试他人的软件程序</p><h4 id="2、提供弹性的云服务"><a href="#2、提供弹性的云服务" class="headerlink" title="2、提供弹性的云服务"></a>2、提供弹性的云服务</h4><p>Docker容器可以随开随关，很适合动态的扩容和缩容</p><h4 id="3、组建微服务架构"><a href="#3、组建微服务架构" class="headerlink" title="3、组建微服务架构"></a>3、组建微服务架构</h4><p>通过多个容器，一台机器可以跑多个服务，在本机就可以模拟出微服务架构</p><h3 id="六、Docker安装"><a href="#六、Docker安装" class="headerlink" title="六、Docker安装"></a>六、Docker安装</h3><h4 id="1、Linux安装"><a href="#1、Linux安装" class="headerlink" title="1、Linux安装"></a>1、Linux安装</h4><p>Docker要求CentOS内核版本高于3.10<br>uname -r 查看内核版本</p><p>安装必要的系统工具：<br>yum install -y yum-utils device-mapper-persistent-data lvm2</p><p>添加软件源信息：<br>yum-config-manager –add-repo <a href="http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo" target="_blank" rel="noopener">http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</a></p><p>更新 yum 缓存：<br>yum makecache fast</p><p>安装 Docker-ce：<br>yum -y install docker-ce</p><p>启动 Docker 后台服务<br>systemctl start docker</p><p>测试运行 hello-world<br>docker run hello-world</p><p>看到hello from docker证明安装成功</p><h4 id="2、windows安装"><a href="#2、windows安装" class="headerlink" title="2、windows安装"></a>2、windows安装</h4><p>win10专业版，直接安装 docker for windows 即可</p><p>win10普通版、win7 win8 ,需要安装 docker tool box</p><p>toolbox 配置：<br>右键  Docker Quickstart Terminal<br>“C:\Program Files\Git\bin\bash.exe” <code>--login</code> -i “C:\Program Files\Docker Toolbox\start.sh”<br>把这个位置配成你本机的git位置                        修改后面这个脚本</p><p>DOCKER_MACHINE=”D:\Docker Toolbox\docker-machine.exe”</p><p>STEP=”Looking for vboxmanage.exe”<br>VBOXMANAGE=”C:\Program Files\Oracle\VirtualBox\VBoxManage.exe”</p><pre><code>#if [ ! -z &quot;$VBOX_MSI_INSTALL_PATH&quot; ]; then#VBOXMANAGE=&quot;${VBOX_MSI_INSTALL_PATH}VBoxManage.exe&quot;#else#VBOXMANAGE=&quot;${VBOX_INSTALL_PATH}VBoxManage.exe&quot;#fi</code></pre><h3 id="七、image文件"><a href="#七、image文件" class="headerlink" title="七、image文件"></a>七、image文件</h3><p>Docker把应用程序及其依赖，打包在image文件里面，只有通过image文件，才能生成docker容器</p><p>Docker可以根据image文件生成容器实例</p><p>image文件可以继承。在实际开发中，一个image文件往往通过继承另一个image文件，加上一些个性化的设置而生成</p><p>启动容器<br>docker run hello-world</p><p>列出所有image文件<br>docker image ls</p><p>删除image文件<br>docker image rm image文件名</p><p>使用docker-machine stop default停掉Docker的虚拟机<br>使用docker-machine start default开启Docker的虚拟机</p><h3 id="八、配置阿里云docker镜像加速器"><a href="#八、配置阿里云docker镜像加速器" class="headerlink" title="八、配置阿里云docker镜像加速器"></a>八、配置阿里云docker镜像加速器</h3><ol><li><p>注册阿里云，获得专属加速器地址</p></li><li><p>hsiehchou@DESKTOP-KJDN870 MINGW64 /d/Docker Toolbox<br>$ docker-machine ssh default<br>docker@default:<del>$ sudo sed -i “s|EXTRA_ARGS=’|EXTRA_ARGS=’–regis<br>try-mirror=https://<code>********</code>.mirror.aliyuncs.com |g” /var/lib/boot<br>2docker/profile<br>docker@default:</del>$ exit</p></li><li><p>hsiehchou@DESKTOP-KJDN870 MINGW64 /d/Docker Toolbox<br>$ docker-machine restart default</p></li></ol><h3 id="九、安装redis"><a href="#九、安装redis" class="headerlink" title="九、安装redis"></a>九、安装redis</h3><h4 id="1、搜索镜像"><a href="#1、搜索镜像" class="headerlink" title="1、搜索镜像"></a>1、搜索镜像</h4><p>docker search redis</p><h4 id="2、拉取镜像"><a href="#2、拉取镜像" class="headerlink" title="2、拉取镜像"></a>2、拉取镜像</h4><p>docker pull redis</p><h4 id="3、启动redis"><a href="#3、启动redis" class="headerlink" title="3、启动redis"></a>3、启动redis</h4><p>docker run <code>--name</code> myredis -p 6379:6379 -d redis redis-server</p><p>-d表示后台运行</p><p>-p表示端口号，左边的6379表示win10系统端口考，右边表示容器中redis端口号</p><p><code>--name</code>表示运行redis镜像的实例名称</p><h4 id="4、进入Image的小环境"><a href="#4、进入Image的小环境" class="headerlink" title="4、进入Image的小环境"></a>4、进入Image的小环境</h4><p>docker exec -it  <code>COXTAINER ID</code> bash<br>redis-cli</p><p>hsiehchou@DESKTOP-KJDN870 MINGW64 /d/Docker Toolbox<br>$ docker run <code>--name</code> myredis12 -p 6379:6379 -d redis redis-server<br>9e37ae7338d05b83f666a95fe3677aa85b1481c8fa519a79e9008a5a5e9e909d</p><p>hsiehchou@DESKTOP-KJDN870 MINGW64 /d/Docker Toolbox<br>$ docker ps<br>CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES<br>9e37ae7338d0        redis               “docker-entrypoint.s”   5 seconds ago       Up 6 seconds        0.0.0.0:6379-&gt;6379/tcp   myredis12</p><p>hsiehchou@DESKTOP-KJDN870 MINGW64 /d/Docker Toolbox<br>$ docker exec -it 9e37ae7338d0 bash<br>root@9e37ae7338d0:/data# redis-cli<br>127.0.0.1:6379&gt;</p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker入门 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git简单操作</title>
      <link href="/2019/04/23/git-jian-dan-cao-zuo/"/>
      <url>/2019/04/23/git-jian-dan-cao-zuo/</url>
      
        <content type="html"><![CDATA[<p><strong>git 版本控制系统</strong><br>git是一个版本控制系统</p><h3 id="一、什么是版本控制系统"><a href="#一、什么是版本控制系统" class="headerlink" title="一、什么是版本控制系统"></a>一、什么是版本控制系统</h3><h4 id="1、概念"><a href="#1、概念" class="headerlink" title="1、概念"></a>1、概念</h4><p>版本控制是一种 记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统</p><p>（<em>）记录文件的所有历史变化<br>（</em>）随时可恢复到任何一个历史状态<br>（<em>）多人协作开发或修改<br>（</em>）错误恢复<br>（*）多功能并行开发</p><p>产品–&gt; 新加功能A —&gt; 单独拉一个新分支 –&gt; 开发完成后合并到master或者丢弃</p><h4 id="2、分类"><a href="#2、分类" class="headerlink" title="2、分类"></a>2、分类</h4><p>本地版本控制系统<br>集中化版本控制系统        SVN<br>分布式版本控制系统        Git</p><h4 id="3、基本概念"><a href="#3、基本概念" class="headerlink" title="3、基本概念"></a>3、基本概念</h4><p>repository ：存放所有文件及其历史信息<br>checkout    ：取出或切换到执行版本的文件<br>version    ：表示一个版本<br>tag    ：记录标识一个主要版本。2.0 3.0。用来标识一个特定的version</p><h4 id="4、不同版本控制系统优缺点"><a href="#4、不同版本控制系统优缺点" class="headerlink" title="4、不同版本控制系统优缺点"></a>4、不同版本控制系统优缺点</h4><p><strong>本地</strong><br>优点：<br>简单，很多系统中内置。适合保存文本文件（配置文件、文章、信件）</p><p>缺点：<br>    只支持管理少量的文件，不支持基于项目的管理<br>    支持的文件类型单一<br>    不支持网络，无法实现多人协作</p><p><strong>集中式版本控制系统</strong><br>优点：<br>    适合多人团队协作开发<br>    代码集中化管理</p><p>缺点：<br>    单点故障<br>    必须联网工作，无法单机工作</p><p>解决：<br><strong>分布式版本控制系统</strong><br>集合集中式版本控制系统优点<br>支持离线工作，先提交到本地仓库，再在某个时间上传到远程仓库<br>每个计算机都是一个完整仓库：强备份</p><h3 id="二、git分布式版本管理系统"><a href="#二、git分布式版本管理系统" class="headerlink" title="二、git分布式版本管理系统"></a>二、git分布式版本管理系统</h3><p>由Linux创始人开发，作为Linux内核代码管理系统使用</p><p>Git在设计时考虑了很多方面设计目标</p><p>特点：<br>速度<br>简单的设计<br>对非线性开发模式的强力支持（允许上千个并行开发的分支）<br>完全分布式<br>有能力管理超大规模项目（挑战：速度和数据量）</p><p>Git原理：保存快照而非保存区别<br>Git保存时，相当于保存了当下所有文件的一个整体快照<br>所以，每个版本都是独立的。随时想取某一个版本，可以很快取出来</p><h3 id="三、安装git"><a href="#三、安装git" class="headerlink" title="三、安装git"></a>三、安装git</h3><p>Git 的工作区域：</p><p>Git repository：：    最终确定的文件保存到仓库，作为一个新的版本<br>staging area：            暂存已经修改的文件<br>woking directory：    工作目录</p><p>安装git<br>从 <a href="https://git-scm.com/" target="_blank" rel="noopener">https://git-scm.com/</a> 下载windows版本git<br>全使用默认值，一直下一步</p><h3 id="四、创建仓库和基本操作"><a href="#四、创建仓库和基本操作" class="headerlink" title="四、创建仓库和基本操作"></a>四、创建仓库和基本操作</h3><p>git安装好后，需要一些基本设置<br>设置用户名：git config –global user.name “hsiehchou”<br>设置邮箱：git config –global user.email  “<a href="mailto:417952939@qq.com">417952939@qq.com</a>“</p><p>查看所有设置：git config –list</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git init</strong><br>Reinitialized existing Git repository in C:/Users/hsiehchou/Desktop/git demo/.git/</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ ll -a</strong><br>total 52<br>drwxr-xr-x 1 hsiehchou 197121 0 4月  24 12:45 ./<br>drwxr-xr-x 1 hsiehchou 197121 0 4月  24 10:44 ../<br>drwxr-xr-x 1 hsiehchou 197121 0 4月  24 12:46 .git/</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git status</strong><br>On branch master</p><p>No commits yet</p><p>nothing to commit (create/copy files and use “git add” to track)<br>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ touch README</strong></p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ vi README</strong></p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git status</strong><br>On branch master</p><p>No commits yet</p><p><strong>未追踪的文件</strong><br><strong>Untracked files</strong>:<br>  (use “git add <code>&lt;file&gt;</code>…” to include in what will be committed)<br> README</p><p>nothing added to commit but untracked files present (use “git add” to track)</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>新建文件，默认是未追踪的文件<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git add README</strong><br>warning: LF will be replaced by CRLF in README.<br>The file will have its original line endings in your working directory.</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git status</strong><br>On branch master</p><p>No commits yet</p><p>Changes to be committed:<br>  (use “git rm –cached <code>&lt;file&gt;</code>…” to unstage)<br> new file:   README</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>git add 提交到了暂存区域<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git commit -m “add README”</strong><br>[master (root-commit) f50e736] add README<br> 1 file changed, 1 insertion(+)<br> create mode 100644 README</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br><strong>查看历史</strong><br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git log</strong><br>commit f50e7362b5573e9b9862f7fb65a9e3f6fa98913a (HEAD -&gt; master)<br>Author: hsiehchou <a href="mailto:&#52;&#49;&#x37;&#57;&#x35;&#x32;&#x39;&#51;&#57;&#x40;&#x71;&#x71;&#46;&#x63;&#x6f;&#x6d;">&#52;&#49;&#x37;&#57;&#x35;&#x32;&#x39;&#51;&#57;&#x40;&#x71;&#x71;&#46;&#x63;&#x6f;&#x6d;</a><br>Date:   Wed Apr 24 12:52:51 2019 +0800<br>add README</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br><strong>修改文件</strong><br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ vim README</strong></p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br><strong>检测到被修改了</strong><br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git status</strong><br>On branch master<br>Changes not staged for commit:<br>  (use “git add <code>&lt;file&gt;</code>…” to update what will be committed)<br>  (use “git checkout – <code>&lt;file&gt;</code>…” to discard changes in working directory)</p><p> modified:   README</p><p>no changes added to commit (use “git add” and/or “git commit -a”)</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br><strong>提交到仓库</strong><br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git commit -a -m “modify README”</strong><br>warning: LF will be replaced by CRLF in README.<br>The file will have its original line endings in your working directory<br>[master 278ec6a] modify README<br> 1 file changed, 1 insertion(+)</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br><strong>删除文件</strong><br>rm README<br>git rm README<br>git commit -m “delete README”</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ rm README</strong></p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git rm README</strong><br>rm ‘README’</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git commit -m “delete README”</strong><br>[master b82ec4f] delete README<br> 1 file changed, 2 deletions(-)<br> delete mode 100644 README</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br><strong>checkout 某个版本</strong><br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git log</strong><br>commit b82ec4f7ccf718abac6dc630b7049618d179418f (HEAD -&gt; master)<br>Author: hsiehchou <a href="mailto:&#x34;&#49;&#x37;&#57;&#x35;&#x32;&#x39;&#51;&#x39;&#64;&#113;&#x71;&#x2e;&#99;&#x6f;&#x6d;">&#x34;&#49;&#x37;&#57;&#x35;&#x32;&#x39;&#51;&#x39;&#64;&#113;&#x71;&#x2e;&#99;&#x6f;&#x6d;</a><br>Date:   Wed Apr 24 12:56:45 2019 +0800</p><p> delete README</p><p>commit 278ec6a869f73af71539785f6893259726f9902e<br>Author: hsiehchou <a href="mailto:&#x34;&#x31;&#x37;&#x39;&#53;&#x32;&#57;&#x33;&#57;&#x40;&#113;&#113;&#x2e;&#99;&#111;&#109;">&#x34;&#x31;&#x37;&#x39;&#53;&#x32;&#57;&#x33;&#57;&#x40;&#113;&#113;&#x2e;&#99;&#111;&#109;</a><br>Date:   Wed Apr 24 12:56:00 2019 +0800</p><p>modify README</p><p>commit f50e7362b5573e9b9862f7fb65a9e3f6fa98913a<br>Author: hsiehchou <a href="mailto:&#52;&#49;&#55;&#x39;&#x35;&#50;&#57;&#x33;&#x39;&#x40;&#113;&#x71;&#x2e;&#x63;&#x6f;&#109;">&#52;&#49;&#55;&#x39;&#x35;&#50;&#57;&#x33;&#x39;&#x40;&#113;&#x71;&#x2e;&#x63;&#x6f;&#109;</a><br>Date:   Wed Apr 24 12:52:51 2019 +0800</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo ((b82ec4f…))<br><strong>$ git checkout 278ec6a869f73af71539785f6893259726f9902e</strong><br>Previous HEAD position was b82ec4f delete README<br>HEAD is now at 278ec6a modify README</p><p>You are in ‘detached HEAD’ state. You can look around, make experimental<br>changes and commit them, and you can discard any commits you make in this<br>state without impacting any branches by performing another checkout.</p><p>If you want to create a new branch to retain commits you create, you may<br>do so (now or later) by using -b with the checkout command again. Example:</p><p>  git checkout -b <code>&lt;new-branch-name&gt;</code></p><p>HEAD is now at 278ec6a modify README</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo ((278ec6a…))<br><strong>$ ll</strong><br>total 1<br>-rw-r–r– 1 hsiehchou 197121 22 4月  24 12:58 README</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo ((278ec6a…))<br><strong>$ cat README</strong><br>Hello World!<br>fsdggd</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo ((278ec6a…))<br><strong>$ git checkout master</strong><br>Previous HEAD position was 278ec6a modify README<br>Switched to branch ‘master’</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ ll</strong><br>total 0</p><h3 id="五、git远程仓库"><a href="#五、git远程仓库" class="headerlink" title="五、git远程仓库"></a>五、git远程仓库</h3><p>实现代码共享<br>远程仓库实际保存了本地.git文件夹下的东西，内容几乎一样<br>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ touch README2</strong></p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ vim README2</strong></p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git add README2</strong><br>warning: LF will be replaced by CRLF in README2.<br>The file will have its original line endings in your working directory</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git commit -a -m “README2”</strong><br>[master 0c4f333] README2<br> 1 file changed, 1 insertion(+)<br> create mode 100644 README2</p><p><strong>git 远程仓库访问协议</strong>：<br>ssh协议<br>git协议<br>http https协议：一般用于开源项目</p><p>常用远程仓库实现：<br>1、github<br>2、自己搭建git仓库服务器 gitlab、码云</p><p>举例：在自己的github中，关联本地仓库<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ cd ~/.ssh</strong><br>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/.ssh<br><strong>$ ll</strong><br>total 9<br>-rw-r–r– 1 hsiehchou 197121 1823 4月  15 23:35 id_rsa<br>-rw-r–r– 1 hsiehchou 197121  398 4月  15 23:35 id_rsa.pub<br>-rw-r–r– 1 hsiehchou 197121 1197 4月  16 14:09 known_hosts</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/.ssh<br><strong>$ cat id_rsa.pub</strong></p><p> <strong>创建公钥</strong><br>ssh-keygen -t rsa -C “<a href="mailto:417952939@qq.com">417952939@qq.com</a>“命令</p><p>测试是否能够正常连接github<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ ssh -T <a href="mailto:git@github.com">git@github.com</a></strong><br>Hi hsiehchou! You’ve successfully authenticated, but GitHub does not provide shell access.</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git remote add origin <a href="mailto:git@github.com">git@github.com</a>:hsiehchou/git-test.git</strong></p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ ll</strong><br>total 1<br>-rw-r–r– 1 root 197121 15 4月  21 22:13 README2</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++<br>hsiehchou@DESKTOP-KJDN870 MINGW64 ~/Desktop/git demo (master)<br><strong>$ git push -u origin master</strong><br>Enumerating objects: 11, done.<br>Counting objects: 100% (11/11), done.<br>Delta compression using up to 4 threads<br>Compressing objects: 100% (4/4), done.<br>Writing objects: 100% (11/11), 826 bytes | 103.00 KiB/s, done.<br>Total 11 (delta 0), reused 0 (delta 0)<br>To github.com:hsiehchou/git-test.git</p><ul><li>[new branch]      master -&gt; master<br>Branch ‘master’ set up to track remote branch ‘master’ from ‘origin’.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop HA和HBase HA</title>
      <link href="/2019/04/22/hadoop-ha-he-hbase-ha/"/>
      <url>/2019/04/22/hadoop-ha-he-hbase-ha/</url>
      
        <content type="html"><![CDATA[<p><strong>Hadoop  HBase HA</strong></p><p>保证所有的服务器时间都相同</p><h3 id="一、Hadoop-HA"><a href="#一、Hadoop-HA" class="headerlink" title="一、Hadoop HA"></a>一、Hadoop HA</h3><p><strong>HDFS HA</strong></p><p>/root/hd/hadoop-2.8.4/etc/hadoop 下是所有hadoop配置文件</p><h4 id="1、core-site-xml"><a href="#1、core-site-xml" class="headerlink" title="1、core-site.xml"></a>1、core-site.xml</h4><pre><code>&lt;configuration&gt;    &lt;property&gt;         &lt;name&gt;fs.defaultFS&lt;/name&gt;         &lt;value&gt;hdfs://mycluster&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;          &lt;value&gt;hsiehchou123:2181,hsiehchou124:2181&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;         &lt;value&gt;/root/hd/hadoop-2.8.4/tmp&lt;/value&gt;:    &lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="2、hdfs-site-xml"><a href="#2、hdfs-site-xml" class="headerlink" title="2、hdfs-site.xml"></a>2、hdfs-site.xml</h4><pre><code>&lt;configuration&gt;    &lt;property&gt;         &lt;name&gt;dfs.nameservices&lt;/name&gt;         &lt;value&gt;mycluster&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;         &lt;value&gt;nn1,nn2&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;         &lt;value&gt;hsiehchou121:8020&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;         &lt;value&gt;hsiehchou122:8020&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;         &lt;value&gt;hsiehchou121:50070&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;         &lt;value&gt;hsiehchou122:50070&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;         &lt;value&gt;qjournal://hsiehchou123:8485;hsiehchou124:8485/mycluster&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;         &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;         &lt;value&gt;sshfence&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;         &lt;value&gt;/root/.ssh/id_dsa&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;          &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>NameNode节点一般配置2台；<strong>qjournal</strong>—— journal节点一般配置3台<br>我这里开始只有四台，所以，journal节点我只分配了两台</p><h4 id="3、yarn-site-xml"><a href="#3、yarn-site-xml" class="headerlink" title="3、yarn-site.xml"></a>3、yarn-site.xml</h4><pre><code>&lt;configuration&gt;    &lt;!-- Site specific YARN configuration properties --&gt;    &lt;!-- Site specific YARN configuration properties --&gt;    &lt;property&gt;         &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;         &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;         &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;          &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;          &lt;value&gt;yarncluster&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;          &lt;value&gt;rm1,rm2&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;          &lt;value&gt;hsiehchou121&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;          &lt;value&gt;hsiehchou122&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;          &lt;value&gt;hsiehchou123,hsiehchou124&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;          &lt;value&gt;32768&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;        &lt;value&gt;32768&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;        &lt;value&gt;4096&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;          &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;        &lt;value&gt;24&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;        &lt;value&gt;/tmp/yarn-logs&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>scp -r  hadoop/ hsiehchou122:/root/hd/hadoop-2.8.4/etc<br>scp -r  hadoop/ hsiehchou123:/root/hd/hadoop-2.8.4/etc<br>scp -r  hadoop/ hsiehchou124:/root/hd/hadoop-2.8.4/etc</p><p>配置好后，分发到所有节点，启动ZooKeeper后<br>start-all.sh 即可启动所有</p><h3 id="二、HBase-HA"><a href="#二、HBase-HA" class="headerlink" title="二、HBase HA"></a>二、HBase HA</h3><p>修改配置文件，分发到所有几点，启动即可<br>注意：要启动两个Master，其中一个需要手动启动</p><p>注意：Hbase安装时，需要对应Hadoop版本</p><p>hbase hbase-2.1.4  对应 hadoop  2.8.4</p><p>通常情况下，把Hadoop  core-site hdfs-site 拷贝到hbase conf下</p><p>修改 hbase-env.sh<br>修改  hbase-site.xml</p><h4 id="1、hbase-env-sh"><a href="#1、hbase-env-sh" class="headerlink" title="1、hbase-env.sh"></a>1、hbase-env.sh</h4><p>export JAVA_HOME=/root/hd/jdk1.8.0_192</p><p>export HBASE_MANAGES_ZK=false<br>关闭hbase自带的zookeeper，使用集群zookeeper</p><h4 id="2、hbase-site-xml"><a href="#2、hbase-site-xml" class="headerlink" title="2、hbase-site.xml"></a>2、hbase-site.xml</h4><pre><code>&lt;configuration&gt;&lt;property&gt;&lt;name&gt;hbase.cluster.distributed&lt;/name&gt;&lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt;&lt;name&gt;hbase.rootdir&lt;/name&gt;&lt;value&gt;hdfs://mycluster/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt;&lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;&lt;value&gt;hsiehchou123,hsiehchou124&lt;/value&gt; &lt;/property&gt; &lt;property&gt;&lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;&lt;value&gt;2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt;&lt;name&gt;zookeeper.session.timeout&lt;/name&gt;&lt;value&gt;120000&lt;/value&gt; &lt;/property&gt; &lt;property&gt;&lt;name&gt;hbase.zookeeper.property.tickTime&lt;/name&gt;&lt;value&gt;6000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>启动HBase<br>需要从另一台服务器上单独启动Master<br>./hbase-daemon.sh start master</p><p>通过以下网站可以看到信息<br><a href="http://192.168.116.122:16010/master-status" target="_blank" rel="noopener">http://192.168.116.122:16010/master-status</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hadoop HA </tag>
            
            <tag> HBase HA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>内存数据库专题（MemCached和Redis）</title>
      <link href="/2019/04/18/nei-cun-shu-ju-ku-zhuan-ti-memcached-he-redis/"/>
      <url>/2019/04/18/nei-cun-shu-ju-ku-zhuan-ti-memcached-he-redis/</url>
      
        <content type="html"><![CDATA[<p><strong>内存数据库专题</strong><br>为什么要把数据存入内存？<br>快</p><p>常见的内存数据库：<br>MemCached：看成Redis前身，严格来说，MemCached不能叫数据库，只能叫缓存<br>不支持持久化。如果内存停电，数据丢失</p><p>Redis：内存数据库，支持持久化，支持HA</p><p>Oracle TimesTen</p><p>session一致性</p><p>MemCached + keepalive实现</p><h3 id="一、Memcached"><a href="#一、Memcached" class="headerlink" title="一、Memcached"></a>一、Memcached</h3><h4 id="1、基本原理和体系架构"><a href="#1、基本原理和体系架构" class="headerlink" title="1、基本原理和体系架构"></a>1、基本原理和体系架构</h4><p>（<em>）在内存中，维护了一张巨大的Hash表<br>（</em>）通过路由算法来决定数据存储的位置。—&gt; 客户端路由<br><img src="/medias/Memcached%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E5%92%8C%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84.PNG" alt="Memcached基本原理和体系架构"></p><p>注意：默认，官方版本MemCached实例彼此不会进行通信</p><p>第三方版本可以实现通信</p><h4 id="2、安装配置MemCached"><a href="#2、安装配置MemCached" class="headerlink" title="2、安装配置MemCached"></a>2、安装配置MemCached</h4><p>前提：<br>（1）gcc编译器<br>yum install gcc<br>gcc –version</p><p>（2）libevent库<br>tar -zxvf libevent-2.0.21-stable.tar.gz<br>cd libevent-2.0.21-stable<br>./configure <code>--prefix</code>=/root/hd/libevent<br>make<br>make install</p><p>tar -zxvf memcached-1.4.25.tar.gz<br>cd memcached-1.4.25</p><p>./configure <code>--prefix</code>=/root/hd/memcached <code>--with-libevent</code>=/root/hd/libevent<br>make<br>make install<br>cd bin/<br>./memcached -u root -d -m 128 -p 11211<br>./memcached -u root -d -m 128 -p 11212<br>./memcached -u root -d -m 128 -p 11213<br>ps -ef | grep memcached</p><p><strong>注意</strong>：<br>-u：root用户需要注明（其他用户可以不写）<br>-d：启动守护线程（在后天运行）<br>-m：占用多少内存<br>-p：运行在哪个端口    </p><h4 id="3、操作MemCached"><a href="#3、操作MemCached" class="headerlink" title="3、操作MemCached"></a>3、操作MemCached</h4><p>（*）命令行<br>telnet 192.168.116.121 11211</p><p>保存数据：<br>set 如果key存在，替换原来的值<br>add 如果key存在，返回错误</p><table><thead><tr><th>set key1</th><th align="center">0</th><th align="center">0</th><th align="center">4</th></tr></thead><tbody><tr><td>key名字</td><td align="center">标识位</td><td align="center">数据过期时间0表示不过期</td><td align="center">value的长度</td></tr><tr><td>abcd</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td>get key1</td><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table><p>统计命令<br>stats items<br>stats</p><pre><code>package day1;import net.spy.memcached.MemcachedClient;import net.spy.memcached.internal.OperationFuture;import java.io.IOException;import java.io.Serializable;import java.net.InetSocketAddress;import java.util.ArrayList;import java.util.List;import java.util.concurrent.ExecutionException;import java.util.concurrent.Future;/** * java 操作Memcached */public class Demo1 {    public static void main(String[] args) throws Exception {        //hello();        //testGet();        //setStudent();        testSets() ;    }</code></pre><p>[root@hsiehchou121 memcached]# cd bin/<br>[root@hsiehchou121 bin]# ./memcached -u root -d -m 128 -p 11211<br>[root@hsiehchou121 bin]# ./memcached -u root -d -m 128 -p 11212<br>[root@hsiehchou121 bin]# ./memcached -u root -d -m 128 -p 11213</p><p><strong>插入数据</strong></p><pre><code> public static void hello() throws Exception {        //连接到集群，set key        //建立MemcachedClient实例，指定Memcached服务器的IP地址和端口        MemcachedClient client = new MemcachedClient(new InetSocketAddress(&quot;192.168.116.121&quot;,11211));        Future&lt;Boolean&gt; f = client.set(&quot;key1&quot;, 0, &quot;hello World&quot;);        if (f.get().booleanValue()){            client.shutdown();        }    }</code></pre><p>[root@hsiehchou121 ~]# telnet 192.168.116.121 11211<br>get key1</p><p><strong>查询数据</strong></p><pre><code>public static void testGet() throws Exception{    //连接到集群，set key    //建立MemcachedClient实例，指定Memcached服务器的IP地址和端口    MemcachedClient client = new MemcachedClient(new InetSocketAddress(&quot;192.168.116.121&quot;,11211));    //按照key取值，不存在的话返回null    Object o = client.get(&quot;key1&quot;);    System.out.println(&quot;取到的值为： &quot; + o);    client.shutdown();}</code></pre><p>取到的值为：hello World</p><p><strong>插入类</strong></p><pre><code>   public static void setStudent() throws  Exception{        //连接到集群，set key        //建立MemcachedClient实例，指定Memcached服务器的IP地址和端口        MemcachedClient client = new MemcachedClient(new InetSocketAddress(&quot;192.168.116.121&quot;,11211));        Future&lt;Boolean&gt; f = client.set(&quot;stu1&quot;, 0, new Student());        if (f.get().booleanValue()){            client.shutdown();        }    }</code></pre><p>[root@hsiehchou121 ~]# telnet 192.168.116.121 11211<br>get stu1</p><p><strong>基于客户端的分布式插入数据</strong></p><pre><code> public static void testSets() throws Exception{        //测试客户端路由算法        //构造每台Memcached服务器加入List        List&lt;InetSocketAddress&gt; list = new ArrayList&lt;&gt;();        list.add(new InetSocketAddress(&quot;192.168.116.121&quot;,11211));        list.add(new InetSocketAddress(&quot;192.168.116.121&quot;,11212));        list.add(new InetSocketAddress(&quot;192.168.116.121&quot;,11213));        //建立Memcached Client实例        MemcachedClient client = new MemcachedClient(list);        for (int i = 0; i &lt; 20; i++){            System.out.println(&quot;插入数据：&quot; + i);            client.set(&quot;key&quot;+i,0, &quot;value&quot;+i);//(key1,value1)(key2,value2)            Thread.sleep(1000);        }        client.shutdown();    }</code></pre><p>[root@hsiehchou121 ~]# telnet 192.168.116.121 11211<br>get key1<br>VALUE key1 0 6<br>value1<br>[root@hsiehchou121 ~]# telnet 192.168.116.121 11212<br>get key2<br>VALUE key2 0 6<br>value2<br>[root@hsiehchou121 ~]# telnet 192.168.116.121 11213<br>get key3<br>VALUE key3 0 6<br>value3</p><pre><code>}class Student implements Serializable{}</code></pre><h4 id="4、MemCached路由算法"><a href="#4、MemCached路由算法" class="headerlink" title="4、MemCached路由算法"></a>4、MemCached路由算法</h4><p>1）<strong>求余数hash算法</strong><br>用key做hash运算得到一个整数，根据余数路由<br>例如：服务器端有三台MemCached服务器<br>根据key，做hash运算<br>7%3=1，那么就路由到第2台服务器<br>6%3=0，那么路由到第1台服务器<br>5%3=2，那么路由到第3台服务器</p><p>优点：数据分布均衡在多台服务器中，适合大多数据需求<br>缺点：如果需要扩容或者有宕机的情况，会造成数据的丢失</p><p>2）<strong>一致性hash算法</strong><br><strong>基本原理</strong>：<br>key1  1-333 放在node1上<br>key2 334-666放在node2上<br>key3 667-1000放在hsiehchou121上</p><p><strong>一致性hash算法下扩容</strong><br>key1  1-333 放在node1上<br>key2 334-666放在node2上<br>key3 667-831放在hsiehchou121上<br><strong>key4 832-1000放在node4上</strong><br>如果扩容，增加一个新的节点，只影响扩容的节点，对其他节点不影响</p><p><strong>一致性hash算法下DOWN机</strong><br>key1  1-333 放在node1上<br>key2 334-666放在node2上<br>key3 667-1000放在hsiehchou121上<br>如果宕机，对key1 和 key2不产生影响，只对key3产生影响</p><h4 id="5、MemCached的主主复制和HA"><a href="#5、MemCached的主主复制和HA" class="headerlink" title="5、MemCached的主主复制和HA"></a>5、MemCached的主主复制和HA</h4><p>1）Memcached主主复制<br><strong>架构图</strong>   </p><table><thead><tr><th align="center">主服务器</th><th align="center"></th><th align="center">主服务器</th></tr></thead><tbody><tr><td align="center">memcached</td><td align="center">&lt;——-&gt;</td><td align="center">memcached</td></tr></tbody></table><ol><li>安装具有复制功能的memcached版本<br>tar zxvf memcached-1.2.8-repcached-2.2.tar.gz<br>cd memcached-1.2.8-repcached-2.2<br>./configure <code>--prefix</code>=/root/hd/memcached_replication <pre><code>      `--with-libevent`=/root/hd/libevent/ `--enable-replication`</code></pre>make<br>make install</li></ol><p><strong>出现以下错误</strong></p><pre><code>memcached.c: 696: error: `IOV MAX` undeclared (first use in this function)memcached.c: 696: error: (Each undeclared identifier is reported only oncememcached.c: 696: error: for each function it appears in.)</code></pre><p><strong>解决办法</strong><br><strong>编辑memcached.c文件如下</strong><br>55 /* FreeBSD 4.x doesn’t have IOV_MAX exposed. */<br>56 #ifndef IOV_MAX<br>57 #if defined(<strong>FreeBSD</strong>) || defined(<strong>APPLE</strong>)<br>58 # define IOV_MAX 1024<br>59 #endif<br>60 #endif</p><p><strong>修改成如下形式</strong><br>55 /* FreeBSD 4.x doesn’t have IOV_MAX exposed. */<br>56 #ifndef IOV_MAX<br>57 //#if defined(<strong>FreeBSD</strong>) || defined(<strong>APPLE</strong>)<br>58 # define IOV_MAX 1024<br>59 //#endif<br>60 #endif</p><p>启动第一台MemCached，使用-x指定对端服务器的地址<br>./memcached -u root -d  -m 128 -x 192.168.116.121</p><p>启动第二台MemCached，使用-x指定对端服务器的地址<br>./memcached -u root -d  -m 128 -x 192.168.116.122</p><p><strong>出现以下错误</strong><br>./memcached: error while loading shared libraries: libevent-2.0.so.5: cannot open shared object file: No such file or directory</p><p><strong>解决办法</strong><br>查找 libevent-2.0.so.5<br>whereis libevent-2.0.so.5</p><p>使用ldd命令查看memcached命令，发现找不到<br>[root@hsiehchou121 bin]# ldd /root/hd/memcached-1.2.8-repcached-2.2/bin/memcached<br>linux-gate.so.1 =&gt; (0x00255000)<br>libevent-2.0.so.5 =&gt; not found<br>libc.so.6 =&gt; /lib/libc.so.6 (0x00110000)<br>/lib/ld-linux.so.2(0x003a4000)</p><p><strong>建立软连接</strong><br>ln -s /root/hd/libevent/lib/libevent-2.0.so.5 /usr/lib/libevent-2.0.so.5</p><p>2）Memcached的HA（High Availablity）<br>Keepalived是一个交换机制的软件。Keepalived的作用是检测服务器的状态，如果有一台web服务器死机，或工作出现故障，Keepalived将检测到，并将有故障的服务器从系统中剔除，同时使用其他服务器代替该服务器的工作，当服务器工作正常后Keepalived自动将服务器加入到服务器群中，这些工作全部自动完成，不需要人工干涉，需要人工做的只是修复故障的服务器</p><p>利用Keepalived实现MemCached的主主复制高可用架构<br>    Keepalived在memcached主服务器产生一个虚拟IP（VIP）<br>    Keepalived可以通过不断的检测memcached主服务器的11211端口是否正常工作，<br>    如果发现memcached Down机，虚拟IP就从主服务器移到从服务器</p><p>配置Keepalived（每台机器都要配置）<br>    rpm -ivh keepalived-1.2.13-4.el6.i686.rpm</p><p>    配置：主从节点都要配置，配置文件<br>    /etc/keepalived/keepalived.conf</p><p><strong>主节点配置信息</strong></p><pre><code>! Configuration File for keepalivedglobal_defs {    notification_email {      417952939@qq.com    }    notification_email_from collen_training@126.com    smtp_server 127.0.0.1    smtp_connect_timeout 30    router_id LVS_DEVEL}vrrp_instance VI_1 {    state MASTER    interface eth0    virtual_router_id 51    priority 101    advert_int 1    authentication {     auth_type PASS     auth_pass 1111    }    virtual_ipaddress {      192.168.116.88    }}</code></pre><p><strong>从节点配置信息</strong></p><pre><code>! Configuration File for keepalivedglobal_defs {    notification_email {       417952939@qq.com    }    notification_email_from collen_training@126.com    smtp_server 127.0.0.1    smtp_connect_timeout 30    router_id LVS_DEVEL}vrrp_instance VI_1 {    state MASTER    interface eth0    virtual_router_id 51    priority 100    advert_int 1    authentication {      auth_type PASS      auth_pass 1111    }    virtual_ipaddress {      192.168.116.88    }}</code></pre><p>验证Keepalived: 使用命令 <code>ip ad sh</code> 查看虚拟ip地址<br>inet 192.168.116.88/32 scope global eth0</p><h3 id="二、Redis"><a href="#二、Redis" class="headerlink" title="二、Redis"></a>二、Redis</h3><h4 id="1、Redis简介"><a href="#1、Redis简介" class="headerlink" title="1、Redis简介"></a>1、Redis简介</h4><p>（1）Redis的前身：Memcached<br>（2）和Memcached区别“<br>（<em>）支持持久化：RDB快照、AOF日志<br>（</em>）支持丰富的数据类型</p><h4 id="2、安装Redis"><a href="#2、安装Redis" class="headerlink" title="2、安装Redis"></a>2、安装Redis</h4><p>833  tar -zxvf redis-3.0.5.tar.gz<br>839  cd redis-3.0.5/<br>841  make<br>842  make PREFIX=/root/hd/redis install</p><p>redis-benchmark ：        Redis提供的压力测试工具。模拟产生客户端的压力<br>redis-check-aof：            检查aof日志文件<br>redis-check-dump：        检查rdb文件<br>redis-cli：                Redis客户端脚本<br>redis-sentinel：            哨兵<br>redis-server：            Redis服务器脚本</p><p>核心配置文件:redis.conf<br>[root@hsiehchou121 redis-3.0.5]# cp redis.conf /root/hd/redis/<br>[root@hsiehchou121 redis]# mkdir conf<br>[root@hsiehchou121 redis]# mv redis.conf conf/<br>[root@hsiehchou121 conf]# vi redis.conf </p><p>42 <strong>daemonize</strong> yes  //后台方式运行<br>50 port 6379</p><p>启动redis ./bin/redis-server conf/redis.conf<br>检测是否启动好<br>[root@hsiehchou121 redis]# ./bin/redis-server conf/redis.conf </p><h4 id="3、操作Redis"><a href="#3、操作Redis" class="headerlink" title="3、操作Redis"></a>3、操作Redis</h4><p>1）命令行<br>redis-cli<br>./bin/redis-cli</p><p>127.0.0.1:6379&gt; set key1 value1<br>OK<br>127.0.0.1:6379&gt; get key1<br>“value1”<br>127.0.0.1:6379&gt; keys *</p><p>1) “key1”</p><p>对数据的操作：<br>127.0.0.1:6379&gt; set money 100<br>OK<br>127.0.0.1:6379&gt; incr money<br>(integer) 101<br>127.0.0.1:6379&gt; get money<br>“101”<br>127.0.0.1:6379&gt; incrby money 10000<br>(integer) 10101</p><p>2）数据类型<br>①　字符串<br>127.0.0.1:6379&gt; set key1 “hello”<br>OK<br>127.0.0.1:6379&gt; get key1<br>“hello”<br>127.0.0.1:6379&gt;<br>127.0.0.1:6379&gt; append key1 “<code>*******</code>“<br>(integer) 12<br>127.0.0.1:6379&gt; get key1<br>“hello<code>*******</code>“</p><p>②　链表<br>127.0.0.1:6379&gt;<br>127.0.0.1:6379&gt; lpush list 11 22 33 44 55<br>(integer) 5<br>127.0.0.1:6379&gt; lrange list 0 2</p><p>1) “55”<br>2) “44”<br>3) “33”<br>127.0.0.1:6379&gt; lrange list 0 -1</p><p>1) “55”<br>2) “44”<br>3) “33”<br>4) “22”<br>5) “11”<br>127.0.0.1:6379&gt; lpop list<br>“55”</p><p>③　Hash<br>127.0.0.1:6379&gt; hset hashkey1 name ls<br>(integer) 1<br>127.0.0.1:6379&gt; hset hashkey2 age 23<br>(integer) 1<br>127.0.0.1:6379&gt; hmset user001 name ls age 23 gender mals<br>OK<br>127.0.0.1:6379&gt; hmset user002 name xz age 24 gender mals<br>OK<br>127.0.0.1:6379&gt; hmget user001 name age gender</p><p>1) “ls”<br>2) “23”<br>3) “mals”<br>127.0.0.1:6379&gt; hgetall user001</p><p>1) “name”<br>2) “ls”<br>3) “age”<br>4) “23”<br>5) “gender”<br>6) “mals”</p><p>④　无序集合<br>无序，不可重复的集合<br>127.0.0.1:6379&gt; sadd setkey1 11 22 33 44 55<br>(integer) 5<br>127.0.0.1:6379&gt; sadd setkey2 33 44 55 66 77 88<br>(integer) 6<br>127.0.0.1:6379&gt; smembers setkey1</p><p>1) “11”<br>2) “22”<br>3) “33”<br>4) “44”<br>5) “55”</p><p>sdiif： 差集<br>sinter： 交集<br>suntion：并集</p><p>⑤　有序集合<br>有序可以重复的集合，根据一个score来进行排序<br>127.0.0.1:6379&gt; zadd chinese 90 Tom 92 Nary 83 Nike<br>(integer) 3<br>127.0.0.1:6379&gt; zrange chinese 0 100</p><p>1) “Nike”<br>2) “Tom”<br>3) “Nary”<br>127.0.0.1:6379&gt; zrange chinese 0 100 withscores</p><p>1) “Nike”<br>2) “83”<br>3) “Tom”<br>4) “90”<br>5) “Nary”<br>6) “92”</p><p>⑥　Redis数据类型案例分析：网站统计用户登录的次数<br>a.    1亿个用户，有经常登录的，也有不经常登录的<br>b.    如何来记录用户的登录信息<br>c.    如何查询活跃用户：比如：一周内，登录3次的</p><p>    解决方案一：采用关系型数据库<br>建立表：记录一周内，每天登录的情况</p><p>采用关系型数据库保存登录信息存在的问题，每天产生一亿条数据，一周就是7亿条数据</p><p>    解决方案二：采用Redis存储登录信息<br>可以使用Redis的setbit，登录与否：有1和0就可以表示<br>一亿个用户，每天是否登录，用1或者0表示即可，每天产生约12M的数据<br>一亿个用户一周的登录信息：<br>12M*7=84M</p><pre><code>3）Java Api①　基本操作    @Test    public void testString(){        Jedis jedis = new Jedis(&quot;192.168.116.121&quot;,6379);        //添加数据        jedis.set(&quot;name&quot;,&quot;ls&quot;);//向key--&gt;name中放入了value--&gt;ls        System.out.println(jedis.get(&quot;name&quot;));//执行结果：ls        jedis.append(&quot;name&quot;,&quot; is my lover&quot;);//拼接        System.out.println(jedis.get(&quot;name&quot;));//执行结果：ls is my lover        jedis.del(&quot;name&quot;);//删除某个键        System.out.println(jedis.get(&quot;name&quot;));//执行结果：null        //设置多个键值对        jedis.mset(&quot;name&quot;,&quot;tom&quot;,&quot;age&quot;,&quot;23&quot;,&quot;qq&quot;,&quot;123456789&quot;);        jedis.incr(&quot;age&quot;);//进行加1操作        System.out.println(jedis.get(&quot;name&quot;) + &quot;-&quot; + jedis.get(&quot;age&quot;) + &quot;-&quot; + jedis.get(&quot;qq&quot;));//执行结果：tom-24-123456789        jedis.disconnect();    }</code></pre><p>②　连接池</p><pre><code>public class RedisUtils {    private static JedisPool jedisPool = null;    static {        try {            JedisPoolConfig config = new JedisPoolConfig();            //可用连接实例的最大数目，默认值为8 如果赋值为-1，则表示不限制            config.setMaxTotal(1024);            //控制一个pool最多有多少个状态为idle（空闲的）的jedis实例，默认值也是8            config.setMaxIdle(200);            //等待可用连接的最大时间，单位毫秒，默认值为-1，表示永不超时            config.setMaxWaitMillis(10000);            //在borrow一个jedis实例时，是否提前进行validate操作，如果为true，则得到的jedis实例均是可用的            config.setTestOnBorrow(true);            jedisPool = new JedisPool(config, &quot;192.168.116.121&quot;);        }catch (Exception e) {            e.printStackTrace();        }    }    public synchronized static Jedis getJedis(){        try {            if (jedisPool != null)                return jedisPool.getResource();        }catch (Exception e){            e.printStackTrace();        }        return null;    }    public static void returnResource(final Jedis jedis){        if (jedis != null)            jedisPool.returnResource(jedis);    }}</code></pre><p>③　使用Redis实现分布式锁<br>使用Maven搭建工程：</p><pre><code>&lt;dependency&gt;    &lt;groupId&gt;redis.clients&lt;/groupId&gt;    &lt;artifactId&gt;jedis&lt;/artifactId&gt;    &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt;</code></pre><h4 id="4、Redis的事务：不是真正的事务，是一种模拟"><a href="#4、Redis的事务：不是真正的事务，是一种模拟" class="headerlink" title="4、Redis的事务：不是真正的事务，是一种模拟"></a>4、Redis的事务：不是真正的事务，是一种模拟</h4><p>1）复习：事务（关系型数据库）<br>（*）什么是事务？<br>事务有一组DML(Data Manipulation Language)语句组成。DML 插入更新删除操作</p><p>（*）事务的特点<br>要么都成功，要么都失败</p><p>（*）Oracle中事务的本质：将事务的DML操作写入日志。日志写入成功，则事务执行成功</p><p>2）Redis事务的本质：将一组操作放入队列中，一次执行（批处理）</p><p>3）对比Oracle和Redis事务的区别</p><table><thead><tr><th align="center">操作</th><th align="center">Oracle</th><th align="center">Redis</th></tr></thead><tbody><tr><td align="center">开启事务</td><td align="center">自动开启</td><td align="center">命令：multi</td></tr><tr><td align="center">执行语句</td><td align="center">DML</td><td align="center">Redis命令</td></tr><tr><td align="center">提交事务</td><td align="center">显式提交：commit；隐式提交:DDL语句（create table）</td><td align="center">命令：exec 执行放在multi里面的操作</td></tr><tr><td align="center">回滚事务</td><td align="center">显式回滚：rollback；隐式回滚：系统掉电，客户端退出</td><td align="center">命令：discard 把队列中的操作废弃掉</td></tr></tbody></table><p><strong>注意</strong>：不是真正的事务，只是一种模拟</p><p>4）举例：模拟银行转账<br>set tom 1000<br>set mike 1000<br>tom –&gt; mike 转账操作必须在事务中，要么都成功，要么都不成功<br>multi<br>decrby tom 100<br>incrby mike 100<br>exec</p><p>127.0.0.1:6379&gt; set tom 1000<br>OK<br>127.0.0.1:6379&gt; set mike 1000<br>OK<br>127.0.0.1:6379&gt; multi<br>OK<br>127.0.0.1:6379&gt; decrby tom 100<br>QUEUED<br>127.0.0.1:6379&gt; incrby mike 100<br>QUEUED<br>127.0.0.1:6379&gt; exec</p><p>1) (integer) 900<br>2) (integer) 1100</p><p>5）举例：买票<br>set tom 1000<br>set ticket 1<br>multi<br>decrby tom 500<br>decr ticket<br>exec</p><p>在exec前，从另一个窗口，decr ticket<br>127.0.0.1:6379&gt; set tom 1000<br>OK<br>127.0.0.1:6379&gt; set ticket 1<br>OK<br>127.0.0.1:6379&gt; multi<br>OK<br>127.0.0.1:6379&gt; decrby tom 500<br>QUEUED<br>127.0.0.1:6379&gt; decr ticket<br>QUEUED<br>127.0.0.1:6379&gt; exec</p><p>1) (integer) 500<br>2) (integer) -1</p><h4 id="5、Redis锁机制"><a href="#5、Redis锁机制" class="headerlink" title="5、Redis锁机制"></a>5、Redis锁机制</h4><p>执行事务操作的时候，如果监视的值发生了变化，则提交失败<br>命令：<strong>watch</strong></p><p>举例：<strong>买票</strong><br>set tom 1000<br>set ticket 1<br>watch ticket —–&gt; 相当于给ticket加了锁。认为在下面执行事务的时候，值不会变<br>multi<br>decrby tom 500<br>decr ticket<br>exec</p><p>127.0.0.1:6379&gt; set tom 1000<br>OK<br>127.0.0.1:6379&gt; set ticket 1<br>OK<br>127.0.0.1:6379&gt; watch ticket<br>OK<br>127.0.0.1:6379&gt; multi<br>OK<br>127.0.0.1:6379&gt; decrby tom 500<br>QUEUED<br>127.0.0.1:6379&gt; decr ticket<br>QUEUED<br>127.0.0.1:6379&gt; exec<br>(nil)<br>127.0.0.1:6379&gt; get tom<br>“1000”</p><p><code>nil</code> 代表操作没有执行或者执行失败</p><p><strong>Java应用程序中的事务和锁</strong><br>①　事务</p><pre><code>@Testpublic void testTransaction(){    Jedis jedis = new Jedis(&quot;192.168.116.121&quot;,6379);    Transaction tc = null;    try{        //开启事务        tc = jedis.multi();        tc.decrBy(&quot;tom&quot;, 100);        tc.incrBy(&quot;mike&quot;, 100);        //提交事务        tc.exec();    }catch (Exception e){        e.printStackTrace();        //回滚事务        tc.discard();    }    jedis.disconnect();}</code></pre><p>②　锁</p><pre><code> @Test    public void testLock(){        Jedis jedis = new Jedis(&quot;192.168.116.121&quot;,6379);        //对ticket加锁，如果在事务执行过程中，该值有变化，则抛出异常        jedis.watch(&quot;ticket&quot;);        Transaction tc = null;        try{            //开启事务            tc = jedis.multi();            tc.decr(&quot;ticket&quot;);//车票数量减一            Thread.sleep(5000);            tc.decrBy(&quot;tom&quot;, 100);//扣tom 100块钱买票的钱            //提交事务            tc.exec();        }catch (Exception e){            e.printStackTrace();            //回滚事务            tc.discard();        }        jedis.disconnect();    }</code></pre><h4 id="6、Redis的消息机制：消息系统"><a href="#6、Redis的消息机制：消息系统" class="headerlink" title="6、Redis的消息机制：消息系统"></a>6、Redis的消息机制：消息系统</h4><p>1）消息的类型<br>（<em>）Queue消息：队列，点对点<br>（</em>）Topic消息：主题，群发：发布消息，订阅消息</p><p>2）Redis消息机制<br>只支持Topic消息</p><p>命令：发布消息 publish     格式：publish channel名称 “消息内容”</p><p>订阅：subscribe     格式：subscribe channel名称</p><p>psubscribe 订阅消息  —— 可以用通配符来订阅消息<br>    格式：psubscribe channel*名称</p><p>3）常用的消息系统：<br>Redis 只支持 Topic<br>Kafka 只支持Topic 需要Zookeeper支持<br>JMS Java Messging Service java消息服务标准。支持Queue Topic<br>产品：Weblogic</p><p>例子：<br>窗口1（发）：<br>127.0.0.1:6379&gt; PUBLISH c1 hello<br>(integer) 2<br>127.0.0.1:6379&gt; PUBLISH c1 test<br>(integer) 2</p><p>窗口2（订）：<br>127.0.0.1:6379&gt; SUBSCRIBE c1<br>Reading messages… (press Ctrl-C to quit)</p><p>1) “subscribe”<br>2) “c1”<br>3) (integer) 1</p><p>1) “message”<br>2) “c1”<br>3) “hello”</p><p>1) “message”<br>2) “c1”<br>3) “test”</p><p>窗口3（订）：<br>127.0.0.1:6379&gt; SUBSCRIBE c1<br>Reading messages… (press Ctrl-C to quit)</p><p>1) “subscribe”<br>2) “c1”<br>3) (integer) 1</p><p>1) “message”<br>2) “c1”<br>3) “hello”</p><p>1) “message”<br>2) “c1”<br>3) “test”</p><p><strong>通过通配符订阅</strong><br>窗口1（发）：<br>127.0.0.1:6379&gt; PUBLISH c2 hello<br>(integer) 1<br>127.0.0.1:6379&gt; PUBLISH c4 hello<br>(integer) 1</p><p>窗口2（发）：<br>127.0.0.1:6379&gt; PUBLISH c1 dfg<br>(integer) 1</p><p>窗口3（订）：<br>127.0.0.1:6379&gt; PSUBSCRIBE c*<br>Reading messages… (press Ctrl-C to quit)</p><p>1) “psubscribe”<br>2) “c*”<br>3) (integer) 1</p><p>1) “pmessage”<br>2) “c*”<br>3) “c2”<br>4) “hello”</p><p>1) “pmessage”<br>2) “c*”<br>3) “c4”<br>4) “hello”</p><p>1) “pmessage”<br>2) “c*”<br>3) “c1”<br>4) “dfg”</p><p><strong>使用Java程序实现消息的发布与订阅</strong><br>需要继承JedisPubSub类</p><pre><code>@Test    public void testMessage(){        Jedis jedis = new Jedis(&quot;192.168.116.121&quot;, 6379);        //subscribe和psubcribe不能同时订阅        jedis.subscribe(new MyListener(), &quot;channel&quot;);        //jedis.psubscribe(new MyListener(), &quot;channel*&quot;);    }    class MyListener extends JedisPubSub{        public void onMessage(String channel, String message){            System.out.println(&quot;onMessage channel is &quot; + channel + &quot; message is &quot; + message);        }        public void onPMessage(String pattern, String channel, String message){            System.out.println(&quot;onPMessage channel is &quot; + pattern);            System.out.println(&quot;onPMessage channel is &quot; + channel);            System.out.println(&quot;onPMessage message is &quot; + message);        }        public void onPSubscribe(String arg0, int arg1){}        public void onPUnsubscribe(String arg0, int arg1){}        public void onSubscribe(String arg0, int arg1){}        public void onUnsubscribe(String arg0, int arg1){}    }</code></pre><h4 id="7、Redis持久化"><a href="#7、Redis持久化" class="headerlink" title="7、Redis持久化"></a>7、Redis持久化</h4><p>本质：<strong>备份和恢复</strong><br>1）<strong>RDB快照</strong>：默认<br>（*）看成一种快照，备份。每隔段时间，将内存汇总的数据保存到硬盘上。产生RDB文件</p><p>（<em>）*</em>RDB 生成策略**<br><strong>redis.conf中</strong><br>147 save 900 1       900秒内，有1个key发生变化，执行RDB<br>148 save 300 10      300内，如果有10个key发生变化，执行RDB<br>149 save 60 10000    60秒内，如果有10000个key发生变化，执行RDB</p><p>save —- 时间 —– 发生变化的key的个数</p><p>（*）其他参数<br>164 stop-writes-on-bgsave-error yes  当后台写进程出错时，禁止写入新的数据</p><p>170 rdbcompression yes      是否压缩。如果看重性能，设置成no<br>    压缩会节省空间，但会影响备份和恢复性能</p><p>182 dbfilename dump.rdb  RDB的文件名字<br>192 dir ./  RDB的文件地址</p><p>（*）RDB的优点和缺点<br>优点：快，恢复速度快<br>缺点：在两次RDB之间，可能会造成数据的丢失<br>解决：AOF</p><p>2）<strong>AOF日志</strong><br>客户端在操作Redis时，把操作记录到文件中，如果发生崩溃，读取日志，把操作完全执行一遍</p><p>（*）默认是禁用<br>509 appendonly no  参数修改成yes</p><p>（*）AOF记录策略<br>538 # appendfsync always           每条操作都记录日志：优点安全 缺点：慢<br>539 appendfsync everysec    每秒写入一次<br>540 # appendfsync no            由操作系统来决定记录日志的方式。不会用的到。</p><p>（*）AOF日志重写：overwrite<br>举例：<br>set money 0<br>incr money<br>..100次</p><p>set money 100</p><p> ./redis-benchmark -n 100000<br> 模拟客户端100000次请求</p><p>（<em>）参数设置<br>561 no-appendfsync-on-rewrite *</em>no**   执行重写的时候，不写入新的aof日志<br>//561 no-appendfsync-on-rewrite yes  生成rdb的时候，是否不写入aof<br>580 auto-aof-rewrite-percentage 100 aof文件比上次重写时，超过的百分比<br>581 auto-aof-rewrite-min-size 64mb  执行重写的文件大小。到64M触发重写</p><p>3）当两个同时存在时，优先执行哪个？<br>504 # If the AOF is enabled on startup Redis will load the AOF, that is the file<br>505 # with the better durability guarantees.</p><p><strong>AOF开启时，优先使用AOF</strong></p><h4 id="8、Redis的主从复制"><a href="#8、Redis的主从复制" class="headerlink" title="8、Redis的主从复制"></a>8、Redis的主从复制</h4><p>1）<strong>Redis主从复制集群</strong><br>作用：<br>主从复制，主从备份，防止主节点down机<br>任务分离：分摊主节点压力。读写分离</p><p>Memcacached：主主复制<br>Redis：主从复制</p><p>Redis集群两种部署方式<br><strong>星型模型</strong>：<br>优点：效率高，两个slave地位一样，可以直接从主节点取出信息<br>缺点：HA比较麻烦</p><p><strong>线性模型</strong>：<br>优点：HA简单，master宕机后，可以直接切换到slave1<br>缺点：效率不如星型模型</p><p>cp redis.conf redis6379.conf<br>cp redis.conf redis6380.conf<br>cp redis.conf redis6381.conf </p><p><strong>主节点</strong>（redis6379.conf ）：关闭rdb aof<br>509 appendonly no<br>147 #save 900 1<br>148 #save 300 10<br>149 #save 60 10000</p><p><strong>从节点</strong>（redis6380.conf redis6381.conf）<br>不同机器有的可以不改<br>改端口号<br>50 port 6380<br>改aof rdb文件名：<br>182 dbfilename dump6380.rdb<br>211 slaveof 192.168.116.121 6379<br>513 appendfilename “appendonly6380.aof”</p><p><strong>改端口号</strong><br>50 port 6381<br>改aof rdb文件名：<br>182 dbfilename dump6381.rdb<br>211 slaveof 192.168.116.121 6379<br>513 appendfilename “appendonly6381.aof”</p><p>[root@hsiehchou121 redis]# ./bin/redis-server ./conf/redis6379.conf<br>[root@hsiehchou121 redis]# ./bin/redis-server ./conf/redis6380.conf<br>[root@hsiehchou121 redis]# ./bin/redis-server ./conf/redis6381.conf </p><p>[root@hsiehchou121 redis]# ps -ef | grep redis<br>root       6371      1  1 16:31 ?        00:00:00 ./bin/redis-server *:6379<br>root       6375      1  4 16:31 ?        00:00:01 ./bin/redis-server *:6380<br>root       6381      1  0 16:31 ?        00:00:00 ./bin/redis-server *:6381<br>root       6395   5432  0 16:31 pts/0    00:00:00 grep –color=auto redis</p><p>[root@hsiehchou121 redis]# ./bin/redis-cli -p 6379<br>127.0.0.1:6379&gt; set tom 10000<br>OK<br>127.0.0.1:6379&gt; quit<br>[root@hsiehchou121 redis]# ./bin/redis-cli -p 6380<br>127.0.0.1:6380&gt; get tom<br>“10000”</p><p>默认情况下，从节点只读，不可以写入数据</p><p>注意：一次性启动从节点不要太多</p><p><img src="/medias/Redis%20%E4%B8%BB%E4%BB%8E%E6%9C%8D%E5%8A%A1%E7%9A%84%E9%80%9A%E4%BF%A1%E5%8E%9F%E7%90%86.PNG" alt="Redis 主从服务的通信原理">            </p><p>2）<strong>Redis的分片</strong><br>多个从节点分摊读的压力</p><p>客户端代理分片工具：Twemproxy</p><p><strong>解压</strong><br>[root@hsiehchou121 nutcracker-0.3.0]# ./configure <code>--prefix</code>=/root/hd/nutcracker<br>[root@hsiehchou121 nutcracker-0.3.0]# make<br>[root@hsiehchou121 nutcracker-0.3.0]# make install</p><p>cp /root/hd/nutcracker-0.3.0/conf/nutcracker.yml ./conf/</p><p><strong>修改server信息</strong></p><pre><code>alpha:  listen: 127.0.0.1:22121  hash: fnv1a_64  distribution: ketama  auto_eject_hosts: true  redis: true  server_retry_timeout: 2000  server_failure_limit: 1  servers:   - 192.168.116.121:6380:1   - 192.168.116.121:6381:1</code></pre><p><strong>启动redis</strong><br>[root@hsiehchou121 redis]# ./bin/redis-server ./conf/redis6379.conf<br>[root@hsiehchou121 redis]# ./bin/redis-server ./conf/redis6380.conf<br>[root@hsiehchou121 redis]# ./bin/redis-server ./conf/redis6381.conf </p><p><strong>检查配置文件是否正确</strong><br>[root@hsiehchou121 nutcracker]# ./sbin/nutcracker -t conf/nutcracker.yml</p><p><strong>启动代理分片</strong><br>[root@hsiehchou121 nutcracker]# ./sbin/nutcracker -d -c conf/nutcracker.yml</p><p>[root@hsiehchou121 redis]# ./bin/redis-cli -p 22121 访问</p><h4 id="9、Redis的HA（哨兵机制）"><a href="#9、Redis的HA（哨兵机制）" class="headerlink" title="9、Redis的HA（哨兵机制）"></a>9、Redis的HA（哨兵机制）</h4><p><strong>主从结构，存在单点故障问题</strong></p><p>redis2.4版本之后有</p><p>redis-sentinel 就是哨兵</p><p>vi redis6380.conf<br>211 slaveof 192.168.116.121 6379</p><p>vi redis6381.conf<br>211 slaveof 192.168.116.121 6379</p><p><strong>配置</strong>：<br>cp /root/hd/redis-3.0.5/sentinel.conf  ./conf/</p><p>vim sentinel.conf </p><p>53 sentinel monitor mymaster 192.168.116.121 6379 1</p><p>[root@hsiehchou121 redis]# ./bin/redis-sentinel conf/sentinel.conf</p><p>看日志：<br>3085:X 23 Apr 17:04:17.522 # +monitor master mymaster 192.168.116.121 6379 quorum 1<br>3085:X 23 Apr 17:04:18.524 * +slave slave 192.168.116.121:6380 192.168.116.121 6380 @ mymaster 192.168.116.121 6379<br>3085:X 23 Apr 17:04:18.526 * +slave slave 192.168.116.121:6381 192.168.116.121 6381 @ mymaster 192.168.116.121 6379</p><p>kill master 检测到<br>看日志：<br>try-failover master mymaster 192.168.109.134 6379<br>检测到6379挂了</p><p>3085:X 23 Apr 17:05:14.647 # +selected-slave slave 192.168.116.121:6381 192.168.116.121 6381 @ mymaster 192.168.116.121 6379<br>select slave 选举新的主节点</p><p>3085:X 23 Apr 17:05:16.830 * +slave slave 192.168.116.121:6380 192.168.116.121 6380 @ mymaster 192.168.116.121 6381<br>把其他的从节点连接到主节点上</p><p><strong>注意</strong>:一定要按步骤来，一步一步配置</p><p><strong>亲测排坑</strong><br><strong>划重点</strong><br>此处的Redis的HA高可用的redis主节点和从节点的变化会导致<strong>sentinel monitor mymaster</strong>（sentinel.conf的第53行）和<strong>slaveof</strong>一起变化（从节点的第211行）。而且这个过程是不可逆的，就是更新了变只有自己手动去修改下。</p><p>所以，如果停止重新运行，便会报错，需要自己自行修改这些内容。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> 内存数据库 </tag>
            
            <tag> MemCached </tag>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark MLlib和Spark GraphX</title>
      <link href="/2019/04/11/spark-mllib-he-spark-graphx/"/>
      <url>/2019/04/11/spark-mllib-he-spark-graphx/</url>
      
        <content type="html"><![CDATA[<p><strong>Spark MLlib</strong></p><p>MLlib 是 Spark 可以扩展的机器学习库</p><p>MLlib is Apache Spark’s scalable machine learning library.</p><h3 id="一、MLlib概述"><a href="#一、MLlib概述" class="headerlink" title="一、MLlib概述"></a>一、MLlib概述</h3><p>MLlib 是 Spark 可以扩展的机器学习库</p><p>Spark在机器学习方面具有得天独厚的有事，有以下几个原因：</p><h4 id="1、机器学习算法"><a href="#1、机器学习算法" class="headerlink" title="1、机器学习算法"></a>1、机器学习算法</h4><p>一般都有多个步骤迭代计算，需要在多次迭代后，获得足够小的误差或者收敛才会停止</p><pre><code>double wucha = 1.0while(wucha&gt;=0.00001){    建模  wucha -= 某个值}</code></pre><p>模型计算完毕</p><p>当迭代使用Hadoop的MapReduce计算框架时，每次都要读写硬盘以及任务启动工作，导致很大的IO开销</p><p>而Spark基于内存的计算模型天生擅长迭代计算。只有在必要时，才会读写硬盘</p><p>所以Spark是机器学习比较理想的平台</p><h4 id="2、通信"><a href="#2、通信" class="headerlink" title="2、通信"></a>2、通信</h4><p>Hadoop的MapReduce计算框架，通过heartbeat方式来进行通信和传递数据，执行速度慢</p><p>spark 有高效的 Akka 和 Netty 的通信系统，通行效率高</p><p>Spark MLlib 是Spark 对常用的机器学习算法的实现库，同时包括相关测试和数据生成器</p><h3 id="二、什么是机器学习"><a href="#二、什么是机器学习" class="headerlink" title="二、什么是机器学习"></a>二、什么是机器学习</h3><h4 id="1、机器学习的定义"><a href="#1、机器学习的定义" class="headerlink" title="1、机器学习的定义"></a>1、机器学习的定义</h4><p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P,<br>if its performance at tasks in T, as measured by P, improves with experience E</p><p>三个关键词：算法、经验、模型评价</p><p>在数据的基础上，通过算法构建出模型，并进行评价<br>如果达到要求，则用该模型测试其他数据<br>如果不达到要求，要调整算法来重新建立模型，再次进行评估<br>循环往复，知道获得满意的经验</p><p>应用：金融反欺诈、语音识别、自然语言处理、翻译、模式识别、智能控制等等</p><h4 id="2、基于大数据的机器学习"><a href="#2、基于大数据的机器学习" class="headerlink" title="2、基于大数据的机器学习"></a>2、基于大数据的机器学习</h4><p>传统的机器学习算法，由于技术和单机存储的现值，只能在少量数据上使用<br>即，依赖于数据抽样<br>问题：很难做好随机，导致学习的模型不准确</p><p>在大数据上进行机器学习，直接处理全量数据并进行大量迭代计算</p><p>Spark本身计算优势，适合机器学习</p><p>另外 spark-shell pyspark 都可以提供及时查询工具</p><h4 id="3、MLlib"><a href="#3、MLlib" class="headerlink" title="3、MLlib"></a>3、MLlib</h4><p>MLlib是Spark机器学习库，简化机器学习的工程实践工作，方便扩展到更大规模<br>集成了通用的学习算法：分类、回归、聚类、协同过滤、降维等等</p><p>另外，MLlib本身在Spark中，数据清洗、SQL、建模放在一起</p><p><strong>sample_linear_regression_data.txt</strong><br>1 1:1.9<br>2 1:3.1<br>3 1:4<br>3.5 1:4.45<br>4 1:5.02<br>9 1:9.97<br>-2 1:-0.98</p><pre><code>package day7import org.apache.spark.sql.SparkSessionimport org.apache.spark.ml.regression.LinearRegression/* * 1.3850645873427236 1:0.14476184437006356 2:-0.11280617018445871 3:-0.4385084538142101 4:-0.5961619435136434 5:0.419554626795412 6:-0.5047767472761191 7:0.457180284958592 8:-0.9129360314541999 9:-0.6320022059786656 10:-0.44989608519659363 *  */object Demo1 {  def main(args: Array[String]): Unit = {    val spark = SparkSession.builder().appName(&quot;Demo1&quot;).master(&quot;local&quot;).getOrCreate()    val data_path = &quot;H:\\sample_linear_regression_data.txt&quot;    //读取训练数据    val trainning = spark.read.format(&quot;libsvm&quot;).load(data_path)    //定义模型    val lr = new LinearRegression().setMaxIter(10000)    //训练模型    val lrModel = lr.fit(trainning)    //获取模型训练结果    val trainningSummary = lrModel.summary    //获取预测值    trainningSummary.predictions.show()    //获取误差    print(trainningSummary.rootMeanSquaredError)    spark.stop()  }}</code></pre><h2 id="Spark-Graphx"><a href="#Spark-Graphx" class="headerlink" title="Spark Graphx"></a>Spark Graphx</h2><h3 id="一、Spark-Graphx-是什么？"><a href="#一、Spark-Graphx-是什么？" class="headerlink" title="一、Spark Graphx 是什么？"></a>一、Spark Graphx 是什么？</h3><p>1、是Spark 的一个模块，主要用于进行以图为核心的计算，还有分布式图计算</p><p>2、Graphx 底层基于RDD计算，和RDD共用一种存储形态。在展示形态上，可以用数据集来表示，也可以用图来表示</p><h3 id="二、Spark-GraphX-有哪些抽象？"><a href="#二、Spark-GraphX-有哪些抽象？" class="headerlink" title="二、Spark GraphX 有哪些抽象？"></a>二、Spark GraphX 有哪些抽象？</h3><h4 id="1、顶点"><a href="#1、顶点" class="headerlink" title="1、顶点"></a>1、顶点</h4><p>RDD[(VertexId,VD)]表示<br>VertexId 代表了顶点的ID，是Long类型<br>VD 是顶点的属性，可以是任何类型</p><h4 id="2、边"><a href="#2、边" class="headerlink" title="2、边"></a>2、边</h4><p>RDD[Edge[ED]]表示<br>Edge表示一个边<br>包含一个ED类型参数来设定属性<br>另外，边还包含了源顶点ID和目标顶点ID</p><h4 id="3、三元组"><a href="#3、三元组" class="headerlink" title="3、三元组"></a>3、三元组</h4><p>三元组结构用RDD[EdgeTriplet[VD,ED]]表示<br>三元组包含一个边、边的属性、源顶点ID、源顶点属性、目标顶点ID、目标顶点属性</p><h4 id="4、图"><a href="#4、图" class="headerlink" title="4、图"></a>4、图</h4><p>Graph表示，通过顶点和边来构建</p><pre><code>package day7import org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.graphx.Edgeimport org.apache.spark.graphx.Graphobject Demo2 {  def main(args: Array[String]): Unit = {    val conf = new SparkConf().setAppName(&quot;Demo2&quot;).setMaster(&quot;local&quot;)    //创建Spark Context对象    val sc = new SparkContext(conf)    //定义点    val users = sc.parallelize(Array((3L,(&quot;TIme&quot;,&quot;student&quot;)),(5L,(&quot;Andy&quot;,&quot;student&quot;)),        (7L,(&quot;Mary&quot;,&quot;student&quot;)),(2L,(&quot;Lily&quot;,&quot;post&quot;))))    //定义边    val relationship = sc.parallelize(Array(Edge(3L,7L,&quot;col&quot;),Edge(5L,3L,&quot;ad&quot;),Edge(2L,5L,&quot;col&quot;),Edge(5L,7L,&quot;heh&quot;)))     //构建图    val graph = Graph(users, relationship)    //图的操作    val post_count = graph.vertices.filter{ case (id,(name,pos)) =&gt; pos==&quot;post&quot;}.count    println(&quot;post count is &quot; + post_count)    val edges_count = graph.edges.filter(e =&gt; e.srcId &gt; e.dstId).count()    println(&quot;the value is &quot; + edges_count)  }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 调优</title>
      <link href="/2019/04/07/spark-diao-you/"/>
      <url>/2019/04/07/spark-diao-you/</url>
      
        <content type="html"><![CDATA[<p><strong>Spark 调优</strong>    </p><p>问题：只要会用就可以，为什么还要精通内核源码与调优？<br>Spark 性能优化概览：<br>Spark的计算本质是，分布式计算<br>所以，Spark程序的性能可能因为集群中的任何因素出现瓶颈：CPU、网络带宽、或者内存</p><p>CPU、网络带宽，是运维来维护的<br>聚焦点：内存</p><p>如果内存能够容纳下所有的数据，那就不需要调优了<br>如果内存比较紧张，不足以放下所有数据（10亿量级—500G）,需要对内存的使用进行性能优化<br>比如：使用某些方法减少内存的消耗</p><p>Spark性能优化，主要针对在内存的使用调优</p><p>Spark性能优化的技术：<br>1、使用高性能序列化类库<br>2、优化数据结构<br>3、对于多次使用的RDD进行持久化、checkpoint<br>4、持久化级别：MEMORY_ONLY  —&gt;  MEMORY_ONLY_SER 序列化<br>5、Java虚拟机垃圾回收调优<br>6、Shuffle调优，1.x版本中，90%的性能问题，都是由于Shuffle导致的。</p><p>其他性能优化：<br>1、提高并行度<br>2、广播共享数据<br>等等。。。</p><h3 id="一、诊断Spark内存使用"><a href="#一、诊断Spark内存使用" class="headerlink" title="一、诊断Spark内存使用"></a>一、诊断Spark内存使用</h3><p>首先要看到内存使用情况，才能进行针对性的优化</p><h4 id="1、内存花费"><a href="#1、内存花费" class="headerlink" title="1、内存花费"></a>1、内存花费</h4><p>（1）每个Java对象，都有一个对象头，占用16字节，包含一些对象的元信息，比如指向他的类的指针<br>如果对象本身很小，比如int，但是他的对象头比对象自己还大</p><p>（2）Java的String对象，会比他内存的原始数据，多出40个字节<br>String内部使用的char数组来保存内部的字符串序列，并且还要保存诸如输出长度之类的信息<br>char使用的是UTF-16编码，每个字符会占2个字节。比如，包含10个字符的String，2*10+40=60字节</p><p>（3）Java中的集合类型，比如HashMap和LinkedList，内部使用链表数据结构<br>链表中的每个数据，使用Entry对象包装<br>Entry对象，不光有对象头，还有指向下一个Entry的指针，占用8字节</p><p>（4）元素类型为原始数据类型（int），内部通常会使用原始数据类型的包装类型（Integer）来存储元素</p><h4 id="2、如何判断Spark程序消耗内存情况？"><a href="#2、如何判断Spark程序消耗内存情况？" class="headerlink" title="2、如何判断Spark程序消耗内存情况？"></a>2、如何判断Spark程序消耗内存情况？</h4><p>预估<br>（1）设置RDD的并行度<br>两种方法创建RDD，parallelize()  textFile() 在这两个方法中，传入第二个参数，设置RDD的partition数量<br>在SparkConfig中设置一个参数：<br>spark.default.parallelism<br>可以统一设置这个application中所有RDD的partition数量</p><p>（2）将RDD缓存 cache()</p><p>（3）观察日志：<br>driver的日志<br>/root/hd/spark-2.1.0-bin-hadoop2.7/work</p><p>Master和Worker的日志<br>/root/hd/spark-2.1.0-bin-hadoop2.7/logs</p><pre><code>scala&gt; val rdd1 = sc.textFile(&quot;/root/hd/tmp_files/test_Cache.txt&quot;)rdd1: org.apache.spark.rdd.RDD[String] = /root/hd/tmp_files/test_Cache.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:24scala&gt; rdd1.cacheres1: rdd1.type = /root/hd/tmp_files/test_Cache.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:24scala&gt; rdd1.countres2: Long = 921911                                                             scala&gt; rdd1.countres3: Long = 921911</code></pre><p>/root/hd/spark-2.1.0-bin-hadoop2.7/work<br>19/04/17 17:02:10 INFO MemoryStore: Block rdd_1_1 stored as values in memory (estimated size 22.9 MB, free 343.1 MB)<br>19/04/17 17:02:10 INFO MemoryStore: Block rdd_1_0 stored as values in memory (estimated size 22.9 MB, free 320.2 MB)</p><p>（4）将这个内存信息相加，就是RDD内存占用量<br>22.9MB+22.9MB=45.8MB</p><h3 id="二、使用高性能序列化类库"><a href="#二、使用高性能序列化类库" class="headerlink" title="二、使用高性能序列化类库"></a>二、使用高性能序列化类库</h3><h4 id="1、数据序列化概述"><a href="#1、数据序列化概述" class="headerlink" title="1、数据序列化概述"></a>1、数据序列化概述</h4><p>数据序列化，就是将对象或者数据结构，转换成特定的格式，使其可在网络中传输，或存储在内存或文件中<br>反序列化，是相反的操作，将对象从序列化数据中还原出来<br>序列化后的数据格式，可以是二进制，xml，Json等任何格式<br>对象、数据序列化的重点在于数据的交换与传输</p><p>在任何分布式系统中，序列化都是扮演着一个重要的角色<br>如果使用的序列化技术，操作很慢，或者序列化后的数据量还是很大，会让分布式系统应用程序性能下降很多<br>所以，Spark性能优化的第一步，就是进行序列化的性能优化</p><p>Spark自身默认会在一些地方对数据进行序列化，比如Shuffle。另外，我们使用了外部数据（自定义类型），也要让其课序列化</p><p>Spark本身对序列化的便捷性和性能进行了取舍<br>默认情况下：Spark倾向于序列化的便捷性，使用了Java自身提供的序列化机制，很方便使用</p><p>但是，Java序列化机制性能不高，序列化速度慢，序列化后数据较大，比较占用内存空间</p><h4 id="2、Kryo"><a href="#2、Kryo" class="headerlink" title="2、Kryo"></a>2、Kryo</h4><p>Spark支持使用Kryo类库来进行序列化<br>速度快，占用空间更小，比Java序列化数据占用空间小10倍</p><h4 id="3、如何使用kryo序列化机制"><a href="#3、如何使用kryo序列化机制" class="headerlink" title="3、如何使用kryo序列化机制"></a>3、如何使用kryo序列化机制</h4><p>（1）设置Spark Conf<br>bin/spark-submit will also read configuration options from </p><p><strong>conf/spark-defaults.conf</strong>,<br>in which each line consists of a key and a value separated by whitespace. For example:</p><p>spark.master            spark://5.6.7.8:7077<br>spark.executor.memory   4g<br>spark.eventLog.enabled  true<br>spark.serializer        org.apache.spark.serializer.KryoSerializer</p><p>（2）使用kryo是，要求需要序列化的类，要提前注册，以获得高性能<br>conf.registerKryoClasses(Array(classOf[Count],……))<br>conf.registerKryoClasses(Array(classOf[类], classOf[类], ……))</p><h4 id="4、kryo类库的优化"><a href="#4、kryo类库的优化" class="headerlink" title="4、kryo类库的优化"></a>4、kryo类库的优化</h4><p>（1）优化缓存大小<br>如果注册的自定义类型，本身特别大（100个字段），会导致要序列化的对象太大<br>此时需要对kyro本身进行优化。因为kryo内部的缓存，可能不能存放这么大的class对象<br>spark.kryoserializer.buffer.max  设置这个参数，将其调大</p><p>（2）预先注册自定义类型<br>虽然不注册自定义类型，kryo也可以正常工作，但会保存一份他的全限定类名，耗费内存<br>推荐预先注册要序列化的自定义类型</p><h3 id="三、优化数据结构"><a href="#三、优化数据结构" class="headerlink" title="三、优化数据结构"></a>三、优化数据结构</h3><h4 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h4><p>要减少内存的消耗，除了使用高效的序列化类库外，还要优化数据结构<br>避免Java语法特性中所导致的额外内存开销</p><p>核心：优化算子函数内部使用到的局部数据或算子函数外部的数据<br>目的：减少对内存的消耗和占用</p><h4 id="2、如何做"><a href="#2、如何做" class="headerlink" title="2、如何做"></a>2、如何做</h4><p>（1）优先使用数组以及字符串，而不是集合类。即：优先使用Array，而不是ArrayList、LinkedList、HashMap<br>使用int[] 会比List<code>&lt;Integer&gt;</code> 节省内存</p><p>（2）将对象转换成字符串<br>企业中，将HashMap、List这种数据，统一用String拼接成特殊格式的字符串</p><p>Map&lt;Integer,Person&gt; persons = new HashMap&lt;Integer,Person&gt;()</p><p>可以优化为：<br>“id:name,address”<br>String persons = “1:Andy,Beijing|2:Tom,Tianjin….”</p><p>（3）避免使用多层嵌套对象结构<br>举例：<br>下面的例子不好，因为Teacher类的内部又嵌套了大量的小的Student对象<br>public class Teacher{ private …..; privage List<code>&lt;Student&gt;</code> students = new ArrayList()}</p><p>解决：转换成字符串进行处理<br>{“teacherId”: 1, “students”:[{“stuId”:1…..},{}]}</p><p>（4）对于能够避免的场景，尽量使用int代替String<br>虽然String比List效率高，但int类型占用更少内存</p><p>比如：数据库主键，id，推荐使用自增的id，而不是uuid</p><h3 id="四、rdd-cache-checkpoint"><a href="#四、rdd-cache-checkpoint" class="headerlink" title="四、rdd.cache checkpoint"></a>四、rdd.cache checkpoint</h3><h3 id="五、持久化级别"><a href="#五、持久化级别" class="headerlink" title="五、持久化级别"></a>五、持久化级别</h3><p>MEMORY_ONLY  —&gt;  MEMORY_ONLY_SER 序列化</p><h3 id="六、Java虚拟机的调优"><a href="#六、Java虚拟机的调优" class="headerlink" title="六、Java虚拟机的调优"></a>六、Java虚拟机的调优</h3><h4 id="1、概述-1"><a href="#1、概述-1" class="headerlink" title="1、概述"></a>1、概述</h4><p>如果在持久化RDD的时候，持久化了大量的数据，那么Java虚拟机的垃圾回收就可能成为一个瓶颈</p><p>Java虚拟机会定期进行垃圾回收，此时会追踪所有Java对象，并且在垃圾回收时，找到那些已经不再使用的对象</p><p>清理旧对象，给新对象腾出空间</p><p>垃圾回收的性能开销，是与内存中的对象数量成正比</p><p>在做Java虚拟机调优之前，必须先做好上面的调优工作，这样才有意义</p><p>必须注意顺序（先进行完上面的调优，再进行JVM调优）</p><h4 id="2、Spark-GC原理"><a href="#2、Spark-GC原理" class="headerlink" title="2、Spark GC原理"></a>2、Spark GC原理</h4><p><strong>垃圾回收期GC</strong></p><p>垃圾回收器，寻找那些对象已经不再使用，将其清除出去</p><p>GC对性能的影响在于，如果内存中数据量较大，会很频繁地造成内存空间不够，导致GC频繁发生，而GC本身是有性能消耗的，如果频繁发生，对性能影响严重</p><p>此外，如果数据量过大，每次要回收的数据量也很大，导致GC慢</p><p>另外GC发生的时候，GC是一个线程，我们Task运行时线程叫做工作线程。GC运行时会让工作线程停下来，让GC单独运行，影响Spark应用程序的运行速度，降低了性能</p><p><strong>核心：不让GC频繁发生</strong></p><p><img src="/medias/sparkGC%E5%8E%9F%E7%90%86.PNG" alt="sparkGC原理"></p><h4 id="3、监测垃圾回收"><a href="#3、监测垃圾回收" class="headerlink" title="3、监测垃圾回收"></a>3、监测垃圾回收</h4><p>我们可以进行监测，比如多久进行一次垃圾回收以及耗费的时间等等。</p><p>spark-submit脚本中，添加一个配置<br>–conf “spark.executor.extraJavaOptions=-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimesStamps”</p><p>注意：这个是输出到worker日志中，而不是driver日志<br>/root/hd/spark-2.1.0-bin-hadoop2.7/logs  worker日志<br>/root/hd/spark-2.1.0-bin-hadoop2.7/work  driver日志</p><h4 id="4、优化Executor内存比例"><a href="#4、优化Executor内存比例" class="headerlink" title="4、优化Executor内存比例"></a>4、优化Executor内存比例</h4><p>目的：减少GC次数</p><p>对于GC调优来说，最重要的就是调节，RDD的缓存占用的内存空间与算子执行时创建对象所占用的内存空间的比例</p><p>Executor:Task=3:2  （Executor 占60%给RDD缓存，Task占40%）<br>对于默认情况，Spark使用每个Executor 60% 的内存空间来缓存RDD，在task运行期间所创建的对象，只有40%内存空间来存放</p><p>在默认情况下，很可能发生的事情，分配给Task的内存不够，<br>导致新创建对象时，很快占满内存，GC启动，找到不再使用的对象，清楚内存</p><p>所以，如果Task分配内存过小，可能会导致GC频繁发生，工作线程停止</p><p>可以通过调节比例，将RDD缓存空间占比调节到40%，降低Task GC频率</p><p>需要配合其他优化：<br>kryo优化<br>持久化级别优化<br>数据结构优化</p><p>使用：conf.set(“spark.storage.memoryFraction”,0.5)</p><p><img src="/medias/spark%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D.PNG" alt="spark内存分配."></p><h4 id="5、Java-GC-调优-（-）"><a href="#5、Java-GC-调优-（-）" class="headerlink" title="5、Java GC 调优 （-）"></a>5、Java GC 调优 （-）</h4><p>让GC更快的处理完</p><h3 id="七、shuffle原理"><a href="#七、shuffle原理" class="headerlink" title="七、shuffle原理"></a>七、shuffle原理</h3><h4 id="1、优化前"><a href="#1、优化前" class="headerlink" title="1、优化前"></a>1、优化前</h4><p>假设一个节点上，有4个shufflemaptask，有两个CPU core</p><p>每个MapTask会为每个ReduceTask创建一份bucket缓存，以及对应的ShuffleBlockFile磁盘文件</p><p>问题：假设有100个MapTask，100个ReduceTask，会产生100*100即10000个文件，磁盘io过多，影响性能</p><p>假设另外一个节点上，运行了4个ReduceTask</p><p>每个ReduceTask拉取过来的数据，其实会组成内部的RDD，叫ShuffledRDD，优先放入内存，如果不够，写入磁盘</p><p>每个ReduceTask针对数据进行聚合，最后生成MapPartitionsRDD，执行reduceByKey操作希望的到那个RDD<br><img src="/medias/shuffle%E4%BC%98%E5%8C%96%E5%89%8D.PNG" alt="Shuffle优化前"></p><h4 id="2、优化后"><a href="#2、优化后" class="headerlink" title="2、优化后"></a>2、优化后</h4><p>在Spark新版本中，引入了consolidation的机制，提出了ShuffleGroup概念</p><p>一个ShuffleMapTask执行完后，写入本地文件不会变，但是，下一个ShuffleSMapTask运行的时候，可以直接将数据写入之前的本地文件</p><p>相当于，多个ShuffleMapTask的输出进行了合并，大大减少文件数量</p><p>1和2可以乘坐一组ShuffleGroup，每个文件中，都存储了多个ShuffleMapTask的数量，每个ShuffleMapTask的数据叫做segment，通过外部索引，来标记每个ShuffleMapTask的数据以及偏移量，对不同的ShuffleMapTask的数据进行区分<br><img src="/medias/shuffle%E4%BC%98%E5%8C%96%E5%90%8E.PNG" alt="shuffle优化后"></p><h3 id="八、其他调优"><a href="#八、其他调优" class="headerlink" title="八、其他调优"></a>八、其他调优</h3><h4 id="1、提高并行度"><a href="#1、提高并行度" class="headerlink" title="1、提高并行度"></a>1、提高并行度</h4><h4 id="2、广播共享数据"><a href="#2、广播共享数据" class="headerlink" title="2、广播共享数据"></a>2、广播共享数据</h4>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Streaming</title>
      <link href="/2019/04/03/spark-streaming-ji-chu/"/>
      <url>/2019/04/03/spark-streaming-ji-chu/</url>
      
        <content type="html"><![CDATA[<p><strong>Spark Streaming</strong><br>流式计算框架，类似于Storm</p><p>常用的实时计算引擎（流式计算）<br>1、Apache Storm：真正的流式计算</p><p>2、Spark Streaming ：严格上来说，不是真正的流式计算（实时计算）<br>把连续的流式数据，当成不连续的RDD<br>本质：是一个离散计算（不连续）</p><p>3、Apache Flink：真正的流式计算。与Spark Streaming相反<br>把离散的数据，当成流式数据来处理</p><p>4、JStorm</p><h3 id="一、Spark-Streaming基础"><a href="#一、Spark-Streaming基础" class="headerlink" title="一、Spark Streaming基础"></a>一、Spark Streaming基础</h3><h4 id="1、什么是-Spark-Streaming"><a href="#1、什么是-Spark-Streaming" class="headerlink" title="1、什么是 Spark Streaming"></a>1、什么是 Spark Streaming</h4><p>Spark Streaming makes it easy to build scalable fault-tolerant streaming applications.<br>易于构建灵活的、高容错的流式系统</p><p>Spark Streaming是核心Spark API的扩展，可实现可扩展、高吞吐量、可容错的实时数据流处理。数据可以从诸如Kafka，Flume，Kinesis或TCP套接字等众多来源获取，并且可以使用由高级函数（如map，reduce，join和window）开发的复杂算法进行流数据处理。最后，处理后的数据可以被推送到文件系统，数据库和实时仪表板。而且，您还可以在数据流上应用Spark提供的机器学习和图处理算法</p><p>特点：<br>1、易用，已经集成到Spark中<br>2、容错性：底层RDD，RDD本身具有容错机制<br>3、支持多种语言：Java Scala Python</p><p>Spark Streaming将连续的数据流抽象为discretizedstream或DStream。在内部，DStream 由一个RDD序列表示</p><h4 id="2、演示官方的Demo"><a href="#2、演示官方的Demo" class="headerlink" title="2、演示官方的Demo"></a>2、演示官方的Demo</h4><p>往Spark Streaming中发送字符串，Spark 接收到以后，进行计数<br>使用消息服务器 netcat Linux自带<br>yum install nc.x86_64</p><p>nc -l 1234</p><p>注意：总核心数 大于等于2。一个核心用于接收数据，另一个用于处理数据</p><p>在netcat中写入数据 Spark Streaming可以取到</p><pre><code>[root@hsiehchou121 spark-2.1.0-bin-hadoop2.7]# ./bin/run-example streaming.NetworkWordCount localhost 1234</code></pre><h4 id="3、开发自己的NetWorkWordCount程序"><a href="#3、开发自己的NetWorkWordCount程序" class="headerlink" title="3、开发自己的NetWorkWordCount程序"></a>3、开发自己的NetWorkWordCount程序</h4><p>和Spark Core类似</p><p><strong>代码</strong></p><pre><code>package day5import org.apache.spark.streaming.StreamingContextimport org.apache.spark.SparkConfimport org.apache.spark.streaming.Secondsimport org.apache.spark.storage.StorageLevelimport org.apache.log4j.Loggerimport org.apache.log4j.Level/** * 开发自己的流式计算程序 *  * 知识点 * 1、创建一个StreamingContext对象  ----》核心：创建一个DStream *  * 2、DStream的表现形式：就是一个RDD *  * 3、使用DStream把连续的数据流变成不连续的RDD *  * Spark Streaming 最核心的内容 */object MyNetworkWordCount {  def main(args: Array[String]): Unit = {    //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建一个StreamingContext对象    //local[2]表示开启了两个线程    val conf = new SparkConf().setAppName(&quot;MyNetworkWordCount&quot;).setMaster(&quot;local[2]&quot;)    //Seconds(3)表示采样时间间隔     val ssc = new StreamingContext(conf, Seconds(3))    //创建DStream，从netcat服务器上接收数据    val lines = ssc.socketTextStream(&quot;192.168.116.121&quot;, 1234, StorageLevel.MEMORY_ONLY)    //lines中包含了netcat服务器发送过来的数据    //分词操作    val words = lines.flatMap(_.split(&quot; &quot;))    //计数    val wordCount = words.map((_,1)).reduceByKey(_+_)    //打印结果    wordCount.print()    //启动StreamingContext进行计算    ssc.start()    //等待任务结束    ssc.awaitTermination()  }}</code></pre><p><strong>程序中的几点说明</strong></p><p>appName参数是应用程序在集群UI上显示的名称</p><p>master是Spark，Mesos或YARN集群的URL，或者一个特殊的“local [*]”字符串来让程序以本地模式运行</p><p>当在集群上运行程序时，不需要在程序中硬编码master参数，而是使用spark-submit提交应用程序并将master的URL以脚本参数的形式传入。但是，对于本地测试和单元测试，您可以通过“local[*]”来运行Spark Streaming程序（请确保本地系统中的cpu核心数够用）</p><p>StreamingContext会内在的创建一个SparkContext的实例（所有Spark功能的起始点），你可以通过ssc.sparkContext访问到这个实例</p><p>批处理的时间窗口长度必须根据应用程序的延迟要求和可用的集群资源进行设置</p><p><strong>请务必记住以下几点</strong></p><p>一旦一个StreamingContext开始运作，就不能设置或添加新的流计算</p><p>一旦一个上下文被停止，它将无法重新启动</p><p>同一时刻，一个JVM中只能有一个StreamingContext处于活动状态</p><p>StreamingContext上的stop()方法也会停止SparkContext。 要仅停止StreamingContext（保持SparkContext活跃），请将stop() 方法的可选参数stopSparkContext设置为false</p><p>只要前一个StreamingContext在下一个StreamingContext被创建之前停止（不停止SparkContext），SparkContext就可以被重用来创建多个StreamingContext</p><p>问题：Hello Hello<br>Hello World</p><p>现在现象：（Hello,2）<br>    (Hello , 1) (World , 1)</p><p>能不能累加起来？保存记录下以前的状态？<br>能，能<br>通过Spark Streaming提供的算子来实现</p><h3 id="二、高级特性"><a href="#二、高级特性" class="headerlink" title="二、高级特性"></a>二、高级特性</h3><h4 id="1、什么是DStream？离散流"><a href="#1、什么是DStream？离散流" class="headerlink" title="1、什么是DStream？离散流"></a>1、什么是DStream？离散流</h4><p>把连续的数据变成不连续的RDD<br>因为DStream的特性，导致Spark Streaming不是真正的流式计算</p><p><strong>离散流</strong>（DStreams）：Discretized Streams<br>DiscretizedStream或DStream 是Spark Streaming对流式数据的基本抽象。它表示连续的数据流，这些连续的数据流可以是从数据源接收的输入数据流，也可以是通过对输入数据流执行转换操作而生成的经处理的数据流。在内部，DStream由一系列连续的RDD表示</p><p>举例分析：<br>在之前的MyNetworkWordCount 的例子中，我们将一行行文本组成的流转换为单词流，具体做法为：将flatMap操作应用于名为lines的 DStream中的每个RDD上，以生成words DStream的RDD</p><p><strong>DStream中的转换操作（transformation）</strong></p><table><thead><tr><th align="center">Transformation</th><th align="center">Meaning</th></tr></thead><tbody><tr><td align="center">map(func)</td><td align="center">利用函数func处理DStreamd的每个元素，返回一个新的DStream</td></tr><tr><td align="center">flatMap(func)</td><td align="center">于map相似，但是每个输入项可被映射为0个或者多个输出项</td></tr><tr><td align="center">filter(func)</td><td align="center">返回一个新的DStream，它仅仅包含源DStream中满足函数func的项</td></tr><tr><td align="center">repartition(numPartitions)</td><td align="center">通过创建更多或者更少的partition改变这个DStream的并行级别（level of parallelism）</td></tr><tr><td align="center">union(otherStream)</td><td align="center">返回一个新的DStream，它包含源DStream和otherStream的联合元素</td></tr><tr><td align="center">count()</td><td align="center">通过计算源DStream中每个RDD的元素数量，返回一个包含单元素（single-element）RDDs的新DStream</td></tr><tr><td align="center">reduce(func)</td><td align="center">利用函数func聚焦源DStream中每个RDD的元素，返回一个包含单元素（single-element）RDDs的新DStream，函数应该是相关联的，以使计算可以并行化</td></tr><tr><td align="center">countByValue()</td><td align="center">这个算子应用于元素类型为K的DStream上，返回一个（K,long）对的新DStream，每个键的值实在原DStream的每个RDD中的频率</td></tr><tr><td align="center">reduceByKey(func,[numTasks])</td><td align="center">当在一个由（K,V）对组成的DStream上调用这个算子，返回一个新的由（K,V）对组成的DStream，每一个key的值均由给定的reduce函数聚集起来。注意：在默认情况下，这个算子利用了Spark默认的并发任务数去分组，你可以用numTasks参数设置不同的任务数</td></tr><tr><td align="center">join(otherStream,[numTasks])</td><td align="center">当应用于两个DStream（一个包含（K,V）对，一个包含（K,W）对），返回一个包含（K,（V,W））对的新DStream</td></tr><tr><td align="center">cogroup(otherStream,[numTasks])</td><td align="center">当应用于两个DStream（一个包含(K,V)对，一个包含（K,W）对），返回一个包含（K,Seq[v],Seq[W]）的元组</td></tr><tr><td align="center">transform(func)</td><td align="center">通过对源DStream的每个RDD应用RDD-to-RDD函数，创建一个新的DStream，这个可以在DStream中的任何RDD操作中使用</td></tr><tr><td align="center">updateStateByKey(func)</td><td align="center">利用给定的函数更新DStream的状态，返回一个新”state”的DStream</td></tr></tbody></table><h4 id="2、重点算子讲解"><a href="#2、重点算子讲解" class="headerlink" title="2、重点算子讲解"></a>2、重点算子讲解</h4><p>（1）updateStateByKey(func)<br>默认情况下，Spark Streaming不记录之前的状态，每次发数据，都会从0开始<br>现在使用本算子，实现累加操作</p><p>操作允许不断用新信息更新它的同时保持任意状态<br>定义状态-状态可以是任何的数据类型<br>定义状态更新函数-怎样利用更新前的状态和从输入流里面获取的新值更新状态</p><p>重写MyNetworkWordCount程序，累计每个单词出现的频率（注意：累计）</p><pre><code>package day5import org.apache.spark.streaming.StreamingContextimport org.apache.spark.SparkConfimport org.apache.spark.streaming.Secondsimport org.apache.spark.storage.StorageLevelimport org.apache.log4j.Loggerimport org.apache.log4j.Levelimport javax.swing.text.DefaultEditorKit.PreviousWordAction/** * 实现累加操作 */object MyTotalNetworkWordCount {  def main(args: Array[String]): Unit = {    //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建一个StreamingContext对象    //local[2]表示开启了两个线程    val conf = new SparkConf().setAppName(&quot;MyTotalNetworkWordCount &quot;).setMaster(&quot;local[2]&quot;)    //Seconds(3)表示采样时间间隔     val ssc = new StreamingContext(conf, Seconds(3))    //设置检查点目录，保存之前的状态信息    ssc.checkpoint(&quot;hdfs://hsiehchou121:9000/tmp_files/chkp&quot;)    //创建DStream 从netcat服务器上接收数据    val lines = ssc.socketTextStream(&quot;192.168.116.121&quot;, 1234, StorageLevel.MEMORY_ONLY)    val words = lines.flatMap(_.split(&quot; &quot;))    val wordPair = words.map((_,1))    /**     * 定义一个值函数，进行累加运算     * 1、当前值是多少（参数1）     * 2、之前的结果是多少（参数2）     */    val addFunc = (currentValues:Seq[Int], previousValues:Option[Int]) =&gt;{      //进行累加运算      //1、把当前的序列进行累加      val currentTotal = currentValues.sum      //2、在之前的值上再累加      Some(currentTotal + previousValues.getOrElse(0))    }    //进行累加运算    val total = wordPair.updateStateByKey(addFunc)    total.print()    ssc.start()    ssc.awaitTermination()  }}</code></pre><p>我在执行过程中遇到访问权限问题<br>解决如下：<br>在hadoop的etc/hadoop/下的hdfs-site.xml中增加如下内容即可</p><pre><code>    &lt;property&gt;         &lt;name&gt;dfs.permissions&lt;/name&gt;         &lt;value&gt;false&lt;/value&gt;    &lt;/property&gt;     &lt;property&gt;        &lt;name&gt;dfs.safemode.threshold.pct&lt;/name&gt;        &lt;value&gt;0f&lt;/value&gt;    &lt;/property&gt;  </code></pre><p>（2）transform(func)<br>通过RDD-to-RDD函数作用于源DStream中的各个RDD，可以是任意的RDD操作，从而返回一个新的RDD</p><pre><code>package day5import org.apache.spark.streaming.StreamingContextimport org.apache.spark.SparkConfimport org.apache.spark.streaming.Secondsimport org.apache.spark.storage.StorageLevelimport org.apache.log4j.Loggerimport org.apache.log4j.Level/** * 开发自己的流式计算程序 *  * 知识点 * 1、创建一个StreamingContext对象  ----》核心：创建一个DStream *  * 2、DStream的表现形式：就是一个RDD *  * 3、使用DStream把连续的数据流变成不连续的RDD *  * Spark Streaming 最核心的内容 */object MyNetworkWordCount {  def main(args: Array[String]): Unit = {    //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建一个StreamingContext对象    //local[2]表示开启了两个线程    val conf = new SparkConf().setAppName(&quot;MyNetworkWordCount &quot;).setMaster(&quot;local[2]&quot;)    //Seconds(3)表示采样时间间隔     val ssc = new StreamingContext(conf, Seconds(3))    //创建DStream，从netcat服务器上接收数据    val lines = ssc.socketTextStream(&quot;192.168.116.121&quot;, 1234, StorageLevel.MEMORY_ONLY)    //lines中包含了netcat服务器发送过来的数据    //分词操作    val words = lines.flatMap(_.split(&quot; &quot;))    //计数    val wordPair = words.transform(x =&gt; x.map(x =&gt; (x, 1)))    //打印结果    wordPair.print()    //启动StreamingContext进行计算    ssc.start()    //等待任务结束    ssc.awaitTermination()  }}</code></pre><h4 id="3、窗口操作"><a href="#3、窗口操作" class="headerlink" title="3、窗口操作"></a>3、窗口操作</h4><p>窗口：对落在窗口内的数据进行处理，也是一个DStream，RDD</p><pre><code>package day5import org.apache.spark.streaming.StreamingContextimport org.apache.spark.SparkConfimport org.apache.spark.streaming.Secondsimport org.apache.spark.storage.StorageLevelimport org.apache.log4j.Loggerimport org.apache.log4j.Level/** * 窗口操作： * 需求：每10秒钟，把过去30秒的数据读取进来 */object MyNetworkWordCountByWindow {  def main(args: Array[String]): Unit = {    //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建一个StreamingContext对象    //local[2]表示开启了两个线程    val conf = new SparkConf().setAppName(&quot;MyNetworkWordCountByWindow&quot;).setMaster(&quot;local[2]&quot;)    //Seconds(3)表示采样时间间隔     val ssc = new StreamingContext(conf, Seconds(1))    //创建DStream 从netcat服务器上接收数据    val lines = ssc.socketTextStream(&quot;192.168.116.121&quot;, 1234, StorageLevel.MEMORY_ONLY)    //lines中包含了netcat服务器发送过来的数据    //分词操作 给每个单词记一次数    val words = lines.flatMap(_.split(&quot; &quot;)).map((_,1))    /*     * reduceByKeyAndWindow 函数的三个参数     * 1、需要进行什么操作     * 2、窗口的大小30秒     * 3、窗口滑动的距离10秒     */    val result = words.reduceByKeyAndWindow((x:Int,y:Int)=&gt;(x+y),Seconds(30),Seconds(10))    result.print()    ssc.start()    ssc.awaitTermination()    /*     * The slide duration of windowed DStream (10000 ms) must be a multiple of the slide      * duration of parent DStream (3000 ms)     *      * 注意：窗口滑动距离必须是采样时间的整数倍     */  }}</code></pre><p>举例：每10秒钟把过去30秒的数据采集过来<br>注意：先启动nc  再启动程序 local[2]</p><h4 id="4、集成Spark-SQL-使用SQL语句来处理流式数据"><a href="#4、集成Spark-SQL-使用SQL语句来处理流式数据" class="headerlink" title="4、集成Spark SQL: 使用SQL语句来处理流式数据"></a>4、集成Spark SQL: 使用SQL语句来处理流式数据</h4><pre><code>package day5import org.apache.spark.streaming.StreamingContextimport org.apache.spark.SparkConfimport org.apache.spark.streaming.Secondsimport org.apache.spark.storage.StorageLevelimport org.apache.log4j.Loggerimport org.apache.log4j.Levelimport org.apache.spark.sql.SparkSession/** * 集成Spark SQL : 在Spark Streaming中使用SQL语句 */object MyNetworkWordCountWithSQL {   def main(args: Array[String]): Unit = {     //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建一个StreamingContext对象    //local[2]表示开启了两个线程    val conf = new SparkConf().setAppName(&quot;MyNetworkWordCountByWindow&quot;).setMaster(&quot;local[2]&quot;)    //Seconds(3)表示采样时间间隔     val ssc = new StreamingContext(conf, Seconds(10))    //创建DStream 从netcat服务器上接收数据    val lines = ssc.socketTextStream(&quot;192.168.116.121&quot;, 1234, StorageLevel.MEMORY_ONLY)    //进行单词计数    val words = lines.flatMap(_.split(&quot; &quot;))    //集成Spark SQL 使用SQL语句实现WordCount    words.foreachRDD(rdd =&gt;{      //创建一个Spark Session对象      //通过ssc.sparkContext.getConf 直接获取此session的conf      val spark = SparkSession.builder().config(ssc.sparkContext.getConf).getOrCreate()      //把RDD转换成DataFrame  需要用到隐式转换      import spark.implicits._      val df1 = rdd.toDF(&quot;word&quot;)//表df1 只有一个列名 名字叫word      //创建视图      df1.createOrReplaceTempView(&quot;words&quot;)      //执行SQL  通过SQL实现wordcount      spark.sql(&quot;select word,count(1) from words group by word&quot;).show      }    )    ssc.start()    ssc.awaitTermination()   }}</code></pre><h4 id="5、缓存和持久化：和RDD一样"><a href="#5、缓存和持久化：和RDD一样" class="headerlink" title="5、缓存和持久化：和RDD一样"></a>5、缓存和持久化：和RDD一样</h4><p>与RDD类似，DStreams还允许开发人员将流数据保留在内存中。也就是说，在DStream上调用persist() 方法会自动将该DStream的每个RDD保留在内存中。如果DStream中的数据将被多次计算（例如，相同数据上执行多个操作），这个操作就会很有用。对于基于窗口的操作，如reduceByWindow和reduceByKeyAndWindow以及基于状态的操作，如updateStateByKey，数据会默认进行持久化。 因此，基于窗口的操作生成的DStream会自动保存在内存中，而不需要开发人员调用persist()</p><p>对于通过网络接收数据（例如Kafka，Flume，sockets等）的输入流，默认持久化级别被设置为将数据复制到两个节点进行容错</p><p>注意，与RDD不同，DStreams的默认持久化级别将数据序列化保存在内存中</p><h4 id="6、支持检查点：和RDD一样"><a href="#6、支持检查点：和RDD一样" class="headerlink" title="6、支持检查点：和RDD一样"></a>6、支持检查点：和RDD一样</h4><p>流数据处理程序通常都是全天候运行，因此必须对应用中逻辑无关的故障（例如，系统故障，JVM崩溃等）具有弹性。为了实现这一特性，Spark Streaming需要checkpoint足够的信息到容错存储系统，以便可以从故障中恢复</p><p>①　一般会对两种类型的数据使用检查点：<br>1）元数据检查点（Metadatacheckpointing） - 将定义流计算的信息保存到容错存储中（如HDFS）。这用于从运行streaming程序的driver程序的节点的故障中恢复。元数据包括以下几种：<br>    配置（Configuration） - 用于创建streaming应用程序的配置信息</p><p>    DStream操作（DStream operations） - 定义streaming应用程序的DStream操作集合</p><p>    不完整的batch（Incomplete batches） - jobs还在队列中但尚未完成的batch</p><p>2）数据检查点（Datacheckpointing） - 将生成的RDD保存到可靠的存储层。对于一些需要将多个批次之间的数据进行组合的stateful变换操作，设置数据检查点是必需的。在这些转换操作中，当前生成的RDD依赖于先前批次的RDD，这导致依赖链的长度随时间而不断增加，由此也会导致基于血统机制的恢复时间无限增加。为了避免这种情况，stateful转换的中间RDD将定期设置检查点并保存到到可靠的存储层（例如HDFS）以切断依赖关系链</p><p>总而言之，元数据检查点主要用于从driver程序故障中恢复，而数据或RDD检查点在任何使用stateful转换时是必须要有的</p><p>②　何时启用检查点：<br>对于具有以下任一要求的应用程序，必须启用检查点：<br>1）使用状态转：如果在应用程序中使用updateStateByKey或reduceByKeyAndWindow（具有逆函数），则必须提供检查点目录以允许定期保存RDD检查点<br>2）从运行应用程序的driver程序的故障中恢复：元数据检查点用于使用进度信息进行恢复</p><p>③　如何配置检查点：<br>可以通过在一些可容错、高可靠的文件系统（例如，HDFS，S3等）中设置保存检查点信息的目录来启用检查点。这是通过使用streamingContext.checkpoint(checkpointDirectory)完成的。设置检查点后，您就可以使用上述的有状态转换操作。此外，如果要使应用程序从驱动程序故障中恢复，您应该重写streaming应用程序以使程序具有以下行为：<br>1）当程序第一次启动时，它将创建一个新的StreamingContext，设置好所有流数据源，然后调用start()方法。<br>2）当程序在失败后重新启动时，它将从checkpoint目录中的检查点数据重新创建一个StreamingContext。<br>使用StreamingContext.getOrCreate可以简化此行为</p><p>④　改写之前的WordCount程序，使得每次计算的结果和状态都保存到检查点目录下<br>hdfs dfs -ls /spark_checkpoint</p><h3 id="三、数据源"><a href="#三、数据源" class="headerlink" title="三、数据源"></a>三、数据源</h3><p>Spark Streaming是一个流式计算引擎，就需要从外部数据源来接收数据</p><h4 id="1、基本的数据源"><a href="#1、基本的数据源" class="headerlink" title="1、基本的数据源"></a>1、基本的数据源</h4><p>文件流：监控文件系统的变化，如果文件有增加，读取文件中的内容</p><p>希望Spark Streaming监控一个文件夹，如果有变化，则把变化采集过来</p><p><strong>此功能为</strong>修改文件里面的内容，并修改文件名，才能检测到，单修改一个是不起作用的</p><pre><code>package day5import org.apache.spark.streaming.StreamingContextimport org.apache.spark.SparkConfimport org.apache.spark.streaming.Secondsimport org.apache.spark.storage.StorageLevelimport org.apache.log4j.Loggerimport org.apache.log4j.Level/** * 测试文件流 */object FileStreaming {  def main(args: Array[String]): Unit = {    //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建一个StreamingContext对象    //local[2]表示开启了两个线程    val conf = new SparkConf().setAppName(&quot;MyNetworkWordCountByWindow&quot;).setMaster(&quot;local[2]&quot;)    //Seconds(3)表示采样时间间隔     val ssc = new StreamingContext(conf, Seconds(1))    //直接监控某个目录，如果有新文件产生，就读取出来    val lines = ssc.textFileStream(&quot;H:\\other\\test_file_stream&quot;)    lines.print()    ssc.start()    ssc.awaitTermination()  }}</code></pre><p>RDD队列流：可以从队列中获取数据（不常用）</p><pre><code>package day5import org.apache.spark.streaming.StreamingContextimport org.apache.spark.SparkConfimport org.apache.spark.streaming.Secondsimport org.apache.spark.storage.StorageLevelimport org.apache.log4j.Loggerimport org.apache.log4j.Levelimport org.apache.spark.sql.SparkSessionimport org.apache.spark.rdd.RDDimport scala.collection.mutable.Queue/** * RDD队列流 */object RDDQueueStream {  def main(args: Array[String]): Unit = {     //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建一个StreamingContext对象    //local[2]表示开启了两个线程    val conf = new SparkConf().setAppName(&quot;RDDQueueStream&quot;).setMaster(&quot;local[2]&quot;)    //Seconds(3)表示采样时间间隔     val ssc = new StreamingContext(conf, Seconds(3))    //需要先创建一个队列RDD[Int]    val rddQueue = new Queue[RDD[Int]]()    //往队列里面添加数据 ----&gt; 创建数据源    for(i &lt;- 1 to 3){      rddQueue += ssc.sparkContext.makeRDD(1 to 10)      //为了便于观察      Thread.sleep(1000)    }    //从队列中接收数据，创建DStream    val inputDStream = ssc.queueStream(rddQueue)    //处理数据    val result = inputDStream.map(x =&gt; (x, x*2))    result.print()    ssc.start()    ssc.awaitTermination()   }}</code></pre><p>套接字流：socketTextStream</p><h4 id="2、高级数据源"><a href="#2、高级数据源" class="headerlink" title="2、高级数据源"></a>2、高级数据源</h4><p>（1）Flume<br>Spark SQL 对接flume有多种方式：<br>push方式：flume将数据推送给Spark Streaming<br>flume/myagent/a4.conf</p><pre><code># bin/flume-ng agent -n a4 -f myagent/a4.conf -c conf -Dflume.root.logger=INFO.console# 定义agent名，source、channel、sink的名称a4.sources = r1a4.channels = c1a4.sinks = k1# 具体定义sourcea4.sources.r1.type = spooldira4.sources.r1.spoolDir = /root/hd/tmp_files/logs# 具体定义channela4.channels.c1.type = memorya4.channels.c1.capacity = 10000a4.channels.c1.transactionCapacity = 100# 具体定义sinka4.sinks = k1a4.sinks.k1.type = avroa4.sinks.k1.channel = c1a4.sinks.k1.hostname = 192.168.116.1a4.sinks.k1.port = 1234# 组装 source、channel、sinka4.sources.r1.channels = c1a4.sinks.k1.channel = c1</code></pre><pre><code>package day5import org.apache.spark.streaming.StreamingContextimport org.apache.spark.SparkConfimport org.apache.spark.streaming.Secondsimport org.apache.log4j.Loggerimport org.apache.log4j.Levelimport org.apache.spark.streaming.flume.FlumeUtilsobject MyFlumeStream {  def main(args: Array[String]): Unit = {    //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建一个StreamingContext对象    //local[2]表示开启了两个线程    val conf = new SparkConf().setAppName(&quot;MyFlumeStream&quot;).setMaster(&quot;local[2]&quot;)    //Seconds(3)表示采样时间间隔     val ssc = new StreamingContext(conf, Seconds(3))    //对象flume    //创建一个flumeEvent  从flume中接收push来的数据，也是一个DStream    //flume将数据push到&quot;192.168.116.1&quot;,1234  Spark Streaming在这里监听    val flumeEventDStream = FlumeUtils.createStream(ssc, &quot;192.168.116.1&quot;, 8888)    //将FlumeEvent中的事件转换成字符串    val lineDStream = flumeEventDStream.map(e =&gt; {       new String(e.event.getBody.array)     })    //输出结果    lineDStream.print()    ssc.start()    ssc.awaitTermination()  }}</code></pre><p>custom sink 模式：比第一种有更好的健壮性和容错性。使用这种方式，flume配置一个sink<br>a1.conf</p><pre><code>#bin/flume-ng agent -n a1 -f myagent/a1.conf -c conf -Dflume.root.logger=INFO,consolea1.channels = c1a1.sinks = k1a1.sources = r1a1.sources.r1.type = spooldira1.sources.r1.spoolDir = /root/hd/tmp_files/logsa1.channels.c1.type = memorya1.channels.c1.capacity = 100000a1.channels.c1.transactionCapacity = 100000a1.sinks.k1.type = org.apache.spark.streaming.flume.sink.SparkSinka1.sinks.k1.channel = c1a1.sinks.k1.hostname = 192.168.116.121a1.sinks.k1.port = 1234#组装source、channel、sinka1.sources.r1.channels = c1a1.sinks.k1.channel = c1</code></pre><p>使用官方提供的spark sink组件</p><p>需要把 spark-streaming-flume-sink_2.10-2.1.0.jar 拷贝到flume lib下<br>需要把 spark-streaming-flume-sink_2.10-2.1.0.jar 拷贝到IDE的lib下添加到build path中</p><p>（2）Kafka<br>在讲Kafka时，举例</p><h3 id="四、性能优化的参数"><a href="#四、性能优化的参数" class="headerlink" title="四、性能优化的参数"></a>四、性能优化的参数</h3><p>性能优化：<br>    spark submit的时候，程序报OOM错误<br>    程序跑的很慢</p><h4 id="1、减少批数据的执行时间"><a href="#1、减少批数据的执行时间" class="headerlink" title="1、减少批数据的执行时间"></a>1、减少批数据的执行时间</h4><p>在Spark中有几个优化可以减少批处理的时间：<br>①　数据接收的并行水平<br>通过网络(如kafka，flume，socket等)接收数据需要这些数据反序列化并被保存到Spark中。如果数据接收成为系统的瓶颈，就要考虑并行地接收数据。注意，每个输入DStream创建一个receiver（运行在worker机器上）接收单个数据流。创建多个输入DStream并配置它们可以从源中接收不同分区的数据流，从而实现多数据流接收。例如，接收两个topic数据的单个输入DStream可以被切分为两个kafka输入流，每个接收一个topic。这将在两个worker上运行两个receiver，因此允许数据并行接收，提高整体的吞吐量。多个DStream可以被合并生成单个DStream，这样运用在单个输入DStream的transformation操作可以运用在合并的DStream上</p><p>②　数据处理的并行水平<br>如果运行在计算stage上的并发任务数不足够大，就不会充分利用集群的资源。默认的并发任务数通过配置属性来确定spark.default.parallelism</p><p>③　数据序列化<br>可以通过改变序列化格式来减少数据序列化的开销。在流式传输的情况下，有两种类型的数据会被序列化：<br>1）输入数据<br>2）由流操作生成的持久RDD<br>在上述两种情况下，使用Kryo序列化格式可以减少CPU和内存开销</p><h4 id="2、设置正确的批容量"><a href="#2、设置正确的批容量" class="headerlink" title="2、设置正确的批容量"></a>2、设置正确的批容量</h4><p>为了Spark Streaming应用程序能够在集群中稳定运行，系统应该能够以足够的速度处理接收的数据（即处理速度应该大于或等于接收数据的速度）。这可以通过流的网络UI观察得到。批处理时间应该小于批间隔时间</p><p>根据流计算的性质，批间隔时间可能显著的影响数据处理速率，这个速率可以通过应用程序维持。可以考虑WordCountNetwork这个例子，对于一个特定的数据处理速率，系统可能可以每2秒打印一次单词计数（批间隔时间为2秒），但无法每500毫秒打印一次单词计数。所以，为了在生产环境中维持期望的数据处理速率，就应该设置合适的批间隔时间(即批数据的容量)</p><p>找出正确的批容量的一个好的办法是用一个保守的批间隔时间（5-10,秒）和低数据速率来测试你的应用程序</p><h4 id="3、内存调优"><a href="#3、内存调优" class="headerlink" title="3、内存调优"></a>3、内存调优</h4><p>在这一节，我们重点介绍几个强烈推荐的自定义选项，它们可以减少Spark Streaming应用程序垃圾回收的相关暂停，获得更稳定的批处理时间</p><p>1）Default persistence level of DStreams：和RDDs不同的是，默认的持久化级别是序列化数据到内存中（DStream是StorageLevel.MEMORY_ONLY_SER，RDD是StorageLevel.MEMORY_ONLY）。即使保存数据为序列化形态会增加序列化/反序列化的开销，但是可以明显的减少垃圾回收的暂停</p><p>2）Clearing persistent RDDs：默认情况下，通过Spark内置策略（LUR），Spark Streaming生成的持久化RDD将会从内存中清理掉。如果spark.cleaner.ttl已经设置了，比这个时间存在更老的持久化RDD将会被定时的清理掉。正如前面提到的那样，这个值需要根据Spark Streaming应用程序的操作小心设置。然而，可以设置配置选项spark.streaming.unpersist为true来更智能的去持久化（unpersist）RDD。这个配置使系统找出那些不需要经常保有的RDD，然后去持久化它们。这可以减少Spark RDD的内存使用，也可能改善垃圾回收的行为</p><p>3）Concurrent garbage collector：使用并发的标记-清除垃圾回收可以进一步减少垃圾回收的暂停时间。尽管并发的垃圾回收会减少系统的整体吞吐量，但是仍然推荐使用它以获得更稳定的批处理时间</p><p>方法：调整spark参数<br>    conf.set…</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark SQL</title>
      <link href="/2019/03/31/spark-sql/"/>
      <url>/2019/03/31/spark-sql/</url>
      
        <content type="html"><![CDATA[<p>Spark SQL 类似于Hive</p><h3 id="一、Spark-SQL-基础"><a href="#一、Spark-SQL-基础" class="headerlink" title="一、Spark SQL 基础"></a>一、Spark SQL 基础</h3><h4 id="1、什么是Spark-SQL"><a href="#1、什么是Spark-SQL" class="headerlink" title="1、什么是Spark SQL"></a>1、什么是Spark SQL</h4><p>Spark SQL is Apache Spark’s module for working with structured data.<br>Spark SQL 是spark 的一个模块。来处理 结构化 的数据<br>不能处理非结构化的数据</p><p>特点： </p><p><strong>1）容易集成</strong> </p><p>不需要单独安装</p><p><strong>2）统一的数据访问方式</strong> </p><p>结构化数据的类型：JDBC JSon Hive parquer文件 都可以作为Spark SQL 的数据源<br>对接多种数据源，且使用方式类似</p><p><strong>3）完全兼容hive</strong> </p><p>把Hive中的数据，读取到Spark SQL中运行</p><p><strong>4）支持标准的数据连接</strong></p><p>JDBC</p><h4 id="2、为什么学习Spark-SQL"><a href="#2、为什么学习Spark-SQL" class="headerlink" title="2、为什么学习Spark SQL"></a>2、为什么学习Spark SQL</h4><p>执行效率比Hive高</p><p>hive 2.x 执行引擎可以使用 Spark</p><h4 id="3、核心概念：表（DataFrame-DataSet）"><a href="#3、核心概念：表（DataFrame-DataSet）" class="headerlink" title="3、核心概念：表（DataFrame DataSet）"></a>3、核心概念：表（DataFrame DataSet）</h4><p>mysql中的表：表结构、数据<br>DataFrame：Schema、RDD（数据）</p><p>DataSet 在spark1.6以后，对DataFrame做了一个封装</p><h4 id="4、创建DataFrame"><a href="#4、创建DataFrame" class="headerlink" title="4、创建DataFrame"></a>4、创建DataFrame</h4><p>（<em>）测试数据：员工表、部门表<br>第一种方式：使用case class<br>*</em>1）定义Schema**<br>样本类来定义Schema</p><p>case class 特点：<br>可以支持模式匹配，使用case class建立表结构</p><p>7521, WARD, SALESMAN,7698, 1981/2/22, 1250, 500, 30</p><p>case class Emp(empno:Int,ename:String,job:String,mgr:Int,hiredate:String,sal:Int,comm:Int,deptno:Int)</p><p><strong>2）读取文件</strong><br>val lines = sc.textFile(“/root/hd/tmp_files/emp.csv”).map(_.split(“,”))</p><p><strong>3）把每行数据，映射到Emp上</strong><br>val allEmp = lines.map(x =&gt; Emp(x(0).toInt,x(1),x(2),x(3).toInt,x(4),x(5).toInt,x(6).toInt,x(7).toInt))</p><p><strong>4）生成DataFrame</strong><br>val df1 = allEmp.toDF</p><p>df1.show</p><p><strong>第二种方式 使用Spark Session</strong><br>（1）什么是Spark Session<br><strong>Spark session available as ‘spark’.</strong><br>2.0以后引入的统一访问方式。可以访问所有的Spark组件</p><p>def createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame</p><p>（2）使用StructType来创建Schema</p><p>val struct =<br>StructType(<br>StructField(“a”, IntegerType, true) ::<br>StructField(“b”, LongType, false) ::<br>StructField(“c”, BooleanType, false) :: Nil)</p><p>case class Emp(<br>empno:Int,<br>ename:String,<br>job:String,<br>mgr:Int,<br>hiredate:String,<br>sal:Int,<br>comm:Int,<br>deptno:Int)</p><p>—————–分割———————-<br>import org.apache.spark.sql.types._</p><p>val myschema = StructType(<br>List(<br>StructField(“empno”,DataTypes.IntegerType),<br>StructField(“ename”,DataTypes.StringType),<br>StructField(“job”,DataTypes.StringType),<br>StructField(“mgr”,DataTypes.IntegerType),<br>StructField(“hiredate”,DataTypes.StringType),<br>StructField(“sal”,DataTypes.IntegerType),<br>StructField(“comm”,DataTypes.IntegerType),<br>StructField(“deptno”,DataTypes.IntegerType)<br>))</p><p>准备数据 RDD[Row]<br>import org.apache.spark.sql.Row</p><p>val allEmp = lines.map(x =&gt; Row(x(0).toInt,x(1),x(2),x(3).toInt,x(4),x(5).toInt,x(6).toInt,x(7).toInt))</p><p>val df2 = spark.createDataFrame(allEmp,myschema)</p><p>df2.show</p><p><strong>第三种方式</strong> </p><p>直接读取一个带格式的文件<br>在/root/hd/spark-2.1.0-bin-hadoop2.7/examples/src/main/resources有现成的json代码</p><p>val df3 = spark.read 读文件，默认是Parquet文件<br>val df3 = spark.read.json(“/uroot/hd/tmp_files/people.json”)</p><p>df3.show</p><p>val df4 = spark.read.format(“json”).load(“/root/hd/tmp_files/people.json”)</p><p>df4.show</p><h4 id="5、操作DataFrame"><a href="#5、操作DataFrame" class="headerlink" title="5、操作DataFrame"></a>5、操作DataFrame</h4><p><strong>1）DSL语句</strong><br>mybatis Hibernate</p><p>df1.printSchema</p><p>df1.select(“ename”,”sal”).show</p><p>df1.select($”ename”,$”sal”,$”sal”+100).show<br>$”sal” 可以看做是一个变量</p><p>查询薪水大于2000的员工<br>df1.filter($”sal” &gt; 2000).show</p><p>求每个部门的员工人数<br>df1.groupBy($”deptno”).count.show</p><p>相当于select deptno,count(1) from emp group by deptno</p><p><strong>2）SQL语句</strong> </p><p>注意：不能直接执行SQL，需要生成一个视图，再执行sql</p><p>scala&gt; df1.create<br>createGlobalTempView createOrReplaceTempView createTempView</p><p>一般用到 createOrReplaceTempView createTempView<br>视图：类似于表，但不保存数据</p><p>df1.createOrReplaceTempView(“emp”)</p><p>操作：<br>spark.sql(“select * from emp”).show</p><p>查询薪水大于2000的员工<br>spark.sql(“select * from emp where sal &gt; 2000”).show</p><p>求每个部门的员工人数<br>spark.sql(“select deptno,count(1) from emp group by deptno”).show</p><p><strong>3）多表查询</strong> </p><p>10,ACCOUNTING,NEW YORK</p><p>case class Dept(deptno:Int,dname:String,loc:String)<br>val lines = sc.textFile(“/root/hd/tmp_files/dept.csv”).map(_.split(“,”))<br>val allDept = lines.map(x=&gt;Dept(x(0).toInt,x(1),x(2)))</p><p>df5.createOrReplaceTempView(“dept”)</p><p>spark.sql(“select dname,ename from emp,dept where emp.deptno=dept.deptno”).show</p><h4 id="6、操作DataSet"><a href="#6、操作DataSet" class="headerlink" title="6、操作DataSet"></a>6、操作DataSet</h4><p>Dataset是一个分布式的数据收集器。这是在Spark1.6之后新加的一个接口，兼顾了RDD的优点（强类型，可以使用功能强大的lambda）以及Spark SQL的执行器高效性的优点。所以可以把DataFrames看成是一种特殊的Datasets，即：Dataset(Row)</p><p>Dataset跟DataFrame类似，是一套新的接口，是高级的Dataframe</p><p>举例： </p><p><strong>1）创建DataSet</strong></p><p>（1）使用序列来创建DataSet<br>定义一个case class<br>case class MyData(a:Int,b:String)</p><p>生成序列，并创建DataSet<br>val ds = Seq(MyData(1,”Tom”),MyData(2,”Merry”)).toDS</p><p>.toDS 生成DataSet</p><p>ds.show</p><p>（2）使用JSON数据来创建DataSet</p><p>定义case class<br>case class Person(name:String,age:BigInt)</p><p>通过Json数据来生成DataFrame<br>val df = spark.read.format(“json”).load(“/root/hd/tmp_files/people.json”)</p><p>将DataFrame转换成DataSet<br>df.as[Person].show</p><p>df.as[Person] 就是一个DataSet</p><p>（3）使用其他数据<br>RDD操作和DataFrame操作相结合 —&gt; DataSet</p><p>读取数据，创建DataSet<br>val linesDS = spark.read.text(“/root/hd/tmp_files/test_WordCount.txt”).as[String]</p><p>对DataSet进行操作：<br>val words = linesDS.flatMap(.split(” “)).filter(.length &gt; 3)</p><p>words.show<br>words.collect</p><p>执行一个WordCount程序<br>val result = linesDS.flatMap(.split(” “)).map((,1)).groupByKey( x =&gt; x._1).count<br>result.show</p><p>排序：</p><pre><code>result.orderBy($&quot;value&quot;).showresult.orderBy($&quot;count(1)&quot;).show</code></pre><p><strong>2）DataSet操作案例</strong> </p><p>使用emp.json 生成一个DataFrame<br>val empDF = spark.read.json(“/root/hd/tmp_files/emp.json”)</p><p>查询工资大于3000的员工<br>empDF.where($”sal” &gt;= 3000).show</p><p>创建case class</p><p>case class Emp(empno:BigInt,ename:String,job:String,mgr:String,hiredate:String,sal:BigInt,comm:String,deptno:BigInt)</p><p>生成DataSet<br>val empDS = empDF.as[Emp]</p><p>查询工资大于3000的员工<br>empDS.filter(_.sal &gt; 3000).show</p><p>查询10号部门的员工<br>empDS.filter(_.deptno == 10).show</p><p><strong>3）多表查询</strong> </p><p>（1）创建部门表<br>val deptRDD = sc.textFile(“/root/hd/tmp_files/dept.csv”).map(_.split(“,”))<br>case class Dept(deptno:Int,dname:String,loc:String)</p><p>val deptDS = deptRDD.map( x=&gt; Dept(x(0).toInt,x(1),x(2))).toDS</p><p>（2）创建员工表<br>case class Emp(empno:Int,ename:String,job:String,mgr:Int,hiredate:String,sal:Int,comm:Int,deptno:Int)<br>val empRDD = sc.textFile(“/root/hd/tmp_files/emp.csv”).map(_.split(“,”))</p><p>7369,SMITH,CLERK,7902,1980/12/17,800,0,20<br>val empDS = empRDD.map(x=&gt; Emp(x(0).toInt,x(1),x(2),x(3).toInt,x(4),x(5).toInt,x(6).toInt,x(7).toInt)).toDS</p><p>（3）执行多表查询：等值连接<br>val result = deptDS.join(empDS,”deptno”)<br>result.show<br>result.printSchema</p><p>val result1 = deptDS.joinWith(empDS, deptDS(“deptno”) === empDS(“deptno”) )<br>result1.show<br>result1.printSchema</p><p>join 和 joinWith 区别：连接后schema不同</p><p>join ：将两张表展开成一张更大的表<br>joinWith ：把两张表的数据分别做成一列，然后直接拼在一起</p><p><strong>4）多表连接后再筛选</strong> </p><p>deptDS.join(empDS,”deptno”).where(“deptno == 10”).show</p><p>result.explain：执行计划</p><h4 id="7、Spark-SQL-中的视图"><a href="#7、Spark-SQL-中的视图" class="headerlink" title="7、Spark SQL 中的视图"></a>7、Spark SQL 中的视图</h4><p>视图是一个虚表，不存储数据<br>两种类型： </p><p><strong>1）普通视图（本地视图）</strong></p><p>只在当前Session中有效createOrReplaceTempView createTempView</p><p><strong>2）全局视图</strong> </p><p>createGlobalTempView<br>在不同的Session中都有用，把全局视图创建在命名空间中：global_temp中。类似于一个库</p><p>scala&gt; df1.create<br>createGlobalTempView createOrReplaceTempView createTempView</p><p>举例：<br>创建一个新session，读取不到emp视图，报错<br>df1.createOrReplaceTempView(“emp”)<br>spark.sql(“select * from emp”).show<br>spark.newSession.sql(“select * from emp”)</p><p>以下两种方式均可读到全局视图中的数据<br>df1.createGlobalTempView(“emp1”)</p><p>spark.newSession.sql(“select * from global_temp.emp1”).show</p><p>spark.sql(“select * from global_temp.emp1”).show</p><h3 id="二、使用数据源"><a href="#二、使用数据源" class="headerlink" title="二、使用数据源"></a>二、使用数据源</h3><p>在Spark SQL中，可以使用各种各样的数据源来操作。 结构化</p><h4 id="1、使用load函数、save函数"><a href="#1、使用load函数、save函数" class="headerlink" title="1、使用load函数、save函数"></a>1、使用load函数、save函数</h4><p>load函数是加载数据，save是存储数据</p><p>注意：使用load 或 save时，默认是Parquet文件。列式存储文件</p><p>举例:<br>读取 users.parquet 文件<br>val userDF = spark.read.load(“/root/hd/tmp_files/users.parquet”)</p><p>userDF.printSchema<br>userDF.show</p><p>val userDF = spark.read.load(“/root/hd/tmp_files/emp.json”)</p><p>保存parquet文件</p><pre><code>userDF.select($&quot;name&quot;,$&quot;favorite_color&quot;).write.save(&quot;/root/hd/tmp_files/parquet&quot;)</code></pre><p>读取刚刚写入的文件：<br>val userDF1 = spark.read.load(“/root/hd/tmp_files/parquet/part-00000-f9a3d6bb-d481-4fc9-abf6-5f20139f97c5.snappy.parquet”)—&gt; 不推荐</p><p>生产中直接读取存放的目录即可：<br>val userDF2 = spark.read.load(“/root/hd/tmp_files/parquet”)</p><p>读json文件 必须format<br>val userDF = spark.read.format(“json”).load(“/root/hd/tmp_files/emp.json”)<br>val userDF3 = spark.read.json(“/root/hd/tmp_files/emp.json”)</p><p>关于<strong>save函数</strong>： </p><p>调用save函数的时候，可以指定存储模式，追加、覆盖等等<br>userDF.write.save(“/root/hd/tmp_files/parquet”)</p><p>userDF.write.save(“/root/hd/tmp_files/parquet”)<br>org.apache.spark.sql.AnalysisException: path file:/root/hd/tmp_files/parquet already exists.;</p><p>save的时候覆盖<br>userDF.write.mode(“overwrite”).save(“/root/hd/tmp_files/parquet”)</p><p>将结果保存成表<br>userDF.select($”name”).write.saveAsTable(“table1”)</p><p>scala&gt; userDF.select($”name”).write.saveAsTable(“table1”)</p><p>scala&gt; spark.sql(“select * from table1”).show<br>+——+<br>| name|<br>+——+<br>|Alyssa|<br>| Ben|<br>+——+</p><h4 id="2、Parquet文件"><a href="#2、Parquet文件" class="headerlink" title="2、Parquet文件"></a>2、Parquet文件</h4><p>列式存储文件，是Spark SQL 默认的数据源<br>就是一个普通的文件</p><p>举例：<br>1）把其他文件，转换成Parquet文件<br>调用save函数<br>把数据读进来，再写出去，就是Parquet文件</p><p>val empDF = spark.read.json(“/root/hd/tmp_files/emp.json”)<br>empDF.write.mode(“overwrite”).save(“/root/hd/tmp_files/parquet”)<br>empDF.write.mode(“overwrite”).parquet(“/root/hd/tmp_files/parquet”)</p><p>val emp1 = spark.read.parquet(“/root/hd/tmp_files/parquet”)<br>emp1.createOrReplaceTempView(“emp1”)<br>spark.sql(“select * from emp1”)</p><p>2）支持Schema的合并<br>项目开始 表结构简单 schema简单<br>项目越来越大 schema越来越复杂</p><p>举例：<br>通过RDD来创建DataFrame<br>val df1 = sc.makeRDD(1 to 5).map( i =&gt; (i,i*2)).toDF(“single”,”double”)<br>“single”,”double” 是表结构<br>df1.show</p><p>df1.write.mode(“overwrite”).save(“/root/hd/tmp_files/test_table/key=1”)</p><p>val df2 = sc.makeRDD(6 to 10).map( i =&gt; (i,i*3)).toDF(“single”,”triple”)<br>df2.show<br>df2.write.mode(“overwrite”).save(“/root/hd/tmp_files/test_table/key=2”)</p><p>合并两个部分<br>val df3 = spark.read.parquet(“/root/hd/tmp_files/test_table”)</p><p>val df3 = spark.read.option(“mergeSchema”,true).parquet(“/root/hd/tmp_files/test_table”)</p><p><strong>key是可以随意取名字的，两个key需要一致，不然合并会报错</strong></p><p>通过RDD来创建DataFrame<br>val df1 = sc.makeRDD(1 to 5).map( i =&gt; (i,i*2)).toDF(“single”,”double”)<br>“single”,”double” 是表结构<br>df1.show</p><p>df1.write.mode(“overwrite”).save(“/root/hd/tmp_files/test_table/kt=1”)</p><p>val df2 = sc.makeRDD(6 to 10).map( i =&gt; (i,i*3)).toDF(“single”,”triple”)<br>df2.show<br>df2.write.mode(“overwrite”).save(“/root/hd/tmp_files/test_table/kt=2”)</p><p>合并两个部分<br>val df3 = spark.read.parquet(“/root/hd/tmp_files/test_table”)</p><p>val df3 = spark.read.option(“mergeSchema”,true).parquet(“/root/hd/tmp_files/test_table”)</p><h4 id="3、json文件"><a href="#3、json文件" class="headerlink" title="3、json文件"></a>3、json文件</h4><p>读取Json文件，生成DataFrame<br>val peopleDF = spark.read.json(“/root/hd/tmp_files/people.json”)</p><p>peopleDF.printSchema</p><p>peopleDF.createOrReplaceTempView(“peopleView”)</p><p>spark.sql(“select * from peopleView”).show</p><p>Spark SQL 支持统一的访问接口。对于不同的数据源，读取进来，生成DataFrame后，操作完全一样</p><h4 id="4、JDBC"><a href="#4、JDBC" class="headerlink" title="4、JDBC"></a>4、JDBC</h4><p>使用JDBC操作关系型数据库，加载到Spark中进行分析和处理</p><p>方式一：</p><pre><code>./spark-shell --master spark://hsiehchou121:7077 --jars /root/hd/tmp_files/mysql-connector-java-8.0.12.jar --driver-class-path /root/hd/tmp_files/mysql-connector-java-8.0.12.jar </code></pre><pre><code>val mysqlDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;,&quot;jdbc:mysql://192.168.116.1/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;).option(&quot;driver&quot;,&quot;com.mysql.cj.jdbc.Driver&quot;).option(&quot;user&quot;,&quot;root&quot;).option(&quot;password&quot;,&quot;123456&quot;).option(&quot;dbtable&quot;,&quot;emp&quot;).load</code></pre><pre><code>val mysqlDF = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;,&quot;jdbc:mysql://192.168.116.1/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;).option(&quot;driver&quot;,&quot;com.mysql.cj.jdbc.Driver&quot;).option(&quot;user&quot;,&quot;root&quot;).option(&quot;password&quot;,&quot;123456&quot;).option(&quot;dbtable&quot;,&quot;emp&quot;).loadmysqlDF.show</code></pre><p><strong>问题解决</strong></p><p>如果遇到下面问题，就是你本机的mysql数据库没有权限给你虚拟机访问<br>java.sql.SQLException: null, message from server: “Host ‘hsiehchou121’ is not allowed to connect to this MySQL server”</p><p><strong>解决方案</strong></p><p>1）进入你本机的数据库<br>mysql -u root -p<br>2）use mysql;<br>3）修改root用户前面的Host，改为%，意思是全部IP都能访问<br>4）flush privileges;</p><p><strong>方式二</strong><br>定义一个Properties类<br>import java.util.Properties<br>val mysqlProps = new Properties()<br>mysqlProps.setProperty(“driver”,”com.mysql.cj.jdbc.Driver”)<br>mysqlProps.setProperty(“user”,”root”)<br>mysqlProps.setProperty(“password”,”123456”)</p><p>val mysqlDF1 = spark.read.jdbc(“jdbc:mysql://192.168.116.1:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8”,”emp”,mysqlProps)</p><p>mysqlDF1.show</p><h4 id="5、使用Hive"><a href="#5、使用Hive" class="headerlink" title="5、使用Hive"></a>5、使用Hive</h4><p>比较常见<br>（<em>）spark SQL 完全兼容hive<br>（</em>）需要进行配置<br>拷贝一下文件到spark/conf目录下：<br>Hive 配置文件： hive-site.xml<br>Hadoop 配置文件：core-site.xml hdfs-site.xml</p><p>配置好后，重启spark</p><p><strong>在hive的lib下和spark的jars下面增加mysql-connector-java-8.0.12.jar这边连接数据库的jar包</strong></p><p>启动Hadoop ：start-all.sh<br>启动 hive：</p><pre><code>hsiehchou121cd hive/bin/./hive --service metastorehsiehchou122cd hive/bin./hive</code></pre><p><strong>hsiehchou121启动问题</strong></p><p>java.sql.SQLSyntaxErrorException: Table ‘hive.version’ doesn’t exist<br>解决：去mysql数据库中的hive库下面创建version表<br>这里需要给本地的hive库创建下hive所必须用的表</p><p>我们去/root/hd/hive/scripts/metastore/upgrade/mysql这里面找到hive-schema-1.2.0.mysql.sql，将里面的sql语句在hive库中执行</p><p>hive-txn-schema-0.14.0.mysql.sql，这个也做好执行下，用于事务管理</p><p><strong>显示当前所在库名字</strong> </p><p>set hive.cli.print.current.db=true;</p><p>j将emp.csv上传到hdfs中的/tmp_files/下面<br>hdfs dfs -put emp.csv /tmp_files</p><p>在hive中创建emp_default表</p><pre><code>hive (default)&gt; create table emp(empno int,ename string,job string,mgr int,hiredate string,sal int,comm int,deptno int)              &gt; row format              &gt; delimited fields              &gt; terminated by &quot;,&quot;;hive (default)&gt; load data inpath &#39;/tmp_files/emp.csv&#39; into table emp;Time taken: 1.894 secondshive (default)&gt; show tables;hive (default)&gt; select * from emp;</code></pre><p>hdfs dfs -put /root/hd/tmp_files/emp.csv /tmp_files</p><pre><code>[root@hsiehchou121 bin]# ./spark-shell --master spark://hsiehchou121:7077</code></pre><p>启动Spatk时，如果出现如下错误<br>java.sql.SQLSyntaxErrorException: Table ‘hive.partitions’ doesn’t exist<br>在MySQL数据库里面创建partitions表</p><p>scala&gt; spark.sql(“select * from emp_default”).show<br>scala&gt; spark.sql(“select * from default.emp_default”).show</p><p>spark.sql(“create table company.emp_4(empno Int,ename String,job String,mgr String,hiredate String,sal Int,comm String,deptno Int)row format delimited fields terminated by ‘,’”)<br>spark.sql(“load data local inpath ‘/root/hd/tmp_files/emp.csv’ overwrite into table company.emp_4”)</p><h3 id="三、在IDE中开发Spark-SQL"><a href="#三、在IDE中开发Spark-SQL" class="headerlink" title="三、在IDE中开发Spark SQL"></a>三、在IDE中开发Spark SQL</h3><h4 id="1、创建DataFrame-StructType方式"><a href="#1、创建DataFrame-StructType方式" class="headerlink" title="1、创建DataFrame StructType方式"></a>1、创建DataFrame StructType方式</h4><pre><code>package day4import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.types.StructTypeimport org.apache.spark.sql.types.StructFieldimport org.apache.spark.sql.types.IntegerTypeimport org.apache.spark.sql.types.StringTypeimport org.apache.spark.sql.Rowimport org.apache.log4j.Loggerimport org.apache.log4j.Level/** * 创建DataFrame StructType方式 */object Demo1 {  def main(args: Array[String]): Unit = {    //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建Spark Session对象    val spark = SparkSession.builder().master(&quot;local&quot;).appName(&quot;Demo1&quot;).getOrCreate()    //从指定的地址创建RDD对象    val personRDD = spark.sparkContext.textFile(&quot;H:\\other\\students.txt&quot;).map(_.split(&quot;\t&quot;))    //通过StructType方式指定Schema    val schema = StructType(      List(        StructField(&quot;id&quot;, IntegerType),        StructField(&quot;name&quot;, StringType),        StructField(&quot;age&quot;, IntegerType)))    //将RDD映射到rowRDD上，映射到Schema上    val rowRDD = personRDD.map(p =&gt; Row(p(0).toInt,p(1),p(2).toInt))    val personDataFrame = spark.createDataFrame(rowRDD, schema)    //注册视图    personDataFrame.createOrReplaceTempView(&quot;t_person&quot;)    //执行SQL语句  desc降序   asc 升序    val df = spark.sql(&quot;select * from t_person order by age desc&quot;)    df.show    spark.stop()  }}</code></pre><h4 id="2、使用case-class来创建DataFrame"><a href="#2、使用case-class来创建DataFrame" class="headerlink" title="2、使用case class来创建DataFrame"></a>2、使用case class来创建DataFrame</h4><pre><code>package day4import org.apache.log4j.Loggerimport org.apache.log4j.Levelimport org.apache.spark.sql.SparkSession/** * 使用case class来创建DataFrame */object Demo2 {  def main(args: Array[String]): Unit = {    //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建Spark Session对象    val spark = SparkSession.builder().master(&quot;local&quot;).appName(&quot;Demo1&quot;).getOrCreate()    //从指定的地址创建RDD对象    val lineRDD = spark.sparkContext.textFile(&quot;H:\\other\\students.txt&quot;).map(_.split(&quot;\t&quot;))    //把数据与case class做匹配    val studentRDD = lineRDD.map(x =&gt; Student(x(0).toInt,x(1),x(2).toInt))    //生成DataFrame    import spark.sqlContext.implicits._    val studentDF = studentRDD.toDF()    //注册视图,执行SQL    studentDF.createOrReplaceTempView(&quot;student&quot;)    spark.sql(&quot;select * from student&quot;).show    spark.stop()  }}//定义case classcase class Student(stuId:Int, stuName:String, stuAge:Int)</code></pre><h4 id="3、写入MySQL"><a href="#3、写入MySQL" class="headerlink" title="3、写入MySQL"></a>3、写入MySQL</h4><pre><code>package day4import org.apache.log4j.Loggerimport org.apache.log4j.Levelimport org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.types.IntegerTypeimport org.apache.spark.sql.types.StringTypeimport org.apache.spark.sql.Rowimport org.apache.spark.sql.types.StructTypeimport org.apache.spark.sql.types.StructFieldimport java.util.Properties/** * 写入mysql */object Demo3 {  def main(args: Array[String]): Unit = {    //减少Info日志的打印    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)    //创建Spark Session对象    val spark = SparkSession.builder().master(&quot;local&quot;).appName(&quot;Demo1&quot;).getOrCreate()    //从指定的地址创建RDD对象    val lineRDD = spark.sparkContext.textFile(&quot;H:\\other\\students.txt&quot;).map(_.split(&quot;\t&quot;))    //通过StructType方式指定Schema    val schema = StructType(      List(        StructField(&quot;personID&quot;, IntegerType),        StructField(&quot;personName&quot;, StringType),        StructField(&quot;personAge&quot;, IntegerType)))    //将RDD映射到rowRDD上，映射到Schema上    val rowRDD = lineRDD.map(p =&gt; Row(p(0).toInt,p(1),p(2).toInt))    val personDataFrame = spark.createDataFrame(rowRDD, schema)    personDataFrame.createOrReplaceTempView(&quot;myperson&quot;)    val result = spark.sql(&quot;select * from myperson&quot;)    result.show    //把结果存入mysql中    val props = new Properties()    props.setProperty(&quot;user&quot;, &quot;root&quot;)    props.setProperty(&quot;password&quot;, &quot;123456&quot;)    props.setProperty(&quot;driver&quot;, &quot;com.mysql.cj.jdbc.Driver&quot;)    result.write.mode(&quot;append&quot;).jdbc(&quot;jdbc:mysql://localhost:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;, &quot;student&quot;, props)    spark.stop()  }}</code></pre><h4 id="4、使用Spark-SQL-读取Hive中的数据，将计算结果存入MySQL"><a href="#4、使用Spark-SQL-读取Hive中的数据，将计算结果存入MySQL" class="headerlink" title="4、使用Spark SQL 读取Hive中的数据，将计算结果存入MySQL"></a>4、使用Spark SQL 读取Hive中的数据，将计算结果存入MySQL</h4><pre><code>package day4import org.apache.spark.sql.SparkSessionimport java.util.Properties/** * 使用Spark SQL 读取Hive中的数据，将计算结果存入mysql */object Demo4 {  def main(args: Array[String]): Unit = {    //创建SparkSession    val spark = SparkSession.builder().appName(&quot;Demo4&quot;).enableHiveSupport().getOrCreate()    //执行SQL    val result = spark.sql(&quot;select deptno,count(1) from company.emp group by deptno&quot;)    //将结果保存到mysql中     val props = new Properties()    props.setProperty(&quot;user&quot;, &quot;root&quot;)    props.setProperty(&quot;password&quot;, &quot;123456&quot;)    props.setProperty(&quot;driver&quot;, &quot;com.mysql.cj.jdbc.Driver&quot;)    result.write.jdbc(&quot;jdbc:mysql://192.168.116.1:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;, &quot;emp_stat&quot;, props)    spark.stop()  } }</code></pre><p><strong>提交任务</strong></p><pre><code>[root@hsiehchou121 bin]# ./spark-submit --master spark://hsiehchou121:7077 --jars /root/hd/tmp_files/mysql-connector-java-8.0.12.jar --driver-class-path /root/hd/tmp_files/mysql-connector-java-8.0.12.jar --class day4.Demo4 /root/hd/tmp_files/Demo4.jar</code></pre><h3 id="四、性能优化"><a href="#四、性能优化" class="headerlink" title="四、性能优化"></a>四、性能优化</h3><p>与RDD类似</p><h4 id="1、把内存中缓存表的数据"><a href="#1、把内存中缓存表的数据" class="headerlink" title="1、把内存中缓存表的数据"></a>1、把内存中缓存表的数据</h4><p>直接读取内存的值，来提高性能</p><p>RDD中如何缓存：<br>rdd.cache 或者 rdd.persist</p><p>在Spark SQL中，使用SparkSession.sqlContext.cacheTable</p><p>spark中所有context对象<br>1）sparkContext ： SparkCore<br>2）sql Context ： SparkSQL<br>3）Streaming Context ：SparkStreaming</p><p>统一起来：SparkSession</p><p>操作mysql，启动spark shell 时，需要：<br>./spark-shell <code>--master</code> spark://hsiehchou121:7077 <code>--jars</code> /root/hd/tmp_files/mysql-connector-java-8.0.12.jar <code>--driver-class-path</code> /root/hd/tmp_files/mysql-connector-java-8.0.12.jar</p><p>val mysqlDF = spark.read.format(“jdbc”).option(“driver”,”com.mysql.cj.jdbc.Driver”).option(“url”,”jdbc:mysql://192.168.116.1:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8”).option(“user”,”root”).option(“password”,”123456”).option(“dbtable”,”emp”).load</p><p>mysqlDF.show<br>mysqlDF.createOrReplaceTempView(“emp”)</p><p>spark.sqlContext.cacheTable(“emp”) —-&gt; 标识这张表可以被缓存，数据还没有真正被缓存<br>spark.sql(“select * from emp”).show —-&gt; 依然读取mysql<br>spark.sql(“select * from emp”).show —-&gt; 从缓存中读取数据</p><p>spark.sqlContext.clearCache</p><p>清空缓存后，执行查询，会触发查询mysql数据库</p><h4 id="2、了解性能优化的相关参数"><a href="#2、了解性能优化的相关参数" class="headerlink" title="2、了解性能优化的相关参数"></a>2、了解性能优化的相关参数</h4><p>将数据缓存到内存中的相关优化参数<br>spark.sql.inMemoryColumnarStorage.compressed<br>默认为 true<br>Spark SQL 将会基于统计信息自动地为每一列选择一种压缩编码方式</p><p>spark.sql.inMemoryColumnarStorage.batchSize<br>默认值：10000<br>缓存批处理大小。缓存数据时, 较大的批处理大小可以提高内存利用率和压缩率，但同时也会带来 OOM（Out Of Memory）的风险</p><p>其他性能相关的配置选项（不过不推荐手动修改，可能在后续版本自动的自适应修改）<br>spark.sql.files.maxPartitionBytes<br>默认值：128 MB<br>读取文件时单个分区可容纳的最大字节数</p><p>spark.sql.files.openCostInBytes<br>默认值：4M<br>打开文件的估算成本, 按照同一时间能够扫描的字节数来测量。当往一个分区写入多个文件的时候会使用。高估更好, 这样的话小文件分区将比大文件分区更快 (先被调度)</p><p>spark.sql.autoBroadcastJoinThreshold<br>默认值：10M<br>用于配置一个表在执行 join 操作时能够广播给所有 worker 节点的最大字节大小。通过将这个值设置为 -1 可以禁用广播。注意，当前数据统计仅支持已经运行了 ANALYZE TABLE COMPUTE STATISTICS noscan 命令的 Hive Metastore 表</p><p>spark.sql.shuffle.partitions<br>默认值：200<br>用于配置 join 或聚合操作混洗（shuffle）数据时使用的分区数</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Core</title>
      <link href="/2019/03/29/spark-core/"/>
      <url>/2019/03/29/spark-core/</url>
      
        <content type="html"><![CDATA[<p>Spark生态圈：<br>Spark Core ： RDD（弹性分布式数据集）<br>Spark SQL<br>Spark Streaming<br>Spark MLLib ：协同过滤，ALS，逻辑回归等等 –&gt; 机器学习<br>Spark Graphx ： 图计算</p><h3 id="一、Spark-Core"><a href="#一、Spark-Core" class="headerlink" title="一、Spark Core"></a>一、Spark Core</h3><h4 id="1、什么是Spark？特点"><a href="#1、什么是Spark？特点" class="headerlink" title="1、什么是Spark？特点"></a>1、什么是Spark？特点</h4><p><a href="https://spark.apache.org/" target="_blank" rel="noopener">https://spark.apache.org/</a><br>Apache Spark™ is a unified analytics engine for large-scale data processing.<br>特点：快、易用、通用性、兼容性（完全兼容Hadoop）</p><p>快：快100倍（Hadoop 3 之前）<br>易用：支持多种语言开发<br>通用性：生态系统全<br>易用性：兼容Hadoop</p><h3 id="二、安装和部署Spark、Spark-的-HA"><a href="#二、安装和部署Spark、Spark-的-HA" class="headerlink" title="二、安装和部署Spark、Spark 的 HA"></a>二、安装和部署Spark、Spark 的 HA</h3><h4 id="1、Spark体系结构"><a href="#1、Spark体系结构" class="headerlink" title="1、Spark体系结构"></a>1、Spark体系结构</h4><p>Spark的运行方式</p><p>Yarn</p><p>Standalone：本机调试（Demo）</p><p>Worker：从节点。每个服务器上，资源和任务的管理者。只负责管理一个节点</p><p>执行过程：<br>一个Worker 有多个 Executor。 Executor是任务的执行者，按阶段（stage）划分任务。————&gt; RDD</p><p>客户端：Driver Program 提交任务到集群中<br>1）spark-submit<br>2）spark-shell</p><h4 id="2、Spark的搭建"><a href="#2、Spark的搭建" class="headerlink" title="2、Spark的搭建"></a>2、Spark的搭建</h4><p>1）准备工作：JDK 配置主机名 免密码登录</p><p>2）伪分布式模式<br>在一台虚拟机上模拟分布式环境（Master和Worker在一个节点上）<br>配置spark-env.sh<br>vi spark-env.sh</p><pre><code>export JAVA_HOME=/root/hd/jdk1.8.0_192export SPARK_MASTER_HOST=hsiehchou121export SPARK_MASTER_PORT=7077</code></pre><p>配置slaves<br>vi slaves<br>hsiehchou121</p><p>浏览器访问hsiehchou121:8080</p><p>在Spark中使用Scala语言</p><pre><code>[root@hsiehchou121 bin]# ./spark-shell --master spark://hsiehchou121:7077</code></pre><p>3）全分布式环境<br>修改slave文件 拷贝到其他三台服务器 启动</p><h4 id="3、Spark的-HA"><a href="#3、Spark的-HA" class="headerlink" title="3、Spark的 HA"></a>3、Spark的 HA</h4><p>回顾HA（高可用）<br>（<em>）HDFS Yarn Hbase Spark 主从结构<br>（</em>）单点故障</p><p>（1）基于文件目录的单点恢复<br>主要用于开发或测试环境。当spark提供目录保存spark Application和worker的注册信息，并将他们的恢复状态写入该目录中，这时，一旦Master发生故障，就可以通过重新启动Master进程（sbin/start-master.sh），恢复已运行的spark Application和worker的注册信息</p><p>基于文件系统的单点恢复，主要是在spark-en.sh里对SPARK_DAEMON_JAVA_OPTS设置</p><table><thead><tr><th align="center">配置参数</th><th align="center">参考值</th></tr></thead><tbody><tr><td align="center">spark.deploy.recoveryMode</td><td align="center">设置为FILESYSTEM开启单点恢复功能，默认值：NONE</td></tr><tr><td align="center">spark.deploy.recoveryDirectory</td><td align="center">Spark 保存恢复状态的目录</td></tr></tbody></table><p><strong>参考</strong>：<br>export SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=FILESYSTEM -Dspark.deploy.recoveryDirectory=/root/hd/spark-2.1.0-bin-hadoop2.7/recovery”</p><p>（*）本质：还是只有一个主节点Master，创建了一个恢复目录，保存集群状态和任务的信息<br>当Master挂掉，重新启动时，会从恢复目录下读取状态信息，恢复出来原来的状态</p><p>用途：这个只用于开发和测试，但是生产使用用ZooKeeper</p><p>export SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=FILESYSTEM -Dspark.deploy.recoveryDirectory=/root/hd/spark-2.1.0-bin-hadoop2.7/recovery”</p><p>（2）基于ZooKeeper ：和Hadoop类似<br>ZooKeeper提供了一个Leader Election机制，利用这个机制可以保证虽然集群存在多个Master，但是只有一个是Active的，其他的都是Standby。当Active的Master出现故障时，另外的一个Standby Master会被选举出来。由于集群的信息，包括Worker， Driver和Application的信息都已经持久化到ZooKeeper，因此在切换的过程中只会影响新Job的提交，对于正在进行的Job没有任何的影响</p><table><thead><tr><th align="center">配置参数</th><th align="center">参考值</th></tr></thead><tbody><tr><td align="center">spark.deploy.recoveryMode</td><td align="center">设置为ZOOKEEPER开启单点恢复功能，默认值：NONE</td></tr><tr><td align="center">spark.deploy.zookeeper.url</td><td align="center">ZooKeeper集群的地址</td></tr><tr><td align="center">spark.deploy.zookeeper.dir</td><td align="center">Spark信息在ZK中的保存目录，默认：/spark</td></tr></tbody></table><p><strong>参考</strong>：<br>export SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181,hsiehchou124:2181 -Dspark.deploy.zookeeper.dir=/spark”</p><p>（*）复习一下zookeeper：<br>相当于一个数据库，把一些信息存放在zookeeper中，比如集群的信息<br>数据同步功能，选举功能，分布式锁功能</p><p>数据同步：给一个节点中写入数据，可以同步到其他节点</p><p>选举：Zookeeper中存在不同的角色，Leader Follower。如果Leader挂掉，重新选举Leader</p><p>分布式锁：秒杀。以目录节点的方式来保存数据</p><p>修改 spark-env.sh</p><pre><code>export JAVA_HOME=/root/hd/jdk1.8.0_192#export SPARK_MASTER_HOST=hsiehchou121#export SPARK_MASTER_PORT=7077#export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=FILESYSTEM -Dspark.deploy.recoveryDirectory=/root/hd/spark-2.1.0-bin-hadoop2.7/recovery&quot;export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181,hsiehchou124:2181 -Dspark.deploy.zookeeper.dir=/spark&quot;</code></pre><p>同步到其他三台服务器<br>[root@hsiehchou121 spark-2.1.0-bin-hadoop2.7]# scp conf/spark-env.sh hsiehchou122:/root/hd/spark-2.1.0-bin-hadoop2.7/conf<br>[root@hsiehchou121 spark-2.1.0-bin-hadoop2.7]# scp conf/spark-env.sh hsiehchou123:/root/hd/spark-2.1.0-bin-hadoop2.7/conf<br>[root@hsiehchou121 spark-2.1.0-bin-hadoop2.7]# scp conf/spark-env.sh hsiehchou124:/root/hd/spark-2.1.0-bin-hadoop2.7/conf</p><p>在hsiehchou121 start-all hsiehchou121 master hsiehchou122 Worker hsiehchou123 Worker hsiehchou124 Worker<br>在hsiehchou121 start-master hsiehchou121 master hsiehchou122 master（standby） hsiehchou122 Worker hsiehchou123 Worker hsiehchou124 Worker</p><p>在hsiehchou121 上kill master<br>hsiehchou122 master（Active） hsiehchou122 Worker hsiehchou123 Worker hsiehchou124 Worker</p><p>在网页<a href="http://192.168.116.122:8080/" target="_blank" rel="noopener">http://192.168.116.122:8080/</a> 可以看到相应信息</p><h3 id="三、执行Spark的任务：两个工具"><a href="#三、执行Spark的任务：两个工具" class="headerlink" title="三、执行Spark的任务：两个工具"></a>三、执行Spark的任务：两个工具</h3><h4 id="1、spark-submit：用于提交Spark的任务"><a href="#1、spark-submit：用于提交Spark的任务" class="headerlink" title="1、spark-submit：用于提交Spark的任务"></a>1、spark-submit：用于提交Spark的任务</h4><p>任务：jar</p><p>举例：蒙特卡洛求PI（圆周率）</p><p>./spark-submit <code>--master</code> spark://hsiehchou121:7077 <code>--class</code><br><code>--class</code>指明主程序的名字</p><pre><code>[root@hsiehchou121 /]#cd /root/hd/spark-2.1.0-bin-hadoop2.7/bin[root@hsiehchou121 bin]# ./spark-submit --master spark://hsiehchou121:7077 --class org.apache.spark.examples.SparkPi /root/hd/spark-2.1.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.1.0.jar 100</code></pre><p>其中100指定执行的次数</p><h4 id="2、spark-shell-相当于REPL"><a href="#2、spark-shell-相当于REPL" class="headerlink" title="2、spark-shell 相当于REPL"></a>2、spark-shell 相当于REPL</h4><p>spark-shell是Spark自带的交互式Shell程序，方便用户进行交互式编程，用户可以在该命令行下用scala编写spark程序<br>（*）启动Spark Shell：spark-shell<br>也可以使用以下参数：<br>参数说明：<br><code>--master</code> spark://hsiehchou121:7077 指定Master的地址<br><code>--executor-memory</code> 2g 指定每个worker可用内存为2G<br><code>--total-executor-cores</code> 2 指定整个集群使用的cup核数为2个<br>例如：</p><p>spark-shell <code>--master</code> spark://hsiehchou121:7077 <code>--executor-memory</code> 2g <code>--total-executor-cores</code> 2<br>注意：<br>如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系</p><p>作为一个独立的Application运行<br>两种模式：<br>（1）<strong>本地模式</strong><br>spark-shell 后面不接任何参数，代表本地模式<br>./spark-shell<br>Spark context available as ‘sc’ (master = local[<em>], app id = local-1554372019995).<br>sc 是 SparkContext 对象名。 local[</em>] 代表本地模式，不提交到集群中运行</p><p>（2）<strong>集群模式</strong><br>[root@hsiehchou121 bin]# ./spark-shell <code>--master</code> spark://hsiehchou121:7077<br>提交到集群运行<br>Spark context available as ‘sc’ (master = spark://hsiehchou121:7077, app id = app-20190404190030-0000).</p><p>master = spark://hsiehchou121:7077<br>Spark session available as ‘spark’<br>Spark Session 是 2.0 以后提供的，利用 SparkSession 可以访问spark所有组件</p><p>示例：<br><strong>WordCount程序</strong> </p><p>程序如下：</p><pre><code>sc.textFile(&quot;hdfs://192.168.116.121:9000/data.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;hdfs://192.168.116.121:9000/output/wc&quot;)</code></pre><p>说明：<br>sc是SparkContext对象，该对象时提交spark程序的入口<br>textFile(“hdfs://192.168.116.121:9000/data.txt”)是hdfs中读取数据<br>flatMap(<em>.split(” “))先map在压平<br>map((</em>,1))将单词和1构成元组<br>reduceByKey(+)按照key进行reduce，并将value累加<br>saveAsTextFile(“hdfs://192.168.116.121:9000/output/wc”)将结果写入到hdfs中</p><p>（*）处理本地文件，把结果打印到屏幕上<br>vi /root/hd/tmp_files/test_WordCount.txt<br>I love China<br>I love Jiangsu<br>Jiangsu is a beautiful place in China</p><pre><code>scala&gt; sc.textFile(&quot;/root/hd/tmp_files/test_WordCount.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).collectres0: Array[(String, Int)] = Array((is,1), (love,2), (China,2), (a,1), (Jiangsu,2), (I,2), (in,1), (place,1), (beautiful,1))</code></pre><p>（*）处理HDFS文件，结果保存在HDFS上</p><pre><code>[root@hsiehchou121 tmp_files]# hdfs dfs -mkdir /tmp_files[root@hsiehchou121 tmp_files]# hdfs dfs -copyFromLocal ~/hd/tmp_files/test_WordCount.txt /tmp_files</code></pre><pre><code>scala&gt; sc.textFile(&quot;hdfs://hsiehchou121:9000/tmp_files/test_WordCount.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;hdfs://hsiehchou121:9000/out/0404/test_WordCount&quot;)</code></pre><p>-rw-r–r– 3 root supergroup 0 2019-04-04 19:12 /out/0404/test_WordCount/_SUCCESS<br>-rw-r–r– 3 root supergroup 16 2019-04-04 19:12 /out/0404/test_WordCount/part-00000<br>-rw-r–r– 3 root supergroup 65 2019-04-04 19:12 /out/0404/test_WordCount/part-00001</p><p>_SUCCESS 代表程序执行成功</p><p>part-00000 part-00001 结果文件，分区。里面内容不重复</p><p>（*）单步运行WordCount —-&gt; RDD<br>scala&gt; val rdd1 = sc.textFile(“/root/hd/tmp_files/test_WordCount.txt”)<br>rdd1: org.apache.spark.rdd.RDD[String] = /root/hd/tmp_files/test_WordCount.txt MapPartitionsRDD[12] at textFile at <code>&lt;console&gt;:</code>24</p><p>scala&gt; rdd1.collect<br>res5: Array[String] = Array(I love China, I love Jiangsu, Jiangsu is a beautiful place in China)</p><p>scala&gt; val rdd2 = rdd1.flatMap(_.split(“ “))<br>rdd2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[14] at flatMap at <code>&lt;console&gt;</code>:26</p><p>scala&gt; rdd2.collect<br>res6: Array[String] = Array(I, love, China, I, love, Jiangsu, Jiangsu, is, a, beautiful, place, in, China)</p><p>scala&gt; val rdd3 = rdd2.map((_,1))<br>rdd3: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[15] at map at <code>&lt;console&gt;</code>:28</p><p>scala&gt; rdd3.collect<br>res7: Array[(String, Int)] = Array((I,1), (love,1), (China,1), (I,1), (love,1), (Jiangsu,1), (Jiangsu,1), (is,1), (a,1), (beautiful,1), (place,1), (in,1), (China,1))</p><p>scala&gt; val rdd4 = rdd3.reduceByKey(<em>+</em>)<br>rdd4: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[16] at reduceByKey at <code>&lt;console&gt;</code>:30</p><p>scala&gt; rdd4.collect<br>res8: Array[(String, Int)] = Array((is,1), (love,2), (China,2), (a,1), (Jiangsu,2), (I,2), (in,1), (place,1), (beautiful,1))</p><p><strong>RDD 弹性分布式数据集</strong> </p><p>（1）依赖关系 ： 宽依赖和窄依赖<br>（2）算子：<br>函数：<br>Transformation ： 延时计算 map flatMap textFile<br>Action ： 立即触发计算 collect</p><p>说明：</p><p><strong>scala复习</strong> </p><p>（*）flatten：把嵌套的结果展开<br>scala&gt; List(List(2,4,6,8,10),List(1,3,5,7,9)).flatten<br>res21: List[Int] = List(2, 4, 6, 8, 10, 1, 3, 5, 7, 9)</p><p>（*）flatmap : 相当于一个 map + flatten<br>scala&gt; var myList = List(List(2,4,6,8,10),List(1,3,5,7,9))<br>myList: List[List[Int]] = List(List(2, 4, 6, 8, 10), List(1, 3, 5, 7, 9))</p><p>scala&gt; myList.flatMap(x=&gt;x.map(_*2))<br>res22: List[Int] = List(4, 8, 12, 16, 20, 2, 6, 10, 14, 18)</p><p>myList.flatMap(x=&gt;x.map(_*2))</p><p>执行过程：<br>（1）将 List(2, 4, 6, 8, 10), List(1, 3, 5, 7, 9) 调用 map(_*2) 方法。x 代表一个List<br>（2）flatten<br>（3）在IDE中开发scala版本和Java版本的WorkCount</p><h3 id="四、WordCount（Scala版本和Java版本）"><a href="#四、WordCount（Scala版本和Java版本）" class="headerlink" title="四、WordCount（Scala版本和Java版本）"></a>四、WordCount（Scala版本和Java版本）</h3><h4 id="1、Scala版本的WordCount"><a href="#1、Scala版本的WordCount" class="headerlink" title="1、Scala版本的WordCount"></a>1、Scala版本的WordCount</h4><p>新建一个工程，把jar引入到工程中</p><pre><code>package day1import org.apache.spark.SparkConfimport org.apache.spark.SparkContextobject WordCount {  def main(args: Array[String]): Unit = {    //创建一个Spark的配置文件    val conf = new SparkConf().setAppName(&quot;My Scala WordCount 0404&quot;).setMaster(&quot;local&quot;)    //创建SparkContext对象    val sc = new SparkContext(conf)    //1.从本地模式运行 .setMaster(&quot;local&quot;)   //val result = sc.textFile(&quot;hdfs://hsiehchou121:9000/tmp_files/test_WordCount.txt&quot;)      //.flatMap(_.split(&quot; &quot;))      //.map((_,1))      //.reduceByKey(_+_)    //result.foreach(println)    //2、在集群模式运行    val result = sc.textFile(args(0))      .flatMap(_.split(&quot; &quot;))      .map((_, 1))      .reduceByKey(_ + _)      .saveAsTextFile(args(1))    sc.stop()  }}</code></pre><p>export Demo1.jar 点击下一步，把jar包上传到服务器上/root/hd/tmp_files/下</p><p>在spark里面的bin目录下输入</p><p>[root@hsiehchou121 bin]# ./spark-submit <code>--master</code> spark://hsiehchou121:7077 <code>--class</code> day1.WordCount /root/hd/tmp_files/Demo1.jar hdfs://hsiehchou121:9000/tmp_files/test_WordCount.txt hdfs://hsiehchou121:9000/out/0405/Demo1</p><h4 id="2、Java版本的WordCount"><a href="#2、Java版本的WordCount" class="headerlink" title="2、Java版本的WordCount"></a>2、Java版本的WordCount</h4><pre><code>package day1;import java.util.Arrays;import java.util.Iterator;import java.util.List;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import scala.Tuple2;public class JavaWordCount {    public static void main(String[] args) {        SparkConf conf = new SparkConf().setAppName(&quot;JavaWordCount&quot;).setMaster(&quot;local&quot;);        //创建SparkContext对象        JavaSparkContext sc = new JavaSparkContext(conf);        //读入数据        JavaRDD&lt;String&gt; lines = sc.textFile(&quot;hdfs://192.168.116.121:9000/tmp_files/test_WordCount.txt&quot;);        //分词，第一个参数表示读进来的每一句话，第二个参数表示返回值        JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String,String&gt;(){            @Override            public Iterator&lt;String&gt; call(String input) throws Exception {                return Arrays.asList(input.split(&quot; &quot;)).iterator();            }        });        //每一个单词记一个数        JavaPairRDD&lt;String,Integer&gt; ones = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() {            @Override            public Tuple2&lt;String, Integer&gt; call(String input) throws Exception {                return new Tuple2&lt;String, Integer&gt;(input,1);            }        });        //执行reduce操作        JavaPairRDD&lt;String, Integer&gt; counts = ones.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() {            @Override            public Integer call(Integer arg0, Integer arg1) throws Exception {                return arg0 + arg1;            }        });        List&lt;Tuple2&lt;String,Integer&gt;&gt; output = counts.collect();        for(Tuple2&lt;String, Integer&gt; tuple:output) {            System.out.println(tuple._1 + &quot;:&quot; + tuple._2);        }        sc.stop();    }}</code></pre><p>[root@hsiehchou121 bin]# ./spark-submit <code>--master</code> spark://hsiehchou121:7077 <code>--class</code> day1.JavaWordCount /root/hd/tmp_files/Demo2.jar</p><h3 id="五、分析Spark的任务流程"><a href="#五、分析Spark的任务流程" class="headerlink" title="五、分析Spark的任务流程"></a>五、分析Spark的任务流程</h3><h4 id="1、分析WordCount程序处理过程"><a href="#1、分析WordCount程序处理过程" class="headerlink" title="1、分析WordCount程序处理过程"></a>1、分析WordCount程序处理过程</h4><p><img src="/medias/WordCount%E7%A8%8B%E5%BA%8F%E5%88%86%E6%9E%90.PNG" alt="WordCount程序分析"></p><h4 id="2、Spark调度任务的过程"><a href="#2、Spark调度任务的过程" class="headerlink" title="2、Spark调度任务的过程"></a>2、Spark调度任务的过程</h4><p>提交到及群众运行任务时，spark执行任务调度<br><img src="/medias/spark%E7%9A%84%E8%B0%83%E7%94%A8%E4%BB%BB%E5%8A%A1%E8%BF%87%E7%A8%8B.PNG" alt="spark的调用任务过程"></p><h3 id="六、RDD和RDD特性、RDD的算子"><a href="#六、RDD和RDD特性、RDD的算子" class="headerlink" title="六、RDD和RDD特性、RDD的算子"></a>六、RDD和RDD特性、RDD的算子</h3><h4 id="1、RDD：弹性分布式数据集"><a href="#1、RDD：弹性分布式数据集" class="headerlink" title="1、RDD：弹性分布式数据集"></a>1、RDD：弹性分布式数据集</h4><p>（<em>）Spark中最基本的数据抽象<br>（</em>）RDD的特性</p><p>Internally, each RDD is characterized by five main properties:<br>*</p><p>A list of partitions<br>1）是一组分区<br>RDD由分区组成，每个分区运行在不同的Worker上，通过这种方式来实现分布式计算</p><p><img src="/medias/RDD.PNG" alt="RDD"></p><p>A function for computing each split<br>在RDD中，提供算子处理每个分区中的数据</p><p>-A list of dependencies on other RDDs<br>RDD存在依赖关系：宽依赖和窄依赖</p><p>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)<br>可以自定义分区规则来创建RDD</p><p>Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)<br>优先选择离文件位置近的节点来执行</p><p><strong>如何创建RDD</strong><br>（1）通过SparkContext.parallelize方法来创建</p><p>scala&gt; val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8),3)<br>rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[32] at parallelize at <code>&lt;console&gt;</code>:29</p><p>scala&gt; rdd1.partitions.length<br>res35: Int = 3</p><p>scala&gt; val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8),2)<br>rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[33] at parallelize at <code>&lt;console&gt;</code>:29</p><p>scala&gt; rdd1.partitions.length<br>res36: Int = 2</p><p>（2）通过外部数据源来创建<br>sc.textFile()</p><p>scala&gt; val rdd2 = sc.textFile(“/root/hd/tmp_files/test_WordCount.txt”)<br>rdd2: org.apache.spark.rdd.RDD[String] = /usr/local/tmp_files/test_WordCount.txt MapPartitionsRDD[35] at textFile at <code>&lt;console&gt;</code>:29</p><h4 id="2、-算子"><a href="#2、-算子" class="headerlink" title="2、 算子"></a>2、 算子</h4><p><strong>1）Transformation</strong><br>map(func)：相当于for循环，返回一个新的RDD</p><p>filter(func)：过滤<br>flatMap(func)：flat+map 压平</p><p>mapPartitions(func)：对RDD中的每个分区进行操作<br>mapPartitionsWithIndex(func)：对RDD中的每个分区进行操作，可以取到分区号</p><p>sample(withReplacement, fraction, seed)：采样</p><p><strong>集合运算</strong><br>union(otherDataset)：对源RDD和参数RDD求并集后返回一个新的RDD<br>intersection(otherDataset)：对源RDD和参数RDD求交集后返回一个新的RDD</p><p>distinct([numTasks]))：去重</p><p><strong>聚合操作</strong>：<strong>group by</strong><br>groupByKey([numTasks]) ：在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD<br>reduceByKey(func, [numTasks])：在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置<br>aggregateByKey(zeroValue)(seqOp,combOp,[numTasks])：按照key进行聚合</p><p><strong>排序</strong><br>sortByKey([ascending], [numTasks])：在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD<br>sortBy(func,[ascending], [numTasks])：与sortByKey类似，但是更灵活</p><p>join(otherDataset, [numTasks])：在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD<br>cogroup(otherDataset, [numTasks])：在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable,Iterable))类型的RDD<br>cartesian(otherDataset)<br>pipe(command, [envVars])<br>coalesce(numPartitions)</p><p><strong>重分区</strong>：<br>repartition(numPartitions)<br>repartitionAndSortWithinPartitions(partitioner)</p><p>举例：<br>（1）创建一个RDD，每个元素乘以2，再排序<br>scala&gt; val rdd1 = sc.parallelize(Array(3,4,5,100,79,81,6,8))<br>rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <code>&lt;console&gt;</code>:24</p><p>scala&gt; val rdd2 = rdd1.map(_*2)<br>rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at <code>&lt;console&gt;</code>:26</p><p>scala&gt; rdd2.collect<br>res0: Array[Int] = Array(6, 8, 10, 200, 158, 162, 12, 16)                       </p><p>scala&gt; rdd2.sortBy(x=&gt;x,true).collect<br>res1: Array[Int] = Array(6, 8, 10, 12, 16, 158, 162, 200)</p><p>scala&gt; rdd2.sortBy(x=&gt;x,false).collect<br>res2: Array[Int] = Array(200, 162, 158, 16, 12, 10, 8, 6)     </p><p>def sortBy[K](f: (T) ⇒ K, ascending: Boolean = true)<br>过滤出大于20的元素：</p><p>scala&gt; val rdd3 = rdd2.filter(_&gt;20)<br>rdd3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[53] at filter at <code>&lt;console&gt;</code>:33+</p><p>scala&gt; rdd3.collect<br>res3: Array[Int] = Array(200, 158, 162)   </p><p>（2）字符串（字符）类型的RDD<br>scala&gt; val rdd4 = sc.parallelize(Array(“a b c”,”d e f”,”g h i”))<br>rdd4: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[28] at parallelize at <code>&lt;console&gt;</code>:24</p><p>scala&gt; rdd4.flatMap(_.split(“ “)).collect<br>res4: Array[String] = Array(a, b, c, d, e, f, g, h, i)</p><h4 id="3、RDD的集合运算"><a href="#3、RDD的集合运算" class="headerlink" title="3、RDD的集合运算"></a>3、RDD的集合运算</h4><p>scala&gt; val rdd5 = sc.parallelize(List(1,2,3,6,7,8,100))<br>rdd5: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24</p><p>scala&gt; val rdd6 = sc.parallelize(List(1,2,3,4))<br>rdd6: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24</p><p>scala&gt; val rdd7 = rdd5.union(rdd6)<br>rdd7: org.apache.spark.rdd.RDD[Int] = UnionRDD[2] at union at <console>:28</p><p>scala&gt; rdd7.collect<br>res5: Array[Int] = Array(1, 2, 3, 6, 7, 8, 100, 1, 2, 3, 4)                     </p><p>scala&gt; rdd7.distinct.collect<br>res6: Array[Int] = Array(100, 4, 8, 1, 6, 2, 3, 7)</p><h4 id="4、分组操作：reduceByKey"><a href="#4、分组操作：reduceByKey" class="headerlink" title="4、分组操作：reduceByKey"></a>4、分组操作：reduceByKey</h4><key value>scala> val rdd1 = sc.parallelize(List(("Time",1800),("Dadi",2400),("Giu",1600)))rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[6] at parallelize at <console>:24<p>scala&gt; val rdd2 = sc.parallelize(List((“Dadi”,1300),(“Time”,2900),(“Mi”,600)))<br>rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[7] at parallelize at <console>:24</p><p>scala&gt; val rdd3 = rdd1 union rdd2<br>rdd3: org.apache.spark.rdd.RDD[(String, Int)] = UnionRDD[8] at union at <console>:28</p><p>scala&gt; rdd3.collect<br>res3: Array[(String, Int)] = Array((Time,1800), (Dadi,2400), (Giu,1600), (Dadi,1300), (Time,2900), (Mi,600))</p><p>scala&gt; val rdd4 = rdd3.groupByKey<br>rdd4: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[9] at groupByKey at <console>:30</p><p>scala&gt; rdd4.collect<br>res4: Array[(String, Iterable[Int])] = Array((Mi,CompactBuffer(600)),<br>      (Time,CompactBuffer(1800, 2900)),<br>      (Dadi,CompactBuffer(2400, 1300)),<br>      (Giu,CompactBuffer(1600)))</p><p>scala&gt; rdd3.reduceByKey(<em>+</em>).collect<br>res5: Array[(String, Int)] = Array((Mi,600), (Time,4700), (Dadi,3700), (Giu,1600))<br>reduceByKey will provide much better performance.<br>官方不推荐使用 groupByKey 推荐使用 reduceByKey</p><h4 id="5、cogroup"><a href="#5、cogroup" class="headerlink" title="5、cogroup"></a>5、cogroup</h4><p>在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable,Iterable))类型的RDD</p><p>对两个RDD中的KV元素，每个RDD中相同key中的元素分别聚合成一个集合。与reduceByKey不同的是针对两个RDD中相同的key的元素进行合并，与groupByKey返回值上与区别</p><p>scala&gt; val rdd1 = sc.parallelize(List((“Tim”,1),(“Tim”,2),(“Jert”,3),(“kiy”,2)))<br>rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[11] at parallelize at <console>:24</p><p>scala&gt; val rdd1 = sc.parallelize(List((“Tim”,1),(“Tim”,2),(“Jert”,3),(“Kiy”,2)))<br>rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[12] at parallelize at <console>:24</p><p>scala&gt; val rdd2 = sc.parallelize(List((“Jert”,2),(“Tim”,1),(“Sun”,2)))<br>rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[13] at parallelize at <console>:24</p><p>scala&gt; val rdd3 = rdd1.cogroup(rdd2)<br>rdd3: org.apache.spark.rdd.RDD[(String, (Iterable[Int], Iterable[Int]))] = MapPartitionsRDD[15] at cogroup at <console>:28</p><p>scala&gt; rdd3.collect<br>res6: Array[(String, (Iterable[Int], Iterable[Int]))] = Array(<br>(Tim,(CompactBuffer(1, 2),CompactBuffer(1))),<br>(Sun,(CompactBuffer(),CompactBuffer(2))),<br>(Kiy,(CompactBuffer(2),CompactBuffer())),<br>(Jert,(CompactBuffer(3),CompactBuffer(2))))</p><h4 id="6、reduce操作（Action）"><a href="#6、reduce操作（Action）" class="headerlink" title="6、reduce操作（Action）"></a>6、reduce操作（Action）</h4><p><strong>聚合操作</strong></p><p>scala&gt; val rdd1 = sc.parallelize(List(1,2,3,4,5))<br>rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[16] at parallelize at <console>:24</p><p>scala&gt; rdd1.reduce(<em>+</em>)<br>res7: Int = 15</p><h4 id="7、需求：按照value排序"><a href="#7、需求：按照value排序" class="headerlink" title="7、需求：按照value排序"></a>7、需求：按照value排序</h4><p>做法：<br>1）交换，把key 和 value交换，然后调用sortByKey方法<br>2）再次交换</p><p>scala&gt; val rdd1 = sc.parallelize(List((“tim”,1),(“jery”,3),(“kef”,2),(“sun”,2)))<br>rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[17] at parallelize at <console>:24</p><p>scala&gt; val rdd2 = sc.parallelize(List((“jery”,1),(“tim”,3),(“sun”,5),(“kef”,1)))<br>rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[18] at parallelize at <console>:24</p><p>scala&gt; val rdd3 = rdd1.union(rdd2)<br>rdd3: org.apache.spark.rdd.RDD[(String, Int)] = UnionRDD[19] at union at <console>:28</p><p>scala&gt; val rdd4 = rdd3.reduceByKey(<em>+</em>)<br>rdd4: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[20] at reduceByKey at <console>:30</p><p>scala&gt; rdd4.collect<br>res8: Array[(String, Int)] = Array((tim,4), (kef,3), (sun,7), (jery,4))</p><p>scala&gt; val rdd5 = rdd4.map(t=&gt;(t._2,t._1)).sortByKey(false).map(t=&gt;(t._2,t._1))<br>rdd5: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[25] at map at <console>:32</p><p>scala&gt; rdd5.collect<br>res10: Array[(String, Int)] = Array((sun,7), (tim,4), (jery,4), (kef,3))</p><p>（2）Action<br>reduce(func)：通过func函数聚集RDD中的所有元素，这个功能必须是课交换且可并联的</p><p>collect()：在驱动程序中，以数组的形式返回数据集的所有元素<br>count()：返回RDD的元素个数<br>first()：返回RDD的第一个元素（类似于take(1)）<br>take(n)：返回一个由数据集的前n个元素组成的数组</p><p>takeSample(withReplacement,num, [seed])：返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子</p><p>takeOrdered(n, [ordering])：takeOrdered和top类似，只不过以和top相反的顺序返回元素</p><p>saveAsTextFile(path)：将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本</p><p>saveAsSequenceFile(path) ：将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统</p><p>saveAsObjectFile(path) ：saveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中</p><p>countByKey()：针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数</p><p>foreach(func)：在数据集的每一个元素上，运行函数func进行更新。<br>与map类似，没有返回值</p><p>3）特性<br>（1）<strong>RDD的缓存机制</strong><br>RDD通过persist方法或cache方法可以将前面的计算结果缓存，但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用</p><p>通过查看源码发现cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在object StorageLevel中定义的</p><p>缓存有可能丢失，或者存储存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition</p><p>（<em>）作用：提高性能<br>（</em>）使用：标识RDD可以被缓存 persist cache<br>（*）可以缓存的位置：</p><p>val NONE = new StorageLevel(false, false, false, false)<br>val DISK_ONLY = new StorageLevel(true, false, false, false)<br>val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)<br>val MEMORY_ONLY = new StorageLevel(false, true, false, true)<br>val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)<br>val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)<br>val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)<br>val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)<br>val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)<br>val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)<br>val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)<br>val OFF_HEAP = new StorageLevel(true, true, true, false, 1)</p><p>/**</p><ul><li>Persist this RDD with the default storage level (<code>MEMORY_ONLY</code>).</li><li>/<br>def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)</li></ul><p>/**</p><ul><li>Persist this RDD with the default storage level (<code>MEMORY_ONLY</code>).</li><li>/<br>def cache(): this.type = persist()<br>举例：测试数据，92万条<br>进入spark-shell命令</li></ul><p>./spark-shell <code>--master</code> spark://hsiehchou121:7077</p><p>scala&gt; val rdd1 = sc.textFile(“hdfs://192.168.116.121:9000/tmp_files/test_Cache.txt”)<br>rdd1: org.apache.spark.rdd.RDD[String] = hdfs://192.168.116.121:9000/tmp_files/test_Cache.txt MapPartitionsRDD[3] at textFile at <code>&lt;console&gt;</code>:24</p><p>scala&gt; rdd1.count  –&gt; 直接出发计算<br>res0: Long = 921911                                                       </p><p>scala&gt; rdd1.cache  –&gt; 标识RDD可以被缓存，不会触发计算<br>res1: rdd1.type = hdfs://192.168.116.121:9000/tmp_files/test_Cache.txt MapPartitionsRDD[3] at textFile at <code>&lt;console&gt;</code>:24</p><p>scala&gt; rdd1.count   –&gt; 和第一步一样，触发计算，但是，把结果进行缓存<br>res2: Long = 921911                                                          </p><p>scala&gt; rdd1.count   –&gt;  从缓存中直接读出结果<br>res3: Long = 921911</p><p>（2）<strong>RDD的容错机制：通过检查点来实现</strong><br>检查点（本质是通过将RDD写入Disk做检查点）是为了通过lineage（血统）做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销</p><p>设置checkpoint的目录，可以是本地的文件夹、也可以是HDFS。一般是在具有容错能力，高可靠的文件系统上(比如HDFS, S3等)设置一个检查点路径，用于保存检查点数据</p><p>/**</p><p>Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint</p><p>directory set with SparkContext#setCheckpointDir and all references to its parent</p><p>RDDs will be removed. This function must be called before any job has been</p><p>executed on this RDD. It is strongly recommended that this RDD is persisted in</p><p>memory, otherwise saving it on a file will require recomputation.<br>*/</p><p>（*）复习检查点：<br>HDFS中的检查点：有SecondaryNamenode来实现日志的合并</p><p>（*）RDD的检查点：容错<br>概念：血统 Lineage<br>理解：表示任务执行的生命周期<br>WordCount textFile —&gt; redceByKey</p><p>如果血统越长，越容易出错</p><p>假如有检查点，可以从最近的一个检查点开始，往后面计算。不用重头计算</p><p>（*）RDD检查点的类型<br>（1）基于本地目录：需要将Spark shell 或者任务运行在本地模式上（setMaster(“local”)）<br>开发和测试</p><p>（2）HDFS目录：用于生产<br>sc.setCheckPointDir(目录)</p><p>举例：<strong>设置检查点</strong><br>scala&gt; var rdd1 = sc.textFile(“hdfs://192.168.116.121:9000/tmp_files/test_Cache.txt”)<br>rdd1: org.apache.spark.rdd.RDD[String] = hdfs://192.168.116.121:9000/tmp_files/test_Cache.txt MapPartitionsRDD[1] at textFile at <code>&lt;console&gt;</code>:24</p><p>设置检查点目录：<br>scala&gt; sc.setCheckpointDir(“hdfs://192.168.116.121:9000/sparkchkpt”)</p><p>标识rdd1可以执行检查点操作<br>scala&gt; rdd1.checkpoint</p><p>scala&gt; rdd1.count<br>res2: Long = 921911</p><p>（3）*<em>依赖关系：宽依赖，窄依赖 *</em></p><p><strong>Stage是每一个job处理过程要分为的几个阶段</strong></p><p><img src="/medias/Stage%E5%88%92%E5%88%86.PNG" alt="Stage划分"></p><p>划分任务执行的stage<br>RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）</p><p><strong>窄依赖</strong>指的是每一个父RDD的Partition最多被子RDD的一个Partition使用（一（父）对一（子））<br>总结：<strong>窄依赖</strong>我们形象的比喻为<strong>独生子女</strong></p><p><strong>宽依赖</strong>指的是多个子RDD的Partition会依赖同一个父RDD的Partition（一（父）对多（子））<br>总结：<strong>宽依赖</strong>我们形象的比喻为<strong>超生</strong></p><p>DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此<strong>宽依赖是划分Stage的依据</strong></p><h3 id="七、RDD的高级算子"><a href="#七、RDD的高级算子" class="headerlink" title="七、RDD的高级算子"></a>七、RDD的高级算子</h3><h4 id="1、mapPartitionsWithIndex"><a href="#1、mapPartitionsWithIndex" class="headerlink" title="1、mapPartitionsWithIndex"></a>1、mapPartitionsWithIndex</h4><p>对RDD中的每个分区（带有下标）进行操作，下标用index表示<br>通过这个算子，我们可以获取分区号</p><p>def mapPartitionsWithIndex&lt;a href=”<br>f: %28Int, Iterator%5bT%5d%29 ⇒ Iterator%5bU%5d,<br>preservesPartitioning: Boolean = false”&gt;U(implicit arg0: ClassTag[U]): RDD[U]</p><p>通过将函数应用于此RDD的每个分区来返回新的RDD，同时跟踪原始分区的索引</p><p>preservesPartitioning指输入函数是否保留分区器，除非是一对RDD并且输入函数不修改keys，否则应该是false</p><p>参数：f是个函数参数 f 中第一个参数是Int，代表分区号，第二个Iterator[T]代表分区中的元素</p><p>举例：把分区中的元素，包括分区号，都打印出来</p><p>scala&gt; val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8),3)<br>rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[3] at parallelize at <code>&lt;console&gt;</code>:24</p><p>scala&gt; def fun1(index:Int, iter:Iterator[Int]) : Iterator[String] = {<br>     | iter.toList.map(x =&gt; “[partId: “+ index +” , value = “ + x + “ ]”).iterator<br>     | }<br>fun1: (index: Int, iter: Iterator[Int])Iterator[String]</p><p>scala&gt; rdd1.mapPartitionsWithIndex(fun1).collect<br>res3: Array[String] = Array(<br>[partId: 0 , value = 1 ], [partId: 0 , value = 2 ],<br>[partId: 1 , value = 3 ], [partId: 1 , value = 4 ], [partId: 1 , value = 5 ],<br>[partId: 2 , value = 6 ], [partId: 2 , value = 7 ], [partId: 2 , value = 8 ])</p><h4 id="2、aggregate"><a href="#2、aggregate" class="headerlink" title="2、aggregate"></a>2、aggregate</h4><p>聚合操作。类似于分组<br>（*）先对局部进行聚合操作，再对全局进行聚合操作</p><p><strong>调用聚合操作</strong><br>scala&gt; val rdd2 = sc.parallelize(List(1,2,3,4,5),2)<br>rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at <code>&lt;console&gt;</code>:24</p><p>scala&gt; rdd2.mapPartitionsWithIndex(fun1).collect<br>res4: Array[String] = Array(<br>[partId : 0 , value = 1 ], [partId : 0 , value = 2 ],<br>[partId : 1 , value = 3 ], [partId : 1 , value = 4 ], [partId : 1 , value = 5 ])</p><p>scala&gt; import scala.math._<br>import scala.math._</p><pre><code>scala&gt; rdd2.aggregate(0)(max(_,_),_+_)res6: Int = 7</code></pre><p>说明：aggregate</p><pre><code>(0) 初始值是 0 (max(_,_) 局部操作的函数,   _+_   全局操作的函数)</code></pre><pre><code>scala&gt; rdd2.aggregate(100)(max(_,_),_+_)res8: Int = 300</code></pre><p>分析结果：初始值是100，代表每个分区多了一个100<br>全局操作，也多了一个100<br>100+100+100 = 300</p><p>对RDD中的元素进行求和<br>RDD.map</p><p><strong>聚合操作（效率大于map）</strong></p><pre><code>scala&gt; rdd2.aggregate(0)(_+_,_+_)res9: Int = 15</code></pre><p>相当于MapReduce 的 Combiner</p><pre><code>scala&gt; rdd2.aggregate(10)(_+_,_+_)res10: Int = 45</code></pre><p>（*）对字符串操作</p><p>scala&gt; val rdd2 = sc.parallelize(List(“a”,”b”,”c”,”d”,”e”,”f”),2)<br>rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[7] at parallelize at <code>&lt;console&gt;</code>:27</p><pre><code>scala&gt; rdd2.aggregate(&quot;&quot;)(_+_,_+_)res11: String = abcdefscala&gt; rdd2.aggregate(&quot;*&quot;)(_+_,_+_)res12: String = **def*abc</code></pre><p>结果分析:<br>*abc *def</p><p>*<em>def</em>abc</p><p>（*）复杂的例子<br>1）<br>scala&gt; val rdd3 = sc.parallelize(List(“12”,”23”,”345”,”4567”),2)<br>rdd3: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[8] at parallelize at <code>&lt;console&gt;</code>:27</p><p>scala&gt; def fun1(index:Int, iter:Iterator[String]) : Iterator[String] = {<br> | iter.toList.map(x =&gt; “[partId : “ + index + “ , value = “ + x + “ ]”).iterator<br> | }</p><p>scala&gt; rdd3.mapPartitionsWithIndex(fun1).collect<br>res17: Array[String] = Array(<br>[partId : 0 , value = 12 ], [partId : 0 , value = 23 ],<br>[partId : 1 , value = 345 ], [partId : 1 , value = 4567 ])</p><p>scala&gt; rdd3.aggregate(“”)((x,y)=&gt; math.max(x.length,y.length).toString,(x,y)=&gt;x+y)<br>res13: String = 42<br>执行过程：<br>第一个分区：<br>第一次比较： “” “12” 长度最大值 2 2–&gt;”2”<br>第二次比较： “2” “23” 长度最大值 2 2–&gt;”2”</p><p>第二个分区：<br>第一次比较： “” “345” 长度最大值 3 3–&gt;”3”<br>第二次比较： “3” “4567” 长度最大值 4 4–&gt;”4”<br>结果：24 或者42</p><p>2）<br>scala&gt; rdd3.aggregate(“”)((x,y)=&gt; math.min(x.length,y.length).toString,(x,y)=&gt;x+y)<br>res18: String = 11<br>执行过程：<br>第一个分区：<br>第一次比较： “” “12” 长度最小值 0 0–&gt;”0”<br>第二次比较： “0” “23” 长度最小值 1 1–&gt;”1”</p><p>第二个分区：<br>第一次比较： “” “345” 长度最小值 0 0–&gt;”0”<br>第二次比较： “0” “4567” 长度最小值 1 1–&gt;”1”</p><p>val rdd3 = sc.parallelize(List(“12”,”23”,”345”,””),2)<br>rdd3.aggregate(“”)((x,y)=&gt; math.min(x.length,y.length).toString,(x,y)=&gt;x+y)</p><p>scala&gt; val rdd3 = sc.parallelize(List(“12”,”23”,”345”,””),2)<br>rdd3: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[10] at parallelize at <console>:27</p><p>scala&gt; rdd3.aggregate(“”)((x,y)=&gt; math.min(x.length,y.length).toString,(x,y)=&gt;x+y)<br>res19: String = 10</p><p>scala&gt; rdd3.aggregate(“”)((x,y)=&gt; math.min(x.length,y.length).toString,(x,y)=&gt;x+y)<br>res20: String = 01<br>3）aggregateByKey：类似于aggregate，区别：操作的是 key value 的数据类型</p><p>scala&gt; val pairRDD = sc.parallelize(List((“cat”,2),(“cat”,5),(“mouse”,4),(“cat”,12),(“dog”,12),(“mouse”,2)),2)<br>pairRDD: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[0] at parallelize at <code>&lt;console&gt;</code>:24</p><p>scala&gt; def fun3(index:Int, iter:Iterator[(String,Int)]) : Iterator[String] = {<br>     | iter.toList.map(x=&gt;”partId : “ + index + “ , value = “ + x + “ ]”).iterator<br>     | }<br>fun3: (index: Int, iter: Iterator[(String, Int)])Iterator[String]</p><p>scala&gt; pairRDD.mapPartitionsWithIndex(fun3).collect<br>res0: Array[String] = Array(<br>partId : 0 , value = (cat,2) ], partId : 0 , value = (cat,5) ], partId : 0 , value = (mouse,4) ],<br>partId : 1 , value = (cat,12) ], partId : 1 , value = (dog,12) ], partId : 1 , value = (mouse,2) ])</p><p>1.将每个动物园（分区）中，动物数最多的动物，进行求和<br>动物园0<br>[partId : 0 , value = (cat,2) ], [partId : 0 , value = (cat,5) ], [partId : 0 , value = (mouse,4) ],</p><p>动物园1<br>[partId : 1 , value = (cat,12) ], [partId : 1 , value = (dog,12) ], [partId : 1 , value = (mouse,2) ])</p><pre><code>pairRDD.aggregateByKey(0)(math.max(_,_),_+_)scala&gt; pairRDD.aggregateByKey(0)(math.max(_,_),_+_).collectres1: Array[(String, Int)] = Array((dog,12), (cat,17), (mouse,6))   </code></pre><p>2.将所有动物求和</p><pre><code>pairRDD.aggregateByKey(0)(_+_,_+_).collectscala&gt; pairRDD.reduceByKey(_+_).collectres27: Array[(String, Int)] = Array((dog,12), (cat,19), (mouse,6))</code></pre><p>aggregateByKey效率更高</p><p>4）<strong>coalesce与repartition</strong><br>与分区有关<br>都是对RDD进行重分区</p><p>区别：<br>coalesce 默认不会进行Shuffle 默认 false 如需修改分区，需置为true</p><p>repartition 会进行Shuffle</p><p>scala&gt; val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9),2)<br>rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at <code>&lt;console&gt;</code></p><p>scala&gt; val rdd2 = rdd1.repartition(3)<br>rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[9] at repartition at <code>&lt;console&gt;</code>:26</p><p>scala&gt; rdd2.partitions.length<br>res4: Int = 3</p><p>scala&gt; val rdd3 = rdd1.coalesce(3,true)<br>rdd3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[13] at coalesce at <code>&lt;console&gt;</code>:26</p><p>scala&gt; rdd3.partitions.length<br>res5: Int = 3</p><p>scala&gt; val rdd4 = rdd1.coalesce(4)<br>rdd4: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[14] at coalesce at <code>&lt;console&gt;</code>:26</p><p>scala&gt; rdd4.partitions.length<br>res6: Int = 2</p><p>5）<strong>其他高级算子</strong><br>比较好的高级算子的博客（推荐）<br><a href="http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html" target="_blank" rel="noopener">http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html</a></p><h3 id="八、编程案例"><a href="#八、编程案例" class="headerlink" title="八、编程案例"></a>八、编程案例</h3><h4 id="1、分析日志"><a href="#1、分析日志" class="headerlink" title="1、分析日志"></a>1、分析日志</h4><p>需求：找到访问量最高的两个网页<br>（<em>）第一步：对网页的访问量求和<br>（</em>）第二步：排序，降序 </p><p><strong>日志数据</strong><br>192.168.88.1 - - [30/Jul/2017:12:53:43 +0800] “GET /MyDemoWeb/ HTTP/1.1” 200 259<br>192.168.88.1 - - [30/Jul/2017:12:53:43 +0800] “GET /MyDemoWeb/head.jsp HTTP/1.1” 200 713<br>192.168.88.1 - - [30/Jul/2017:12:53:43 +0800] “GET /MyDemoWeb/body.jsp HTTP/1.1” 200 240<br>192.168.88.1 - - [30/Jul/2017:12:54:37 +0800] “GET /MyDemoWeb/oracle.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:38 +0800] “GET /MyDemoWeb/hadoop.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:38 +0800] “GET /MyDemoWeb/java.jsp HTTP/1.1” 200 240<br>192.168.88.1 - - [30/Jul/2017:12:54:40 +0800] “GET /MyDemoWeb/oracle.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:40 +0800] “GET /MyDemoWeb/hadoop.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:41 +0800] “GET /MyDemoWeb/mysql.jsp HTTP/1.1” 200 241<br>192.168.88.1 - - [30/Jul/2017:12:54:41 +0800] “GET /MyDemoWeb/hadoop.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:42 +0800] “GET /MyDemoWeb/web.jsp HTTP/1.1” 200 239<br>192.168.88.1 - - [30/Jul/2017:12:54:42 +0800] “GET /MyDemoWeb/oracle.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:52 +0800] “GET /MyDemoWeb/oracle.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:52 +0800] “GET /MyDemoWeb/hadoop.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:53 +0800] “GET /MyDemoWeb/oracle.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:54 +0800] “GET /MyDemoWeb/mysql.jsp HTTP/1.1” 200 241<br>192.168.88.1 - - [30/Jul/2017:12:54:54 +0800] “GET /MyDemoWeb/hadoop.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:54 +0800] “GET /MyDemoWeb/hadoop.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:56 +0800] “GET /MyDemoWeb/web.jsp HTTP/1.1” 200 239<br>192.168.88.1 - - [30/Jul/2017:12:54:56 +0800] “GET /MyDemoWeb/java.jsp HTTP/1.1” 200 240<br>192.168.88.1 - - [30/Jul/2017:12:54:57 +0800] “GET /MyDemoWeb/oracle.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:57 +0800] “GET /MyDemoWeb/java.jsp HTTP/1.1” 200 240<br>192.168.88.1 - - [30/Jul/2017:12:54:58 +0800] “GET /MyDemoWeb/oracle.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:58 +0800] “GET /MyDemoWeb/hadoop.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:59 +0800] “GET /MyDemoWeb/oracle.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:54:59 +0800] “GET /MyDemoWeb/hadoop.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:55:43 +0800] “GET /MyDemoWeb/mysql.jsp HTTP/1.1” 200 241<br>192.168.88.1 - - [30/Jul/2017:12:55:43 +0800] “GET /MyDemoWeb/oracle.jsp HTTP/1.1” 200 242<br>192.168.88.1 - - [30/Jul/2017:12:55:43 +0800] “GET /MyDemoWeb/web.jsp HTTP/1.1” 200 239<br>192.168.88.1 - - [30/Jul/2017:12:55:43 +0800] “GET /MyDemoWeb/hadoop.jsp HTTP/1.1” 200 242</p><pre><code>package day2import org.apache.spark.SparkConfimport org.apache.spark.SparkContextobject MyTomcatLogCount {  def main(args: Array[String]): Unit = {    val conf  = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;MyTomcatLogCount&quot;)    val sc = new SparkContext(conf)    /**     * 读入日志解析     *      * 192.168.88.1 - - [30/Jul/2017:12:54:52 +0800] &quot;GET /MyDemoWeb/oracle.jsp HTTP/1.1&quot; 200 242     *      */    val rdd1 = sc.textFile(&quot;H:\\other\\localhost_access_log.txt&quot;)      .map(       line =&gt; {         //解析字符串， 得到jsp的名字         //1.解析两个引号之间的字符串         val index1 = line.indexOf(&quot;\&quot;&quot;)         val index2 = line.lastIndexOf(&quot;\&quot;&quot;)         val line1 = line.substring(index1+1,index2)//GET /MyDemoWeb/oracle.jsp HTTP/1.1         //得到两个空格的位置         val index3 = line1.indexOf(&quot; &quot;)         val index4 = line1.lastIndexOf(&quot; &quot;)         val line2 = line1.substring(index3+1,index4)///MyDemoWeb/oracle.jsp         //得到jsp的名字         val jspName = line2.substring(line2.lastIndexOf(&quot;/&quot;))//oracle.jsp         (jspName,1)       }      )    //统计出每个jsp的次数               val rdd2 = rdd1.reduceByKey(_+_)                               //使用value排序    val rdd3 = rdd2.sortBy(_._2, false)    rdd3.take(2).foreach(println)    sc.stop()  }}</code></pre><p>结果：<br>(/hadoop.jsp,9)<br>(/oracle.jsp,9)</p><h4 id="2、创建自定义分区"><a href="#2、创建自定义分区" class="headerlink" title="2、创建自定义分区"></a>2、创建自定义分区</h4><p>根据jsp文件的名字，将各自的访问日志放入到不同的分区文件中</p><pre><code>package day2import org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.Partitionerimport scala.collection.mutable.HashMapobject MyTomcatLogPartitioner {  def main(args: Array[String]): Unit = {    System.setProperty(&quot;hadoop.home.dir&quot;, &quot;E:\\hadoop-2.7.3&quot;)    val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;MyTomcatLogPartitioner&quot;)    val sc = new SparkContext(conf)     /**     * 读入日志解析     *      * 192.168.88.1 - - [30/Jul/2017:12:54:52 +0800] &quot;GET /MyDemoWeb/oracle.jsp HTTP/1.1&quot; 200 242     *      */    val rdd1 = sc.textFile(&quot;H:\\other\\localhost_access_log.txt&quot;)      .map(       line =&gt; {         //解析字符串， 得到jsp的名字         //1.解析两个引号之间的字符串         val index1 = line.indexOf(&quot;\&quot;&quot;)         val index2 = line.lastIndexOf(&quot;\&quot;&quot;)         val line1 = line.substring(index1+1,index2)//GET /MyDemoWeb/oracle.jsp HTTP/1.1         //得到两个空格的位置         val index3 = line1.indexOf(&quot; &quot;)         val index4 = line1.lastIndexOf(&quot; &quot;)         val line2 = line1.substring(index3+1,index4)///MyDemoWeb/oracle.jsp         //得到jsp的名字         val jspName = line2.substring(line2.lastIndexOf(&quot;/&quot;))//oracle.jsp         (jspName,line)       }      )                              //定义分区规则    //得到不重复的jsp的名字    val rdd2 = rdd1.map(_._1).distinct().collect()    //创建分区规则    val myPartitioner = new MyWebPartitioner(rdd2)    val rdd3 = rdd1.partitionBy(myPartitioner)    //将rdd3 输出    rdd3.saveAsTextFile(&quot;H:\\other\\test_partition&quot;)      sc.stop()  }}class MyWebPartitioner(jspList : Array[String]) extends Partitioner{  //定义一个集合来保存分区条件， String 代表jsp的名字， Int 代表序号  val partitionMap = new HashMap[String,Int]()  var partID = 0 //初始分区号  for (jsp &lt;- jspList){    partitionMap.put(jsp, partID)    partID += 1  }  //定义有多少个分区  def numPartitions : Int = partitionMap.size  //根据jsp，返回对应的分区  def getPartition(key : Any) : Int = partitionMap.getOrElse(key.toString(),0)}</code></pre><h4 id="3、使用JDBCRDD-操作数据库"><a href="#3、使用JDBCRDD-操作数据库" class="headerlink" title="3、使用JDBCRDD 操作数据库"></a>3、使用JDBCRDD 操作数据库</h4><p>将RDD的数据保存到mysql数据库中</p><pre><code>package day2import java.sql.DriverManagerimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.rdd.JdbcRDD/** * 需求找出工资小于等于2000大于900的员工 * select * from emp where sal &gt; ? and sal &lt;= ? */object MyMysqlDemo {  val connection = () =&gt; {    Class.forName(&quot;com.mysql.cj.jdbc.Driver&quot;).newInstance()    DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;,&quot;root&quot;,&quot;123456&quot;)  }  def main(args: Array[String]): Unit = {    System.setProperty(&quot;hadoop.home.dir&quot;, &quot;E:\\hadoop-2.7.3&quot;)    val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;MyMysqlDemo&quot;)    val sc = new SparkContext(conf)    val mysqlRDD = new JdbcRDD(sc, connection, &quot;select * from emp where sal &gt; ? and sal &lt;= ?&quot;, 900, 2000, 2, r =&gt; {      val ename = r.getString(2)      val sal = r.getInt(4)      (ename, sal)    })    val result = mysqlRDD.collect()    println(result.toBuffer)    sc.stop()      }}</code></pre><p>mysql的company的emp数据<br>1 Tom 10 2400<br>2 Alis 11 1900<br>3 Kei 12 1500<br>4 Mi 11 900<br>结果<br>ArrayBuffer((Alis,1900), (Kei,1500))</p><p>JdbcRDD参数说明</p><table><thead><tr><th align="center">参数名称</th><th align="center">类型</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">sc</td><td align="center">org.apache.spark.SparkContext</td><td align="center">Spark Context对象</td></tr><tr><td align="center">getConnection</td><td align="center">scala.Function0[java.sql.Connection]</td><td align="center">得到一个数据库Connection</td></tr><tr><td align="center">sql</td><td align="center">scala.Predef.String</td><td align="center">执行的SQL语句</td></tr><tr><td align="center">lowerBound</td><td align="center">scala.Long</td><td align="center">下边界值，即：SQL的第一个参数</td></tr><tr><td align="center">upperBound</td><td align="center">scala.Long</td><td align="center">上边界值，即：SQL的第二个参数</td></tr><tr><td align="center">numPartitions</td><td align="center">scala.Int</td><td align="center">分区的个数，即：启动多少个Executor</td></tr><tr><td align="center">mapRow</td><td align="center">scala.Function1[java.sql.ResultSet, T]</td><td align="center">得到的结果集</td></tr></tbody></table><p>JdbcRDD的缺点：从上面的参数说明可以看出，JdbcRDD有以下两个缺点：<br>（1）执行的SQL必须有两个参数，并类型都是Long<br>（2）得到的结果是ResultSet，即：只支持select操作</p><h4 id="4、操作数据库：把结果存放到数据库中"><a href="#4、操作数据库：把结果存放到数据库中" class="headerlink" title="4、操作数据库：把结果存放到数据库中"></a>4、操作数据库：把结果存放到数据库中</h4><pre><code>package day3import org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport java.sql.Connectionimport java.sql.DriverManagerimport java.sql.PreparedStatement/** * 把Spark结果存放到mysql数据库中 */object MyTomcatLogCountToMysql {  def main(args: Array[String]): Unit = {    //创建SparkContext    val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;MyTomcatLogCountToMysql&quot;)    val sc = new SparkContext(conf)     /**     * 读入日志解析     *      * 192.168.88.1 - - [30/Jul/2017:12:54:52 +0800] &quot;GET /MyDemoWeb/oracle.jsp HTTP/1.1&quot; 200 242     *      */    val rdd1 = sc.textFile(&quot;H:\\other\\localhost_access_log.txt&quot;)      .map(       line =&gt; {         //解析字符串， 得到jsp的名字         //1.解析两个引号之间的字符串         val index1 = line.indexOf(&quot;\&quot;&quot;)         val index2 = line.lastIndexOf(&quot;\&quot;&quot;)         val line1 = line.substring(index1+1,index2)//GET /MyDemoWeb/oracle.jsp HTTP/1.1         //得到两个空格的位置         val index3 = line1.indexOf(&quot; &quot;)         val index4 = line1.lastIndexOf(&quot; &quot;)         val line2 = line1.substring(index3+1,index4)///MyDemoWeb/oracle.jsp         //得到jsp的名字         val jspName = line2.substring(line2.lastIndexOf(&quot;/&quot;))//oracle.jsp         (jspName,1)     }    )      //存入数据库//    var conn : Connection = null//    var pst : PreparedStatement = null//    //    try{//        /**//         * create table mydata(jspname varchar(50), countNumber Int);//         * //         * foreach 没有返回值 ， 在本需求中，只需要写数据库，不需要返回新的RDD，所以用foreach即可//         * //         * 运行Task not serializable//         *///        conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;,&quot;root&quot;,&quot;123456&quot;) //        pst = conn.prepareStatement(&quot;insert into mydata values (?,?)&quot;)//    //        rdd1.foreach(f =&gt; {//          pst.setString(1, f._1)//          pst.setInt(2, f._2)//          //          pst.executeUpdate()//        })//    }catch{//      case t : Throwable =&gt; t.printStackTrace()//    }finally{//      if(pst != null) pst.close()//      if(conn != null)  conn.close()//    }//    sc.stop()    //第一种修改方式    //存入数据库//    var conn : Connection = null//    var pst : PreparedStatement = null//    //    try{//      rdd1.foreach(f =&gt; {//        conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;,&quot;root&quot;,&quot;123456&quot;) //        pst = conn.prepareStatement(&quot;insert into mydata values (?,?)&quot;)//  //        pst.setString(1, f._1)//        pst.setInt(2, f._2)//        //        pst.executeUpdate()//      })//    }catch{//      case t : Throwable =&gt; t.printStackTrace()//    }finally{//      if(pst != null) pst.close()//      if(conn != null)  conn.close()//    }//    sc.stop()      /*     * 第一种修改方式功能上可以实现，但每条数据都会创建连接，对数据库造成很大压力     *      * 针对分区来操作：一个分区建立一个连接即可     */     rdd1.foreachPartition(saveToMysql)      sc.stop()  }  def saveToMysql(it : Iterator[(String, Int)]) = {      var conn : Connection = null      var pst : PreparedStatement = null      try{        conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/company?serverTimezone=UTC&amp;characterEncoding=utf-8&quot;,&quot;root&quot;,&quot;123456&quot;)         pst = conn.prepareStatement(&quot;insert into mydata values (?,?)&quot;)        it.foreach(f =&gt; {          pst.setString(1, f._1)          pst.setInt(2, f._2)          pst.executeUpdate()        })      }catch{        case t : Throwable =&gt; t.printStackTrace()      }finally{        if(pst != null) pst.close()        if(conn != null)  conn.close()      }  }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Akka练习</title>
      <link href="/2019/03/27/akka-lian-xi/"/>
      <url>/2019/03/27/akka-lian-xi/</url>
      
        <content type="html"><![CDATA[<h4 id="Actor并发模型"><a href="#Actor并发模型" class="headerlink" title="Actor并发模型"></a>Actor并发模型</h4><p><strong>Java中的并发开发</strong><br>Java的并发编程是基于 共享数据 和 加锁 的一种机制。锁的是共享数据<br>synchronized</p><p><strong>Scala中的并发开发</strong><br>不共享数据。依赖于 消息传递 的一种并发编程模式</p><p>如果 Actor A 和 Actor B要相互沟通<br>1、A要给B传递一个消息，B有一个收件箱，B轮询自己的收件箱<br>2、如果B看到A的消息，解析A的消息并执行相应操作<br>3、有可能会回复 A 消息</p><p><strong>Actor示例</strong></p><pre><code>package day6import akka.actor.{Actor, ActorSystem, Props}/**  * Actor示例  */class HelloActor extends Actor{  override def receive: Receive = {    case &quot;Hello&quot; =&gt; println(&quot;Hello Receive&quot;)    case _ =&gt; println(&quot;aaa&quot;)  }}object Demo1 extends App {  //新建一个ActorSystem  val system = ActorSystem(&quot;HelloSystem&quot;)  //构造函数  val helloActor = system.actorOf(Props[HelloActor],&quot;helloactor&quot;)  //发消息  helloActor ! &quot;Hello&quot;  helloActor ! &quot;Hello2234&quot;}</code></pre><p><strong>建立两个Actor 相互传递消息</strong></p><pre><code>package day6import akka.actor.{Actor, ActorRef, ActorSystem, Props}/**  * 建立两个Actor 相互传递消息  *  * 定义消息：样本类、区分 消息的不同  *///消息的定义case object PingMessagecase object PongMessagecase object StartMessagecase object StopMessageclass Ping(pong : ActorRef) extends Actor{  var count = 0  def incrementAndPing {    count += 1;    println(&quot;Ping&quot;)  }  override def receive: Receive = {    case StartMessage =&gt;      incrementAndPing      pong ! PingMessage    case PongMessage =&gt;      if(count &gt; 9){        sender() ! StopMessage      }else {        incrementAndPing        pong ! PingMessage      }  }}class Pong extends Actor{  override def receive: Receive = {    case PingMessage =&gt;    println(&quot;pong&quot;)    //给ping回复消息    sender ! PongMessage    case StopMessage =&gt;      println(&quot;pong Stop&quot;)      context.stop(self)      //context.system.finalize()  }}object Demo2 extends App{  val system = ActorSystem(&quot;PingPongSystem&quot;)  val pong =  system.actorOf(Props[Pong],name=&quot;pong&quot;)  val ping = system.actorOf(Props(new Ping(pong)),name=&quot;ping&quot;)  ping ! StartMessage}</code></pre><p>AKKA 负责来回传递消息</p><p><strong>Scala项目</strong></p><h4 id="实现一个主从管理系统"><a href="#实现一个主从管理系统" class="headerlink" title="实现一个主从管理系统"></a>实现一个主从管理系统</h4><p><img src="/medias/NewAkkaSystem.PNG" alt="NewAkkaSystem"></p><p>Worker类<br>Master类</p><p>ActorMessage类<br>WorkerInfo类</p><p>ActorMessage 类：定义消息 5种消息</p><p><strong>WorkerInfo.scala</strong></p><pre><code>package akka/**  * 保存worker的基本信息  */class WorkerInfo(val id : String, val workerHost : String, val memory : String, val cores : String) {  //保存心跳信息  var lastHeartBeat : Long = System.currentTimeMillis()  override def toString : String = s&quot;WorkerInfo($id, $workerHost, $memory, $cores)&quot;}</code></pre><p><strong>ActorMessage.scala</strong></p><pre><code>package akka/**  * 样本类，保存所有信息  *///worker ----&gt; master注册节点case class RegisterWorker(val id : String, val workerHost : String, val memory : String, val cores : String)//worker ----&gt; master 发送心跳信号case class HeartBeat(val workerId : String)//master ----&gt; worker 注册完成 ACKcase class RegisteredWorker(val workerHost : String)//master ----&gt; master 检查超时节点case class CheckTimeOutWorker()//worker ----&gt; worker 提醒自己发送心跳信号case class SendHeartBeat()</code></pre><p><strong>Worker.scala</strong></p><pre><code>package akkaimport java.util.UUIDimport akka.actor._import com.typesafe.config.ConfigFactoryimport scala.concurrent.duration._import scala.concurrent.ExecutionContext.Implicits.globalclass Worker extends Actor {  //Worker端持有Master端的引用（代理对象）  //因为worker会给Master发送信息，所以才要这个对象  var master : ActorSelection = null  ////生成一个UUID，作为Worker的标识  val id = UUID.randomUUID().toString  //构造方法执行完执行一次  override def preStart(): Unit = {    //Worker向MasterActorSystem发送建立连接请求    master = context.system.actorSelection(&quot;akka.tcp://MasterActorSystem@localhost:8881/user/Master&quot;)    //Worker向Master发送注册消息    master ! RegisterWorker(id, &quot;localhost&quot;, &quot;10240&quot;, &quot;8&quot;)  }  //该方法会被反复执行，用于接收消息，通过case class模式匹配接收消息  override def receive : Receive = {    //Master向Worker的反馈信息    case RegisteredWorker(masterURL) =&gt; {      //启动定时任务，向Master发送心跳      context.system.scheduler.schedule(0 millis, 5000 millis, self, SendHeartBeat)    }    case SendHeartBeat =&gt; {      println(&quot;worker send hearbeat&quot;)      master ! HeartBeat(id)    }  }}object Worker extends App{  val clientPort = 8803  //创建ActorSystem的必要参数  val configStr =    s&quot;&quot;&quot;       |akka.actor.provider = &quot;akka.remote.RemoteActorRefProvider&quot;       |akka.remote.netty.tcp.port = $clientPort       &quot;&quot;&quot;.stripMargin  val conf = ConfigFactory.parseString(configStr)  //创建ActorSystem  val actorSystem = ActorSystem(&quot;MasterActorSystem&quot;,conf)  //启动Actor，Master会被实例化，生命周期方法会被调用  actorSystem.actorOf(Props[Worker],&quot;Worker&quot;)}</code></pre><p><strong>Master.scala</strong></p><pre><code>package akkaimport akka.actor.{Actor, ActorSystem, Props}import com.typesafe.config.ConfigFactoryimport scala.collection.mutableimport scala.concurrent.duration._import scala.concurrent.ExecutionContext.Implicits.globalclass Master extends Actor {  //保存WorkerId 和 Worker信息的 map  val idToWorker = new mutable.HashMap[String, WorkerInfo]  //保存所有worker信息的Set  val workers = new mutable.HashSet[WorkerInfo]  //Worker超时时间  val WORKER_TIMEOUT = 10 * 1000  //构造方法执行完执行一次  override def preStart(): Unit = {    //启动定时器，定时执行    //设置在5毫秒之后，间隔10秒，给自己发一个CheckOfTimeOutWorker    context.system.scheduler.schedule(0 millis, 5000 millis, self, CheckTimeOutWorker)  }  //该方法会被反复执行，用于接收消息，通过case class模式匹配接收消息  override def receive: Receive = {    //Worker向Master发送的注册消息    case RegisterWorker(id, workerHost, memory, cores) =&gt; {        if(!idToWorker.contains(id)){          val worker = new WorkerInfo(id,workerHost,memory,cores)          workers.add(worker)          idToWorker(id) = worker          println(&quot;nrew register worker: &quot; + worker)          sender ! RegisteredWorker(worker.id)        }      }    //Worker向Master发送的心跳消息    case HeartBeat(workerId) =&gt; {      val workerInfo = idToWorker(workerId)      println(&quot;get heartbeat message from: &quot;+ workerInfo)      workerInfo.lastHeartBeat = System.currentTimeMillis()    }    //Master自己向自己发送的定期检查超时Worker的消息    case CheckTimeOutWorker =&gt; {      //检查超时的worker      val currentTime = System.currentTimeMillis()      val toRemove = workers.filter( w =&gt; currentTime - w.lastHeartBeat &gt; WORKER_TIMEOUT).toArray      for(worker &lt;- toRemove){        workers -= worker        idToWorker.remove(worker.id)      }      println(&quot;Worker size: &quot; + workers.size)    }  }}object Master extends App {  val host = &quot;localhost&quot;  val port = 8881  //创建ActorSystem的必要参数  val configStr =    s&quot;&quot;&quot;       |akka.actor.provider = &quot;akka.remote.RemoteActorRefProvider&quot;       |akka.remote.netty.tcp.hostname = &quot;$host&quot;       |akka.remote.netty.tcp.port = &quot;$port&quot;     &quot;&quot;&quot;.stripMargin  val conf = ConfigFactory.parseString(configStr)  //创建ActorSystem  val actorSystem = ActorSystem(&quot;MasterActorSystem&quot;,conf)  //启动Actor Master会被实例化 生命周期的方法会被调用  actorSystem.actorOf(Props[Master],&quot;Master&quot;)}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Akka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala编程</title>
      <link href="/2019/03/25/scala-bian-cheng/"/>
      <url>/2019/03/25/scala-bian-cheng/</url>
      
        <content type="html"><![CDATA[<h3 id="一、Scala函数式编程"><a href="#一、Scala函数式编程" class="headerlink" title="一、Scala函数式编程"></a>一、Scala函数式编程</h3><p>多范式：面向对象，函数式编程（程序实现起来简单）</p><p>举例：WordCount<br>sc 是 SparkContext , 非常重要</p><p>一行：</p><pre><code>var result = sc.textFile(&quot;hdfs://xxxx/xxx/data.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).collect</code></pre><h4 id="1、复习函数"><a href="#1、复习函数" class="headerlink" title="1、复习函数"></a>1、复习函数</h4><p>关键字 def</p><h4 id="2、匿名函数"><a href="#2、匿名函数" class="headerlink" title="2、匿名函数"></a>2、匿名函数</h4><p>没有名字的函数 </p><p><strong>举例</strong></p><pre><code>scala&gt; var myarray = Array(1,2,3)myarray: Array[Int] = Array(1, 2, 3)scala&gt; def fun1(x:Int):Int = x*3fun1: (x: Int)Intscala&gt; (x:Int) =&gt; x*3res0: Int =&gt; Int = &lt;function1&gt;</code></pre><p>问题：怎么去调用？高阶函数</p><pre><code>scala&gt; fun1(3)res1: Int = 9scala&gt; myarray.foreach(println)123//调用匿名函数scala&gt; myarray.map((x:Int) =&gt; x*3)res3: Array[Int] = Array(3, 6, 9)</code></pre><p><code>(_,1)  (_+_)</code> 都是匿名函数</p><h4 id="3、高阶函数（带有函数参数的函数）"><a href="#3、高阶函数（带有函数参数的函数）" class="headerlink" title="3、高阶函数（带有函数参数的函数）"></a>3、高阶函数（带有函数参数的函数）</h4><p>把一个函数作为另外一个函数的参数值</p><p>定义一个高阶函数：<br>对10做某种运算</p><pre><code>scala&gt; def someAction(f:(Double)=&gt;(Double)) = f(10)someAction: (f: Double =&gt; Double)Double</code></pre><p><strong>解释</strong></p><p>(Double)=&gt;(Double) 代表了f 的类型：入参是double，返回值也是double的函数</p><pre><code>import scala.math._scala&gt; someAction(sqrt)res5: Double = 3.1622776601683795scala&gt; someAction(sin)res6: Double = -0.5440211108893698scala&gt; someAction(cos)res7: Double = -0.8390715290764524scala&gt; someAction(println)&lt;console&gt;:16: error: type mismatch;found   : () =&gt; Unitrequired: Double =&gt; Double   someAction(println)                ^def someAction(f:(Double)=&gt;(Double)) = f(10)someAction(sqrt) = sqrt(10)</code></pre><h4 id="4、高阶函数的实例"><a href="#4、高阶函数的实例" class="headerlink" title="4、高阶函数的实例"></a>4、高阶函数的实例</h4><p>scala中提供了常用的高阶函数</p><p>（1）map : 相当于一个循环，对某个集合中的每个元素都进行操作（接收一个函数），返回一个新的集合</p><p>scala&gt; var numbers = List(1,2,3,4,5,6,7,8,9,10)<br>numbers: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</p><p>scala&gt; numbers.map((i:Int)=&gt;i*2)<br>res2: List[Int] = List(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)</p><p>scala&gt; numers.map(_*2)<br>res3: List[Int] = List(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)</p><p>说明：<br>(i:Int)=&gt;i<em>2 与 _</em>2 等价的</p><p>(i:Int,j:Int)=&gt;i+j   <code>_+_</code><br>scala&gt; numbers<br>res4: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)<br>不改变numbers本身的值</p><p>（2）foreach：相当于一个循环，对某个集合中的每个元素都进行操作（接收一个函数），不返回结果<br>scala&gt; numbers.foreach(println)<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10</p><p>numbers.foreach(_*2)<br>没有返回值</p><p>（3） filter ：过滤，选择满足的数据<br>举例：查询能被2整除的数字</p><p>scala&gt; numbers.filter((i:Int)=&gt;i%2==0)<br>res5: List[Int] = List(2, 4, 6, 8, 10)</p><p>filter函数，参数要求：要求一个返回 bool 值的函数，筛选出所有为true的数据</p><p>（4）zip操作：合并两个集合<br>scala&gt; List(1,2,3).zip(List(4,5,6))<br>res6: List[(Int, Int)] = List((1,4), (2,5), (3,6))</p><p>//少的话匹配不上就不合并<br>scala&gt; List(1,2,3).zip(List(4,5))<br>res7: List[(Int, Int)] = List((1,4), (2,5))</p><p>scala&gt; List(1).zip(List(4,5))<br>res8: List[(Int, Int)] = List((1,4))</p><p>（5）partition ： 根据断言（就是某个条件，匿名函数）的结果，来进行分区<br>举例：<br>能被2整除的分成一个区，不能被2整除的分成另外一个区<br>scala&gt; numbers.partition((i:Int)=&gt;i%2==0)<br>res9: (List[Int], List[Int]) = (List(2, 4, 6, 8, 10),List(1, 3, 5, 7, 9))</p><p>（6）find ： 查找第一个满足条件的元素<br>scala&gt; numbers.find(_%3==0)<br>res10: Option[Int] = Some(3)</p><p>_%3==0 (i:Int)=&gt;i%3==0 一样</p><p>（7）flatten：把嵌套的结果展开<br>scala&gt; List(List(2,4,6,8,10),List(1,3,5,7,9)).flatten<br>res11: List[Int] = List(2, 4, 6, 8, 10, 1, 3, 5, 7, 9)</p><p>（8）flatmap : 相当于一个 map + flatten<br>scala&gt; var myList = List(List(2,4,6,8,10),List(1,3,5,7,9))<br>myList: List[List[Int]] = List(List(2, 4, 6, 8, 10), List(1, 3, 5, 7, 9))</p><p>scala&gt; myList.flatMap(x=&gt;x.map(_*2))<br>res22: List[Int] = List(4, 8, 12, 16, 20, 2, 6, 10, 14, 18)</p><p>myList.flatMap(x=&gt;x.map(_*2))</p><p>执行过程：<br>1、将 List(2, 4, 6, 8, 10), List(1, 3, 5, 7, 9) 调用 map(_*2) 方法。x 代表一个List<br>2、flatten</p><pre><code>package day4/**  * 对比flatmap与map  */object Demo2 {  /**    * flatmap执行分析    * 1 List(1*2)   List(2)    * 2 List(2*2)   List(4)    * 3 List(&#39;a&#39;,&#39;b&#39;)    *    * List(List(2),List(4),List(&#39;a&#39;,&#39;b&#39;)).flatten    * List(2,4,&#39;a&#39;,&#39;b&#39;)    */  def flatMap(){    val li = List(1,2,3)    val res = li.flatMap(x=&gt;      x match{      case 3=&gt;List(&#39;a&#39;,&#39;b&#39;)      case _=&gt;List(x*2)    })    println(res)  }  /**    * map过程解析    *     * 1 2  ----&gt;List(2,2,3)    * 2 4  ----&gt;List(2,4,5)    * 3 List(&#39;a&#39;,&#39;b&#39;)----&gt;List(2,4,List(&#39;a&#39;,&#39;b&#39;))    */  def map(){    val li = List(1,2,3)    val res = li.map(x=&gt;      x match{      case 3=&gt;List(&#39;a&#39;,&#39;b&#39;)      case _=&gt;x*2    })    println(res)  }  def main(args: Array[String]): Unit = {    flatMap()    map()  }}</code></pre><h4 id="5、概念：闭包、柯里化"><a href="#5、概念：闭包、柯里化" class="headerlink" title="5、概念：闭包、柯里化"></a>5、概念：闭包、柯里化</h4><p>（1）闭包：就是函数的嵌套<br>在一个函数的里面，包含了另一个函数的定义<br>可以在内函数中访问外函数的变量</p><p>举例：</p><pre><code>def mulBy(factor:Double) = (x:Double)=&gt;x*factor        外                           内</code></pre><p>乘以三：</p><pre><code>scala&gt; def mulBy(factor:Double) = (x:Double)=&gt;x*factormulBy: (factor: Double)Double =&gt; Doublescala&gt; var triple = mulBy(3)triple: Double =&gt; Double = &lt;function1&gt;相当于 triple(x:Double) = x*3scala&gt; triple(10)res1: Double = 30.0scala&gt; triple(20)res2: Double = 60.0scala&gt; var half = mulBy(0.5)half: Double =&gt; Double = &lt;function1&gt;scala&gt; half(10)res3: Double = 5.0</code></pre><p>引入柯里化：<br>scala&gt; mulBy(3)(10)<br>res4: Double = 30.0</p><p>（2）柯里化<br>概念：柯里化函数：是把具有多个参数的函数，转化为一个函数链，每个节点上都是单一函数</p><p>def add(x:Int,y:Int) = x+y</p><p>def add(x:Int)(y:Int) = x+y</p><p>转化步骤：</p><p>原始：def add(x:Int,y:Int) = x+y</p><p>闭包：def add(x:Int) = (y:Int) =&gt; x+y</p><p>简写：def add(x:Int)(y:Int) = x+y</p><p>scala&gt; def add(x:Int)(y:Int) = x+y<br>add: (x: Int)(y: Int)Int</p><p>scala&gt; add(1)(2)<br>res5: Int = 3</p><h3 id="二、Scala集合"><a href="#二、Scala集合" class="headerlink" title="二、Scala集合"></a>二、Scala集合</h3><h4 id="1、可变集合和不可变集合（Map）"><a href="#1、可变集合和不可变集合（Map）" class="headerlink" title="1、可变集合和不可变集合（Map）"></a>1、可变集合和不可变集合（Map）</h4><p>immutable mutable<br>举例：<br>scala&gt; def math = scala.collection.immutable.Map(“Tom”-&gt;80,”Lily”-&gt;20)<br>math: scala.collection.immutable.Map[String,Int]</p><p>scala&gt; def math = scala.collection.mutable.Map(“Tom”-&gt;80,”Lily”-&gt;20,”Mike”-&gt;95)<br>math: scala.collection.mutable.Map[String,Int]</p><p>集合的操作：<br>获取集合中的值<br>scala&gt; math.get(“Tom”)<br>res1: Option[Int] = Some(80)</p><p>scala&gt; math(“Tom”)<br>res2: Int = 80</p><p>scala&gt; math(“Tom123”)<br>java.util.NoSuchElementException: key not found: Tom123<br>at scala.collection.MapLike$class.default(MapLike.scala:228)<br>at scala.collection.AbstractMap.default(Map.scala:59)<br>at scala.collection.mutable.HashMap.apply(HashMap.scala:65)<br>… 32 elided</p><p>scala&gt; math.get(“Tom123”)<br>res3: Option[Int] = None</p><p>scala&gt; math.contains(“Tom123”)<br>res4: Boolean = false</p><p>scala&gt; math.getOrElse(“Tom123”,-1)<br>res5: Int = -1</p><p>更新集合中的值：注意：必须是可变集合<br>scala&gt; math<br>res6: scala.collection.mutable.Map[String,Int] = Map(Mike -&gt; 95, Tom -&gt; 80, Lily -&gt; 20)</p><p>scala&gt; math(“Tom”)=0<br>scala&gt; math<br>res7: scala.collection.mutable.Map[String,Int] = Map(Mike -&gt; 95, Tom -&gt; 80, Lily -&gt; 20)</p><p>造成上述现象的原因，没有import包，如果import以后，问题解决：<br>scala&gt; import scala.collection.mutable._<br>import scala.collection.mutable._</p><p>scala&gt; var math = Map(“Tom”-&gt;80,”Lily”-&gt;20,”Mike”-&gt;95)<br>math: scala.collection.mutable.Map[String,Int] = Map(Mike -&gt; 95, Tom -&gt; 80, Lily -&gt; 20)</p><p>scala&gt; math(“Tom”)=0<br>scala&gt; math<br>res8: scala.collection.mutable.Map[String,Int] = Map(Mike -&gt; 95, Tom -&gt; 0, Lily -&gt; 20)</p><p>添加新的元素<br>scala&gt; math(“Tom”)=80<br>scala&gt; math += “Bob”-&gt;85<br>res9: scala.collection.mutable.Map[String,Int] = Map(Bob -&gt; 85, Mike -&gt; 95, Tom -&gt; 80, Lily -&gt; 20)</p><p>移出一个元素<br>scala&gt; math -= “Bob”<br>res10: scala.collection.mutable.Map[String,Int] = Map(Mike -&gt; 95, Tom -&gt; 80, Lily -&gt; 20)</p><h4 id="2、列表：可变列表，不可变列表"><a href="#2、列表：可变列表，不可变列表" class="headerlink" title="2、列表：可变列表，不可变列表"></a>2、列表：可变列表，不可变列表</h4><p>不可变列表 List<br>scala&gt; var myList=List(1,2,3)<br>myList: List[Int] = List(1, 2, 3)</p><p>scala&gt; val nullList:List[Nothing] = List()<br>nullList: List[Nothing] = List()</p><p>//二维列表<br>scala&gt; val dim : List[List[Int]] = List(List(1,2,3),List(4,5,6))<br>dim: List[List[Int]] = List(List(1, 2, 3), List(4, 5, 6))</p><p>scala&gt; myList.head<br>res11: Int = 1</p><p>scala&gt; myList.tail<br>res12: List[Int] = List(2, 3)</p><p>注意：tail 是除了第一个元素外，其他的元素</p><p>可变列表：LinedList 在 scala.collection.mutable 包中</p><p>scala&gt; var myList = scala.collection.mutable.LinkedList(1,2,3,4)<br>warning: there was one deprecation warning; re-run with -deprecation for details<br>myList: scala.collection.mutable.LinkedList[Int] = LinkedList(1, 2, 3, 4)</p><p>需求：把上面列表中，每一个元素都乘以2</p><p>游标，指向列表的开始</p><pre><code>var cur = myList//Nil意思为空while(cur != Nil ){    //把当前元素乘以2    cur.elem = cur.elem*2    //移动指针到下一个元素    cur = cur.next}</code></pre><p>scala&gt; var cur = myList<br>cur: scala.collection.mutable.LinkedList[Int] = LinkedList(1, 2, 3, 4)</p><p>scala&gt; while(cur != Nil ){<br>| cur.elem = cur.elem*2<br>| cur = cur.next<br>| }</p><p>scala&gt; myList<br>res13: scala.collection.mutable.LinkedList[Int] = LinkedList(2, 4, 6, 8)</p><p>scala&gt; myList.map(_*2)<br>warning: there was one deprecation warning; re-run with -deprecation for details<br>res14: scala.collection.mutable.LinkedList[Int] = LinkedList(4, 8, 12, 16)</p><h4 id="3、序列"><a href="#3、序列" class="headerlink" title="3、序列"></a>3、序列</h4><p>（*）数据库中也有序列：sequence 、 auto increment<br>（1）作为主键，实现自动增长<br>（2）提高性能，序列在Oracle是在内存中的</p><p>（*）Vector Range<br>举例：<br>Vector 是一个带下标的序列，我们可以通过下标来访问Vector中的元素<br>scala&gt; var v = Vector(1,2,3,4,5,6)<br>v: scala.collection.immutable.Vector[Int] = Vector(1, 2, 3, 4, 5, 6)</p><p>Range ： 是一个整数的序列<br>scala&gt; Range(0,5)<br>res15: scala.collection.immutable.Range = Range(0, 1, 2, 3, 4)</p><p>从0开始，到5 ，但不包括5</p><p>scala&gt; println(0 until 5)<br>Range(0, 1, 2, 3, 4)</p><p>scala&gt; println(0 to 5)<br>Range(0, 1, 2, 3, 4, 5)</p><p>Range可以相加<br>scala&gt; (‘0’ to ‘9’) ++ (‘A’ to ‘Z’)<br>res16: scala.collection.immutable.IndexedSeq[Char] = Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z)</p><p>把Range转换成list<br>scala&gt; 1 to 5 toList<br>warning: there was one feature warning; re-run with -feature for details<br>res17: List[Int] = List(1, 2, 3, 4, 5)</p><h4 id="4、集（Set）"><a href="#4、集（Set）" class="headerlink" title="4、集（Set）"></a>4、集（Set）</h4><p>不重复元素的集合，默认是HashSet，与java类似<br>scala&gt; var s1 = Set(1,2,10,8)<br>s1: scala.collection.immutable.Set[Int] = Set(1, 2, 10, 8)</p><p>scala&gt; s1 + 10<br>res18: scala.collection.immutable.Set[Int] = Set(1, 2, 10, 8)</p><p>scala&gt; s1 + 7<br>res19: scala.collection.immutable.Set[Int] = Set(10, 1, 2, 7, 8)</p><p>scala&gt; s1<br>res20: scala.collection.immutable.Set[Int] = Set(1, 2, 10, 8)</p><p>创建一个可排序的Set SortedSet<br>scala&gt; var s2 = scala.collection.mutable.SortedSet(1,2,3,10,8)<br>s2: scala.collection.mutable.SortedSet[Int] = TreeSet(1, 2, 3, 8, 10)</p><p>判断元素是否存在<br>scala&gt; s2.contains(1)<br>res21: Boolean = true</p><p>scala&gt; s2.contains(1231231)<br>res22: Boolean = false</p><p>集的运算：union并集 intersect 交集 diff 差集<br>scala&gt; var s1 = Set(1,2,3,4,5,6)<br>s1: scala.collection.immutable.Set[Int] = Set(5, 1, 6, 2, 3, 4)</p><p>scala&gt; var s2 = Set(5,6,7,8,9,10)<br>s2: scala.collection.immutable.Set[Int] = Set(5, 10, 6, 9, 7, 8)</p><p>scala&gt; s1 union s2<br>res23: scala.collection.immutable.Set[Int] = Set(5, 10, 1, 6, 9, 2, 7, 3, 8, 4)</p><p>scala&gt; s1 intersect s2<br>res24: scala.collection.immutable.Set[Int] = Set(5, 6)</p><p>scala&gt; s1 diff s2<br>res25: scala.collection.immutable.Set[Int] = Set(1, 2, 3, 4)</p><p>数据库里面的union操作，要求：<br>列数一样<br>列的类型一样</p><pre><code>select A,B from ****union select C,D from *****</code></pre><p>函数的定义+返回值<br>def sum():Int =</p><p>python数据分析<br>pyspark</p><h4 id="5、模式匹配"><a href="#5、模式匹配" class="headerlink" title="5、模式匹配"></a>5、模式匹配</h4><p>相当于java中的switch case语句 但是 功能更强大</p><pre><code>package day5/**  * 模式匹配  */object Demo1 {  def main(args: Array[String]): Unit = {    //1、相当于java switch case    var chi = &#39;-&#39;    var sign = 0    chi match {      case &#39;+&#39; =&gt; sign = 1      case &#39;-&#39; =&gt; sign = -1      case _ =&gt; sign = 0    }    println(sign)    /**      * 2、scala中的守卫，case _ if 匹配某种类型的所有值      * 需求：匹配所有的数字      */    var ch2 = &#39;5&#39;    var result : Int = -1    ch2 match {      case &#39;+&#39; =&gt; println(&quot;这是一个加号&quot;)      case &#39;-&#39; =&gt; println(&quot;这是一个减号&quot;)      //这里的10表示转换成1十进制      case _ if Character.isDigit(ch2) =&gt; result=Character.digit(ch2,10)      case _ =&gt; println(&quot;其他&quot;)    }    println(result)    /**      * 3、在模式匹配中使用变量      * 如果改成var mystr = &quot;Hello W+rld&quot;      * 打印：加号      *      * 匹配中，则相当于break      */    var mystr = &quot;Hello World&quot;    //取出某个字符，赋给模式匹配的变量    mystr(7) match {      case &#39;+&#39; =&gt; println(&quot;加号&quot;)      case &#39;-&#39; =&gt; println(&quot;减号&quot;)      //case 语句中使用变量 ch代表传递进来的字符      case ch =&gt; println(ch)    }    /**      * 4、匹配类型 instance of      * 用法：case x : Int =&gt;      *      * Any : 表示任何类型，相当于java中的Object      * Unit : 表示没有值， void      * Nothing : 表示在函数抛出异常时，返回值就是Nothing      *           是scala类层级中的最低端，是任何其他类型的子类型      * Null : 表示引用类型的子类，值：null      *      * 特殊类型      * Option : 表示一个值是可选的（有值或者无值）      * Some : 如果值存在，Option[T] 就是一个Some[T]      * None : 如果值不存在，Option[T] 就是一个None      *      * scala&gt; var myMap = Map(&quot;Time&quot;-&gt;96)      * myMap: scala.collection.immutable.Map[String,Int] = Map(Time -&gt; 96)      *      * scala&gt; myMap.get(&quot;Time&quot;)      * res0: Option[Int] = Some(96)      *      * scala&gt; myMap.get(&quot;Time12342&quot;)      * res1: Option[Int] = None      *      * Nil : 空的List      *      * 四个N总结：None Nothing Null Nil      * None : 如果值不存在，Option[T] 就是一个None      * Nothing : 如果方法抛出异常时，则异常的返回值类型就是Nothing      * Null : 可以赋值给所以的引用类型，但是不能赋值给值类型      *       class Student      *       var s1 = new Student      *       s1 = null      * Nil : 空的List      */    var v4 : Any = 100    v4 match {      case x : Int =&gt; println(&quot;这是一个整数&quot;)      case s : String =&gt; println(&quot;这是一个字符串&quot;)      case _ =&gt; println(&quot;这是其他类型&quot;)    }    //5、匹配数组和列表    var myArray = Array(1,2,3)    myArray match {      case Array(0) =&gt; println(&quot;数组中只有一个0&quot;)      case Array(x,y) =&gt; println(&quot;数组中包含两个元素&quot;)      case Array(x,y,z) =&gt; println(&quot;数组中包含三个元素&quot;)      case Array(x,_*) =&gt; println(&quot;这是一个数组，包含多个元素&quot;)    }    var myList = List(1,2,3)    myList match {      case List(0) =&gt; println(&quot;列表中只有一个0&quot;)      case List(x,y) =&gt; println(&quot;列表中包含两个元素，和是&quot; + (x+y))      case List(x,y,z) =&gt; println(&quot;列表中包含三个元素，和是&quot; + (x+y+z))      case List(x,_*) =&gt; println(&quot;列表中包含多个元素，和是&quot; + myList.sum)    }  }}</code></pre><h4 id="6、样本类"><a href="#6、样本类" class="headerlink" title="6、样本类"></a>6、样本类</h4><p>定义： case class</p><pre><code>package day5/**  * 使用case class 来实现模式匹配  */class Vehiclecase class Car(name:String) extends  Vehiclecase class Bike(name:String) extends  Vehicleobject Demo2 {  def main(args: Array[String]): Unit = {    var aCar : Vehicle = new Car(&quot;Car&quot;)    aCar match {      case Car(name) =&gt; println(&quot;汽车 &quot; + name)      case Bike(name) =&gt; println(&quot;自行车 &quot; + name)      case _ =&gt; println(&quot;其他&quot;)    }  }}</code></pre><p>作用：<br>（1）支持模式匹配，instanceof<br>（2）定一个 Spark SQL 中的 schema ： 表结构</p><pre><code>scala&gt; class Fruitdefined class Fruitscala&gt; class Banana(name:String) extends Fruitdefined class Bananascala&gt; class Apple(name:String) extends Fruitdefined class Applescala&gt; var a = new Apple(&quot;Apple&quot;)a: Apple = Apple@572e6fd9scala&gt; println(a.isInstanceOf[Fruit])truescala&gt; println(a.isInstanceOf[Banana])&lt;console&gt;:16: warning: fruitless type test: a value of type Apple cannot also be a Banana       println(a.isInstanceOf[Banana])                             ^false</code></pre><h3 id="三、Scala高级特性"><a href="#三、Scala高级特性" class="headerlink" title="三、Scala高级特性"></a>三、Scala高级特性</h3><h4 id="1、泛型"><a href="#1、泛型" class="headerlink" title="1、泛型"></a>1、泛型</h4><p>和java类似 T</p><p><strong>1）泛型类</strong><br>定义类的时候，可以带有一个泛型的参数<br>例子：</p><pre><code>package day5/**  * 泛型类  *///需求：操作一个整数class GenericClassInt{  //定义一个整数的变量  private var content : Int = 10  //定义set get  def set(value : Int) = content = value  def get() : Int = content}//需求：操作一个字符串class GenericClassString{  //定义一个空字符串  private var content : String = &quot;&quot;  //定义set get  def set(value : String) = content = value  def get() : String = content}class GenericClass[T]{  //定义变量  //注意：初始值用_来表示  private var content : T = _  //定义set get  def set(value : T) = content = value  def get() : T = content}object Demo3{  def main(args: Array[String]): Unit = {    //定义一个Int 类型    var v1 = new GenericClass[Int]    v1.set(1000)    println(v1.get())    //定义一个String 类型    var v2 = new GenericClass[String]    v2.set(&quot;Ni&quot;)    println(v2.get())  }}</code></pre><p><strong>2）泛型函数</strong></p><p>定义一个函数，可以带有一个泛型的参数</p><p>scala&gt; def mkIntArray(elem:Int<em>)=`Array[Int](elem:_</em>)`<br>mkIntArray: (elem: Int*)Array[Int]</p><p>scala&gt; mkIntArray(1,2,3)<br>res5: Array[Int] = Array(1, 2, 3)</p><p>scala&gt; mkIntArray(1,2,3,4,5)<br>res6: Array[Int] = Array(1, 2, 3, 4, 5)</p><p>scala&gt; def mkStringArray(elem:String<em>)=`Array[String](elem:_</em>)`<br>mkStringArray: (elem: String*)Array[String]</p><p>scala&gt; mkStringArray(“a”,”b”)<br>res7: Array[String] = Array(a, b)</p><p>scala&gt; def mkArray[T:ClassTag]</p><p>ClassTag ： 表示scala在运行时候的状态信息，这里表示调用时候数据类型</p><p>scala&gt; import scala.reflect.ClassTag<br>import scala.reflect.ClassTag</p><pre><code>scala&gt; def mkArray[T:ClassTag](elem:T*)= Array[T](elem:_*)mkArray: [T](elem: T*)(implicit evidence$1: scala.reflect.ClassTag[T])Array[T]</code></pre><p>scala&gt; mkArray(1,2)<br>res8: Array[Int] = Array(1, 2)</p><p>scala&gt; mkArray(“Hello”,”aaa”)<br>res9: Array[String] = Array(Hello, aaa)</p><p>scala&gt; mkArray(“Hello”,1)<br>res10: Array[Any] = Array(Hello, 1)<br>泛型：但凡有重复的时候，考虑使用泛型</p><p><strong>3）上界和下界</strong></p><p>Int x<br>规定x的取值范围 100 &lt;= x &lt;=1000</p><p>泛型的取值范围：<br>T</p><p>类的继承关系 A —&gt; B —&gt; C —&gt; D 箭头指向子类</p><p>定义T的取值范围 D &lt;: T &lt;: B</p><p>T 的 取值范围 就是 B C D</p><p>&lt;: 就是上下界的表示方法</p><p>概念<br>上界 S &lt;： T 规定了 S的类型必须是 T的子类或本身<br>下界 U &gt;： T 规定了 U的类型必须是 T的父类或本身</p><p>例子：</p><pre><code>package day5/**  * 主界  *///定义父类class Vehicle{  //函数：驾驶  def drive() = println(&quot;Driving&quot;)}//定义两个子类class Car extends Vehicle{  override def drive() : Unit = println(&quot;Car Driving&quot;)}//class Bike extends Vehicle{//  override def drive(): Unit = println(&quot;Bike Driving&quot;)//}class Bike{  def drive(): Unit = println(&quot;Bike Driving&quot;)}object ScalaUpperBoud {  //定义驾驶交通工具的函数  def takeVehicle[T &lt;: Vehicle](v:T) = v.drive()  def main(args: Array[String]): Unit = {    //定义交通工具    var v : Vehicle = new Vehicle    takeVehicle(v)    var c : Car = new Car    takeVehicle(c)    //因为没有继承Vehicle，所以运行报错    var b : Bike = new Bike    takeVehicle(b)  }}</code></pre><pre><code>scala&gt; def addTwoString[T &lt;: String](x:T,y:T) = x +&quot; ********* &quot; + yaddTwoString: [T &lt;: String](x: T, y: T)Stringscala&gt; addTwoString(&quot;Hello&quot;,&quot;World&quot;)res11: String = Hello ********* Worldscala&gt; addTwoString(1,2)&lt;console&gt;:14: error: inferred type arguments [Int] do not conform to method addTwoString&#39;s type parameter bounds [T &lt;: String]       addTwoString(1,2)       ^&lt;console&gt;:14: error: type mismatch; found   : Int(1) required: T       addTwoString(1,2)                    ^&lt;console&gt;:14: error: type mismatch; found   : Int(2) required: T       addTwoString(1,2)                      ^scala&gt; addTwoString(1.toString,2.toString)res13: String = 1 ********* 2</code></pre><p><strong>4）视图界定 View bounds</strong></p><p>就是上界和下界的扩展</p><p>除了可以接收上界和下界规定的类型以外，还可以接收能够通过隐式转换过去的类型</p><p>用 % 来表示</p><pre><code>scala&gt;  def addTwoString[T &lt;% String](x:T,y:T) = x +&quot; ********* &quot; + yaddTwoString: [T](x: T, y: T)(implicit evidence$1: T =&gt; String)Stringscala&gt; addTwoString(1,2)&lt;console&gt;:14: error: No implicit view available from Int =&gt; String.       addTwoString(1,2)                   ^//定义隐式转换函数scala&gt; implicit def int2String(n:Int):String = n.toStringwarning: there was one feature warning; re-run with -feature for detailsint2String: (n: Int)Stringscala&gt; addTwoString(1,2)res14: String = 1 ********* 2</code></pre><p><strong>执行过程</strong> </p><p>1、调用了 int2String Int =&gt; String<br>2、addTwoString(“1”,”2”)</p><p><strong>5）协变和逆变（概念）</strong></p><p>协变：表示在类型参数前面加上 + 。泛型变量的值，可以是本身类型或者其子类类型<br>例子：</p><pre><code>package day5/**  * 协变：表示在类型参数前面加上 + 。泛型变量的值，可以是本身类型或者其子类类型  */class Animalclass Bird extends Animalclass Sparrow extends Bird//定义第四个类，吃东西的类，协变，有继承关系了class EatSomething[+T](t:T)object Demo4 {  def main(args: Array[String]): Unit = {    //定义一个鸟吃东西的对象    var c1 : EatSomething[Bird] =new EatSomething[Bird](new Bird)    //定义一个动物吃东西的对象    var c2 : EatSomething[Animal] = c1    /**      * 问题：能否把c1 赋给c2      * c1 c2都是EatSomething      * c1 c2 没有继承关系      *      * class EatSomething[T](t:T)      * var c2 : EatSomething[Animal] = c1  报错      * 原因 ： EatSomething[Bird] 并没有继承EatSomething[Animal]      *      * class EatSomething[+T](t:T)      * 报错消失      *      * 协变      */    var c3 : EatSomething[Sparrow] = new EatSomething[Sparrow](new Sparrow)    var c4 : EatSomething[Animal] = c3  }}</code></pre><p>逆变：表示在类型参数前面加上 - 。泛型变量的值，可以是本身类型或者其父类类型<br>例子：</p><pre><code>package day5/**  * 逆变：表示在类型参数前面加上 - 。泛型变量的值，可以是本身类型或者其父类类型  */class Animalclass Bird extends Animalclass Sparrow extends Bird//定义第四个类，吃东西的类，逆变class EatSomething[-T](t:T)object Demo5 {  def main(args: Array[String]): Unit = {    //定义一个鸟吃东西的对象    var c1 : EatSomething[Bird] =new EatSomething[Bird](new Bird)    //定义一个动物吃东西的对象    var c2 : EatSomething[Sparrow] = c1  }}</code></pre><h4 id="2、隐式转换"><a href="#2、隐式转换" class="headerlink" title="2、隐式转换"></a>2、隐式转换</h4><p><strong>1）隐式转换函数： implicit</strong></p><pre><code>package day5/**  * 隐式转换  *  * 定义一个隐式转换函数  */class Fruit(name:String){  def getFruitName() : String = name}class Monkey(f:Fruit){  def say()  = println(&quot;Monkey like &quot; + f.getFruitName())}object ImplicitDemo {  def main(args: Array[String]): Unit = {    //定义一个水果对象    var f : Fruit = new Fruit(&quot;Banana&quot;)    f.say()  }  implicit def fruit2Monkey(f:Fruit) : Monkey = {    new Monkey(f)  }}</code></pre><p><strong>2）隐式参数：使用implicit 修饰的函数参数</strong></p><p>定义一个带有隐式参数的函数：</p><pre><code>scala&gt; def testPara(implicit name:String) = println(&quot;The value is &quot; + name)testPara: (implicit name: String)Unitscala&gt; testPara(&quot;AAAA&quot;)The value is AAAAscala&gt; implicit val name : String = &quot;*****&quot;name: String = *****scala&gt; testParaThe value is *****</code></pre><p>定义一个隐式参数，找到两个值中比较小的那个值<br>100 23 –&gt;23<br>“Hello” “ABC” –&gt; ABC</p><pre><code>scala&gt; def smaller[T](a:T,b:T)(implicit order : T =&gt; Ordered[T]) = if(a&lt;b) a else bsmaller: [T](a: T, b: T)(implicit order: T =&gt; Ordered[T])Tscala&gt; smaller(1,2)res18: Int = 1scala&gt; smaller(&quot;Hello&quot;,&quot;ABC&quot;)res19: String = ABC</code></pre><p>解释：<br>order 就是一个隐式参数，我们使用Scala中的 Ordered 类，表示该值可以被排序，也就是可以被比较</p><p>作用：扩充了属性的功能</p><p><strong>3）隐式类 在类名前 加 implicit 关键字</strong> </p><p>作用：扩充类的功能</p><pre><code>package day5/**  * 隐式类  */object Demo6 {  def main(args: Array[String]): Unit = {    //执行两个数字的求和    println(&quot;两个数字的和是： &quot;+1.add(2))    /**      * 定义一个隐式类，类增强1的功能      *      * Calc(x:Int)      * 1是Int类型，所以就会传递进来      *      * 执行过程：      * 1---&gt;Calc类      * var a = new Calc(1)      * 在调用Calc add方法      * a.add(2)      *      */    implicit class Calc(x:Int){      def add(y: Int) : Int = x + y    }  }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala基础</title>
      <link href="/2019/03/23/scala-ji-chu/"/>
      <url>/2019/03/23/scala-ji-chu/</url>
      
        <content type="html"><![CDATA[<p>1、Scala编程语言<br>2、Spark Core ： Spark内核 ，最重要的一个部分<br>3、Spark SQL : 类似于 Hive 和Pig。数据分析引擎。sql语句提交到Spark集群中运行<br>4、Spark Streaming ：类似于 Storm，用于流式计算、实时计算。本质：一个离线计算</p><h3 id="一、Scala基础"><a href="#一、Scala基础" class="headerlink" title="一、Scala基础"></a>一、Scala基础</h3><h4 id="1、Scala简介"><a href="#1、Scala简介" class="headerlink" title="1、Scala简介"></a>1、Scala简介</h4><p>1）Scala是一个多范式的编程语言（支持多种方式的编程）<br>（1）使用面向对象编程：封装、继承、多态<br>（2）使用函数式编程：最大的特定<br>优点：代码非常简洁<br>缺点：可读性太差，尤其是隐式类、隐式函数、隐式参数</p><p>2）安装和配置Scala<br>（1）基于JDK，先安装JDK<br>（2）Scala：2.11.8（Spark 2.1.0）<br>（3）配置环境变量：SCALA_HOME<br>（4）%SCALA_HOME%\bin 配置到path中<br>下载地址：<a href="https://www.scala-lang.org" target="_blank" rel="noopener">https://www.scala-lang.org</a><br>文档地址：<a href="https://www.scala-lang.org/api/2.11.8/#scala.math.package" target="_blank" rel="noopener">https://www.scala-lang.org/api/2.11.8/#scala.math.package</a></p><p>3）开发环境<br>REPL命令行<br>IDEA ： 需要安装Scala插件</p><h4 id="2、Scala中的数据类型和变量常量"><a href="#2、Scala中的数据类型和变量常量" class="headerlink" title="2、Scala中的数据类型和变量常量"></a>2、Scala中的数据类型和变量常量</h4><p>1）注意一点：Scala中所有的数据，都是对象<br>举例：1 Java int 。在Scala中，1 就是一个对象</p><p>2）基本数据类型<br>Byte ：8位有符号数字<br>Short ：16位有符号数字<br>Int ：32位有符号数字<br>Long ： 64位有符号数字<br>Float ：32位有符号数字<br>Double ：64位有符号数字<br>Char ：8位有符号数字<br>Boolean ： 1位有符号数字</p><p>字符串类型<br>String </p><p>字符<br>Char ：8位有符号数字</p><p>Scala中字符串的插值操作：就是相当于字符串的拼接</p><pre><code>scala&gt; var s1 : String = &quot;Hello &quot;s1: String = &quot;Hello &quot;scala&gt; &quot;My name is Tom and ${s1}&quot;res1: String = My name is Tom and ${s1}</code></pre><p>插值操作时，需要加入 s</p><pre><code>scala&gt; s&quot;My name is Tom and ${s1}&quot;res2: String = &quot;My name is Tom and Hello &quot;</code></pre><p>3）变量var和常量val</p><p>scala&gt; val s2 :String = “Hello all”<br>s2: String = Hello all</p><p>scala&gt; s2 = “Hello everyone”<br><console>:12: error: reassignment to val<br>   s2 = “Hello everyone”</p><p>4）Unit类型和Nothing类型<br>（1）Unit类型，就是java中的void，没有返回值</p><pre><code>scala&gt; val f = ()f: Unit = ()</code></pre><p>返回值 Unit类型<br>( ) 代表了一个函数，这个函数没有返回值</p><p>（2）Nothing类型，在执行过程中，产生了异常Exception<br>举例：<br>scala函数：scala中函数非常重要，是scala的头等公民<br>用法很多：函数式编程、高阶函数</p><p>def myFunction = 函数的实现</p><pre><code>scala&gt; def myFun = throw new Exception(&quot;Some Error&quot;)myFun: Nothing</code></pre><h4 id="3、函数（头等公民）"><a href="#3、函数（头等公民）" class="headerlink" title="3、函数（头等公民）"></a>3、函数（头等公民）</h4><p>（1）scala内置函数，可以直接使用的函数</p><pre><code>scala&gt; max(1,2)&lt;console&gt;:12: error: not found: value max       max(1,2)       ^scala&gt; import scala.math   final package mathscala&gt; import scala.math._import scala.math._</code></pre><p>_ 就相当于Java中的 * 代表包内所有东西</p><pre><code>scala&gt; max(1,2)res4: Int = 2</code></pre><p>res4: Int = 2<br>定义了一个变量 res4 ，接收了 max 函数的返回值。Scala中支持类型的推导。<br>res4 = “”</p><p>（2） 自定义函数<br>语法：<br>def 函数名称（[参数名称：参数类型]*） : 返回值类型 = {<br>函数的实现<br>}</p><p>举例：<br>1）求和</p><pre><code>scala&gt; def sum(x:Int,y:Int):Int = x + ysum: (x: Int, y: Int)Intscala&gt; sum(1,2)res5: Int = 3</code></pre><p>2）求阶乘，5！= 5 * 4* 3 2 1 </p><p><strong>递归</strong></p><pre><code>scala&gt; def myFactor(x:Int):Int = {     | if(x&lt;=1)     | 1     | else     | x*myFactor(x-1)     | }myFactor: (x: Int)Intscala&gt; myFactor(5)res6: Int = 120</code></pre><p>注意：没有return语句<br>函数的最后一句话，就是函数的返回值</p><p>3）求输入的年份是否是闰年<br>闰年：<br>普通闰年：可以被4整除但是不能被100整除的年份<br>世纪闰年：可以被400整除的年份</p><pre><code>scala&gt; def isLeapYear(x:Int) = {     | if(( x%4 == 0 &amp;&amp; x%100 != 0) || (x%400==0)) true     | else false     | }isLeapYear: (x: Int)Booleanscala&gt; isLeapYear(2019)res7: Boolean = falsescala&gt; isLeapYear(2008)res8: Boolean = true</code></pre><p><strong>注意</strong></p><p>1、( x%4 == 0 &amp;&amp; x%100 != 0) || (x%400==0)<br>2、函数定义的时候，可以不写返回值，因为scala支持类型推导</p><h4 id="4、循环语句"><a href="#4、循环语句" class="headerlink" title="4、循环语句"></a>4、循环语句</h4><p>1）类似于Java的用法 while dowhile for</p><pre><code>/**      * for循环      *      * 定义一个集合      */    var list = List(&quot;dfg&quot;,&quot;Agddg&quot;,&quot;Fd&quot;)    println(&quot;------------for循环中的第一种写法-------------&quot;)    for( s &lt;- list ) println(s)    println(&quot;------------for循环中的第二种写法-------------&quot;)    //打印长度大于3的名字    for{      s &lt;- list      if(s.length &gt; 3)    }println(s)    println(&quot;------------for循环中的第三种写法-------------&quot;)    //对第二种进一步简化    for( s&lt;- list if s.length &lt;= 3) println(s)    println(&quot;------------for循环中的第四种写法-------------&quot;)    /**      * 1、把list中所有元素都变成大写      * s &lt;- list      * s1 = s.toUpperCase      *      * 2、返回一个新的集合      * yield(s1)      *      */    var newList = for {      s &lt;- list      s1 = s.toUpperCase    }yield(s1)    for(s &lt;- newList) println(s)    println(&quot;------------while循环-------------&quot;)    //定义循环变量    var i = 0    while( i &lt; list.length){      println(list(i))      /**        * 自增        *        * 注意scala中没有i++        */      i += 1    }    println(&quot;------------do while循环-------------&quot;)    //定义循环变量    var j = 0    do{      println(list(j))      j += 1    }while( j &lt; list.length)</code></pre><p>2）foreach循环（Spark算子）</p><pre><code>  println(&quot;------------foreach循环-------------&quot;)    /**      * foreach scala里面有 spark里面      * map      *      * 没有返回值，map有返回值      * foreach是list的一个方法      */    list.foreach(println)    /**      * foreach 说明      * list.foreach(println)      *      *  我们把一个函数传入了foreach      *  高阶函数（函数式编程）      */    /**      * 判断101-200之间有多少个素数      *      * 判断素数的方法：      * x%2 -----x%sqrt(根号)x      * 当都不能被整除的时候就是素数      *      *      * 16      * sqrt(16) = 4      * 2 3 4      *      * 16%2 == 0 ?      * 16%3 == 0 ?      * 16%4 == 0 ?      *      * 编程思路：      * 两层循环：      *   第一层：101-200      *     第二层：2--sqrt第一层      *      */    println(&quot;---------循环嵌套-------------&quot;)    var count : Int = 0 //保存结果    var index_outer = 0    var index_inner = 0    for(index_outer &lt;- 101 until 200){      index_inner = 2      var b = false //标识是否能被整除      breakable{        while(index_inner &lt;= sqrt(index_outer)){          if(index_outer % index_inner ==0) {            b = true            break          }          index_inner += 1        }      }      if(!b) count += 1    }    println(&quot;个数为：&quot; + count)    /**    * 算法分析：    * 1、比较相邻的元素。如果第一个比第二个大，就交换他们两个。    * 2、对每一对相邻元素都做上述工作，循环完第一次后，最后的元素，就是最大的元素。    * 3、针对剩下的元素，重复上面工作（除了最后一个元素）    *    * 程序分析：    * 1、两层循环    * 2、外层循环控制比较的次数    * 3、内层循环控制到达的位置，就是 结束比较 的位置    **/    println(&quot;---------冒泡排序-------------&quot;)    var a = Array(12,3,6,3,6,7,3,8,34,3)    println(&quot;---------排序前---------------&quot;)    a.foreach(println)    for(i &lt;- 0 until a.length - 1){      for(j &lt;- 0 until a.length - i - 1){        if(a(j) &gt; a(j+1)){          //交换          var tmp = a(j)          a(j) = a(j+1)          a(j+1) = tmp        }      }    }    println(&quot;---------排序后---------------&quot;)    a.foreach(println)</code></pre><h4 id="5、Scala的函数参数"><a href="#5、Scala的函数参数" class="headerlink" title="5、Scala的函数参数"></a>5、Scala的函数参数</h4><p>1）函数参数的求值策略<br>（1）call by value :<br>对函数的实参求值，并且只求一次</p><p>（2）call by name : =&gt;<br>函数实参在函数体内部用到的时候，才会被求值</p><p>举例：</p><pre><code>scala&gt; def test1(x:Int,y:Int) = x + xtest1: (x: Int, y: Int)Intscala&gt; test1(3+4,8)res9: Int = 14scala&gt; def test2(x : =&gt; Int,y : =&gt; Int) = x+xtest2: (x: =&gt; Int, y: =&gt; Int)Intscala&gt; test2(3+4,8)res10: Int = 14</code></pre><p>执行过程对比：<br>test1 —&gt; test1(3+4,8) —&gt; test1(7,8) —&gt; 7+7 —&gt; 14<br>test2 —&gt; test2(3+4,8) —&gt; (3+4) + (3+4) —&gt; 14</p><p>（3）复杂的例子<br>def bar(x:Int,y : =&gt; Int) : Int = 1<br>x 是 value y 是 name</p><p>定义一个死循环：<br>def loop() : Int = loop</p><p>调用bar函数的时候：<br>1、bar(1,loop)<br>2、bar(loop,1)–&gt;产生死循环</p><p>哪个方式会产生死循环？</p><pre><code>scala&gt; def bar(x:Int,y : =&gt; Int) : Int = 1bar: (x: Int, y: =&gt; Int)Intscala&gt; def loop() : Int = looploop: ()Intscala&gt; bar(1,loop)res11: Int = 1scala&gt; bar(loop,1)</code></pre><p><strong>解析</strong></p><p>1、虽然 y 是 name, 每次调用的时候会被求值。但是，函数体内，没有调用到y.<br>2、x 是 value，对函数参数求值，并且只求一次。虽然后面没有用到x，但求值时产生了死循环</p><p>2、Scala中函数参数的类型<br>（1）默认参数<br>当你没有给参数值赋值的时候，就会使用默认值</p><pre><code>def fun1(name:String=&quot;Ti&quot;) :String = &quot;Hello &quot; + namescala&gt; def fun1(name:String=&quot;Ti&quot;) :String = &quot;Hello &quot; + namefun1: (name: String)Stringscala&gt; fun1(&quot;An&quot;)res0: String = Hello Anscala&gt; fun1()res1: String = Hello Ti</code></pre><p>（2）代名参数<br>当有多个默认参数的时候，通过代名参数可以确定给哪个函数参数赋值。</p><pre><code>def fun2(str:String = &quot;Hello &quot; , name:String = &quot; Ti &quot; ,age:Int = 20) = str + name + &quot; age is &quot; +agescala&gt; def fun2(str:String = &quot;Hello &quot; , name:String = &quot; Ti &quot; ,age:Int = 20) = str + name + &quot; age is &quot; +agefun2: (str: String, name: String, age: Int)Stringscala&gt; fun2()res2: String = Hello  Ti  age is 20scala&gt; fun2(&quot;An&quot;)res3: String = An Ti  age is 20scala&gt; fun2(name=&quot;An&quot;)res4: String = Hello An age is 20</code></pre><p>（3）可变参数<br>类似于Java中的可变参数，即 参数数量不固定</p><pre><code>scala&gt; def sum(args:Int*)= { | var result = 0 | for(s&lt;-args) result +=s | result | }sum: (args: Int*)Intscala&gt; sum(1,2,3,4)res5: Int = 10scala&gt; sum(1,2,3,4,3,4)res6: Int = 17</code></pre><h4 id="6、懒值（lazy）"><a href="#6、懒值（lazy）" class="headerlink" title="6、懒值（lazy）"></a>6、懒值（lazy）</h4><p>铺垫：Spark的核心是 RDD（数据集合），操作数据集合的数据，使用算子来操作RDD（函数、方法）</p><p>算子：<br>Transformation ： 延时加载，不会触发计算<br>Action ： 会立刻触发计算</p><p>定义：常量如果是lazy的，他的初始化会被延迟，推迟到第一次使用该常量的时候<br>举例：<br>scala&gt; var x : Int = 10<br>x: Int = 10</p><p>scala&gt; val y : Int = x+1<br>y: Int = 11</p><p>y 的值是x+1 定义后会立即进行计算</p><pre><code>scala&gt; lazy val z : Int = x+1z: Int = &lt;lazy&gt;</code></pre><p>z的初始化会被延迟<br>scala&gt; z<br>res0: Int = 11</p><p>当我们第一次使用z的时候，才会触发计算</p><p>读文件：<br>scala&gt; val words = scala.io.Source.fromFile(“E:\student.txt”).mkString<br>words: String =<br>1 Tom 12<br>2 Mary 13<br>3 Lily 15</p><p>scala&gt; lazy val words = scala.io.Source.fromFile(“E:\student.txt”).mkString<br>words: String = <code>&lt;lazy&gt;</code><br>scala&gt; words<br>res1: String =<br>1 Tom 12<br>2 Mary 13<br>3 Lily 15</p><p>scala&gt; lazy val words = scala.io.Source.fromFile(“E:\student121.txt”).mkString<br>words: String = <code>&lt;lazy&gt;</code></p><p>定义成lazy后，初始化被延迟，所以不会抛异常<br>scala&gt; val words = scala.io.Source.fromFile(“E:\student121.txt”).mkString<br>java.io.FileNotFoundException: E:\student121.txt (系统找不到指定的文件。)<br>at java.io.FileInputStream.open0(Native Method)<br>at java.io.FileInputStream.open(FileInputStream.java:195)<br>at java.io.FileInputStream.(FileInputStream.java:138)<br>at scala.io.Source.fromFile(Source.scala:76)<br>at scala.io.Source$.fromFile(Source.scala:54)<br>… 32 elided</p><h4 id="7、例外：Exception"><a href="#7、例外：Exception" class="headerlink" title="7、例外：Exception"></a>7、例外：Exception</h4><p>类似于Java，还是有一些变化<br>文件操作</p><pre><code>scala&gt; var words = scala.io.Source.fromFile(&quot;E:\\server.xml&quot;).mkStringwords: String =&quot;&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--  Licensed to the Apache Software Foundation (ASF) under one or more  contributor license agreements.  See the NOTICE file distributed with  this work for additional information regarding copyright ownership....</code></pre><pre><code>scala&gt; val words = scala.io.Source.fromFile(&quot;E:\\212.txt&quot;).mkStringjava.io.FileNotFoundException: E:\\212.txt(系统找不到指定的文件。) at java.io.FileInputStream.open0(Native Method) at java.io.FileInputStream.open(FileInputStream.java:195) at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:138) at scala.io.Source$.fromFile(Source.scala:91) at scala.io.Source$.fromFile(Source.scala:76) at scala.io.Source$.fromFile(Source.scala:54) ... 32 elided</code></pre><p><strong>try catch finally练习</strong></p><pre><code>   /**      * 1、采用 try catch finally 来捕获异常和处理异常      * 试验一下，scala中文件读取      *      */    try{      // try 代码块里面写 可能抛出异常的函数      println(&quot;------------try catch finally------------------&quot;)      var words = scala.io.Source.fromFile(&quot;E:\\server.xml&quot;).mkString      println(words)    }catch {      case ex: FileNotFoundException =&gt; {        println(&quot;File Not Found Exception&quot;)      }      case ex: IllegalArgumentException =&gt;{        println(&quot;Illegal Argument Exception&quot;)      }      case _:Exception =&gt;{        println(&quot;This is an Exception&quot;)      }    }finally {      println(&quot;This is finally&quot;)    }  /**   * 当没有抛出异常时：   * try  ---&gt;  finally   * 打印：   * This is finally   *   * 当抛出异常时：   * try  --&gt; catch --&gt; finally   * File Not Found Exception   * This is finally   */   /**    * 2、如果一个函数返回值类型是nothing，表示：在函数执行的过程中，产生了异常    *    * scala&gt; def fun1() = throw new Exception(&quot;Exception&quot;)    *  fun1: ()Nothing    */</code></pre><h4 id="8、数组"><a href="#8、数组" class="headerlink" title="8、数组"></a>8、数组</h4><p>1）数组的类型<br>（1）定长数组：Array</p><pre><code>scala&gt; val a = new Array[Int](10)   -----&gt;  (10) 就是数组的长度a: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)scala&gt; val a = new Array[String](10)a: Array[String] = Array(null, null, null, null, null, null, null, null, null, null)</code></pre><p><strong>初始化赋给默认值</strong></p><pre><code>scala&gt; val c : Array[String] = Array(&quot;Tom&quot;,&quot;Lily&quot;)c: Array[String] = Array(Tom, Lily)scala&gt; val c : Array[String] = Array(&quot;Tom&quot;,&quot;Lily&quot;,1)&lt;console&gt;:11: error: type mismatch; found   : Int(1) required: String       val c : Array[String] = Array(&quot;Tom&quot;,&quot;Lily&quot;,1)</code></pre><p>不能往数组中添加不同类型的元素</p><p>（2）变长数组：ArrayBuffer</p><pre><code>scala&gt; val d = ArrayBuffer[Int]()&lt;console&gt;:11: error: not found: value ArrayBuffer       val d = ArrayBuffer[Int]()       ^</code></pre><p>scala&gt; import scala.collection.mutable._<br>import scala.collection.mutable._</p><p><strong>mutable（可变的）</strong></p><p>scala&gt; val d = ArrayBufferInt<br>d: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer()</p><p>scala&gt; d += 1<br>res2: d.type = ArrayBuffer(1)</p><p>scala&gt; d += 2<br>res3: d.type = ArrayBuffer(1, 2)</p><p>scala&gt; d += (1,2,3,4)<br>res4: d.type = ArrayBuffer(1, 2, 1, 2, 3, 4)</p><p>scala&gt; d.<br>++ combinations groupBy mapResult reverse to<br>++: companion grouped max reverseIterator toArray<br>++= compose hasDefiniteSize maxBy reverseMap toBuffer<br>++=: contains hashCode min runWith toIndexedSeq<br>+: containsSlice head minBy sameElements toIterable<br>+= copyToArray headOption mkString scan toIterator<br>+=: copyToBuffer indexOf nonEmpty scanLeft toList </p><ul><li>corresponds indexOfSlice orElse scanRight toMap<br>– count indexWhere padTo segmentLength toSeq<br>–= diff indices par seq toSet </li><li>= distinct init partition size toStream<br>/: drop inits patch sizeHint toString<br>:+ dropRight insert permutations sizeHintBounded toTraversable<br>:\ dropWhile insertAll prefixLength slice toVector<br>&lt;&lt; endsWith intersect prepend sliding transform<br>WithFilter equals isDefinedAt prependAll sortBy transpose<br>addString exists isEmpty product sortWith trimEnd<br>aggregate filter isTraversableAgain readOnly sorted trimStart<br>andThen filterNot iterator reduce span union<br>append find last reduceLeft splitAt unzip<br>appendAll flatMap lastIndexOf reduceLeftOption startsWith unzip3<br>apply flatten lastIndexOfSlice reduceOption stringPrefix update<br>applyOrElse fold lastIndexWhere reduceRight sum updated<br>canEqual foldLeft lastOption reduceRightOption tail view<br>clear foldRight length reduceToSize tails withFilter<br>clone forall lengthCompare remove take zip<br>collect foreach lift repr takeRight zipAll<br>collectFirst genericBuilder map result takeWhile zipWithIndex</li></ul><p>举例：去掉数组中，最后两个元素</p><pre><code>scala&gt; dres5: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(1, 2, 1, 2, 3, 4)scala&gt; d.trimEnd(2)scala&gt; dres7: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(1, 2, 1, 2)</code></pre><p><strong>遍历数组</strong><br>for循环、foreach：</p><pre><code>scala&gt; var a = Array(&quot;Tom&quot;,&quot;Lily&quot;,&quot;Andy&quot;)a: Array[String] = Array(Tom, Lily, Andy)scala&gt; for(s &lt;- a ) println(s)TomLilyAndyscala&gt; a.foreach(println)TomLilyAndy</code></pre><p>数组的常见操作举例：</p><pre><code>scala&gt; val myarray = Array(1,2,7,8,10,3,6)myarray: Array[Int] = Array(1, 2, 7, 8, 10, 3, 6)scala&gt; myarray.maxres10: Int = 10scala&gt; myarray.minres11: Int = 1scala&gt; myarray.sortWith(_&gt;_)res12: Array[Int] = Array(10, 8, 7, 6, 3, 2, 1)scala&gt; myarray.sortWith(_&lt;_)res13: Array[Int] = Array(1, 2, 3, 6, 7, 8, 10)</code></pre><p>解释：(<em>&gt;</em>)<br>完整 ： sortWith函数里面，参数也是一个函数  –&gt; 高阶函数</p><pre><code>_&gt;_   函数def comp(a:Int,b:Int) = {if(a&gt;b) true else false}(a,b) =&gt; {if(a&gt;b) true else false}(a,b) =&gt; {if(a&gt;b) true else false}   ----&gt;  _&gt;_</code></pre><p><em>&gt;</em> 是一个函数，传入两个参数，返回值是Bool （布尔型）</p><p>2）多维数组<br>和Java类似，通过数组的数组来实现</p><pre><code>scala&gt; var matrix = Array.ofDim[Int](3,4)matrix: Array[Array[Int]] = Array(Array(0, 0, 0, 0), Array(0, 0, 0, 0), Array(0, 0, 0, 0))    Array(0, 0, 0, 0)     Array(0, 0, 0, 0)    Array(0, 0, 0, 0)</code></pre><p>三行四列的数组</p><p>scala&gt; matrix(1)(2)=10</p><p>scala&gt; matrix<br>res15: Array[Array[Int]] = Array(Array(0, 0, 0, 0), Array(0, 0, 10, 0), Array(0, 0, 0, 0))</p><p>数组下标是从0开始的</p><p>例子：<br>定义一个二维数组，其中每个元素是一个一维数组，并且长度不固定</p><pre><code>scala&gt; var triangle = new Array[Array[Int]](10)triangle: Array[Array[Int]] = Array(null, null, null, null, null, null, null, null, null, null)</code></pre><p>初始化：</p><pre><code>scala&gt; for(i &lt;- 0 until triangle.length){     | triangle(i) = new Array[Int](i+1)     | }scala&gt; triangle res17: Array[Array[Int]] = Array(Array(0), Array(0, 0), Array(0, 0, 0), Array(0, 0, 0, 0), Array(0, 0, 0, 0, 0), Array(0, 0, 0, 0, 0, 0), Array(0, 0, 0, 0, 0, 0, 0), Array(0, 0, 0, 0, 0, 0, 0, 0), Array(0, 0, 0, 0, 0, 0, 0, 0, 0), Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0))</code></pre><p>二维数组，如果使用 ArrayArray[Int] 声明时：<br>1、首先指定的是外层数据的长度<br>2、初始化内层数组的时候，再指定内层数组的长度</p><h4 id="9、映射-lt-key-value-gt-Map"><a href="#9、映射-lt-key-value-gt-Map" class="headerlink" title="9、映射 &lt;key,value&gt; Map"></a>9、映射 &lt;key,value&gt; Map</h4><p>举例：<br>创建一个map，来保存学生的成绩</p><p>scala&gt; val scores = Map(“Tom” -&gt; 80,”Andy”-&gt;70,”Mike”-&gt;90)<br>scores: scala.collection.mutable.Map[String,Int] = Map(Mike -&gt; 90, Tom -&gt; 80, Andy -&gt; 70)</p><p>1）Map[String,Int] key String value Int<br>2）scala.collection.mutable</p><p>scala中，映射是有两种，一种是可变map，一种是不可变map<br>scala.collection.mutable —&gt; 可变<br>scala.collection.immutable —&gt; 不可变</p><pre><code>scala&gt; val scores2 = scala.collection.immutable.Map(“Tom” -&gt; 80,”Andy”-&gt;70,”Mike”-&gt;90) scores2: scala.collection.immutable.Map[String,Int] = Map(Tom -&gt; 80, Andy -&gt; 70, Mike -&gt; 90)</code></pre><p><strong>映射的初始化</strong></p><pre><code>scala&gt; val scores2 = scala.collection.mutable.Map((“Tom”,80),(“Andy”,70)) scores2: scala.collection.mutable.Map[String,Int] = Map(Tom -&gt; 80, Andy -&gt; 70)scala&gt; scores2=1&lt;console&gt;:15: error: reassignment to val       scores2=1</code></pre><p><strong>映射的操作</strong><br>1）获取映射中的值</p><pre><code>scala&gt; val chinese = scala.collection.mutable.Map((&quot;Tom&quot;,80),(&quot;Andy&quot;,70))chinese: scala.collection.mutable.Map[String,Int] = Map(Tom -&gt; 80, Andy -&gt; 70)scala&gt; chinese(&quot;Tom&quot;)res18: Int = 80scala&gt; chinese.get(&quot;Andy&quot;)res19: Option[Int] = Some(70)scala&gt; chinese(&quot;aaaa&quot;)java.util.NoSuchElementException: key not found: aaaa  at scala.collection.MapLike$class.default(MapLike.scala:228)  at scala.collection.AbstractMap.default(Map.scala:59)  at scala.collection.mutable.HashMap.apply(HashMap.scala:65)  ... 32 elidedscala&gt; chinese.get(&quot;Andy1231312&quot;)res21: Option[Int] = Nonechinese(&quot;aaaa&quot;) get(&quot;Andy1231312&quot;)需求：判断key是否存在，若不存在，返回默认值scala&gt; if(chinese.contains(&quot;aaa&quot;)){     | chinese(&quot;aaa&quot;)     | }else{     | -1     | }res22: Int = -1scala&gt; chinese.getOrElse(&quot;aaaa&quot;,-1)res23: Int = -1</code></pre><p>2）更新映射中的值<br>注意：必须是可变映射</p><pre><code>scala&gt; chineseres24: scala.collection.mutable.Map[String,Int] = Map(Tom -&gt; 80, Andy -&gt; 70)scala&gt; chinese(&quot;Andy&quot;)=20scala&gt; chineseres26: scala.collection.mutable.Map[String,Int] = Map(Tom -&gt; 80, Andy -&gt; 20)</code></pre><p>3）映射的迭代<br>for foreach</p><pre><code>scala&gt; chineseres27: scala.collection.mutable.Map[String,Int] = Map(Tom -&gt; 80, Andy -&gt; 20)scala&gt; for(s&lt;-chinese) println(s)(Tom,80)(Andy,20)scala&gt; chinese.foreach(println)(Tom,80)(Andy,20)</code></pre><p>foreach  高阶函数</p><h4 id="10、元组-Tuple"><a href="#10、元组-Tuple" class="headerlink" title="10、元组 : Tuple"></a>10、元组 : Tuple</h4><p>scala 中的tuple ： 是不同类型值的集合</p><pre><code>scala&gt; val t1 = Tuple(&quot;Tom&quot;,&quot;Lily&quot;,1)&lt;console&gt;:14: error: not found: value Tuple       val t1 = Tuple(&quot;Tom&quot;,&quot;Lily&quot;,1)                ^</code></pre><p>—–Tuple需要指明不同类型值的个数</p><pre><code>scala&gt; val t1 = Tuple3(&quot;Tom&quot;,&quot;Lily&quot;,1)t1: (String, String, Int) = (Tom,Lily,1)</code></pre><p>Tuple3 代表 Tuple中有三个元素</p><pre><code>scala&gt; val t1 = Tuple2(&quot;Lily&quot;,1)t1: (String, Int) = (Lily,1)scala&gt; val t2 = (1,2,4,&quot;Hello&quot;)t2: (Int, Int, Int, String) = (1,2,4,Hello)</code></pre><p><strong>tuple操作</strong></p><p>访问tuple中的元素</p><pre><code>scala&gt; val t1 = Tuple3(&quot;Tom&quot;,&quot;Lily&quot;,1)t1: (String, String, Int) = (Tom,Lily,1)scala&gt; t1._1   _3         copy     hashCode   productArity     productIterator   toString   zipped_2   canEqual   equals   invert     productElement   productPrefix     xscala&gt; t1._1res30: String = Tomscala&gt; t1._3res31: Int = 1</code></pre><p>如何遍历Tuple中的元素<br>    注意：Tuple并没有提供一个foreach函数，我们使用productIterator</p><p>遍历分为两步：<br>1、使用 productIterator 生成一个迭代器<br>2、遍历</p><pre><code>scala&gt; t1.productIterator.!=                copyToBuffer   forall               min                reduceRightOption   toIterable##                corresponds    foreach              minBy              sameElements        toIterator+                 count          formatted            mkString           scanLeft            toList++                drop           getClass             ne                 scanRight           toMap-&gt;                dropWhile      grouped              next               seq                 toSeq/:                duplicate      hasDefiniteSize      nonEmpty           size                toSet:\                ensuring       hasNext              notify             slice               toStream==                eq             hashCode             notifyAll          sliding             toStringGroupedIterator   equals         indexOf              padTo              span                toTraversableaddString         exists         indexWhere           partition          sum                 toVectoraggregate         filter         isEmpty              patch              synchronized        waitasInstanceOf      filterNot      isInstanceOf         product            take                withFilterbuffered          find           isTraversableAgain   reduce             takeWhile           zipcollect           flatMap        length               reduceLeft         to                  zipAllcollectFirst      fold           map                  reduceLeftOption   toArray             zipWithIndexcontains          foldLeft       max                  reduceOption       toBuffer            →copyToArray       foldRight      maxBy                reduceRight        toIndexedSeqscala&gt; t1.productIterator.foreach(println)TomLily1</code></pre><h4 id="11、scala中的文件操作"><a href="#11、scala中的文件操作" class="headerlink" title="11、scala中的文件操作"></a>11、scala中的文件操作</h4><p>类似于java的IO<br>举例：<br>1、读取文件<br>2、读取二进制文件<br>3、从url中获取信息<br>4、写入文件<br>5、Scala中调用Java的类库</p><p><strong>代码练习</strong></p><pre><code>//读取文件中的行var source = fromFile(&quot;E:\\server.xml&quot;)/**  * 1、将整个文件作为字符串输出  *  * 2、将文件的每一行读入输出  */println(&quot;--------mkString------------------&quot;)//println(source.mkString)println(&quot;----------lines-------------------&quot;)//var lines = source.getLines()//lines.foreach(println)println(&quot;-----------读取字符----------------&quot;)//for (c &lt;- source) println(c)println(&quot;-----------读取字URL----------------&quot;)var source2 = fromURL(&quot;https://hsiehchou.com&quot;,&quot;UTF-8&quot;)println(source2.mkString)/**  * 注意:scala中并不支持直接读取二进制文件  *  * 通过调用java的InputStream来实现  */println(&quot;-----------读取二进制文件----------------&quot;)//var file = new File(&quot;E:\\hsiehchou.war&quot;)//构造一个inputstream//var in = new FileInputStream(file)//构造一个buffer// var buffer = new Array[Byte](file.length().toInt)//读取//in.read(buffer)//println(buffer.length)//关闭//in.close()/**  * 写文件  */println(&quot;---------Write File----------------&quot;)var out = new PrintWriter(&quot;E:\\insert.txt&quot;)for(i &lt;- 0 until 10)  out.println(i)out.close()</code></pre><h3 id="二、Scala面向对象"><a href="#二、Scala面向对象" class="headerlink" title="二、Scala面向对象"></a>二、Scala面向对象</h3><p>Scala是一个多范式的编程语言（支持多种方式的编程）<br>类似于Java 有区别</p><h4 id="1、面向对象的概念"><a href="#1、面向对象的概念" class="headerlink" title="1、面向对象的概念"></a>1、面向对象的概念</h4><p>1、封装 ： 把属性和操作属性的方法，写在了一起。class<br>2、继承<br>3、多态</p><p>Java中面向对象的概念，也是用与Scala</p><h4 id="2、定义类：class"><a href="#2、定义类：class" class="headerlink" title="2、定义类：class"></a>2、定义类：class</h4><p>举例：创建一个学生类</p><pre><code>package day3/**  * 学生  */class Student1 {  //定义学生的属性  private var stuId:Int =0  private var stuName:String = &quot;Time&quot;  private var age:Int = 20  //定义方法（函数）get set  def getStuName():String = stuName  def setStuName(newName:String) = this.stuName = newName  def getStuAge():Int = age  def setStuAge(age:Int) = this.age = age}/**  * 注意object 和 class名字可以不一样  *  * 如果一样的话，这个object就叫作class的伴生对象  */object Student1{  def main(args: Array[String]): Unit = {    //测试    //创建一个学生对象    var s1 = new Student1    //访问他的属性并输出    println(s1.getStuName()+&quot;\t&quot;+s1.getStuAge())    //访问set方法    s1.setStuName(&quot;Hsieh&quot;)    s1.setStuAge(23)    println(s1.getStuName()+&quot;\t&quot;+s1.getStuAge())    //直接访问私有属性    println(&quot;-------访问私有属性----------&quot;)    println(s1.stuId+&quot;\t&quot;+s1.stuName+&quot;\t&quot;+s1.age)    /**      * 为什么我们可以访问私有成员      *      * s1.stuId      *      * 属性的set get 方法      * 1、当一个属性是private属性的时候，scala会自动为其生成set get方法      *      * s1.stuId   .stuId调用了get方法  get 方法的名字就叫stuId      *      * 2、如果只希望生成get方法而不生成set方法，可以定义成常量      *      * 3、如果希望属性不能被外部访问，使用private[this]关键字      */  }}</code></pre><h4 id="3、内部类（嵌套类）"><a href="#3、内部类（嵌套类）" class="headerlink" title="3、内部类（嵌套类）"></a>3、内部类（嵌套类）</h4><p>在一个类的内部，定义了另外一个类</p><pre><code>package day3import scala.collection.mutable.ArrayBuffer/**  * 需求：定义一个学生类，同时要保存学生的成绩信息  */class Student2 {  //定义学生的属性  private var stuName : String = &quot;Time&quot;  private var stuAge : Int = 23  //定义一个数组，来保存学生的课程成绩信息  private var courseList = new ArrayBuffer[Course]()  //定义一个函数，用于添加学生课程成绩  def addNewCourse(cname:String, grade:Int): Unit = {    //创建课程成绩信息    var c = new Course(cname,grade)    //添加到学生    courseList += c  }  //定义课程类  class Course(var courseName:String, var grade : Int){  }}object Student2{  def main(args: Array[String]): Unit = {    //创建学生对象    var s = new Student2    //给学生添加课程信息    s.addNewCourse(&quot;Chinese&quot;,78)    s.addNewCourse(&quot;English&quot;,80)    s.addNewCourse(&quot;Math&quot;,90)    println(s.stuName+&quot;\t&quot;+s.stuAge)    println(&quot;-----------课程信息------------&quot;)    for(c&lt;-s.courseList) println(c.courseName+&quot;\t&quot;+c.grade)  }}</code></pre><h4 id="4、类的构造器：两种"><a href="#4、类的构造器：两种" class="headerlink" title="4、类的构造器：两种"></a>4、类的构造器：两种</h4><p>1）主构造器 ： 和类的声明在一起，并且一个类只能有一个主构造器<br>class Course(var courseName:String,var grade : Int)</p><p>2）辅助构造器 ： 一个类可以有多个辅助构造器，通过this来实现</p><h4 id="5、Object对象"><a href="#5、Object对象" class="headerlink" title="5、Object对象"></a>5、Object对象</h4><p>相当于Java中的static<br>1）Object 对象中的内容都是静态的<br>2）如果和类名相同，则成为伴生对象<br>3）Scala中没有static关键字<br>4）举例<br>（1）使用Object来实现单例模式：一个类里面只有一个对象<br>在Java中，把类的构造器定义成private的，并且提供一个getInstance,返回对象</p><p>在Scala中，使用Object实现 </p><p><strong>例子</strong></p><pre><code>package day3/**  * 实现单例模式  */object CreditCard {  //定义一个变量来保存信用卡卡号  private [this] var creditCardNumber : Long = 0  //定义一个函数产生卡号  def generateNum : Long = {    creditCardNumber += 1    creditCardNumber  }  def main(args: Array[String]): Unit = {    println(CreditCard.generateNum)    println(CreditCard.generateNum)    println(CreditCard.generateNum)    println(CreditCard.generateNum)  }}</code></pre><p>（2）使用App对象：应用程序对象<br>好处：可以省略main方法 </p><p><strong>例子</strong></p><pre><code>package day3object HelloWorld extends App{//  def main(args: Array[String]): Unit = {//    println(&quot;Hello World!&quot;)//  }  println(&quot;Hello World!&quot;)  if(args.length&gt;0){    println(&quot;有参数&quot;)  }else{    println(&quot;没有参数&quot;)  }}</code></pre><h4 id="6、apply方法"><a href="#6、apply方法" class="headerlink" title="6、apply方法"></a>6、apply方法</h4><p>val t1 = Tuple3(“Tom”,”Lily”,1)<br>没有new关键字，但是也创建出来对象，用了apply方法</p><pre><code>package day3class Student4(var stuName:String)/**  * 定义Student4的apply方法  */object Student4 {  def apply(name:String) = {    println(&quot;调用apply方法&quot;)    new Student4(name)  }  def main(args: Array[String]): Unit = {    //通过主构造器来创建学生对象    var s1 = new Student4(&quot;Time&quot;)    println(s1.stuName)    //通过apply方法来创建学生对象，省略new 关键字    var s2 = Student4(&quot;Hsiehchou&quot;)    println(s2.stuName)  }}</code></pre><p>注意：apply方法必须写在伴生对象中</p><h4 id="7、继承"><a href="#7、继承" class="headerlink" title="7、继承"></a>7、继承</h4><p>1）extends 和java一样<br>object HelloWorld extends App</p><pre><code>package day3/**  * extends 继承  *  * 父类：Person 人  * 子类：Employee员工  *///定义父类class Person(val name:String,val age:Int){  //定义函数  def sayHello():String = &quot;Hello &quot;+name+&quot; and the age is &quot;+age}//定义子类class Employee(override val name:String,override val age:Int,salary:Int) extends Person(name,age){  //重写父类中的函数  override def sayHello(): String = &quot;子类中的sayHello&quot;}object Demo1 extends App {  //创建Person  var p1 = new Person(&quot;Tim&quot;,23)  println(p1.name+&quot;\t&quot;+p1.age)  println(p1.sayHello())  //创建一个子类对象  var p2:Person = new Employee(&quot;Nike&quot;,35,1000)  println(p2.sayHello())  //匿名子类  var p3:Person = new Person(&quot;Jike&quot;,32){    //在匿名子类中重写sayHello方法    override def sayHello(): String = &quot;匿名子类中的sayHello&quot;  }  println(p3.sayHello())}</code></pre><p>2）抽象类</p><pre><code>package day3/**  * 抽象类：只能用于继承的类，可以包含抽象方法  *///父类：交通工具类abstract class Vehicle {  //定义抽象方法，没有实现的方法  def checkType():String}//子类：自行车、汽车class Car extends Vehicle{  def checkType : String = &quot;I am a car&quot;}class Bike extends Vehicle{  def checkType : String = &quot;I am a bike&quot;}object Demo2{  def main(args: Array[String]): Unit = {    //多态    var v1 : Vehicle = new Car    println(v1.checkType())    var v2 : Vehicle = new Bike    println(v2.checkType())  }}</code></pre><p>3）抽象字段</p><pre><code>package day3/**  * 抽象字段 抽象属性  *  * 定义：没有初始值的字段  */abstract class Person1{  //定义抽象字段  val id:Int  val name:String}//如果不加abstract 报错abstract class Employee1 extends Person1{}//下面两种方式均不会报错class Employee2() extends Person1{  val id:Int = 1  val name:String = &quot;Time&quot;}class Employee3(val id:Int,val name:String) extends Person1{}object Demo3 {}</code></pre><h4 id="8、特质（trait）"><a href="#8、特质（trait）" class="headerlink" title="8、特质（trait）"></a>8、特质（trait）</h4><p>抽象类，支持多重继承<br>本质：scala 的一个抽象类<br>trait</p><pre><code>package day4/**  * trait特质  *  * 定义两个父类，就是两个trait  *  * 父类：人、动作  *  * 子类：学生  */trait Human{  //抽象字段  val id:Int  val name:String}//动作trait Action{  //定义一个抽象函数  def getActionName():String}class Student1(val id:Int,val name:String) extends Human with Action{  override def getActionName(): String = &quot;Action is running&quot;  /**    * 实现多重继承的方式 extends Human with Action    */}object Demo1 {  def main(args: Array[String]): Unit = {    //创建一个学生对象    var s1 = new Student1(1,&quot;Time&quot;)    println(s1.id+&quot;\t&quot;+s1.name)    println(s1.getActionName())  }}</code></pre><p>关键字：extends Human with Action<br>`</p><h4 id="9、包和包对象"><a href="#9、包和包对象" class="headerlink" title="9、包和包对象"></a>9、包和包对象</h4><p>package<br>package object</p><p><strong>Scala中包的定义和使用</strong></p><p><strong>包的定义</strong> </p><p>1）首先是Scala中的包可以像Java一样使用，例如：</p><pre><code>package com.my.ioclass XXX</code></pre><p>2）可以像C#的namespace一样使用package语句，例如：</p><pre><code>package com.my.io{    class XXX}</code></pre><p>3）package也是可以嵌套的，例如：</p><pre><code>package com.my.io{    class XXX    package test{        class T    }}</code></pre><p><strong>包的引入</strong><br>Scala中依然使用import作为引用包的关键字，例如<br>import com.my.io.XXX //可以不写XXX的全路径<br>import com.my.io._ //引用import com.my.io下的所有类型<br>import com.my.io.XXX._ //引用import com.my.io.XXX的所有成员</p><p>而且<strong>Scala中的import可以写在任意地方</strong></p><pre><code>def method(fruit:Fruit){    import fruit._    println(name)}</code></pre><p><strong>包对象</strong><br>包可以包含类、对象和特质，但不能包含函数或者变量的定义。很不幸，这是Java虚拟机的局限</p><p>把工具函数或者常量添加到包而不是某个Utils对象，这是更加合理的做法。Scala中，包对象的出现正是为了解决这个局限</p><p>Scala中的包对象：常量，变量，方法，类，对象，trait（特质）</p><pre><code>package class4/**  * Scala中的包对象：常量、变量、方法、类、对象、trait（特质）  *///定义一个包对象package object MyPackageObject{    //常量    val x:Int = 0    //变量    var y:String = &quot;Hello World&quot;    //方法    def sayHelloWorld():String = &quot;Hello World&quot;    //类    class MyTestClass{    }    //对象object    object MyTestObject{    }    //trait（特质）    trait MyTestTrait{    }}class Demo3{    //测试    def method1() = {        //导入需要的包对象        import class4.MyPackageObject._        //定义MyTestClass的一个对象        var a = new MyTestClass    }   }</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ElasticSearch（二）</title>
      <link href="/2019/03/20/elasticsearch-er/"/>
      <url>/2019/03/20/elasticsearch-er/</url>
      
        <content type="html"><![CDATA[<h3 id="一、Java-API操作"><a href="#一、Java-API操作" class="headerlink" title="一、Java API操作"></a>一、Java API操作</h3><p>Elasticsearch的Java客户端非常强大；它可以建立一个嵌入式实例并在必要时运行管理任务</p><p>运行一个Java应用程序和Elasticsearch时，有两种操作模式可供使用。该应用程序可在Elasticsearch集群中扮演更加主动或更加被动的角色。在更加主动的情况下（称为Node Client），应用程序实例将从集群接收请求，确定哪个节点应处理该请求，就像正常节点所做的一样。（应用程序甚至可以托管索引和处理请求。）另一种模式称为Transport Client，它将所有请求都转发到另一个Elasticsearch节点，由后者来确定最终目标</p><h4 id="1-API基本操作"><a href="#1-API基本操作" class="headerlink" title="1. API基本操作"></a>1. API基本操作</h4><p><strong>1.1 操作环境准备</strong><br>1）创建maven工程<br>2）添加pom文件</p><pre><code>&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;junit&lt;/groupId&gt;        &lt;artifactId&gt;junit&lt;/artifactId&gt;        &lt;version&gt;4.10&lt;/version&gt;        &lt;scope&gt;test&lt;/scope&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;        &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;        &lt;version&gt;6.1.1&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;        &lt;artifactId&gt;transport&lt;/artifactId&gt;        &lt;version&gt;6.1.1&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;        &lt;artifactId&gt;log4j-core&lt;/artifactId&gt;        &lt;version&gt;2.9.0&lt;/version&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;</code></pre><p>3）等待依赖的jar包下载完成<br>当直接在ElasticSearch 建立文档对象时，如果索引不存在的，默认会自动创建，映射采用默认方式</p><p><strong>1.2 获取Transport Client</strong><br>（1）ElasticSearch服务默认端口9300<br>（2）Web管理平台端口9200</p><pre><code>private TransportClient client;@SuppressWarnings(&quot;unchecked&quot;)@Beforepublic void getClient() throws Exception {    // 1 设置连接的集群名称    Settings settings = Settings.builder().put(&quot;cluster.name&quot;, &quot;my-application&quot;).build();    // 2 连接集群    client = new PreBuiltTransportClient(settings);    client.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(&quot;hsiehchou121&quot;), 9300));    // 3 打印集群名称    System.out.println(client.toString());}</code></pre><p>（3）显示log4j2报错，在resource目录下创建一个文件命名为log4j2.xml并添加如下内容</p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;Configuration status=&quot;warn&quot;&gt;    &lt;Appenders&gt;        &lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt;            &lt;PatternLayout pattern=&quot;%m%n&quot;/&gt;        &lt;/Console&gt;    &lt;/Appenders&gt;    &lt;Loggers&gt;        &lt;Root level=&quot;INFO&quot;&gt;            &lt;AppenderRef ref=&quot;Console&quot;/&gt;        &lt;/Root&gt;    &lt;/Loggers&gt;&lt;/Configuration&gt;</code></pre><p><strong>1.3 创建索引</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void createIndex_blog(){    // 1 创建索引    client.admin().indices().prepareCreate(&quot;blog2&quot;).get();    // 2 关闭连接    client.close();}</code></pre><p><strong>1.4 删除索引</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void deleteIndex(){    // 1 删除索引    client.admin().indices().prepareDelete(&quot;blog2&quot;).get();    // 2 关闭连接    client.close();}</code></pre><p><strong>1.5 新建文档（源数据json串）</strong><br>当直接在ElasticSearch建立文档对象时，如果索引不存在的，默认会自动创建，映射采用默认方式<br><strong>源代码</strong></p><pre><code>@Testpublic void createIndexByJson() throws UnknownHostException {    // 1 文档数据准备    String json = &quot;{&quot; + &quot;\&quot;id\&quot;:\&quot;1\&quot;,&quot; + &quot;\&quot;title\&quot;:\&quot;基于Lucene的搜索服务器\&quot;,&quot;            + &quot;\&quot;content\&quot;:\&quot;它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口\&quot;&quot; + &quot;}&quot;;    // 2 创建文档    IndexResponse indexResponse = client.prepareIndex(&quot;blog&quot;, &quot;article&quot;, &quot;1&quot;).setSource(json).execute().actionGet();    // 3 打印返回的结果    System.out.println(&quot;index:&quot; + indexResponse.getIndex());    System.out.println(&quot;type:&quot; + indexResponse.getType());    System.out.println(&quot;id:&quot; + indexResponse.getId());    System.out.println(&quot;version:&quot; + indexResponse.getVersion());    System.out.println(&quot;result:&quot; + indexResponse.getResult());    // 4 关闭连接    client.close();}</code></pre><p><strong>1.6 新建文档（源数据map方式添加json）</strong><br><strong>源代码</strong> </p><pre><code>@Test public void createIndexByMap() {    // 1 文档数据准备    Map&lt;String, Object&gt; json = new HashMap&lt;String, Object&gt;();    json.put(&quot;id&quot;, &quot;2&quot;);    json.put(&quot;title&quot;, &quot;基于Lucene的搜索服务器&quot;);    json.put(&quot;content&quot;, &quot;它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口&quot;);    // 2 创建文档    IndexResponse indexResponse = client.prepareIndex(&quot;blog&quot;, &quot;article&quot;, &quot;2&quot;).setSource(json).execute().actionGet();    // 3 打印返回的结果    System.out.println(&quot;index:&quot; + indexResponse.getIndex());    System.out.println(&quot;type:&quot; + indexResponse.getType());    System.out.println(&quot;id:&quot; + indexResponse.getId());    System.out.println(&quot;version:&quot; + indexResponse.getVersion());    System.out.println(&quot;result:&quot; + indexResponse.getResult());    // 4 关闭连接    client.close();}</code></pre><p><strong>1.7 新建文档（源数据es构建器添加json）</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void createIndex() throws Exception {    // 1 通过es自带的帮助类，构建json数据    XContentBuilder builder = XContentFactory.jsonBuilder().startObject().field(&quot;id&quot;, 3).field(&quot;title&quot;, &quot;基于Lucene的搜索服务器&quot;).field(&quot;content&quot;, &quot;它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。&quot;)            .endObject();    // 2 创建文档    IndexResponse indexResponse = client.prepareIndex(&quot;blog&quot;, &quot;article&quot;, &quot;3&quot;).setSource(builder).get();    // 3 打印返回的结果    System.out.println(&quot;index:&quot; + indexResponse.getIndex());    System.out.println(&quot;type:&quot; + indexResponse.getType());    System.out.println(&quot;id:&quot; + indexResponse.getId());    System.out.println(&quot;version:&quot; + indexResponse.getVersion());    System.out.println(&quot;result:&quot; + indexResponse.getResult());    // 4 关闭连接    client.close();}</code></pre><p><strong>1.8 搜索文档数据（单个索引）</strong><br><strong>源代码</strong> </p><pre><code>@Test public void getData() throws Exception {    // 1 查询文档    GetResponse response = client.prepareGet(&quot;blog&quot;, &quot;article&quot;, &quot;1&quot;).get();    // 2 打印搜索的结果    System.out.println(response.getSourceAsString());    // 3 关闭连接    client.close();}</code></pre><p><strong>1.9 搜索文档数据（多个索引）</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void getMultiData() {    // 1 查询多个文档    MultiGetResponse response = client.prepareMultiGet().add(&quot;blog&quot;, &quot;article&quot;, &quot;1&quot;).add(&quot;blog&quot;, &quot;article&quot;, &quot;2&quot;, &quot;3&quot;).add(&quot;blog&quot;, &quot;article&quot;, &quot;2&quot;).get();    // 2 遍历返回的结果    for(MultiGetItemResponse itemResponse:response){        GetResponse getResponse = itemResponse.getResponse();        // 如果获取到查询结果        if (getResponse.isExists()) {            String sourceAsString = getResponse.getSourceAsString();            System.out.println(sourceAsString);        }    }    // 3 关闭资源    client.close();}</code></pre><p><strong>1.10 更新文档数据（update）</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void updateData() throws Throwable {    // 1 创建更新数据的请求对象    UpdateRequest updateRequest = new UpdateRequest();    updateRequest.index(&quot;blog&quot;);    updateRequest.type(&quot;article&quot;);    updateRequest.id(&quot;3&quot;);    updateRequest.doc(XContentFactory.jsonBuilder().startObject()            // 对没有的字段添加, 对已有的字段替换            .field(&quot;title&quot;, &quot;基于Lucene的搜索服务器&quot;)            .field(&quot;content&quot;,&quot;它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。大数据前景无限&quot;)            .field(&quot;createDate&quot;, &quot;2017-8-22&quot;).endObject());    // 2 获取更新后的值    UpdateResponse indexResponse = client.update(updateRequest).get();    // 3 打印返回的结果    System.out.println(&quot;index:&quot; + indexResponse.getIndex());    System.out.println(&quot;type:&quot; + indexResponse.getType());    System.out.println(&quot;id:&quot; + indexResponse.getId());    System.out.println(&quot;version:&quot; + indexResponse.getVersion());    System.out.println(&quot;create:&quot; + indexResponse.getResult());    // 4 关闭连接    client.close();}</code></pre><p><strong>1.11 更新文档数据（upsert）</strong><br>设置查询条件, 查找不到则添加IndexRequest内容，查找到则按照UpdateRequest更新</p><pre><code>@Testpublic void testUpsert() throws Exception {    // 设置查询条件, 查找不到则添加    IndexRequest indexRequest = new IndexRequest(&quot;blog&quot;, &quot;article&quot;, &quot;5&quot;)            .source(XContentFactory.jsonBuilder().startObject().field(&quot;title&quot;, &quot;搜索服务器&quot;).field(&quot;content&quot;,&quot;它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。&quot;).endObject());    // 设置更新, 查找到更新下面的设置    UpdateRequest upsert = new UpdateRequest(&quot;blog&quot;, &quot;article&quot;, &quot;5&quot;)            .doc(XContentFactory.jsonBuilder().startObject().field(&quot;user&quot;, &quot;李四&quot;).endObject()).upsert(indexRequest);    client.update(upsert).get();    client.close();}</code></pre><p><strong>1.12 删除文档数据（prepareDelete）</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void deleteData() {    // 1 删除文档数据    DeleteResponse indexResponse = client.prepareDelete(&quot;blog&quot;, &quot;article&quot;, &quot;5&quot;).get();    // 2 打印返回的结果    System.out.println(&quot;index:&quot; + indexResponse.getIndex());    System.out.println(&quot;type:&quot; + indexResponse.getType());    System.out.println(&quot;id:&quot; + indexResponse.getId());    System.out.println(&quot;version:&quot; + indexResponse.getVersion());    System.out.println(&quot;found:&quot; + indexResponse.getResult());    // 3 关闭连接    client.close();}</code></pre><h4 id="2-条件查询QueryBuilder"><a href="#2-条件查询QueryBuilder" class="headerlink" title="2. 条件查询QueryBuilder"></a>2. 条件查询QueryBuilder</h4><p><strong>2.1 查询所有（matchAllQuery）</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void matchAllQuery() {    // 1 执行查询    SearchResponse searchResponse = client.prepareSearch(&quot;blog&quot;).setTypes(&quot;article&quot;)            .setQuery(QueryBuilders.matchAllQuery()).get();    // 2 打印查询结果    SearchHits hits = searchResponse.getHits(); // 获取命中次数，查询结果有多少对象    System.out.println(&quot;查询结果有：&quot; + hits.getTotalHits() + &quot;条&quot;);    for (SearchHit hit : hits) {       System.out.println(hit.getSourceAsString());//打印出每条结果    }    // 3 关闭连接    client.close();}</code></pre><p><strong>2.2 对所有字段分词查询（queryStringQuery）</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void query() {    // 1 条件查询    SearchResponse searchResponse = client.prepareSearch(&quot;blog&quot;).setTypes(&quot;article&quot;)            .setQuery(QueryBuilders.queryStringQuery(&quot;全文&quot;)).get();    // 2 打印查询结果    SearchHits hits = searchResponse.getHits(); // 获取命中次数，查询结果有多少对象    System.out.println(&quot;查询结果有：&quot; + hits.getTotalHits() + &quot;条&quot;);    for (SearchHit hit : hits) {       System.out.println(hit.getSourceAsString());//打印出每条结果    }    // 3 关闭连接    client.close();}</code></pre><p><strong>2.3 通配符查询（wildcardQuery）</strong></p><p>：表示多个字符（0个或多个字符）<br>？：表示单个字符<br>源代码</p><pre><code>@Testpublic void wildcardQuery() {    // 1 通配符查询    SearchResponse searchResponse = client.prepareSearch(&quot;blog&quot;).setTypes(&quot;article&quot;)            .setQuery(QueryBuilders.wildcardQuery(&quot;content&quot;, &quot;*全*&quot;)).get();    // 2 打印查询结果    SearchHits hits = searchResponse.getHits(); // 获取命中次数，查询结果有多少对象    System.out.println(&quot;查询结果有：&quot; + hits.getTotalHits() + &quot;条&quot;);    for (SearchHit hit : hits) {       System.out.println(hit.getSourceAsString());//打印出每条结果    }    // 3 关闭连接    client.close();}</code></pre><p><strong>2.4 词条查询（TermQuery）</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void termQuery() {    // 1 第一field查询    SearchResponse searchResponse = client.prepareSearch(&quot;blog&quot;).setTypes(&quot;article&quot;)            .setQuery(QueryBuilders.termQuery(&quot;content&quot;, &quot;全文&quot;)).get();    // 2 打印查询结果    SearchHits hits = searchResponse.getHits(); // 获取命中次数，查询结果有多少对象    System.out.println(&quot;查询结果有：&quot; + hits.getTotalHits() + &quot;条&quot;);    for (SearchHit hit : hits) {       System.out.println(hit.getSourceAsString());//打印出每条结果    }    // 3 关闭连接    client.close();}</code></pre><p><strong>2.5 模糊查询（fuzzy）</strong><br><strong>源代码</strong></p><pre><code>@Testpublic void fuzzy() {    // 1 模糊查询    SearchResponse searchResponse = client.prepareSearch(&quot;blog&quot;).setTypes(&quot;article&quot;)            .setQuery(QueryBuilders.fuzzyQuery(&quot;title&quot;, &quot;lucene&quot;)).get();    // 2 打印查询结果    SearchHits hits = searchResponse.getHits(); // 获取命中次数，查询结果有多少对象    System.out.println(&quot;查询结果有：&quot; + hits.getTotalHits() + &quot;条&quot;);    Iterator&lt;SearchHit&gt; iterator = hits.iterator();    while (iterator.hasNext()) {        SearchHit searchHit = iterator.next(); // 每个查询对象        System.out.println(searchHit.getSourceAsString()); // 获取字符串格式打印    }    // 3 关闭连接    client.close();}</code></pre><h4 id="3-映射相关操作"><a href="#3-映射相关操作" class="headerlink" title="3. 映射相关操作"></a>3. 映射相关操作</h4><p><strong>源代码</strong></p><pre><code>@Testpublic void createMapping() throws Exception {    // 1设置mapping    XContentBuilder builder = XContentFactory.jsonBuilder()            .startObject()                .startObject(&quot;article&quot;)                    .startObject(&quot;properties&quot;)                        .startObject(&quot;id1&quot;)                            .field(&quot;type&quot;, &quot;string&quot;)                            .field(&quot;store&quot;, &quot;yes&quot;)                        .endObject()                        .startObject(&quot;title2&quot;)                            .field(&quot;type&quot;, &quot;string&quot;)                            .field(&quot;store&quot;, &quot;no&quot;)                        .endObject()                        .startObject(&quot;content&quot;)                            .field(&quot;type&quot;, &quot;string&quot;)                            .field(&quot;store&quot;, &quot;yes&quot;)                        .endObject()                    .endObject()                .endObject()            .endObject();    // 2 添加mapping    PutMappingRequest mapping = Requests.putMappingRequest(&quot;blog4&quot;).type(&quot;article&quot;).source(builder);    client.admin().indices().putMapping(mapping).get();    // 3 关闭资源    client.close();}</code></pre><h3 id="二、IK分词器"><a href="#二、IK分词器" class="headerlink" title="二、IK分词器"></a>二、IK分词器</h3><p>针对词条查询（TermQuery）,查看默认中文分词器的效果:<br>curl -XGET ‘<a href="http://hsiehchou:9200/_analyze?pretty&amp;analyzer=standard’" target="_blank" rel="noopener">http://hsiehchou:9200/_analyze?pretty&amp;analyzer=standard’</a> -d ‘中华人民共和国’ </p><pre><code>{     “tokens” : [         {             “token” : “中”,             “start_offset” : 0,             “end_offset” : 1,             “type” : “”,             “position” : 0         },         {             “token” : “华”,             “start_offset” : 1,             “end_offset” : 2,             “type” : “”,             “position” : 1         },         {             “token” : “人”,             “start_offset” : 2,             “end_offset” : 3,             “type” : “”,             “position” : 2         },         {             “token” : “民”,             “start_offset” : 3,             “end_offset” : 4,             “type” : “”,             “position” : 3         },         {             “token” : “共”,             “start_offset” : 4,             “end_offset” : 5,             “type” : “”,             “position” : 4         },         {             “token” : “和”,             “start_offset” : 5,             “end_offset” : 6,             “type” : “”,             “position” : 5         },         {             “token” : “国”,             “start_offset” : 6,             “end_offset” : 7,             “type” : “”,             “position” : 6         }     ] }</code></pre><h4 id="1-IK分词器的安装"><a href="#1-IK分词器的安装" class="headerlink" title="1. IK分词器的安装"></a>1. IK分词器的安装</h4><p><strong>1.1 前期准备工作</strong><br>1）CentOS联网<br>配置CentOS能连接外网。Linux虚拟机ping <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a> 是畅通的</p><p>2）jar包准备<br>（1）elasticsearch-analysis-ik-master.zip<br>(下载地址:<a href="https://github.com/medcl/elasticsearch-analysis-ik" target="_blank" rel="noopener">https://github.com/medcl/elasticsearch-analysis-ik</a>)<br>（2）apache-maven-3.6.0-bin.tar.gz</p><p><strong>1.2 jar包安装</strong><br>1）Maven解压、配置 MAVEN_HOME和PATH。<br>tar -zxvf apache-maven-3.6.0-bin.tar.gz -C /opt/module/<br>sudo vi /etc/profile</p><p><code>#MAVEN_HOME</code><br>export MAVEN_HOME=/opt/module/apache-maven-3.6.0<br>export PATH=<code>$PATH:$MAVEN_HOME/bin</code><br>source /etc/profile<br>验证命令：mvn -version</p><p>2）Ik分词器解压、打包与配置<br><strong>ik分词器解压</strong><br>unzip elasticsearch-analysis-ik-master.zip -d ./<br>进入ik分词器所在目录</p><p>cd elasticsearch-analysis-ik-master<br>使用maven进行打包</p><p>mvn package -Pdist,native -DskipTests -Dtar<br>打包完成之后，会出现 target/releases/elasticsearch-analysis-ik-{version}.zip</p><p>pwd /opt/software/elasticsearch-analysis-ik-master/target/releases<br>对zip文件进行解压，并将解压完成之后的文件拷贝到es所在目录下的/plugins/</p><p>unzip elasticsearch-analysis-ik-6.0.0.zip<br>cp -r elasticsearch /opt/module/elasticsearch-5.6.1/plugins/</p><p>需要修改plugin-descriptor.properties文件，将其中的es版本号改为你所使用的版本号，即完成ik分词器的安装<br>vi plugin-descriptor.properties<br>修改为<br>elasticsearch.version=6.1.1<br>至此，安装完成，重启ES！</p><p>注意：需选择与es相同版本的ik分词器。<br>安装方法（2种）： </p><ol><li><p>./elasticsearch-plugin install <a href="https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.1.1/elasticsearch-analysis-ik-6.1.1.zip" target="_blank" rel="noopener">https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.1.1/elasticsearch-analysis-ik-6.1.1.zip</a></p></li><li><p>cp elasticsearch-analysis-ik-6.1.1.zip ./elasticsearch-6.1.1/plugins/<br>unzip elasticsearch-analysis-ik-6.1.1.zip -d ik-analyzer<br>elasticsearch-plugin install -f file:///usr/local/elasticsearch-analysis-ik-6.1.1.zip</p></li></ol><h4 id="2-IK分词器的使用"><a href="#2-IK分词器的使用" class="headerlink" title="2. IK分词器的使用"></a>2. IK分词器的使用</h4><p><strong>2.1 命令行查看结果</strong><br><strong>ik_smart模式</strong><br>curl -XGET ‘<a href="http://hsiehchou121:9200/_analyze?pretty&amp;analyzer=ik_smart’" target="_blank" rel="noopener">http://hsiehchou121:9200/_analyze?pretty&amp;analyzer=ik_smart’</a> -d ‘中华人民共和国’</p><p>curl -H “Content-Type:application/json” -XGET ‘<a href="http://192.168.116.121:9200/_analyze?pretty’" target="_blank" rel="noopener">http://192.168.116.121:9200/_analyze?pretty’</a> -d ‘{“analyzer”:”ik_smasysctl -prt”,”text”:”中华人民共和国”}’ </p><pre><code>{     “tokens” : [         {             “token” : “中华人民共和国”,             “start_offset” : 0,             “end_offset” : 7,             “type” : “CN_WORD”,             “position” : 0         }     ] }</code></pre><p><strong>ik_max_word模式</strong><br>curl -XGET ‘<a href="http://hadoop121:9200/_analyze?pretty&amp;analyzer=ik_max_word’" target="_blank" rel="noopener">http://hadoop121:9200/_analyze?pretty&amp;analyzer=ik_max_word’</a> -d ‘中华人民共和国’</p><p>curl -H “Content-Type:application/json” -XGET ‘<a href="http://192.168.116.124:9200/_analyze?pretty’" target="_blank" rel="noopener">http://192.168.116.124:9200/_analyze?pretty’</a> -d ‘{“analyzer”:”ik_max_word”,”text”:”中华人民共和国”}’</p><pre><code>{     “tokens” : [         {             “token” : “中华人民共和国”,             “start_offset” : 0,             “end_offset” : 7,             “type” : “CN_WORD”,             “position” : 0         },         {             “token” : “中华人民”,             “start_offset” : 0,             “end_offset” : 4,             “type” : “CN_WORD”,             “position” : 1         },         {             “token” : “中华”,             “start_offset” : 0,             “end_offset” : 2,             “type” : “CN_WORD”,             “position” : 2         },         {             “token” : “华人”,             “start_offset” : 1,             “end_offset” : 3,             “type” : “CN_WORD”,             “position” : 3         },         {             “token” : “人民共和国”,             “start_offset” : 2,             “end_offset” : 7,             “type” : “CN_WORD”,             “position” : 4         },         {             “token” : “人民”,             “start_offset” : 2,             “end_offset” : 4,             “type” : “CN_WORD”,             “position” : 5         },         {             “token” : “共和国”,             “start_offset” : 4,             “end_offset” : 7,             “type” : “CN_WORD”,             “position” : 6         },         {             “token” : “共和”,             “start_offset” : 4,             “end_offset” : 6,             “type” : “CN_WORD”,             “position” : 7         },         {             “token” : “国”,             “start_offset” : 6,             “end_offset” : 7,             “type” : “CN_CHAR”,             “position” : 8         }     ] }</code></pre><p><strong>2.2 JavaAPI操作</strong><br>1）创建索引<br>//创建索引(数据库)</p><pre><code>@Testpublic void createIndex() {    //创建索引    client.admin().indices().prepareCreate(&quot;blog4&quot;).get();    //关闭资源    client.close();}</code></pre><p>2）创建mapping<br>//创建使用ik分词器的mapping</p><pre><code>@Testpublic void createMapping() throws Exception {    // 1设置mapping    XContentBuilder builder = XContentFactory.jsonBuilder()            .startObject()                .startObject(&quot;article&quot;)                    .startObject(&quot;properties&quot;)                    .startObject(&quot;id1&quot;)                        .field(&quot;type&quot;, &quot;string&quot;)                        .field(&quot;store&quot;, &quot;yes&quot;)                        .field(&quot;analyzer&quot;,&quot;ik_smart&quot;)                    .endObject()                    .startObject(&quot;title2&quot;)                        .field(&quot;type&quot;, &quot;string&quot;)                        .field(&quot;store&quot;, &quot;no&quot;)                        .field(&quot;analyzer&quot;,&quot;ik_smart&quot;)                    .endObject()                    .startObject(&quot;content&quot;)                        .field(&quot;type&quot;, &quot;string&quot;)                        .field(&quot;store&quot;, &quot;yes&quot;)                        .field(&quot;analyzer&quot;,&quot;ik_smart&quot;)                    .endObject()                    .endObject()                .endObject()            .endObject();    // 2 添加mapping    PutMappingRequest mapping = Requests.putMappingRequest(&quot;blog4&quot;).type(&quot;article&quot;).source(builder);    client.admin().indices().putMapping(mapping).get();    // 3 关闭资源    client.close();}</code></pre><p>3）插入数据<br>//创建文档,以map形式</p><pre><code>@Testpublic void createDocumentByMap() {    HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;();    map.put(&quot;id1&quot;, &quot;2&quot;);    map.put(&quot;title2&quot;, &quot;Lucene&quot;);    map.put(&quot;content&quot;, &quot;它提供了一个分布式的web接口&quot;);    IndexResponse response = client.prepareIndex(&quot;blog4&quot;, &quot;article&quot;, &quot;3&quot;).setSource(map).execute().actionGet();    //打印返回的结果    System.out.println(&quot;结果:&quot; + response.getResult());    System.out.println(&quot;id:&quot; + response.getId());    System.out.println(&quot;index:&quot; + response.getIndex());    System.out.println(&quot;type:&quot; + response.getType());    System.out.println(&quot;版本:&quot; + response.getVersion());    //关闭资源    client.close();}</code></pre><p>4） 词条查询<br>//词条查询</p><pre><code>@Testpublic void queryTerm() {    SearchResponse response = client.prepareSearch(&quot;blog4&quot;).setTypes(&quot;article&quot;).setQuery(QueryBuilders.termQuery(&quot;content&quot;,&quot;提供&quot;)).get();    //获取查询命中结果    SearchHits hits = response.getHits();    System.out.println(&quot;结果条数:&quot; + hits.getTotalHits());    for (SearchHit hit : hits) {        System.out.println(hit.getSourceAsString());    }}</code></pre><p><strong>Store 的解释</strong><br>官方文档说 store 默认是 no ，想当然的理解为也就是说这个 field 是不会 store 的，但是查询的时候也能查询出来</p><p>经过查找资料了解到原来 store 的意思是，是否在 _source 之外在独立存储一份。这里要说一下 _source 这是源文档，当索引数据的时候， elasticsearch 会保存一份源文档到 _source 。如果文档的某一字段设置了 store 为 yes (默认为 no)，这时候会在 _source 存储之外再为这个字段独立进行存储，这么做的目的主要是针对内容比较多的字段</p><p>如果放到 _source 返回的话，因为_source 是把所有字段保存为一份文档，命中后读取只需要一次 IO，包含内容特别多的字段会很占带宽影响性能。通常我们也不需要完整的内容返回(可能只关心摘要)，这时候就没必要放到 _source 里一起返回了(当然也可以在查询时指定返回字段)</p><h3 id="三、Logstash"><a href="#三、Logstash" class="headerlink" title="三、Logstash"></a>三、Logstash</h3><h4 id="1-Logstash简介"><a href="#1-Logstash简介" class="headerlink" title="1. Logstash简介"></a>1. Logstash简介</h4><p>Logstash is a tool for managing events and logs. You can use it to collect logs, parse them, and store them for later use (like, for searching).</p><p>logstash是一个数据分析软件，主要目的是分析log日志。整一套软件可以当作一个MVC模型，logstash是controller层，Elasticsearch是一个model层，kibana是view层</p><p>首先将数据传给logstash，它将数据进行过滤和格式化（转成JSON格式），然后传给Elasticsearch进行存储、建搜索的索引，kibana提供前端的页面再进行搜索和图表可视化，它是调用Elasticsearch的接口返回的数据进行可视化。logstash和Elasticsearch是用Java写的，kibana使用node.js框架</p><p>这个软件官网有很详细的使用说明，<a href="https://www.elastic.co/，除了docs之外，还有视频教程。这篇博客集合了docs和视频里面一些比较重要的设置和使用" target="_blank" rel="noopener">https://www.elastic.co/，除了docs之外，还有视频教程。这篇博客集合了docs和视频里面一些比较重要的设置和使用</a></p><h4 id="2-Logstash-安装"><a href="#2-Logstash-安装" class="headerlink" title="2. Logstash 安装"></a>2. Logstash 安装</h4><p>直接下载官方发布的二进制包的，可以访问 <a href="https://www.elastic.co/downloads/logstash" target="_blank" rel="noopener">https://www.elastic.co/downloads/logstash</a> 页面找对应操作系统和版本，点击下载即可</p><p>在终端中，像下面这样运行命令来启动 Logstash 进程：<br>输入（读取数据）：file、es。 输出：file、es、kafka</p><p>bin/logstash -e ‘input{stdin{}}output{stdout{codec=&gt;rubydebug}}’<br>-f文件 -e命令 标准输入、输出（命令行）</p><p>注意：如果出现如下报错，请调高虚拟机内存容量<br>Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000c5330000, 986513408, 0) failed; error=’Cannot allocate memory’ (errno=12)</p><p>然后你会发现终端在等待你的输入。没问题，敲入 Hello World，回车，</p><pre><code>{     “@version” =&gt; “1”,     “host” =&gt; “*“,     “message” =&gt; “hello world”,     “@timestamp” =&gt; 2019-03-18T02:51:18.578Z }</code></pre><p>每位系统管理员都肯定写过很多类似这样的命令<br>cat randdata | awk ‘{print $2}’ | sort | uniq -c | tee sortdata</p><p>Logstash 就像管道符一样！<br>你输入(就像命令行的 cat )数据，然后处理过滤(就像 awk 或者 uniq 之类)数据，最后输出(就像 tee )到其他地方</p><h4 id="3-Logstash-配置"><a href="#3-Logstash-配置" class="headerlink" title="3. Logstash 配置"></a>3. Logstash 配置</h4><p><strong>3.1 input配置</strong><br>读取文件(File)</p><pre><code>input {    file {        path =&gt; [&quot;/var/log/*.log&quot;, &quot;/var/log/message&quot;]        type =&gt; &quot;system&quot;        start_position =&gt; &quot;beginning&quot;    }}output{stdout{codec=&gt;rubydebug}}</code></pre><p>有一些比较有用的配置项，可以用来指定 FileWatch 库的行为</p><p><strong>discover_interval</strong><br>logstash 每隔多久去检查一次被监听的 path 下是否有新文件。默认值是 15 秒</p><p><strong>exclude</strong><br>不想被监听的文件可以排除出去，这里跟 path 一样支持 glob 展开</p><p><strong>close_older</strong><br>一个已经监听中的文件，如果超过这个值的时间内没有更新内容，就关闭监听它的文件句柄。默认是 3600 秒，即一小时</p><p><strong>ignore_older</strong><br>在每次检查文件列表的时候，如果一个文件的最后修改时间超过这个值，就忽略这个文件。默认是 86400 秒，即一天</p><p><strong>sincedb_path</strong><br>如果你不想用默认的 $HOME/.sincedb(Windows 平台上在 C:\Windows\System32\config\systemprofile.sincedb)，可以通过这个配置定义 sincedb 文件到其他位置</p><p><strong>sincedb_write_interval</strong><br>logstash 每隔多久写一次 sincedb 文件，默认是 15 秒</p><p><strong>stat_interval</strong><br>logstash 每隔多久检查一次被监听文件状态（是否有更新），默认是 1 秒</p><p><strong>start_position</strong><br>logstash 从什么位置开始读取文件数据，默认是结束位置，也就是说 logstash 进程会以类似 tail -F 的形式运行。如果你是要导入原有数据，把这个设定改成 “beginning”，logstash 进程就从头开始读取，类似 less +F 的形式运行</p><p><strong>启动命令</strong>：../bin/logstash -f ./input_file.conf<br><strong>测试命令</strong>：echo ‘hehe’ &gt;&gt; test.log<br>echo ‘hehe2’ &gt;&gt; message</p><p><strong>标准输入(Stdin)</strong><br>我们已经见过好几个示例使用 stdin 了。这也应该是 logstash 里最简单和基础的插件了</p><pre><code>input {    stdin {        add_field =&gt; {&quot;key&quot; =&gt; &quot;value&quot;}        codec =&gt; &quot;plain&quot;        tags =&gt; [&quot;add&quot;]        type =&gt; &quot;std&quot;    }}output{stdout{codec=&gt;rubydebug}}</code></pre><p>用上面的新 stdin 设置重新运行一次最开始的 hello world 示例。我建议大家把整段配置都写入一个文本文件，然后运行命令：../bin/logstash -f ./input_stdin.conf。输入 “hello world” 并回车后，你会在终端看到如下输出</p><pre><code>{       &quot;message&quot; =&gt; &quot;hello world&quot;,      &quot;@version&quot; =&gt; &quot;1&quot;,    &quot;@timestamp&quot; =&gt; &quot;2014-08-08T06:48:47.789Z&quot;,          &quot;type&quot; =&gt; &quot;std&quot;,          &quot;tags&quot; =&gt; [        [0] &quot;add&quot;    ],           &quot;key&quot; =&gt; &quot;value&quot;,          &quot;host&quot; =&gt; &quot;raochenlindeMacBook-Air.local&quot;}</code></pre><p><strong>解释</strong><br>type 和 tags 是 logstash 事件中两个特殊的字段。通常来说我们会在输入区段中通过 type 来标记事件类型。而 tags 则是在数据处理过程中，由具体的插件来添加或者删除的</p><p>最常见的用法是像下面这样</p><pre><code>input {    stdin {        type =&gt; &quot;web&quot;    }}filter {    if [type] == &quot;web&quot; {        grok {            match =&gt; [&quot;message&quot;, %{COMBINEDAPACHELOG}]        }    }}output {    if &quot;_grokparsefailure&quot; in [tags] {        nagios_nsca {            nagios_status =&gt; &quot;1&quot;        }    } else {        elasticsearch {        }    }}</code></pre><p><strong>3.2 codec配置</strong><br>Codec 是 logstash 从 1.3.0 版开始新引入的概念(Codec 来自 Coder/decoder 两个单词的首字母缩写)</p><p>在此之前，logstash 只支持纯文本形式输入，然后以过滤器处理它。但现在，我们可以在输入期处理不同类型的数据，这全是因为有了 codec 设置</p><p>所以，这里需要纠正之前的一个概念。Logstash 不只是一个input | filter | output 的数据流，而是一个 input | decode | filter | encode | output 的数据流！codec 就是用来 decode、encode 事件的</p><p>codec 的引入，使得 logstash 可以更好更方便的与其他有自定义数据格式的运维产品共存，比如 graphite、fluent、netflow、collectd，以及使用 msgpack、json、edn 等通用数据格式的其他产品等</p><p>事实上，我们在第一个 “hello world” 用例中就已经用过 codec 了 —— rubydebug 就是一种 codec！虽然它一般只会用在 stdout 插件中，作为配置测试或者调试的工具</p><p><strong>采用 JSON 编码</strong><br>在早期的版本中，有一种降低 logstash 过滤器的 CPU 负载消耗的做法盛行于社区(在当时的 cookbook 上有专门的一节介绍)：直接输入预定义好的 JSON 数据，这样就可以省略掉 filter/grok 配置！</p><p>这个建议依然有效，不过在当前版本中需要稍微做一点配置变动 —— 因为现在有专门的 codec 设置</p><p><strong>配置示例</strong></p><pre><code>input {    stdin {        add_field =&gt; {&quot;key&quot; =&gt; &quot;value&quot;}        codec =&gt; &quot;json&quot;        type =&gt; &quot;std&quot;    }}output{stdout{codec=&gt;rubydebug}}</code></pre><p>输入：<br>{“simCar”:18074045598,”validityPeriod”:”1996-12-06”,”unitPrice”:9,”quantity”:19,”amount”:35,”imei”:887540376467915,”user”:”test”}</p><p>运行结果：<br>{<br>“imei” =&gt; 887540376467915,<br>“unitPrice” =&gt; 9,<br>“user” =&gt; “test”,<br>“@timestamp” =&gt; 2019-03-19T05:01:53.451Z,<br>“simCar” =&gt; 18074045598,<br>“host” =&gt; “zzc-203”,<br>“amount” =&gt; 35,<br>“@version” =&gt; “1”,<br>“key” =&gt; “value”,<br>“type” =&gt; “std”,<br>“validityPeriod” =&gt; “1996-12-06”,<br>“quantity” =&gt; 19<br>}</p><p><strong>3.3 filter配置</strong><br><strong>Grok插件</strong></p><p>logstash拥有丰富的filter插件,它们扩展了进入过滤器的原始数据，进行复杂的逻辑处理，甚至可以无中生有的添加新的 logstash 事件到后续的流程中去！Grok 是 Logstash 最重要的插件之一。也是迄今为止使蹩脚的、无结构的日志结构化和可查询的最好方式。Grok在解析 syslog logs、apache and other webserver logs、mysql logs等任意格式的文件上表现完美</p><p>这个工具非常适用于系统日志，Apache和其他网络服务器日志，MySQL日志等</p><p><strong>配置</strong></p><pre><code>input {    stdin {        type =&gt; &quot;std&quot;    }}filter {  grok {    match=&gt;{&quot;message&quot;=&gt; &quot;%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}&quot; }  }}output{stdout{codec=&gt;rubydebug}}</code></pre><p>输入：55.3.244.1 GET /index.html 15824 0.043<br>输出：<br>{<br>“@version” =&gt; “1”,<br>“host” =&gt; “zzc-203”,<br>“request” =&gt; “/index.html”,<br>“bytes” =&gt; “15824”,<br>“duration” =&gt; “0.043”,<br>“method” =&gt; “GET”,<br>“@timestamp” =&gt; 2019-03-19T05:09:55.777Z,<br>“message” =&gt; “55.3.244.1 GET /index.html 15824 0.043”,<br>“type” =&gt; “std”,<br>“client” =&gt; “55.3.244.1”<br>}</p><p>grok模式的语法如下：<br>%{SYNTAX:SEMANTIC}</p><p>SYNTAX：代表匹配值的类型,例如3.44可以用NUMBER类型所匹配,127.0.0.1可以使用IP类型匹配。<br>SEMANTIC：代表存储该值的一个变量名称,例如 3.44 可能是一个事件的持续时间,127.0.0.1可能是请求的client地址。所以这两个值可以用 %{NUMBER:duration} %{IP:client} 来匹配</p><p>你也可以选择将数据类型转换添加到Grok模式。默认情况下，所有语义都保存为字符串。如果您希望转换语义的数据类型，例如将字符串更改为整数，则将其后缀为目标数据类型。例如%{NUMBER:num:int}将num语义从一个字符串转换为一个整数。目前唯一支持的转换是int和float</p><p>Logstash附带约120个模式。你可以在这里找到它们<a href="https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns" target="_blank" rel="noopener">https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns</a></p><p><strong>自定义类型</strong><br>更多时候logstash grok没办法提供你所需要的匹配类型，这个时候我们可以使用自定义</p><p>创建自定义 patterns 文件<br>①创建一个名为patterns其中创建一个文件postfix （文件名无关紧要,随便起）,在该文件中，将需要的模式写为模式名称，空格，然后是该模式的正则表达式。例如：</p><p>POSTFIX_QUEUEID [0-9A-F]{10,11}</p><p>②然后使用这个插件中的patterns_dir设置告诉logstash目录是你的自定义模式。</p><p><strong>配置</strong></p><pre><code>input {    stdin {        type =&gt; &quot;std&quot;    }}filter {  grok {    patterns_dir =&gt; [&quot;./patterns&quot;]    match =&gt; { &quot;message&quot; =&gt; &quot;%{SYSLOGBASE} %{POSTFIX_QUEUEID:queue_id}: %{GREEDYDATA:syslog_message}&quot; }  }}output{stdout{codec=&gt;rubydebug}}</code></pre><p>输入：<br>Jan 1 06:25:43 mailserver14 postfix/cleanup[21403]: BEF25A72965: message-id=&lt;20130101142543.5828399CCAF@mailserver1</p><p>输出：<br>{<br>“queue_id” =&gt; “BEF25A72965”,<br>“message” =&gt; “Jan 1 06:25:43 mailserver14 postfix/cleanup[21403]: BEF25A72965: message-id=&lt;20130101142543.5828399CCAF@mailserver1”,<br>“pid” =&gt; “21403”,<br>“program” =&gt; “postfix/cleanup”,<br>“@version” =&gt; “1”,<br>“type” =&gt; “std”,<br>“logsource” =&gt; “mailserver14”,<br>“host” =&gt; “zzc-203”,<br>“timestamp” =&gt; “Jan 1 06:25:43”,<br>“syslog_message” =&gt; “message-id=&lt;20130101142543.5828399CCAF@mailserver1”,<br>“@timestamp” =&gt; 2019-03-19T05:31:37.405Z<br>}</p><p><strong>GeoIP 地址查询归类</strong><br>GeoIP 是最常见的免费 IP 地址归类查询库，同时也有收费版可以采购。GeoIP 库可以根据 IP 地址提供对应的地域信息，包括国别，省市，经纬度等，对于可视化地图和区域统计非常有用。</p><p><strong>配置</strong></p><pre><code>input {    stdin {        type =&gt; &quot;std&quot;    }}filter {    geoip {        source =&gt; &quot;message&quot;    }}output{stdout{codec=&gt;rubydebug}}</code></pre><p>输入：183.60.92.253<br>输出：<br>{<br>“type” =&gt; “std”,<br>“@version” =&gt; “1”,<br>“@timestamp” =&gt; 2019-03-19T05:39:26.714Z,<br>“host” =&gt; “zzc-203”,<br>“message” =&gt; “183.60.92.253”,<br>“geoip” =&gt; {<br>“country_code3” =&gt; “CN”,<br>“latitude” =&gt; 23.1167,<br>“region_code” =&gt; “44”,<br>“region_name” =&gt; “Guangdong”,<br>“location” =&gt; {<br>“lon” =&gt; 113.25,<br>“lat” =&gt; 23.1167<br>},<br>“city_name” =&gt; “Guangzhou”,<br>“country_name” =&gt; “China”,<br>“continent_code” =&gt; “AS”,<br>“country_code2” =&gt; “CN”,<br>“timezone” =&gt; “Asia/Shanghai”,<br>“ip” =&gt; “183.60.92.253”,<br>“longitude” =&gt; 113.25<br>}<br>}</p><p><strong>3.4 output配置</strong><br>标准输出(Stdout)</p><p>保存成文件(File)<br>通过日志收集系统将分散在数百台服务器上的数据集中存储在某中心服务器上，这是运维最原始的需求。Logstash 当然也能做到这点</p><p>和 LogStash::Inputs::File 不同, LogStash::Outputs::File 里可以使用 sprintf format 格式来自动定义输出到带日期命名的路径</p><p><strong>配置</strong></p><pre><code>input {    stdin {        type =&gt; &quot;std&quot;    }}output {    file {        path =&gt; &quot;../data_test/%{+yyyy}/%{+MM}/%{+dd}/%{host}.log&quot;        codec =&gt; line { format =&gt; &quot;custom format: %{message}&quot;}    }}</code></pre><p>启动后输入，可看到文件</p><p>服务器间传输文件(File)</p><p><strong>配置</strong><br>接收日志服务器配置</p><pre><code>input {  tcp {    mode =&gt; &quot;server&quot;    port =&gt; 9600    ssl_enable =&gt; false  }}filter {    json {        source =&gt; &quot;message&quot;    }}output {    file {        path =&gt; &quot;/usr/local/logstash-6.6.2/data_test/%{+YYYY-MM-dd}/%{servip}-%{filename}&quot;        codec =&gt; line { format =&gt; &quot;%{message}&quot;}    }}</code></pre><p>发送日志服务器配置</p><pre><code>input{    file {        path =&gt; [&quot;/usr/local/logstash-6.6.2/data_test/send.log&quot;]        type =&gt; &quot;ecolog&quot;        start_position =&gt; &quot;beginning&quot;    }}filter {    if [type] =~ /^ecolog/ {        ruby {            code =&gt; &quot;file_name = event.get(&#39;path&#39;).split(&#39;/&#39;)[-1]                     event.set(&#39;file_name&#39;,file_name)                     event.set(&#39;servip&#39;,&#39;接收方ip&#39;)&quot;        }        mutate {            rename =&gt; {&quot;file_name&quot; =&gt; &quot;filename&quot;}        }    }}output {    tcp {        host  =&gt; &quot;接收方ip&quot;        port  =&gt; 9600        codec =&gt; json_lines    }}</code></pre><p>从发送方发送message，接收方可以看到写出文件</p><p>写入到ES<br><strong>配置</strong></p><pre><code>input {    stdin {        type =&gt; &quot;log2es&quot;    }}output {    elasticsearch {        hosts =&gt; [&quot;192.168.109.133:9200&quot;]        index =&gt; &quot;logstash-%{type}-%{+YYYY.MM.dd}&quot;        document_type =&gt; &quot;%{type}&quot;        sniffing =&gt; true        template_overwrite =&gt; true    }}</code></pre><p>在head插件中可以看到数据<br>sniffing ： 寻找其他es节点</p><p><strong>实战举例</strong>：将错误日志写入es<br><strong>配置</strong></p><pre><code>input {    file {        path =&gt; [&quot;/usr/local/logstash-6.6.2/data_test/run_error.log&quot;]        type =&gt; &quot;error&quot;        start_position =&gt; &quot;beginning&quot;    }}output {    elasticsearch {        hosts =&gt; [&quot;192.168.109.133:9200&quot;]        index =&gt; &quot;logstash-%{type}-%{+YYYY.MM.dd}&quot;        document_type =&gt; &quot;%{type}&quot;        sniffing =&gt; true        template_overwrite =&gt; true    }}</code></pre><h3 id="四、Kibana"><a href="#四、Kibana" class="headerlink" title="四、Kibana"></a>四、Kibana</h3><p>Kibana是一个开源的分析和可视化平台，设计用于和Elasticsearch一起工作</p><p>你用Kibana来搜索，查看，并和存储在Elasticsearch索引中的数据进行交互</p><p>你可以轻松地执行高级数据分析，并且以各种图标、表格和地图的形式可视化数据</p><p>Kibana使得理解大量数据变得很容易。它简单的、基于浏览器的界面使你能够快速创建和共享动态仪表板，实时显示Elasticsearch查询的变化</p><p><strong>安装步骤</strong><br>解压：tar -zxvf kibana-6.6.2-linux-x86_64.tar.gz<br>修改 kibana.yml 配置文件： </p><pre><code>server.port: 5601 server.host: “192.168.116.121” ———-部署kinana服务器的ip elasticsearch.hosts: [“http://192.168.116.121:9200“] kibana.index: “.kibana”</code></pre><p>启动kibana，报错：<br>./bin/kibana<br><code>[error][status]</code>[plugin:<a href="mailto:remote_clusters@6.6.2">remote_clusters@6.6.2</a>] Status changed from red to red - X-Pack plugin is not installed on the [data] Elasticsearch cluster.</p><p>解决，卸载x-pack插件<br>elasticsearch-plugin remove x-pack<br>kibana-plugin remove x-pack</p><p>安装好后启动即可。页面操作</p><p>访问页面<br><a href="http://192.168.116.121:5601/" target="_blank" rel="noopener">http://192.168.116.121:5601/</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> ElasticSearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ElasticSearch（一）</title>
      <link href="/2019/03/18/elasticsearch-yi/"/>
      <url>/2019/03/18/elasticsearch-yi/</url>
      
        <content type="html"><![CDATA[<h3 id="1-全文检索技术简介"><a href="#1-全文检索技术简介" class="headerlink" title="1. 全文检索技术简介"></a>1. 全文检索技术简介</h3><h4 id="什么是搜索？"><a href="#什么是搜索？" class="headerlink" title="什么是搜索？"></a>什么是搜索？</h4><p>搜索，就是在任何场景下，找寻你想要的信息，这个时候，会输入一段你要搜索的关键字，然后就期望找到这个关键字相关的有些信息</p><h4 id="如何实现搜索？"><a href="#如何实现搜索？" class="headerlink" title="如何实现搜索？"></a>如何实现搜索？</h4><p>OA系统，比如：通过名字搜索员工等等<br>mysql :<br>select * from employee e where e.name like “%李雷%”;<br>select * from employee e where e.comment like “%好%”;<br>问题：<br>1）性能<br>2）比如搜索“优秀工”，mysql 无法支持</p><h4 id="全文检索"><a href="#全文检索" class="headerlink" title="全文检索"></a>全文检索</h4><p>全文数据库是全文检索系统的主要构成部分。所谓全文数据库是将一个完整的信息源的全部内容转化为计算机可以识别、处理的信息单元而形成的数据集合</p><p>全文数据库不仅存储了信息，而且还有对全文数据进行词、字、段落等更深层次的编辑、加工的功能</p><p>所有全文数据库无一不是海量信息数据库</p><h4 id="倒排索引"><a href="#倒排索引" class="headerlink" title="倒排索引"></a>倒排索引</h4><p>传统数据库存储：<br>| id  |  描述 |<br>| :—:| :—:|<br>|  1  |  优秀员工  |<br>|  2  |  销售冠军  |<br>|  3  |  优秀团队领导|<br>|  4  |  优秀项目  |</p><p>倒排索引处理步骤：<br>1、切词：<br>优秀员工 —— 优秀 员工<br>销售冠军 —— 销售 冠军<br>优秀团队领导 —— 优秀 团队 领导<br>优秀项目 —— 优秀 项目</p><p>2、建立倒排索引：<br>关键词 id<br>| 关键词 |  id  |<br>| :—:| :—:|<br>| 优秀 | 1,3,4 |<br>| 员工 |  1  |<br>| 销售 |  2  |<br>| 团队 |  3  |<br>|。。。| 。。。| </p><h4 id="Lucene"><a href="#Lucene" class="headerlink" title="Lucene"></a>Lucene</h4><p><strong>全文检索引擎</strong><br>Lucene 能够为文本类型的数据建立索引，所以你只要能把你要索引的数据格式转化的文本的，Lucene 就能对你的文档进行索引和搜索。比如你要对一些 HTML 文档，PDF 文档进行索引的话你就首先需要把 HTML 文档和 PDF 文档转化成文本格式的，然后将转化后的内容交给 Lucene 进行索引，然后把创建好的索引文件保存到磁盘或者内存中，最后根据用户输入的查询条件在索引文件上进行查询。不指定要索引的文档的格式也使 Lucene 能够几乎适用于所有的搜索应用程序</p><p>换句话说，使用 Lucene 可以轻松完成上述步骤</p><h4 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h4><p>Elasticsearch 是一个高度可伸缩的开源全文搜索和分析引擎。它允许你以近实时的方式快速存储、搜索和分析大量的数据。它通常被用作基础的技术来赋予应用程序复杂的搜索特性和需求</p><p>Elasticsearch ，是基于 lucene 开发的，隐藏复杂性，提供简单易用的 restful api 接口、java api 接口（还有其他语言的 api 接口）</p><h4 id="Elasticsearch-特点"><a href="#Elasticsearch-特点" class="headerlink" title="Elasticsearch 特点"></a>Elasticsearch 特点</h4><p>可以作为一个大型分布式集群（数百台服务器）技术，处理 PB 级数据，服务大公司；也可以运行在单机上，服务小公司</p><p>Elasticsearch 不是什么新技术，主要是将全文检索、数据分析以及分布式技术，合并在了一起，才形成了独一无二的 ES</p><p>对用户而言，是开箱即用的，非常简单，作为中小型的应用，直接3分钟部署一下 ES ，就可以作为生产环境的系统来使用了，数据量不大，操作不是太复杂</p><p>数据库的功能面对很多领域是不够用的（事务，还有各种联机事务型的操作）；特殊的功能，比如全文检索，同义词处理，相关度排名，复杂数据分析，海量数据的近实时处理； Elasticsearch 作为传统数据库的一个补充，提供了数据库所不能提供的很多功能</p><h4 id="Elasticsearch核心概念"><a href="#Elasticsearch核心概念" class="headerlink" title="Elasticsearch核心概念"></a>Elasticsearch核心概念</h4><p><strong>近实时</strong><br>近实时，两个意思，从写入数据到数据可以被搜索到有一个小延迟（大概1秒）；基于es执行搜索和分析可以达到秒级</p><p><strong>Cluster（集群）</strong><br>集群包含多个节点，每个节点属于哪个集群是通过一个配置（集群名称，默认是elasticsearch）来决定的，对于中小型应用来说，刚开始一个集群就一个节点很正常</p><p><strong>Node（节点）</strong><br>集群中的一个节点，节点也有一个名称（默认是随机分配的），节点名称很重要（在执行运维管理操作的时候），默认节点会去加入一个名称为“elasticsearch”的集群，如果直接启动一堆节点，那么它们会自动组成一个elasticsearch集群，当然一个节点也可以组成一个elasticsearch集群</p><p><strong>Index（索引-数据库）</strong><br>索引包含一堆有相似结构的文档数据，比如可以有一个客户索引，商品分类索引，订单索引，索引有一个名称。一个index包含很多document，一个index就代表了一类类似的或者相同的document。比如说建立一个product index，商品索引，里面可能就存放了所有的商品数据，所有的商品document</p><p><strong>Type（类型-表）</strong><br>每个索引里都可以有一个或多个type，type是index中的一个逻辑数据分类，一个type下的document，都有相同的field，比如博客系统，有一个索引，可以定义用户数据type，博客数据type，评论数据type</p><p>商品index，里面存放了所有的商品数据，商品document<br>但是商品分很多种类，每个种类的document的field可能不太一样，比如说电器商品，可能还包含一些诸如售后时间范围这样的特殊field；生鲜商品，还包含一些诸如生鲜保质期之类的特殊field</p><p>type，日化商品type，电器商品type，生鲜商品type<br>日化商品type：product_id，product_name，product_desc，category_id，category_name</p><p>电器商品type：product_id，product_name，product_desc，category_id，category_name，service_period</p><p>生鲜商品type：product_id，product_name，product_desc，category_id，category_name，eat_period</p><p>每一个type里面，都会包含一堆document</p><pre><code>{  &quot;product_id&quot;: &quot;1&quot;,  &quot;product_name&quot;: &quot;长虹电视机&quot;,  &quot;product_desc&quot;: &quot;4k高清&quot;,  &quot;category_id&quot;: &quot;3&quot;,  &quot;category_name&quot;: &quot;电器&quot;,  &quot;service_period&quot;: &quot;1年&quot;}{  &quot;product_id&quot;: &quot;2&quot;,  &quot;product_name&quot;: &quot;基围虾&quot;,  &quot;product_desc&quot;: &quot;纯天然，冰岛产&quot;,  &quot;category_id&quot;: &quot;4&quot;,  &quot;category_name&quot;: &quot;生鲜&quot;,  &quot;eat_period&quot;: &quot;7天&quot;}</code></pre><p><strong>Document（文档-行）</strong><br>文档是es中的最小数据单元，一个document可以是一条客户数据，一条商品分类数据，一条订单数据，通常用JSON数据结构表示，每个index下的type中，都可以去存储多个document</p><p><strong>Field（字段-列）</strong><br>Field是Elasticsearch的最小单位。一个document里面有多个field，每个field就是一个数据字段</p><pre><code>product document{  &quot;product_id&quot;: &quot;1&quot;,  &quot;product_name&quot;: &quot;高露洁牙膏&quot;,  &quot;product_desc&quot;: &quot;高效美白&quot;,  &quot;category_id&quot;: &quot;2&quot;,  &quot;category_name&quot;: &quot;日化用品&quot;}</code></pre><p><strong>mapping（映射-约束）</strong><br>数据如何存放到索引对象上，需要有一个映射配置，包括：数据类型、是否存储、是否分词等</p><p>这样就创建了一个名为blog的Index。Type不用单独创建，在创建Mapping 时指定就可以。Mapping用来定义Document中每个字段的类型，即所使用的 analyzer、是否索引等属性，非常关键等。创建Mapping 的代码示例如下：</p><pre><code>client.indices.putMapping({    index : &#39;blog&#39;,    type : &#39;article&#39;,    body : {        article: {            properties: {                id: {                    type: &#39;string&#39;,                    analyzer: &#39;ik&#39;,                    store: &#39;yes&#39;,                },                title: {                    type: &#39;string&#39;,                    analyzer: &#39;ik&#39;,                    store: &#39;no&#39;,                },                content: {                    type: &#39;string&#39;,                     analyzer: &#39;ik&#39;,                    store: &#39;yes&#39;,                }            }        }    }});</code></pre><h4 id="elasticsearch与数据库的类比"><a href="#elasticsearch与数据库的类比" class="headerlink" title="elasticsearch与数据库的类比"></a>elasticsearch与数据库的类比</h4><table><thead><tr><th align="center">关系型数据库（比如Mysql）</th><th align="center">非关系型数据库（Elasticsearch）</th></tr></thead><tbody><tr><td align="center">数据库Database</td><td align="center">索引Index</td></tr><tr><td align="center">表Table</td><td align="center">类型Type</td></tr><tr><td align="center">数据行Row</td><td align="center">文档Document</td></tr><tr><td align="center">数据列Column</td><td align="center">字段Field</td></tr><tr><td align="center">约束 Schema</td><td align="center">映射Mapping</td></tr></tbody></table><h4 id="ES存入数据和搜索数据机制"><a href="#ES存入数据和搜索数据机制" class="headerlink" title="ES存入数据和搜索数据机制"></a>ES存入数据和搜索数据机制</h4><p>1）索引对象（index）：存储数据的表结构 ，任何搜索数据，存放在索引对象上 </p><p>2）映射（mapping）：数据如何存放到索引对象上，需要有一个映射配置， 包括：数据类型、是否存储、是否分词等</p><p>3）文档（document）：一条数据记录，存在索引对象上 </p><p>4）文档类型（type）：一个索引对象，存放多种类型数据，数据用文档类型进行标识</p><h3 id="2-安装"><a href="#2-安装" class="headerlink" title="2. 安装"></a>2. 安装</h3><h4 id="单节点安装教程"><a href="#单节点安装教程" class="headerlink" title="单节点安装教程"></a>单节点安装教程</h4><p>java8<br>1）下载es安装包<br>curl -L -O <a href="https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.1.1.tar.gz" target="_blank" rel="noopener">https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.1.1.tar.gz</a></p><p>2）浏览器下载上传到虚拟机安装并创建和修改文件</p><p>解压elasticsearch-6.1.1.tar.gz到/opt/module目录下</p><p>在/opt/module/elasticsearch-6.1.1路径下创建data和logs文件夹<br>mkdir data<br>mkdir logs</p><p>修改配置文件/opt/module/elasticsearch-6.1.1/config/elasticsearch.yml<br>vi elasticsearch.yml</p><pre><code># ----------- Cluster ----------- cluster.name: my-application# ----------- Node ----------- node.name: node-121# ----------- Paths ----------- path.data: /opt/module/elasticsearch-6.1.1/datapath.logs: /opt/module/elasticsearch-6.1.1/logs# ----------- Memory ----------- bootstrap.memory_lock: falsebootstrap.system_call_filter: false# ----------- Network ----------- network.host: 192.168.116.121# ----------- Discovery ----------- discovery.zen.ping.unicast.hosts: [&quot;hsiehchou121&quot;]</code></pre><p>（1）cluster.name<br>如果要配置集群需要两个节点上的elasticsearch配置的cluster.name相同，都启动可以自动组成集群，这里如果不改cluster.name则默认是cluster.name=my-application</p><p>（2）nodename随意取但是集群内的各节点不能相同 </p><p>（3）修改后的每行前面不能有空格，修改后的“：”后面必须有一个空格</p><p>3）配置linux系统环境<br><strong>编辑limits.conf</strong><br>添加类似如下内容<br>sudo vi /etc/security/limits.conf<br>添加如下内容:</p><ul><li>soft nofile 65536</li><li>hard nofile 131072</li><li>soft nproc 4096</li><li>hard nproc 4096</li></ul><p><strong>进入limits.d目录下修改配置文件</strong><br>sudo vi /etc/security/limits.d/20-nproc.conf<br>修改如下内容：</p><ul><li>soft nproc 4096（修改为此参数，6版本的默认就是4096）</li></ul><p><strong>修改配置sysctl.conf</strong><br>sudo vi /etc/sysctl.conf<br>添加下面配置：<br>vm.max_map_count=655360<br>并执行命令：<br>sudo sysctl -p<br>然后，重新启动elasticsearch，即可启动成功</p><p><strong>启动（非root账户下启动）</strong><br>bin/elasticsearch<br>注意：can not run elasticsearch as root</p><p><strong>测试elasticsearch</strong><br>curl <a href="http://hsiehchou121:9200" target="_blank" rel="noopener">http://hsiehchou121:9200</a><br>curl -XGET ‘hsiehchou121:9200/_cat/health?v&amp;pretty’</p><p><strong>注：Linux中新建账户elasticsearch</strong><br>1、Linux中新建用户命令：<br>举例：我们创建一个名字叫 elasticsearch的用户<br>使用root用户操作如下命令：<br>useradd elasticsearch———–创建用户<br>passwd elasticsearch———–为用户设置密码<br>vim /etc/sudoers ———–为用户赋予sudo权限<br>添加 elasticsearch ALL=(ALL) ALL</p><p>2、修改文件夹及其子文件夹属主命令<br>chown -R elasticsearch ./elasticsearch-6.1.1/<br>修改后即可以使用elasticsearch操作此文件夹内容</p><p>4）安装elasticsearch-head.crx插件<br>页面连接按钮前面有个输入框输入：<a href="http://192.168.116.121:9200/" target="_blank" rel="noopener">http://192.168.116.121:9200/</a></p><p>5）命令行验证<br>REST API<br>curl -XGET ‘localhost:9200/_cat/health?v&amp;pretty’<br>epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent<br>1550960314 06:18:34 elasticsearch green 1 1 0 0 0 0 0 0 - 100.0%</p><p>看到status是 green，证明启动成功</p><p><strong>Green</strong> - 一切运行正常(集群功能齐全)<br><strong>Yellow</strong> - 所有数据是可以获取的，但是一些复制品还没有被分配(集群功能齐全)<br><strong>Red</strong> - 一些数据因为一些原因获取不到(集群部分功能不可用)</p><h4 id="多节点集群安装教程"><a href="#多节点集群安装教程" class="headerlink" title="多节点集群安装教程"></a>多节点集群安装教程</h4><p>1）分发Elasticsearch安装包至hsiehchou122、hsiehchou123、hsiehchou124<br>xsync elasticsearch-6.1.1/<br>或者scp -r elasticsearch-6.1.1/ hsiehchou122:/opt/module/</p><p>2）修改hsiehchou121配置信息<br>[elasticsearch@hsiehchou121 config]$ vi elasticsearch.yml<br>添加如下信息：<br>node.master: true<br>node.data: true</p><p>3）修改hsiehchou122配置信息<br>修改Elasticsearch配置信息<br>[elasticsearch@hsiehchou122 config]$ vi elasticsearch.yml<br>node.name: node-122<br>node.master: false<br>node.data: true<br>network.host: 192.168.116.122</p><p>修改Linux相关配置信息（同hsiehchou121 ）</p><p>修改hsiehchou122、hsiehchou123、hsiehchou124 配置信息</p><p>因为是scp的，所以一些跟hsiehchou121一样的配置就不需要修改了</p><p>5）分别启动三台节点的Elasticsearch<br>6）使用插件查看集群状态</p><h4 id="集群安装（详细增加或者修改内容）"><a href="#集群安装（详细增加或者修改内容）" class="headerlink" title="集群安装（详细增加或者修改内容）"></a>集群安装（详细增加或者修改内容）</h4><p>vi /opt/module/elasticsearch-6.1.1/config/elasticsearch.yml<br>hsiehchou121增加<br>node.master: true<br>node.data: true</p><p>hsiehchou121修改<br>discovery.zen.ping.unicast.hosts: [“hsiehchou121”,”hsiehchou122”,”hsiehchou123”,”hsiehchou124”]</p><p>hsiehchou122增加<br>node.master: false<br>node.data: true</p><p>hsiehchou122修改<br>node.name: node-122<br>network.host: 192.168.116.122<br>discovery.zen.ping.unicast.hosts: [“hsiehchou121”,”hsiehchou122”,”hsiehchou123”,”hsiehchou124”]</p><p>hsiehchou123增加<br>node.master: false<br>node.data: true</p><p>hsiehchou123修改<br>node.name: node-123<br>network.host: 192.168.116.123<br>discovery.zen.ping.unicast.hosts: [“hsiehchou121”,”hsiehchou122”,”hsiehchou123”,”hsiehchou124”]</p><p>hsiehchou124增加<br>node.master: false<br>node.data: true</p><p>hsiehchou124修改<br>node.name: node-124<br>network.host: 192.168.116.124<br>discovery.zen.ping.unicast.hosts: [“hsiehchou121”,”hsiehchou122”,”hsiehchou123”,”hsiehchou124”]</p><p><strong>说明</strong><br>cluster.name ：如果要配置集群需要两个节点上的 elasticsearch 配置的 cluster.name ：相同，都启动可以自动组成集群，这里如果不改 cluster.name ：则默认是 cluster.name=my-application<br>nodename ：随意取但是集群内的各节点不能相同</p><p><strong>启动es，报错：</strong><br>[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]<br>[2]: max number of threads [1024] for user [hduser] is too low, increase to at least [4096]<br>[3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]<br>[4]: system call filters failed to install; check the logs and fix your configuration or disable system</p><p>配置 linux 系统环境：<br>切换到 root 用户，编辑 limits.conf 添加类似如下内容<br>vi /etc/security/limits.conf</p><p>添加如下内容：</p><ul><li>soft nofile 65536</li><li>hard nofile 131072</li><li>soft nproc 4096</li><li>hard nproc 4096<br>进入 limits.d 目录下修改配置jian</li></ul><p>vi /etc/security/limits.d/20-nproc.conf<br>把 * soft nproc 1024 改成4096<br>es6版本的我没有要改，默认就是4096</p><p>修改配置 sysctl.conf<br>vi /etc/sysctl.conf<br>添加：<br>vm.max_map_count=655360<br>执行：<br>sysctl -p</p><p>重新登录elasticsearch用户，重新启动es<br>如果还有报错，则需重启虚拟机</p><p>查看集群状态命令：<br>curl -XGET ‘your ip:9200/_cat/health?v&amp;pretty’</p><p>查看所有数据命令：</p><pre><code>[elasticsearch@hsiehchou121 config]$ curl -XGET &#39;192.168.116.121:9200/blog/_search?pretty&#39; -H &#39;Content-Type: application/json&#39; -d&#39;&gt; {&gt;  &quot;query&quot;: { &quot;match_all&quot;: {} }&gt; }&gt; &#39;</code></pre><p>Elasticsearch head插件安装<br>node js下载插件：<a href="https://github.com/mobz/elasticsearch-head" target="_blank" rel="noopener">https://github.com/mobz/elasticsearch-head</a><br>nodejs官网下载安装包：<a href="https://nodejs.org/dist/" target="_blank" rel="noopener">https://nodejs.org/dist/</a><br>node-v6.9.2-linux-x64.tar.xz<br>拷贝<br>安装nodejs：<br>解压<br>配置环境变量：<br>export NODE_HOME=/usr/local/node-v6.9.2-linux-x64<br>export PATH=<code>$PATH:$NODE_HOME/bin</code></p><p>查看node和npm版本：<br>node -v<br>v6.9.2</p><p>npm -v<br>3.10.9</p><p>解压head插件到/opt/module目录下：<br>unzip elasticsearch-head-master.zip</p><p>查看当前head插件目录下有无node_modules/grunt目录：<br>没有的话，执行下面命令创建：<br>npm install grunt <code>--save</code> <code>--registry</code>=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><br>安装head插件：</p><p>npm install -g cnpm <code>--registry</code>=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><br>安装grunt：</p><p>npm install -g grunt-cli <code>--registry</code>=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><br>编辑Gruntfile.js<br>vim Gruntfile.js</p><p><strong>文件93行添加</strong><br><strong>hostname: ‘0.0.0.0’,</strong></p><p>检查head根目录下是否存在base文件夹<br>没有的话，将 _site下的base文件夹及其内容复制到head根目录下<br>mkdir base<br>cp base/* ../base/</p><p>启动grunt server：<br>[root@hsiehchou121 elasticsearch-head-master]# grunt server -d</p><p>如果提示grunt的模块没有安装：<br>Local Npm module “grunt-contrib-clean” not found. Is it installed?<br>Local Npm module “grunt-contrib-concat” not found. Is it installed?<br>Local Npm module “grunt-contrib-watch” not found. Is it installed?<br>Local Npm module “grunt-contrib-connect” not found. Is it installed?<br>Local Npm module “grunt-contrib-copy” not found. Is it installed?<br>Local Npm module “grunt-contrib-jasmine” not found. Is it installed?</p><p>执行以下命令：<br>npm install grunt-contrib-clean -registry=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><br>npm install grunt-contrib-concat -registry=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><br>npm install grunt-contrib-watch -registry=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><br>npm install grunt-contrib-connect -registry=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><br>npm install grunt-contrib-copy -registry=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a><br>npm install grunt-contrib-jasmine -registry=<a href="https://registry.npm.taobao.org" target="_blank" rel="noopener">https://registry.npm.taobao.org</a></p><p>最后一个模块可能安装不成功，但是不影响使用</p><p>浏览器访问head插件：<br><a href="http://192.168.116.121:9100" target="_blank" rel="noopener">http://192.168.116.121:9100</a></p><p><strong>CDH上的elasticsearch的配置</strong><br>vim /etc/security/limits.conf</p><ul><li>soft nofile 65536</li><li>hard nofile 131072</li><li>soft nproc 2048</li><li>hard nproc 4096</li></ul><p>vim /etc/security/limits.d/90-nproc.conf</p><ul><li>soft    nproc     4096<br>root       soft    nproc     unlimited</li></ul><p>vim /etc/sysctl.conf</p><p><strong>添加下面配置</strong><br>vm.max_map_count=655360</p><p>并执行命令：<br>sysctl -p</p><p><a href="http://hadoop2:9200" target="_blank" rel="noopener">http://hadoop2:9200</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> ElasticSearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase 操作</title>
      <link href="/2019/03/15/hbase-cao-zuo/"/>
      <url>/2019/03/15/hbase-cao-zuo/</url>
      
        <content type="html"><![CDATA[<h4 id="1、HBase-API操作"><a href="#1、HBase-API操作" class="headerlink" title="1、HBase API操作"></a>1、HBase API操作</h4><p>1）首先将core-site.xml、hbase-site.xml、hdfs-site.xml引入maven工程的resources下面</p><p>2）配置pom.xml文件<br>增加hbase依赖</p><pre><code>&lt;dependencies&gt;   &lt;dependency&gt;       &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;       &lt;artifactId&gt;hbase-server&lt;/artifactId&gt;       &lt;version&gt;1.3.0&lt;/version&gt;   &lt;/dependency&gt;   &lt;dependency&gt;       &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;       &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;       &lt;version&gt;1.3.0&lt;/version&gt;   &lt;/dependency&gt;&lt;/dependencies&gt;</code></pre><p>3）创建HbaseTest.java</p><pre><code>package com.hsiehchou.hbase;import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.hbase.*; import org.apache.hadoop.hbase.client.*; import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException; import java.util.ArrayList; import java.util.List;public class HbaseTest {     //配置信息     public static Configuration conf;     //获取配置信息     static{         //alt + enter         conf = HBaseConfiguration.create();     }</code></pre><p><strong>判断HBase中表是否存在</strong></p><pre><code>//1.判断HBase中表是否存在public static boolean isExist(String tableName) throws IOException{    //对表操作需要用HbaseAdmin    //HBaseAdmin admin = new HBaseAdmin(conf);老版本    Connection connection = ConnectionFactory.createConnection(conf);    //管理器    HBaseAdmin admin = (HBaseAdmin) connection.getAdmin();    return  admin.tableExists(TableName.valueOf(tableName));}</code></pre><p><strong>在HBase中创建表</strong></p><pre><code>//2.在HBase中创建表public static void createTable(String tableName, String... columnFamily) throws IOException {    //1.如果对表操作需要使用管理器    Connection connection = ConnectionFactory.createConnection(conf);    HBaseAdmin admin = (HBaseAdmin)connection.getAdmin();    //2.创建描述器    HTableDescriptor hd = new HTableDescriptor(TableName.valueOf(tableName));    //3.指定多个列族    for(String cf:columnFamily){        hd.addFamily(new HColumnDescriptor(cf));    }    //4.创建表    admin.createTable(hd);    System.out.println(&quot;表已经创建成功！！！！&quot;);}</code></pre><p><strong>bin/hbase shell操作</strong><br>list<br>scan ‘ni’<br>describe ‘ni’</p><p><strong>向表中添加数据</strong></p><pre><code>//3,向表中添加数据 put   rowkey  cf:列族public static void addData(String tableName, String rowkey, String cf, String column, String value) throws IOException {  Connection connection = ConnectionFactory.createConnection(conf);  Table table = connection.getTable(TableName.valueOf(tableName));  //添加数据 put方式  Put put = new Put(Bytes.toBytes(rowkey));  //指定列族 列 值  put.addColumn(Bytes.toBytes(cf), Bytes.toBytes(column), Bytes.toBytes(value));  table.put(put);}</code></pre><p><strong>删除一行数据</strong></p><pre><code>//4.删除一行数据public static void deleteRow(String tableName, String rowkey) throws IOException {    Connection connection = ConnectionFactory.createConnection(conf);    Table table = connection.getTable(TableName.valueOf(tableName));    Delete delete = new Delete(Bytes.toBytes(rowkey));    table.delete(delete);}</code></pre><p><strong>删除多个rowkey的数据</strong></p><pre><code>//5.删除多个rowkey的数据public static void deleteMore(String tableName, String... rowkey) throws IOException {    Connection connection = ConnectionFactory.createConnection(conf);    Table table = connection.getTable(TableName.valueOf(tableName));    //封装delete    List&lt;Delete&gt; d = new ArrayList&lt;Delete&gt;();    //遍历rowkey    for(String rk:rowkey){        Delete dd = new Delete(Bytes.toBytes(rk));        d.add(dd);    }    table.delete(d);}</code></pre><p><strong>全表扫描</strong></p><pre><code>//6.全表扫描public static void scanAll(String tableName) throws IOException {    Connection connection = ConnectionFactory.createConnection(conf);    Table table = connection.getTable(TableName.valueOf(tableName));    Scan scan = new Scan();    ResultScanner rs = table.getScanner(scan);    //遍历    for(Result r:rs){        //单元格        Cell[] cells = r.rawCells();        for(Cell c:cells) {            System.out.println(&quot;rowkey为：&quot; + Bytes.toString(CellUtil.cloneRow(c)));            System.out.println(&quot;列族为：&quot; + Bytes.toString(CellUtil.cloneFamily(c)));            System.out.println(&quot;值为：&quot; + Bytes.toString(CellUtil.cloneValue(c)));        }    }}</code></pre><p><strong>删除表</strong></p><pre><code>//7.删除表public static void deleteTable(String tableName) throws IOException {    //1.如果对表操作需要使用管理器    Connection connection = ConnectionFactory.createConnection(conf);    HBaseAdmin admin = (HBaseAdmin)connection.getAdmin();    admin.disableTable(tableName);    admin.deleteTable(TableName.valueOf(tableName));}public static void main(String[] args) throws IOException {     //System.out.println(isExist(“user”));     //create ‘表名’,’列族名’     //createTable(“ni”,”info1”,”info2”,”info3”);     //addData(“ni”,”shanghai”,”info1”,”name”,”lilei”);     //deleteRow(“ni”,”shanghai”);     //deleteMore(“ni”,”shanghai1”,”shanghai2”);    //scanAll(“ni”);     deleteTable(“ni”); } }</code></pre><h4 id="2、HBase-MR"><a href="#2、HBase-MR" class="headerlink" title="2、HBase-MR"></a>2、HBase-MR</h4><p>HBase主要擅长的领域是存储数据，不擅长分析数据</p><p>HBase如果想计算的话需要结合Hadoop的MapReduce</p><p>HBase-MR所需的jar包查看<br>bin/hbase mapredcp</p><p>配置临时环境变量</p><p>export HBASE_HOME=/root/hd/hbase-1.3.0<br>export HADOOP_HOME=/root/hd/hadoop-2.8.4<br>export HADOOP_CLASSPATH=<code>${HBASE_HOME}/bin/hbase mapredcp</code><br>跑hbase-mr程序<br>bin/yarn jar /root/hd/hbase-1.3.0/lib/hbase-server-1.3.0.jar rowcounter user</p><h4 id="3、HBase的表操作"><a href="#3、HBase的表操作" class="headerlink" title="3、HBase的表操作"></a>3、HBase的表操作</h4><p><strong>场景一</strong>：<br>region分片<br>指定列的过滤<br>name age high<br>name</p><p><strong>代码实现</strong><br><strong>ReadLoveMapper.java</strong></p><pre><code>package com.hsiehchou.mr;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.CellUtil;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableMapper;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;/** * HBase -MR * mapper类进行对数据的读取操作 * key:ImmutableBytesWritable hbase中的rowkey * value:封装的一条条的数据 */public class ReadLoveMapper extends TableMapper&lt;ImmutableBytesWritable, Put&gt; {    @Override    protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException {        //1.读取数据  根据rowkey拿到数据        Put put = new Put(key.get());        //2.过滤列 Cell单元格        for (Cell c:value.rawCells()){            //拿到info列族数据  如果是info列族  取出  如果不是info 过滤掉            if(&quot;info&quot;.equals(Bytes.toString(CellUtil.cloneFamily(c)))){                //过滤列                if(&quot;name&quot;.equals(Bytes.toString(CellUtil.cloneQualifier(c)))){                    put.add(c);                }            }        }        //3.输出到reducer端        context.write(key,put);    }}</code></pre><p><strong>WriteLoveReducer .java</strong></p><pre><code>package com.hsiehchou.mr;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableReducer;import org.apache.hadoop.io.NullWritable;import java.io.IOException;/** * keyIn:ImmutableBytesWritable * valueIn:Put * keyOut:NullWritable（在put里面已经有了rowkey了，所以不需要了） */public class WriteLoveReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; {    @Override    protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException {        for (Put p:values){            context.write(NullWritable.get(),p);        }    }}</code></pre><p><strong>LoverDriver .java</strong></p><pre><code>package com.hsiehchou.mr;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Scan;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class LoverDriver implements Tool {    private Configuration conf;    public void setConf(Configuration configuration) {        this.conf = HBaseConfiguration.create(configuration);    }    public Configuration getConf() {        return this.conf;    }    public int run(String[] strings) throws Exception {        //1.创建任务        Job job = Job.getInstance(conf);        //2.指定运行的主类        job.setJarByClass(LoverDriver.class);        //3.配置job        Scan scan = new Scan();        //4.设置具体运行的mapper类        TableMapReduceUtil.initTableMapperJob(&quot;love&quot;,                    scan,                    ReadLoveMapper.class,                    ImmutableBytesWritable.class,                    Put.class,                    job                );        //5.设置具体运行的Reducer类        TableMapReduceUtil.initTableReducerJob(&quot;lovemr&quot;,                    WriteLoveReducer.class,                    job                );        //6.设置reduceTask        job.setNumReduceTasks(1);        boolean rs = job.waitForCompletion(true);        return rs?0:1;    }    public static void main(String[] args) {        try {            //状态码            int sts = ToolRunner.run(new LoverDriver(), args);            System.exit(sts);        } catch (Exception e) {            e.printStackTrace();        }    }}</code></pre><p><strong>场景二</strong></p><p>把HDFS中的数据导入到HBase表中<br>HBase-MR</p><p><strong>代码实现</strong> </p><p><strong>ReadHdfsMapper .java</strong></p><pre><code>package com.hsiehchou.mr1;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.util.Bytes;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;/** * 读取hdfs中的数据 * hdfs -&gt;hbase */public class ReadHdfsMapper extends Mapper&lt;LongWritable, Text, ImmutableBytesWritable, Put&gt; {    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        //1.读取数据        String line = value.toString();        //2.切分数据        String[] fields = line.split(&quot;\t&quot;);        //3.封装数据        byte[] rowkey = Bytes.toBytes(fields[0]);        byte[] name = Bytes.toBytes(fields[1]);        byte[] desc = Bytes.toBytes(fields[2]);        //4.封装成put        Put put = new Put(rowkey);        put.addColumn(Bytes.toBytes(&quot;info&quot;),Bytes.toBytes(&quot;name&quot;),name);        put.addColumn(Bytes.toBytes(&quot;info&quot;),Bytes.toBytes(&quot;desc&quot;),desc);        //5.输出到reducer        context.write(new ImmutableBytesWritable(rowkey),put);    }}</code></pre><p><strong>WriteHbaseReducer.java</strong></p><pre><code>package com.hsiehchou.mr1;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableReducer;import org.apache.hadoop.io.NullWritable;import java.io.IOException;public class WriteHbaseReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; {    @Override    protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException {        for(Put p:values){            context.write(NullWritable.get(),p);        }    }}</code></pre><p><strong>LoveDriver.java</strong></p><pre><code>package com.hsiehchou.mr1;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class LoveDriver implements Tool {    private Configuration conf = null;    public void setConf(Configuration configuration) {        this.conf = HBaseConfiguration.create(configuration);    }    public Configuration getConf() {        return this.conf;    }    public int run(String[] strings) throws Exception {        //1.创建job        Job job = Job.getInstance();        job.setJarByClass(LoveDriver.class);        //2.配置mapper        job.setMapperClass(ReadHdfsMapper.class);        job.setMapOutputKeyClass(ImmutableBytesWritable.class);        job.setMapOutputValueClass(Put.class);        //3.配置reducer        TableMapReduceUtil.initTableReducerJob(&quot;lovehdfs&quot;, WriteHbaseReducer.class, job);        //4.输入配置 hdfs读数据 inputformat        FileInputFormat.addInputPath(job,new Path(&quot;/lovehbase/&quot;));        //5.需要配置outputformat吗？不需要 reducer中已经指定了表        return job.waitForCompletion(true)? 0:1;    }    public static void main(String[] args) {        try {            int sts = ToolRunner.run(new LoveDriver(),args);            System.exit(sts);        } catch (Exception e) {            e.printStackTrace();        }    }}</code></pre><h4 id="4、HBase优化"><a href="#4、HBase优化" class="headerlink" title="4、HBase优化"></a>4、HBase优化</h4><p>1）预分区问题<br>region分片？表很大 bigtable</p><p>分布式？数据量大</p><p>region存储数据，如果有多个region，每个region负责维护一部分的rowkey{startrowkey, endrowkey}<br>1<del>10001<br>1</del>2001 1980<br>2001~40002</p><p>分多少片？提前规划好，提高hbase的性能<br>进行存储数据前做好rowkey的预分区优化hbase</p><p>实际操作：<br>create ‘user_p’,’info’,’partition’,SPLITS =&gt;[‘201’,’202’,’203’,’204’]</p><p><strong>Table Regions</strong></p><table><thead><tr><th align="center">Region Server</th><th align="center">Start Key</th><th align="center">End Key</th></tr></thead><tbody><tr><td align="center">hsiehchou123:16020</td><td align="center">-∞</td><td align="center">201</td></tr><tr><td align="center">hsiehchou124:16020</td><td align="center">201</td><td align="center">202</td></tr><tr><td align="center">hsiehchou124:16020</td><td align="center">202</td><td align="center">203</td></tr><tr><td align="center">hsiehchou123:16020</td><td align="center">203</td><td align="center">204</td></tr><tr><td align="center">hsiehchou122:16020</td><td align="center">204</td><td align="center">+∞</td></tr></tbody></table><p>create ‘user_pppp’,’partition’,SPLITS_FILE =&gt; ‘partitions.txt’</p><p>partitions.txt’放在hbase-shell路径下</p><p>2）rowkey如何设计<br>rowkey是数据的唯一标识，这条数据存储在哪个分区由预分区范围决定</p><p>合理设计rowkey<br>如一份数据分为5个region存储<br>但是我们需要尽可能的保持每个region中的数据量差不多</p><p>尽可能的打散数据，平均分配到每个region中即可</p><p>解决方案：<br>生成随机数、hash/散列值<br>原本的rowkey是201，hash后<br>dfgyfugpgdcjhgfd11412nod<br>202变为：<br>21dqddwdgjohfxsovbxiufq12</p><p>字符串拼接：<br>20190316_a3d4<br>20190316_g04f</p><p>反转字符串：<br>201903161-&gt;161309102<br>201903162-&gt;261309102</p><p>3）HBase基础优化<br>HBase用的HDFS存储<br>DataNode允许最大文件打开数<br>默认4096 调大<br>dfs.datanode.max.transfer.threads<br>hdfs-site.xml</p><p>优化等待时间<br>dfs.image.transfer.timeout<br>默认60000毫秒<br>调大</p><p>内存优化：<br>hadoop-env.sh设置内存的堆大小<br>30%~40%最好</p><p>2G<br>512m</p><p>export HADOOP_PORTMAP_OPTS=’-Xmx512m $HADOOP_PORTMAP_OPTS’</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase基础</title>
      <link href="/2019/03/12/hbase-ji-chu/"/>
      <url>/2019/03/12/hbase-ji-chu/</url>
      
        <content type="html"><![CDATA[<h4 id="1、hbase"><a href="#1、hbase" class="headerlink" title="1、hbase"></a>1、hbase</h4><p>google:<br>gfs –&gt; hdfs<br>mapreduce –&gt; mapreduce<br>bigtable –&gt; hbase</p><p>Apache HBase™是Hadoop数据库，是一个分布式，可扩展的大数据存储</p><p>当您需要对大数据进行随机，实时读/写访问时，请使用Apache HBase™。该项目的目标是托管非常大的表 - 数十亿行X百万列 - 在商品硬件集群上。Apache HBase是一个开源的，分布式的，版本化的非关系数据库nosql，模仿Google的Bigtable： Chang等人的结构化数据分布式存储系统。正如Bigtable利用Google文件系统提供的分布式数据存储一样，Apache HBase在Hadoop和HDFS之上提供类似Bigtable的功能</p><h4 id="2、hbase集群角色"><a href="#2、hbase集群角色" class="headerlink" title="2、hbase集群角色"></a>2、hbase集群角色</h4><p>hdfs: NameNode DataNode<br>yarn: ResourceManager NodeManager<br>zookeeper: QuorumPeerMain<br>hbase: HMaster RegionServer</p><p><strong>主从结构</strong><br><strong>HMaster</strong><br>1）对RegionServer监控<br>2）处理一些元数据的变更<br>3）对RegionServer进行故障转移<br>4）空闲时对数据进行负载均衡<br>5）对region进行管理<br>6）发布位置到客户端借助于zookeeper</p><p><strong>RegionServer</strong><br>1）存储hbase实际的数据<br>2）刷新缓存数据到hdfs<br>3）处理Region<br>4）可以进行压缩<br>5）对Hlog进行维护<br>6）对region分片</p><h4 id="3、hbase集群安装部署"><a href="#3、hbase集群安装部署" class="headerlink" title="3、hbase集群安装部署"></a>3、hbase集群安装部署</h4><p>1）需要安装好zookeeper集群</p><p>2）需要安装好hadoop集群<br>hdfs<br>yarn</p><p>3）解压hbase压缩包<br>tar -zxvf hbase-1.3.0-bin.tar.gz</p><p>4）修改配置hbase-env.sh<br>export JAVA_HOME=/root/hd/jdk1.8.0_192<br>export HBASE_MANAGES_ZK=false</p><p>5）配置hbase-site.xml<br>cd /root/hd/hbase-1.3.0<br>vi hbase-site.xml</p><pre><code>&lt;configuration&gt;    &lt;!-- 设置namenode所在位置 通过rootdir设置 也就是设置hdfs中存放的路径 --&gt;    &lt;property&gt;        &lt;name&gt;hbase.rootdir&lt;/name&gt;        &lt;value&gt;hdfs://hsiehchou121:9000/hbase&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 是否开启集群 --&gt;    &lt;property&gt;        &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 0.98 后的新变动，之前版本没有.port,默认端口为 60000 --&gt;    &lt;property&gt;        &lt;name&gt;hbase.master.port&lt;/name&gt;        &lt;value&gt;16000&lt;/value&gt;    &lt;/property&gt;    &lt;!-- zookeeper集群的位置 --&gt;    &lt;property&gt;        &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;        &lt;value&gt;hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181,hsiehchou124:2181&lt;/value&gt;    &lt;/property&gt;    &lt;!-- hbase的元数据信息存储在zookeeper的位置 --&gt;    &lt;property&gt;        &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;        &lt;value&gt;/root/hd/zookeeper-3.4.10/zkData&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>指定从节点regionservers</p><p>vi regionservers</p><p>hsiehchou122<br>hsiehchou123<br>hsiehchou124</p><p>6）解决依赖问题<br>HBase依赖于Hadoop，换成2.8.4的hadoop依赖包<br>hadoop-annotations-2.8.4.jar<br>hadoop-auth-2.8.4.jar<br>hadoop-common-2.8.4.jar<br>hadoop-hdfs-2.8.4.jar<br>hadoop-hdfs-client-2.8.4.jar<br>hadoop-mapreduce-client-app-2.8.4.jar<br>hadoop-mapreduce-client-common-2.8.4.jar<br>hadoop-mapreduce-client-core-2.8.4.jar<br>hadoop-mapreduce-client-hs-2.8.4.jar<br>hadoop-mapreduce-client-hs-plugins-2.8.4.jar<br>hadoop-mapreduce-client-jobclient-2.8.4.jar<br>hadoop-mapreduce-client-jobclient-2.8.4-tests.jar<br>hadoop-mapreduce-client-shuffle-2.8.4.jar<br>hadoop-yarn-api-2.8.4.jar<br>hadoop-yarn-applications-distributedshell-2.8.4.jar<br>hadoop-yarn-applications-unmanaged-am-launcher-2.8.4.jar<br>hadoop-yarn-client-2.8.4.jar<br>hadoop-yarn-common-2.8.4.jar<br>hadoop-yarn-server-applicationhistoryservice-2.8.4.jar<br>hadoop-yarn-server-common-2.8.4.jar<br>hadoop-yarn-server-nodemanager-2.8.4.jar<br>hadoop-yarn-server-resourcemanager-2.8.4.jar<br>hadoop-yarn-server-web-proxy-2.8.4.jar</p><p>zookeeper-3.4.10.jar</p><p>7）软连接core-site.xml hdfs-site.xml<br>ln -s /root/hd/hadoop-2.8.4/etc/hadoop/hdfs-site.xml<br>ln -s /root/hd/hadoop-2.8.4/etc/hadoop/core-site.xml</p><p>8）发送到其他机器<br>scp -r hbase-1.3.0 hsiehchou122:/root/hd<br>scp -r hbase-1.3.0 hsiehchou123:/root/hd<br>scp -r hbase-1.3.0 hsiehchou124:/root/hd</p><p>find /root/hd/hadoop-2.8.4/ -name hadoop-a*</p><p>9）访问ui界面<br><a href="http://192.168.116.121:16010/master-status" target="_blank" rel="noopener">http://192.168.116.121:16010/master-status</a></p><p>启动hbase<br>bin/hbase-daemon.sh start master<br>bin/hbase-daemon.sh start regionserver</p><p>关闭hbase<br>bin/hbase-daemon.sh stop master<br>bin/hbase-daemon.sh stop regionserver</p><h4 id="4、hbase设计架构"><a href="#4、hbase设计架构" class="headerlink" title="4、hbase设计架构"></a>4、hbase设计架构</h4><p>Rowkey行键 类似 id<br>列式存储<br>hbase操作<br>0）启动终端<br>bin/hbase shell</p><p>1）查看表操作<br>list</p><p>2）显示当前服务器状态<br>status ‘hsiehchou121’<br>1 active master, 0 backup masters, 3 servers, 0 dead, 0.5000 ave<br>rage load<br>1 active master: 1个存活的master<br>0 backup masters: 0个备份master<br>3 servers: 3个regionserver<br>0 dead: 没有挂掉的<br>0.5000 average load：平均加载</p><p>3）显示当前用户<br>whoami</p><p>4）创建表<br>create ‘表名’,’列族’<br>create ‘user’,’info1’</p><p>5）添加数据<br>put ‘表名’,’rowkey’,’列族:列’,’值’<br>put ‘user’,’1001’,’info1:name’,’xie’<br>put ‘user’,’1001’,’info1:age’,’19’</p><p><strong>删除需要ctrl+&lt;-</strong></p><p>6）全表扫描<br>scan ‘表名’<br>scan ‘user’</p><p>ROW COLUMN+CELL<br>1001 column=info1:age, timestamp=1552579563486, value=19<br>1001 column=info1:name, timestamp=1552579531260, value=xie</p><p>7）hbase没有修改，只有覆盖<br>put ‘user’,’1001’,’info1:name’,’mi’<br>只要对应上表名、rowkey、列族、列即可</p><p>8）查看表结构<br>describe ‘user’</p><p>9）变更表结构信息<br>alter ‘user’,{NAME =&gt; ‘info1’,VERSIONS=&gt;’8’}</p><p>10）查看指定的数据信息<br>指定具体的rowkey<br>get ‘user’,’1001’<br>指定具体的列<br>get ‘user’,’1001’,’info1:name’</p><p>11）清空表<br>truncate ‘user1’</p><p>12）删除表<br>需要先指定不可用<br>disable ‘表名’<br>drop ‘表名’<br>disable ‘user1’<br>drop ‘user1’</p><p>13）扫描指定范围<br>指定从某一rowkey扫描<br>scan ‘user’,{STARTROW =&gt; ‘1002’}</p><p>包含头不包含尾（1001保留，1002不扫描）<br>scan ‘user’,{STARTROW =&gt; ‘1001’, STOPROW =&gt; ‘1002’}</p><p>14）统计rowkey的个数<br>count ‘user’</p><p>15）退出hbase shell<br>quit</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban</title>
      <link href="/2019/03/08/azkaban/"/>
      <url>/2019/03/08/azkaban/</url>
      
        <content type="html"><![CDATA[<h4 id="1、Azkaban"><a href="#1、Azkaban" class="headerlink" title="1、Azkaban"></a>1、Azkaban</h4><p>官网：<a href="https://azkaban.github.io/" target="_blank" rel="noopener">https://azkaban.github.io/</a><br>Azkaban是一款开源工作流管理器</p><p>Azkaban是在LinkedIn上创建的批处理工作流作业调度程序，用于运行Hadoop作业</p><p>Azkaban通过作业依赖性解决订单，并提供易于使用的Web用户界面来维护和跟踪您的工作流程</p><p>工作流作业：<br>Flume-&gt;HDFS-&gt;MR-&gt;Hive建表-&gt;导入load data脚本<br>自动化调度</p><h4 id="2、Azkaban安装部署"><a href="#2、Azkaban安装部署" class="headerlink" title="2、Azkaban安装部署"></a>2、Azkaban安装部署</h4><p>1）解压<br>首先将压缩包放进/root/hd/azkaban里面<br>azkaban-executor-server-2.5.0.tar.gz –&gt;executor<br>azkaban-sql-script-2.5.0.tar.gz –&gt;azkaban-2.5.0<br>azkaban-web-server-2.5.0.tar.gz –&gt;server<br>tar -zxvf *.tar.gz</p><p>2）进入MySQL创建Azkaban库，然后将解压好的脚本导入<br>create database azkaban;<br>use azkaban;<br>source /root/hd/azkaban/azkaban-2.5.0/create-all-sql-2.5.0.sql;</p><p>3）生成证书（https生成器）<br>keytool -keystore keystore -alias jetty -genkey -keyalg RSA<br>（回车，不用填，到那个CN=Unknown那行下面有个判断，输入y，后面一行继续回车）<br>将keystore移动到server文件夹下</p><p>4）时间同步配置<br>任务调度，所以和本地时间保持一致<br>开启交互窗口：<br>sudo date -s ”<br>hwclock -w</p><p>5）修改server端配置文件<br>cd /root/hd/azkaban/server/conf<br>vi azkaban.properties</p><pre><code># Azkaban Personalization Settingsazkaban.name=Testazkaban.label=My Local Azkabanazkaban.color=#FF3601azkaban.default.servlet.path=/indexweb.resource.dir=web/# 时区default.timezone.id=Asia/Shanghai#Azkaban UserManager class# 用户权限管理默认类user.manager.class=azkaban.user.XmlUserManager# 用户配置user.manager.xml.file=conf/azkaban-users.xml# Loader for projects#配置文件所在位置executor.global.properties=conf/global.propertiesazkaban.project.dir=projects# azkaban目前只支持mysqldatabase.type=mysqlmysql.port=3306# 当前主机名mysql.host=hsiehchou121mysql.database=azkabanmysql.user=rootmysql.password=root# 最大连接数mysql.numconnections=100# Velocity dev modevelocity.dev.mode=false# Azkaban Jetty server properties.# 最大线程数jetty.maxThreads=25jetty.ssl.port=8443jetty.port=8081jetty.keystore=keystorejetty.password=123456jetty.keypassword=123456jetty.truststore=keystorejetty.trustpassword=123456# Azkaban Executor settingsexecutor.port=12321# mail settingsmail.sender=@qq.commail.host=smtp.qq.comjob.failure.email=job.success.email=lockdown.create.projects=falsecache.directory=cache</code></pre><p><strong>azkaban-users.xml</strong></p><pre><code>&lt;azkaban-users&gt;    &lt;user username=&quot;azkaban&quot; password=&quot;azkaban&quot; roles=&quot;admin&quot; groups=&quot;azkaban&quot; /&gt;    &lt;user username=&quot;metrics&quot; password=&quot;metrics&quot; roles=&quot;metrics&quot;/&gt;    &lt;!--增加这一行  role管理员权限:admin--&gt;    &lt;user username=&quot;admin&quot; password=&quot;admin&quot; roles=&quot;admin,metrics&quot;/&gt;     &lt;role name=&quot;admin&quot; permissions=&quot;ADMIN&quot; /&gt;    &lt;role name=&quot;metrics&quot; permissions=&quot;METRICS&quot;/&gt;&lt;/azkaban-users&gt;</code></pre><p>6）修改excutor端配置文件 </p><p><strong>azkaban.properties</strong></p><pre><code># Azkaban# 时区default.timezone.id=Asia/Shanghai# Azkaban JobTypes Plugins# 插件azkaban.jobtype.plugin.dir=plugins/jobtypes#Loader for projectsexecutor.global.properties=conf/global.propertiesazkaban.project.dir=projectsdatabase.type=mysqlmysql.port=3306mysql.host=hsiehchou121mysql.database=azkabanmysql.user=rootmysql.password=rootmysql.numconnections=100# Azkaban Executor settings# 最大线程数executor.maxThreads=50executor.port=12321executor.flow.threads=30</code></pre><h4 id="3、Azkaban实战"><a href="#3、Azkaban实战" class="headerlink" title="3、Azkaban实战"></a>3、Azkaban实战</h4><p>HDFS-&gt;Hive建表-&gt;导入</p><p>1）job1.job</p><p><strong>job1.job</strong><br>type=command<br>command=echo ‘Hello World!’<br>打包成zip包上传到azkaban，执行</p><p>2）job2(a.job和b.job) </p><p><strong>a.job</strong><br>type=command<br>command=echo ‘li’</p><p><strong>b.job</strong><br>type=command<br>dependencies=a<br>command=echo ‘666’<br>打包成zip包上传到azkaban，执行</p><p>3）startyarn.job</p><p><strong>startyarn.job</strong><br>type=command<br>command=/root/hd/hadoop-2.8.4/sbin/start-yarn.sh<br>打包成zip包上传到azkaban，执行</p><p>4）mapreduce.job</p><p><strong>mapreduce.job</strong><br>type=command<br>command=/root/hd/hadoop-2.8.4/bin/hadoop jar hadoop-mapreduce-examples-2.8.4.jar wordcount /wc /wc/out<br>打包成zip包上传到azkaban，执行</p><p>5）Hive操作 </p><p><strong>hive.sql</strong><br>use default;<br>create table azhive(id int, name string) row format delimited fields terminated by ‘\t’;<br>load data inpath ‘/hsiehchou.txt’ into table azhive;</p><p><strong>hivef.job</strong><br>type=command<br>command=/root/hd/hive/bin/hive -f ‘hive.sql’</p><p>打包成zip包上传到Azkaban，执行</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop</title>
      <link href="/2019/03/04/sqoop/"/>
      <url>/2019/03/04/sqoop/</url>
      
        <content type="html"><![CDATA[<h4 id="1、Sqoop"><a href="#1、Sqoop" class="headerlink" title="1、Sqoop"></a>1、Sqoop</h4><p>Flume数据采集 采集日志数据<br>Sqoop数据迁移 HDFS-&gt;MySQL<br>Azkaban任务调度 Flume-&gt;HDFS-&gt;Shell-&gt;Hive-&gt;SQL-&gt;BI</p><p>Sqoop数据迁移=MapReduce<br>处理离线数据<br>整个过程就是数据导入处理导出过程<br>直接使用map</p><p>Sqoop作用：简化开发<br>MySQL-&gt;HDFS<br>MapReduce<br>Sqoop!</p><h4 id="2、概述"><a href="#2、概述" class="headerlink" title="2、概述"></a>2、概述</h4><p>Apache Sqoop（TM）是一种工具，用于在Apache Hadoop和结构化数据存储（如关 系数据库）之间高效传输批量数据 。数据迁移！ </p><p>Sqoop于2012年3月成功从孵化器毕业，现在是一个顶级Apache项目： 更多信息</p><h4 id="3、Sqoop安装部署"><a href="#3、Sqoop安装部署" class="headerlink" title="3、Sqoop安装部署"></a>3、Sqoop安装部署</h4><p>1）下载</p><p>2）上传</p><p>3）解压</p><p>4）重命名<br>mv sqoop-env-template.sh sqoop-env.sh</p><p>5）添加配置信息<br>export HADOOP_COMMON_HOME=/root/hd/hadoop-2.8.4<br>export HADOOP_MAPRED_HOME=/root/hd/hadoop-2.8.4<br>export HIVE_HOME=/root/hd/hive<br>export ZOOCFGDIR=/root/hd/zookeeper-3.4.10/conf</p><p>6）启动查看版本号<br>bin/sqoop version</p><h4 id="4、Sqoop的import导入"><a href="#4、Sqoop的import导入" class="headerlink" title="4、Sqoop的import导入"></a>4、Sqoop的import导入</h4><p>import导入：MySQL-&gt;HDFS<br>export导出：HDFS-&gt;MySQL<br>MySQL-&gt;HDFS操作：<br>1）导入mysql驱动到sqoop/lib下<br>2）命令操作<br>mysql&gt; create database sqoop;<br>mysql&gt; use sqoop;<br>mysql&gt; create table user(id int primary key auto_increment,name varchar(50),addr varchar(300));</p><pre><code>[root@hsiehchou121 sqoop]# bin/sqoop import \&gt; --connect jdbc:mysql://hsiehchou121:3306/sqoop \&gt; --username root \&gt; --password root \&gt; --table user \&gt; --target-dir /sqoop/datas \&gt; --num-mappers 1 \&gt; --fields-terminated-by &quot;\t&quot;</code></pre><p><strong>注意：如果显示mysql的访问权限问题，需要设置mysql的用户权限：所在库 mysql库的user表</strong></p><p>update user set host=’%’ where host=’localhost’;<br>delete from user where Host=’127.0.0.1’;<br>delete from user where Host=’hsiehchou121’;<br>delete from user where Host=’::1’;<br>flush privileges;</p><p><strong>使用query对数据进行过滤</strong></p><pre><code>[root@hsiehchou121 sqoop]# bin/sqoop import \&gt; --connect jdbc:mysql://hsiehchou121:3306/sqoop \&gt; --username root \&gt; --password root \&gt; --target-dir /sqoop/selectimport \&gt; --num-mappers 1 \&gt; --fields-terminated-by &quot;\t&quot; \&gt; --query &#39;select * from user where id&lt;=1 and $CONDITIONS&#39;</code></pre><p><strong>直接过滤字段</strong></p><pre><code>[root@hsiehchou121 sqoop]# bin/sqoop import \&gt; --username root \&gt; --password root \&gt; --connect jdbc:mysql://hsiehchou121:3306/sqoop \&gt; --target-dir /sqoop/selectimport1 \&gt; --num-mappers 1 \&gt; --table user \&gt; --columns addr</code></pre><h4 id="5、MySQL导入到Hive"><a href="#5、MySQL导入到Hive" class="headerlink" title="5、MySQL导入到Hive"></a>5、MySQL导入到Hive</h4><p>在~/.bash_profile里面增加下面配置<br>export HADOOP_CLASSPATH=<code>$HADOOP_CLASSPATH:/root/hd/hive/lib/*</code><br>export HADOOP_USER_HOME=root</p><pre><code>[root@hsiehchou121 sqoop]# bin/sqoop import \&gt; --connect jdbc:mysql://hsiehchou121:3306/sqoop \&gt; --username root \&gt; --password root \&gt; --table user \&gt; --num-mappers 1 \&gt; --hive-import \&gt; --fields-terminated-by &quot;\t&quot; \&gt; --hive-overwrite \&gt; --hive-table user_sqoop</code></pre><h4 id="6、Sqoop的export命令"><a href="#6、Sqoop的export命令" class="headerlink" title="6、Sqoop的export命令"></a>6、Sqoop的export命令</h4><p>Hive-&gt;MySQL<br>Hive导出到MySQL<br>首先清空mysql里面的user：truncate table user;</p><pre><code>[root@hsiehchou121 sqoop]# bin/sqoop export \&gt; --connect jdbc:mysql://hsiehchou121:3306/sqoop \&gt; --username root \&gt; --password root \&gt; --table user \&gt; --num-mappers 1 \&gt; --export-dir /user/hive/warehouse/user_sqoop \&gt; --input-fields-terminated-by &quot;\t&quot;</code></pre><h4 id="7、常用参数"><a href="#7、常用参数" class="headerlink" title="7、常用参数"></a>7、常用参数</h4><p>import ：导入数据到集群<br>export ：从集群导出数据<br>create-hive-table ：创建Hive表<br>import-all-tables ：指定关系型数据库所有表到HDFS集群<br>list-databases ：列出所有数据库<br>list-tables ：列出所有数据库表<br>merge ：合并hdfs中的不同目录下的数据<br>codegen ：获取某张表数据生成JavaBean 并打包</p><pre><code>[root@hsiehchou121 sqoop]# bin/sqoop codegen \&gt; --connect jdbc:mysql://hsiehchou121:3306/sqoop \&gt; --username root \&gt; --password root \&gt; --table user \&gt; --bindir /root/sqoopjar/UserBean \&gt; --class-name UserBean \&gt; --fields-terminated-by &quot;\t&quot;</code></pre><pre><code>[root@hsiehchou121 sqoop]# bin/sqoop list-databases \&gt; --connect jdbc:mysql://hsiehchou121:3306/ \&gt; --username root \&gt; --password root</code></pre><pre><code>[root@hsiehchou121 sqoop]# bin/sqoop merge \&gt; --new-data /testmerge/new/ \&gt; --onto /testmerge/old/ \&gt; --target-dir /testmerge/merged table user \&gt; --jar-file /root/sqoopjar/UserBean/UserBean.jar \&gt; --class-name UserBean \&gt; --merge-key id</code></pre><p><strong>注意</strong>：<br>merge操作是一个新表替代旧表的操作，如果有冲突id的话新表数据替换旧表数据，如果没有冲突则是新表数据添加到旧表的数据</p><p>用户画像 merge<br>身高180 体重70 爱好 …..<br>身高180 体重90 爱好….</p><p>广告大数据 提高销量 广告推送更加精准<br>工业大数据 flink面试</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume</title>
      <link href="/2019/03/02/flume/"/>
      <url>/2019/03/02/flume/</url>
      
        <content type="html"><![CDATA[<h4 id="1、Flume"><a href="#1、Flume" class="headerlink" title="1、Flume"></a>1、Flume</h4><p><strong>概述</strong>：<br>Flume是一种分布式，可靠且可用的服务，用于有效地收集，聚合和移动大量日志数据。它具有基于流数据流的简单灵活的架构。它具有可靠的可靠性机制和许多故障转移和恢复机制，具有强大的容错性。它使用简单的可扩展数据模型，允许在线分析应用程序<br>1）数据采集（爬虫\日志数据\Flume）<br>2）数据存储（HDFS/Hive/HBase(NoSQL)）<br>3）数据计算（MapReduce/Hive/SparkSQL/SparkStreaming/Flink）<br>4）数据可视化</p><h4 id="2、Flume角色"><a href="#2、Flume角色" class="headerlink" title="2、Flume角色"></a>2、Flume角色</h4><p>1）Source<br>数据源，用户采集数据，Source产生数据流，同时会把产生的数据流传输到Channel</p><p>2）Channel<br>传输通道，用于桥接Source和Sink</p><p>3）Sink<br>下沉，用于收集Channel传输的数据，将数据源传递到目标源</p><p>4）Agent<br>在Flume中使用事件作为传输的基本单元</p><h4 id="3、Flume使用"><a href="#3、Flume使用" class="headerlink" title="3、Flume使用"></a>3、Flume使用</h4><p>简单易用，只需要写配置文件即可</p><h4 id="4、Flume安装配置"><a href="#4、Flume安装配置" class="headerlink" title="4、Flume安装配置"></a>4、Flume安装配置</h4><p>1）下载Flume<br>2）上传到Linux<br>3）解压<br>tar -zxvf apache-flume-1.6.0-bin.tar.gz -C /root/hd </p><p>4）重命名<br>mv apache-flume-1.6.0-bin/ flume<br>cp flume-env.sh.template flume-env.sh </p><p>5）修改配置<br>vi flume-env.sh<br>export JAVA_HOME=/root/hd/jdk1.8.0_192</p><h4 id="5、Flume监听端口"><a href="#5、Flume监听端口" class="headerlink" title="5、Flume监听端口"></a>5、Flume监听端口</h4><p>启动命令：<br>bin/flume-ng agent <code>--conf</code> conf/log4j.properties <code>--name</code> a1 <code>--conf-file</code> conf/flumejob_telnet.conf</p><p><strong>我已经排坑了，这里我建议–conf 后面指定的路径建议是全路径，指定到log4j.properties或，我当时老师讲的是直接conf/，我实际操作是有问题的，不能实时的反馈</strong></p><pre><code>bin/flume-ng agent 使用ng启动agent--conf conf/log4j.properties 指定配置所在的文件夹--name a1 指定的agent别名--conf-file conf/flumejob_telnet.conf 文件-Dflume.root.logger=INFO,console 日志级别的反馈**</code></pre><p><strong>flumejob_telnet.conf</strong></p><pre><code>#smple.conf: A single-node Flume configuration# Name the components on this agent 定义变量方便调用 加s可以有多个此角色a1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the source 描述source角色 进行内容定制# 此配置属于tcp source 必须是netcat类型a1.sources.r1.type = netcat a1.sources.r1.bind = localhosta1.sources.r1.port = 44444# Describe the sink 输出日志文件a1.sinks.k1.type = logger# Use a channel which buffers events in memory（file） 使用内存 总大小1000 每次传输100a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channel 一个source可以绑定多个channel # 一个sinks可以只能绑定一个channel  使用的是图二的模型a1.sources.r1.channels = c1a1.sinks.k1.channel = c1</code></pre><pre><code>[root@hsiehchou121 flume]# bin/flume-ng agent \&gt; --conf conf/ \&gt; --name a1 \&gt; --conf-file conf/flumejob_telnet.conf \&gt; -Dflume.root.logger=INFO.console</code></pre><p><strong>yum search telnet</strong> </p><p><strong>yum install telnet.x86_64</strong></p><h4 id="6、flume监听本地linux文件采集到hdfs"><a href="#6、flume监听本地linux文件采集到hdfs" class="headerlink" title="6、flume监听本地linux文件采集到hdfs"></a>6、flume监听本地linux文件采集到hdfs</h4><p>启动命令：<br>bin/flume-ng agent <code>--conf</code> conf/log4j.properties <code>--name</code> a1 <code>--conf-file</code> conf/flumejob_hdfs.conf</p><p><strong>flumejob_hdfs.conf</strong></p><pre><code># Name the components on this agent agent别名设置a1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the source  设置数据源监听本地文件配置# exec 执行一个命令的方式去查看文件 tail -F 实时查看a1.sources.r1.type = exec# 要执行的脚本command tail -F 默认10行 man tail  查看帮助a1.sources.r1.command = tail -F /tmp/root/hive.log# 执行这个command使用的是哪个脚本 -c 指定使用什么命令# whereis bash# bash: /usr/bin/bash /usr/share/man/man1/bash.1.gz a1.sources.r1.shell = /usr/bin/bash -c# Describe the sink a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://hsiehchou121:9000/flume/%Y%m%d/%H#上传文件的前缀a1.sinks.k1.hdfs.filePrefix = logs-#是否按照时间滚动文件夹a1.sinks.k1.hdfs.round = true#多少时间单位创建一个新的文件夹  秒 （默认30s）a1.sinks.k1.hdfs.roundValue = 1#重新定义时间单位（每小时滚动一个文件夹）a1.sinks.k1.hdfs.roundUnit = minute#是否使用本地时间戳a1.sinks.k1.hdfs.useLocalTimeStamp = true#积攒多少个 Event 才 flush 到 HDFS 一次a1.sinks.k1.hdfs.batchSize = 500#设置文件类型，可支持压缩a1.sinks.k1.hdfs.fileType = DataStream#多久生成一个新的文件 秒a1.sinks.k1.hdfs.rollInterval = 30#设置每个文件的滚动大小 字节（最好128M,合理）a1.sinks.k1.hdfs.rollSize = 134217700#文件的滚动与 Event 数量无关a1.sinks.k1.hdfs.rollCount = 0#最小冗余数(备份数 生成滚动功能则生效roll hadoop本身有此功能 无需配置) 1份 不冗余 hdfs已经备份3份a1.sinks.k1.hdfs.minBlockReplicas = 1# Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1</code></pre><pre><code>[root@hsiehchou121 flume]# bin/flume-ng agent \&gt; --conf conf/log4j.properties \&gt; --name a1 \&gt; --conf-file conf/flumejob_hdfs.conf</code></pre><h4 id="7、监听文件夹"><a href="#7、监听文件夹" class="headerlink" title="7、监听文件夹"></a>7、监听文件夹</h4><p><strong>flumejob_dir.conf</strong></p><pre><code># 定义别名a1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = spooldir# 监控的文件夹a1.sources.r1.spoolDir = /root/testdir# 上传成功后显示后缀名 a1.sources.r1.fileSuffix = .COMPLETED# 如论如何 加绝对路径的文件名 默认falsea1.sources.r1.fileHeader = true#忽略所有以.tmp 结尾的文件（正在被写入），不上传# ^以任何开头 出现无限次 以.tmp结尾的a1.sources.r1.ignorePattern = ([^ ]*\.tmp)# Describe the sink 下沉到hdfsa1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://hsiehchou121:9000/flume/testdir/%Y%m%d/%H#上传文件的前缀a1.sinks.k1.hdfs.filePrefix = testdir-#是否按照时间滚动文件夹a1.sinks.k1.hdfs.round = true#多少时间单位创建一个新的文件夹a1.sinks.k1.hdfs.roundValue = 1#重新定义时间单位a1.sinks.k1.hdfs.roundUnit = hour#是否使用本地时间戳a1.sinks.k1.hdfs.useLocalTimeStamp = true#积攒多少个 Event 才 flush 到 HDFS 一次a1.sinks.k1.hdfs.batchSize = 100#设置文件类型，可支持压缩a1.sinks.k1.hdfs.fileType = DataStream#多久生成一个新的文件a1.sinks.k1.hdfs.rollInterval = 600#设置每个文件的滚动大小大概是 128M a1.sinks.k1.hdfs.rollSize = 134217700#文件的滚动与 Event 数量无关a1.sinks.k1.hdfs.rollCount = 0#最小副本数a1.sinks.k1.hdfs.minBlockReplicas = 1# Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1 a1.sinks.k1.channel = c1</code></pre><p>[root@hsiehchou121 conf]# bin/flume-ng agent <code>--conf</code> conf/log4j.properties <code>--name</code> a1 <code>--conf-file</code> conf/flumejob_dir.conf</p><pre><code>[root@hsiehchou121 flume]# bin/flume-ng agent \&gt; --conf conf/log4j.properties \&gt; --name a1 \&gt; --conf-file conf/flumejob_dir.conf </code></pre><h4 id="8、多个channel-sink"><a href="#8、多个channel-sink" class="headerlink" title="8、多个channel/sink"></a>8、多个channel/sink</h4><p>需求：监控hive.log文件，用同时产生两个channel，一个channel对应的sink存储到hdfs中，另外一个channel对应的sink存储到本地 </p><p><strong>flumejob_1.conf</strong></p><pre><code># name the components on this agent 别名设置a1.sources = r1a1.sinks = k1 k2 a1.channels = c1 c2# 将数据流复制给多个 channela1.sources.r1.selector.type = replicating# Describe/configure the source a1.sources.r1.type = execa1.sources.r1.command = tail -F /tmp/root/hive.loga1.sources.r1.shell = /bin/bash -c# Describe the sink# 分两个端口发送数据 a1.sinks.k1.type = avro a1.sinks.k1.hostname = hsiehchou121 a1.sinks.k1.port = 4141a1.sinks.k2.type = avro a1.sinks.k2.hostname = hsiehchou121 a1.sinks.k2.port = 4142# Describe the channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.channels.c2.type = memory a1.channels.c2.capacity = 1000a1.channels.c2.transactionCapacity = 100# Bind the source and sink to the channel a1.sources.r1.channels = c1 c2 a1.sinks.k1.channel = c1a1.sinks.k2.channel = c2</code></pre><p>[root@hsiehchou121 flume]# bin/flume-ng agent <code>--conf</code> conf/log4j.properties <code>--name</code> a1 <code>--conf-file</code> conf/flumejob_1.conf</p><p><strong>flumejob_2.conf</strong></p><pre><code># Name the components on this agent a2.sources = r1a2.sinks = k1 a2.channels = c1# Describe/configure the sourcea2.sources.r1.type = avro # 端口抓取数据a2.sources.r1.bind = hsiehchou121a2.sources.r1.port = 4141# Describe the sink a2.sinks.k1.type = hdfsa2.sinks.k1.hdfs.path = hdfs://hsiehchou121:9000/flume2/%Y%m%d/%H#上传文件的前缀a2.sinks.k1.hdfs.filePrefix = flume2-#是否按照时间滚动文件夹a2.sinks.k1.hdfs.round = true#多少时间单位创建一个新的文件夹a2.sinks.k1.hdfs.roundValue = 1#重新定义时间单位a2.sinks.k1.hdfs.roundUnit = hour#是否使用本地时间戳a2.sinks.k1.hdfs.useLocalTimeStamp = true#积攒多少个 Event 才 flush 到 HDFS 一次a2.sinks.k1.hdfs.batchSize = 100#设置文件类型，可支持压缩a2.sinks.k1.hdfs.fileType = DataStream#多久生成一个新的文件a2.sinks.k1.hdfs.rollInterval = 600#设置每个文件的滚动大小大概是 128M a2.sinks.k1.hdfs.rollSize = 134217700#文件的滚动与 Event 数量无关a2.sinks.k1.hdfs.rollCount = 0#最小副本数a2.sinks.k1.hdfs.minBlockReplicas = 1# Describe the channel a2.channels.c1.type = memory a2.channels.c1.capacity = 1000a2.channels.c1.transactionCapacity = 100# Bind the source and sink to the channel a2.sources.r1.channels = c1a2.sinks.k1.channel = c1</code></pre><p>[root@hsiehchou121 flume]# bin/flume-ng agent <code>--conf</code> conf/log4j.properties <code>--name</code> a2 <code>--conf-file</code> conf/flumejob_1.conf</p><p><strong>flumejob_3.conf</strong></p><pre><code># Name the components on this agent a3.sources = r1a3.sinks = k1 a3.channels = c1# Describe/configure the source a3.sources.r1.type = avroa3.sources.r1.bind = hsiehchou121a3.sources.r1.port = 4142# Describe the sink a3.sinks.k1.type = file_rolla3.sinks.k1.sink.directory = /root/flume2# Describe the channel a3.channels.c1.type = memory a3.channels.c1.capacity = 1000a3.channels.c1.transactionCapacity = 100# Bind the source and sink to the channel a3.sources.r1.channels = c1a3.sinks.k1.channel = c1</code></pre><p>[root@hsiehchou121 flume]# bin/flume-ng agent <code>--conf</code> conf/log4j.properties <code>--name</code> a3 <code>--conf-file</code> conf/flumejob_1.conf</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive函数&amp;压缩</title>
      <link href="/2019/02/28/hive-han-shu-ya-suo/"/>
      <url>/2019/02/28/hive-han-shu-ya-suo/</url>
      
        <content type="html"><![CDATA[<h4 id="1、排序"><a href="#1、排序" class="headerlink" title="1、排序"></a>1、排序</h4><p>Order By:全局排序<br>1）按照员工表的奖金金额进行正序排序<br>select * from emptable order by emptable.comm asc;<br>可以省略asc</p><p>2）按照员工表的奖金金额进行倒序排序<br>select * from emptable order by emptable.comm desc;</p><p>3）按照部门和奖金进行升序排序<br>select * from emptable order by deptno,comm;</p><p>Sort By: <strong>内部排序（区内有序，全局无序）</strong><br>设置reduce个数的属性：set mapreduce.job.reduces = 3;<br>select * from dept_partitions sort by deptno desc;</p><p>Distribute By: <strong>分区排序</strong><br>1）先按照部门编号进行排序再按照地域编号进行降序排序。<br>select * from dept_partitions distribute by deptno sort by loc desc;</p><p>Cluster By: <strong>分桶排序</strong><br>1）按照部门编号进行排序<br>select * from dept_partitions cluster by deptno;</p><p><strong>注意</strong>：如果Distrbute和Sort by 是相同字段时，可以用cluster by代替</p><h4 id="2、分桶"><a href="#2、分桶" class="headerlink" title="2、分桶"></a>2、分桶</h4><p>分桶分的是文件<br>1）创建分桶表<br>clustered by(id) into 4 buckets</p><pre><code>hive&gt; set mapreduce.job.reduces=4;hive&gt; create table emptable_buck(id int, name string)    &gt; clustered by(id) into 4 buckets    &gt; row format    &gt; delimited fields    &gt; terminated by &#39;\t&#39;;</code></pre><p><strong>查看表的描述信息</strong></p><p>hive&gt; desc formatted emptable_buck;</p><p><strong>加载数据</strong></p><pre><code>hive&gt; load data local inpath &#39;/root/hsiehchou.txt&#39; into table emptable_buck;hive&gt; create table emptable_b(id int, name string)    &gt; row format    &gt; delimited fields    &gt; terminated by &#39;\t&#39;;</code></pre><p><strong>清空表</strong></p><pre><code>hive&gt; truncate table emptable_buck;</code></pre><p><strong>加载数据（桶）</strong></p><pre><code>hive&gt; load data local inpath &#39;/root/hsiehchou.txt&#39; into table emptable_b;</code></pre><p><strong>设置桶的环境变量(插入数据时分桶，不开启默认在一个桶里面)</strong></p><pre><code>hive&gt; set hive.enforce.bucketing=true;hive&gt; truncate table emptable_buck;</code></pre><p>用户需要统计一个具有代表性的结果时，并不是全部结果！抽样！<br>(bucket 1 out of 2 on id）<br>1：第一桶数据<br>2：代表拿两桶</p><pre><code>hive&gt; select * from emptable_buck  tablesample(bucket 1 out of 2 on id);</code></pre><h4 id="3、UDF自定义函数"><a href="#3、UDF自定义函数" class="headerlink" title="3、UDF自定义函数"></a>3、UDF自定义函数</h4><p><strong>查看内置函数</strong><br>show functions; </p><p><strong>查看函数的详细内容</strong><br>desc function extended upper;</p><p>UDF:一进一出<br>UDAF:聚合函数 多进一出 count /max/avg<br>UDTF:一进多出</p><p><strong>java</strong><br>导入Hive的lib下的所有jar包<br>编程java代码</p><pre><code>package com.hsiehchou;import org.apache.hadoop.hive.ql.exec.UDF;public class MyConcat extends UDF {    //将大写转换成小写    public String evaluate(String a, String b) {        return a + &quot;******&quot; + String.valueOf(b);    }   }</code></pre><p>export此文件，打包jar，放入hsiehchou121中</p><p>添加临时：<br>add jar /root/Myconcat.jar;<br>create temporary function my_cat as “com.hsiehchou.MyConcat”;</p><pre><code>&lt;!-- 注册永久：hive-site.xml --&gt;&lt;property&gt;&lt;name&gt;hive.aux.jars.path&lt;/name&gt;&lt;value&gt;file:///root/hd/hive/lib/hive.jar&lt;/value&gt;&lt;/property&gt;</code></pre><h4 id="4、Hive压缩"><a href="#4、Hive压缩" class="headerlink" title="4、Hive压缩"></a>4、Hive压缩</h4><p>存储：hdfs<br>计算：mapreduce</p><p><strong>Map输出阶段压缩方式</strong><br>开启hive中间传输数据压缩功能<br>set hive.exec.compress.intermediate=true;</p><p><strong>开启map输出压缩</strong><br>set mapreduce.map.output.compress=true;</p><p><strong>设置snappy压缩方式</strong><br>set mapreduce.map.output.compress.codec=org.apache.hadoop.io.com<br>press.SnappyCodec;</p><p><strong>Reduce输出阶段压缩方式</strong><br>设置hive输出数据压缩功能<br>set hive.exec.compress.output=true;</p><p><strong>设置mr输出数据压缩</strong><br>set mapreduce.output.fileoutputformat.compress=true;</p><p><strong>指定压缩编码</strong><br>set mapreduce.output.fileoutputformat.compress.codec=org.apache.<br>hadoop.io.compress.SnappyCodec;</p><p><strong>指定压缩类型块压缩</strong><br>set mapreduce.output.fileoutputformat.compress.type=BLOCK;</p><p><strong>测试结果</strong><br>insert overwrite local directory ‘/root/datas/rs’ select * from emptable order by sal desc;</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的SQL操作</title>
      <link href="/2019/02/27/hive-de-sql-cao-zuo/"/>
      <url>/2019/02/27/hive-de-sql-cao-zuo/</url>
      
        <content type="html"><![CDATA[<h3 id="1、分区表"><a href="#1、分区表" class="headerlink" title="1、分区表"></a>1、分区表</h3><h4 id="1）创建分区表"><a href="#1）创建分区表" class="headerlink" title="1）创建分区表"></a>1）创建分区表</h4><pre><code>hive&gt; create table dept_partitions()      &gt; partition by()      &gt; row format      &gt; delimited fields      &gt; terminated by &#39;&#39;;</code></pre><p>例：</p><pre><code>hive&gt; create table dept_partitions(deptno int, dept string, loc string)    &gt; partitioned by(day string)    &gt; row format    &gt; delimited fields    &gt; terminated by &#39;\t&#39;;hive&gt; load data local inpath &#39;/root/dept.txt&#39; into table dept_partitions    &gt; partition(day=&#39;0228&#39;);</code></pre><h4 id="2）查询"><a href="#2）查询" class="headerlink" title="2）查询"></a>2）查询</h4><p><strong>全查询</strong><br>hive&gt; select * from dept_partitions;<br>注意：此时查看的是整个分区表中的数据</p><p><strong>单分区查询</strong><br>hive&gt; select * from dept_partitions where day = ‘0228’;<br>注意：此时查看的是指定分区中的数据</p><p><strong>联合查询</strong><br>hive&gt; select * from dept_partitions where day = ‘0228’ union select * from dept_partitions where day = ‘0302’;</p><p><strong>添加单个分区</strong><br>hive&gt; alter table dept_partitions add partition(day = ‘0303’); </p><p><strong>注意</strong>：如果想一次添加多个的话 空格分割即可<br>hive&gt; alter table dept_partitions add partition(day = ‘0304’) partition(day = ‘0305’);</p><p><strong>查看分区</strong><br>hive&gt; show partitions dept_partitions;</p><p><strong>删除分区</strong><br>hive&gt; alter table dept_partitions drop partition(day=’0305’);<br>分区表在hdfs中分目录文件夹</p><p>hive&gt; dfs -mkdir -p /user/hive/warehouse/dept_partitions/day=0305;</p><p>hive&gt; dfs -put /root/dept.txt /user/hive/warehouse/dept_partitions/day=0305;</p><p>hive&gt; show partitions dept_partitions;<br>此时并没有day=0305，需要进行下面操作</p><p><strong>导入数据</strong><br>相当于修复数据：msck repair table dept_partitions;</p><h3 id="2、DML数据操作"><a href="#2、DML数据操作" class="headerlink" title="2、DML数据操作"></a>2、DML数据操作</h3><h4 id="1）数据的导入"><a href="#1）数据的导入" class="headerlink" title="1）数据的导入"></a>1）数据的导入</h4><p>hive&gt; load data [local] inpath ” into table ;</p><h4 id="2）向表中插入数据"><a href="#2）向表中插入数据" class="headerlink" title="2）向表中插入数据"></a>2）向表中插入数据</h4><p>hive&gt; insert into table student_partitions partition(age = 20)  values(1,’re’);<br>向表中插入sql查询结果数据<br>hive&gt; insert overwrite table student_partitions partition(age = 20) select * from hsiehchou where id&lt;3;</p><p>create方式：<br>hive&gt; create table if not exists student_partitions1 as select * from student_partitions where id = 2;</p><h4 id="3）创建表直接加载数据"><a href="#3）创建表直接加载数据" class="headerlink" title="3）创建表直接加载数据"></a>3）创建表直接加载数据</h4><pre><code>hive&gt; create table student_partitions3(id int,name string)      &gt; row format      &gt; delimited fields      &gt; terminated by &#39;\t&#39;      &gt; location &#39;&#39;;</code></pre><p><strong>注意</strong>：locatition路径是hdfs路径<br>关联文件时不能有多级目录！！！<br>例：</p><pre><code>hive&gt; create table student_partitions4(id int,name string)    &gt; row format    &gt; delimited fields    &gt; terminated by &#39;\t&#39;    &gt; location &#39;/wc&#39;;</code></pre><h4 id="4）把操作结果导出到本地linux"><a href="#4）把操作结果导出到本地linux" class="headerlink" title="4）把操作结果导出到本地linux"></a>4）把操作结果导出到本地linux</h4><p>hive&gt; insert overwrite local directory ‘/root/data’ select * from hsiehchou;</p><h4 id="5）把hive中表数据导出到hdfs中"><a href="#5）把hive中表数据导出到hdfs中" class="headerlink" title="5）把hive中表数据导出到hdfs中"></a>5）把hive中表数据导出到hdfs中</h4><p>hive&gt; export table hsiehchou to ‘/hsiehchou’;</p><p>把hdfs数据导入到hive中<br>hive&gt; import table hsiehchou3 from ‘/hsiehchou/’;</p><h4 id="6）清空表数据"><a href="#6）清空表数据" class="headerlink" title="6）清空表数据"></a>6）清空表数据</h4><p>hive&gt; truncate table hsiehchou3;</p><h3 id="3、查询操作"><a href="#3、查询操作" class="headerlink" title="3、查询操作"></a>3、查询操作</h3><p>基础查询<br>select * from table;全表查询<br>hive&gt; select hsiehchou.id,hsiehchou.name from table …;指定列</p><h4 id="1）指定列查询"><a href="#1）指定列查询" class="headerlink" title="1）指定列查询"></a>1）指定列查询</h4><p>hive&gt; select hsiehchou.name from hsiehchou;</p><h4 id="2）指定列查询设置别名"><a href="#2）指定列查询设置别名" class="headerlink" title="2）指定列查询设置别名"></a>2）指定列查询设置别名</h4><p>hive&gt; select hsiehchou.name as myname from hsiehchou;</p><h4 id="3）创建员工表"><a href="#3）创建员工表" class="headerlink" title="3）创建员工表"></a>3）创建员工表</h4><pre><code>hive&gt; create table hive_db.emptable(empno int, ename string , job string,mgr int, birthday string, sal double, comm double, deptno int)    &gt; row format    &gt; delimited fields    &gt; terminated by &#39;\t&#39;;hive&gt; load data local ‘/root/emp.txt’ into table hive_db.emptable;</code></pre><h4 id="4）查询员工姓名和工资-每个员工加薪1000块"><a href="#4）查询员工姓名和工资-每个员工加薪1000块" class="headerlink" title="4）查询员工姓名和工资(每个员工加薪1000块)"></a>4）查询员工姓名和工资(每个员工加薪1000块)</h4><p>hive&gt; select emptable.ename,emptable.sal+1000 salmoney from emptable;</p><h4 id="5）查看公司有多少员工"><a href="#5）查看公司有多少员工" class="headerlink" title="5）查看公司有多少员工"></a>5）查看公司有多少员工</h4><p>hive&gt; select count(1) empnumber from emptable;</p><h4 id="6）查询工资最高的工资"><a href="#6）查询工资最高的工资" class="headerlink" title="6）查询工资最高的工资"></a>6）查询工资最高的工资</h4><p>hive&gt; select max(sal) numberone from emptable;</p><h4 id="7）查询工资最小的工资"><a href="#7）查询工资最小的工资" class="headerlink" title="7）查询工资最小的工资"></a>7）查询工资最小的工资</h4><p>hive&gt; select min(sal) from emptable;</p><h4 id="8）求工资的总和"><a href="#8）求工资的总和" class="headerlink" title="8）求工资的总和"></a>8）求工资的总和</h4><p>hive&gt; select sum(sal) sal_sum from emptable;</p><h4 id="9）求该公司员工工资的平均值"><a href="#9）求该公司员工工资的平均值" class="headerlink" title="9）求该公司员工工资的平均值"></a>9）求该公司员工工资的平均值</h4><p>hive&gt; select avg(sal) sal_avg from emptable;</p><h4 id="10）查询结果只显示前多少条"><a href="#10）查询结果只显示前多少条" class="headerlink" title="10）查询结果只显示前多少条"></a>10）查询结果只显示前多少条</h4><p>hive&gt; select * from emptable limit 4;</p><h4 id="11）where语句"><a href="#11）where语句" class="headerlink" title="11）where语句"></a>11）where语句</h4><p>作用：过滤<br>使用：where子句紧接着from</p><p>求出工资大于2600的员工<br>hive&gt; select * from emptable where sal&gt;2600;</p><p>求出工资在1000~2500范围的员工<br>hive&gt; select * from emptable where sal&gt;1000 and sal&lt;2500;</p><p>或者<br>hive&gt; select * from emptable where sal between 1000 and 2500;</p><p><strong>查询工资在2000和3000这两个数的员工信息</strong><br>hive&gt; select ename from emptable where sal in(2000,3000);</p><h4 id="12）is-null与is-not-null"><a href="#12）is-null与is-not-null" class="headerlink" title="12）is null与is not null"></a>12）is null与is not null</h4><p><strong>空与非空的过滤</strong><br>空<br>hive&gt; select * from emptable where comm is null;</p><p>非空<br>hive&gt; select * from emptable where comm is not null;</p><h4 id="13）like"><a href="#13）like" class="headerlink" title="13）like"></a>13）like</h4><p><strong>模糊查询</strong><br>使用：<br>通配符% 后面零个或者多个字符<br>_代表一个字符</p><p>查询工资以1开头的员工信息<br>hive&gt; select * from emptable where sal like ‘1%’;</p><p>查询工资地第二位是1的员工信息<br>hive&gt; select * from emptable where sal like ‘_1%’;</p><p>_代表一个字符<br>查询工资中有5的员工信息<br>hive&gt; select * from emptable where sal like ‘%5%’;</p><h4 id="14）And-Not-Or"><a href="#14）And-Not-Or" class="headerlink" title="14）And/Not/Or"></a>14）And/Not/Or</h4><p>查询部门号30并且工资大于1000的员工信息<br>hive&gt; select * from emptable where sal&gt;1000 and deptno=30;</p><p>查询部门号30或者工资大于1000的员工信息<br>hive&gt; select * from emptable where sal&gt;1000 or deptno=30;</p><p>查询工资在2000和3000这两个数的员工信息<br>hive&gt; select * from emptable where sal in(2000,3000);</p><p>查询工资不在2000和3000这两个数的员工信息<br>hive&gt; select * from emptable where sal not in(2000,3000);</p><h4 id="15）分组操作"><a href="#15）分组操作" class="headerlink" title="15）分组操作"></a>15）分组操作</h4><p>Group By语句<br>通常和一些聚合函数一起使用 </p><p>求每个部门的平均工资<br>hive&gt; select avg(sal) avg_sal,deptno from emptable group by deptno;<br>having<br>where：后不可以与分组函数，而having可以</p><p>求每个部门的平均工资大于2000的部门<br>hive&gt; select deptno,avg(sal) avg_sal from emptable group by deptno hav<br>ing avg_sal&gt;2000;</p><h3 id="4、Join操作"><a href="#4、Join操作" class="headerlink" title="4、Join操作"></a>4、Join操作</h3><pre><code>hive&gt; create table dept(deptno int, dname string, loc int)      &gt; row format      &gt; delimited fields      &gt; terminated by &#39;\t&#39;;</code></pre><p>员工表中只有部门编号，并没有部门名称<br>部门表中有部门标号和部门名称</p><p><strong>等值join</strong> </p><h4 id="1）查询员工编号、员工姓名、员工所在的部门名称"><a href="#1）查询员工编号、员工姓名、员工所在的部门名称" class="headerlink" title="1）查询员工编号、员工姓名、员工所在的部门名称"></a>1）查询员工编号、员工姓名、员工所在的部门名称</h4><p>hive&gt; select emptable.empno,emptable.ename,dept.dname from emptable join dept on emptable.deptno=dept.deptno;</p><h4 id="2）查询员工编号、员工姓名、员工所在部门名称、部门所在地"><a href="#2）查询员工编号、员工姓名、员工所在部门名称、部门所在地" class="headerlink" title="2）查询员工编号、员工姓名、员工所在部门名称、部门所在地"></a>2）查询员工编号、员工姓名、员工所在部门名称、部门所在地</h4><p>内连接：只有连接的两张表中都存在与条件向匹配的数据才会被保留下来<br>hive&gt; select e.empno,e.ename,d.dname,d.loc from emptable e join dept d on e.deptno=d.deptno;</p><h4 id="3）左外连接-left-join"><a href="#3）左外连接-left-join" class="headerlink" title="3）左外连接(left join)"></a>3）左外连接(left join)</h4><p>查询员工编号，员工姓名，部门名称<br>hive&gt; select e.empno,e.ename,d.deptname from emptable e left join dept d on e.deptno=d.deptno;<br>特点：默认用的Left join 可以省略left<br>保留左表数据，右表没有join上 显示为null</p><h4 id="4）右外连接-right-join"><a href="#4）右外连接-right-join" class="headerlink" title="4）右外连接(right join)"></a>4）右外连接(right join)</h4><p>hive&gt; select e.empno,e.ename,d.dname from emptable e right join dept d on e.deptno=d.deptno;<br>特点：<br>保留右表数据，左表没有join上 显示为null</p><h4 id="5）满外连接-full-join"><a href="#5）满外连接-full-join" class="headerlink" title="5）满外连接(full join)"></a>5）满外连接(full join)</h4><p>hive&gt; select e.empno,e.ename,d.dname from emptable e full join dept d on e.deptno=d.deptno;<br>特点：结果会返回所有表中符合条件的所有记录，如果有字段没有符合条件用null值代替</p><h4 id="6）多表连接"><a href="#6）多表连接" class="headerlink" title="6）多表连接"></a>6）多表连接</h4><pre><code>hive&gt; create table location(loc int, loc_name string)      &gt; row format      &gt; delimited fields      &gt; terminated by &#39;\t&#39;;</code></pre><p><strong>加载数据</strong><br>hive&gt; load data local inpath ‘/root/location.txt’ into table location;</p><p><strong>查询员工名、部门名称、地域名称</strong><br>hive&gt; select e.ename,d.dname,l.loc_name from emptable e join dept d on<br>e.deptno=d.deptno join location l on d.loc=l.loc;</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hive SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive基础</title>
      <link href="/2019/02/25/hive-ji-chu/"/>
      <url>/2019/02/25/hive-ji-chu/</url>
      
        <content type="html"><![CDATA[<h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3><p>官网：<a href="http://hive.apache.org/" target="_blank" rel="noopener">http://hive.apache.org/</a><br>Apache Hive?数据仓库软件有助于使用SQL读取，编写和管理驻留在分布式存储中的大型数据集。可以将结构投影到已存储的数据中。提供了命令行工具和JDBC驱动程序以将用户连接到Hive</p><p>Hive提供了SQL查询功能 hdfs分布式存储</p><p>Hive本质HQL转化为MapReduce程序<br>环境前提：<br>1）启动hdfs集群<br>2）启动yarn集群<br>如果想用hive的话，需要提前安装部署好hadoop集群</p><h3 id="为什么要学习Hive"><a href="#为什么要学习Hive" class="headerlink" title="为什么要学习Hive"></a>为什么要学习Hive</h3><p>简化开发<br>easycoding!<br>高德地图使用Hive</p><p><strong>优势</strong>：<br>1）操作接口采用类sql语法，select * from stu;<br>简单、上手快！<br>2）hive可以替代mr程序，sqoop<br>3）hive可以处理海量数据<br>4）hive支持UDF，自定义函数</p><p><strong>劣势</strong>：<br>1）处理数据延迟高，慢<br>引擎：1.2.2以前版本都是用的mr引擎<br>2.x之后用的是Spark引擎 </p><p>2）HQL的表达能力有限<br>一些sql无法解决的场景，依然需要我们写MapReduce</p><h3 id="hive架构原理解析"><a href="#hive架构原理解析" class="headerlink" title="hive架构原理解析"></a>hive架构原理解析</h3><p>sql-&gt;转换-&gt;MapReduce-&gt;job</p><h3 id="hive安装部署"><a href="#hive安装部署" class="headerlink" title="hive安装部署"></a>hive安装部署</h3><h4 id="1）下载"><a href="#1）下载" class="headerlink" title="1）下载"></a>1）下载</h4><h4 id="2）上传到Linux"><a href="#2）上传到Linux" class="headerlink" title="2）上传到Linux"></a>2）上传到Linux</h4><h4 id="3）解压"><a href="#3）解压" class="headerlink" title="3）解压"></a>3）解压</h4><p>tar -zxvf apache-hive-1.2.2-bin.tar.gz -C hd/ </p><h4 id="4）重命名"><a href="#4）重命名" class="headerlink" title="4）重命名"></a>4）重命名</h4><p>mv apache-hive-1.2.2-bin/ hive</p><h4 id="5）修改配置文件"><a href="#5）修改配置文件" class="headerlink" title="5）修改配置文件"></a>5）修改配置文件</h4><p>mv hive-env.sh.template hive-env.sh<br>vi hive-env.sh<br>HADOOP_HOME=/root/hd/hadoop-2.8.4<br>export HIVE_CONF_DIR=/root/hd/hive/conf</p><h4 id="6）启动"><a href="#6）启动" class="headerlink" title="6）启动"></a>6）启动</h4><p>bin/hive </p><h3 id="配置mysql元数据库"><a href="#配置mysql元数据库" class="headerlink" title="配置mysql元数据库"></a>配置mysql元数据库</h3><h4 id="1）拷贝mysql驱动到hive-lib"><a href="#1）拷贝mysql驱动到hive-lib" class="headerlink" title="1）拷贝mysql驱动到hive/lib"></a>1）拷贝mysql驱动到hive/lib</h4><p>cp/mv hive/lib </p><h4 id="2）添加hive-site-xml"><a href="#2）添加hive-site-xml" class="headerlink" title="2）添加hive-site.xml"></a>2）添加hive-site.xml</h4><pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;&lt;value&gt;jdbc:mysql://hsiehchou121:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt;&lt;description&gt;JDBC connect string for a JDBCmetastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;description&gt;username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;</code></pre><h4 id="3）注意：重启hadoop集群"><a href="#3）注意：重启hadoop集群" class="headerlink" title="3）注意：重启hadoop集群"></a>3）注意：重启hadoop集群</h4><h4 id="4）启动hive"><a href="#4）启动hive" class="headerlink" title="4）启动hive"></a>4）启动hive</h4><p>bin/hive<br>此时mysql中创建metastore元数据库<br>hive&gt; create table hsiehchou(id int, name string)</p><p>row format<br>delimited fields<br>terminated by ‘\t’;<br>OK<br>hive&gt; load data local inpath ‘/root/hsiehchou.txt’ into table hsiehchou;<br>OK<br>hive&gt; select * from hsiehchou;<br>OK<br>1 re<br>2 mi<br>3 zk<br>4 sf<br>5 ls</p><h3 id="杀死hive进程"><a href="#杀死hive进程" class="headerlink" title="杀死hive进程"></a>杀死hive进程</h3><p>[root@hsiehchou121 hive]# ps -aux|grep hive<br>root 3649 3.7 16.9 2027072 239240 pts/0 Tl 21:37 0:31<br>root 4285 0.0 0.0 112648 948 pts/0<br>[root@hsiehchou121 hive]# kill -9 3649</p><h3 id="安装mysql5-6"><a href="#安装mysql5-6" class="headerlink" title="安装mysql5.6"></a>安装mysql5.6</h3><p>yum search libaio # 检索相关信息<br>yum install libaio # 安装依赖包<br>wget <a href="http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm" target="_blank" rel="noopener">http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm</a></p><p>添加 MySQL Yum Repository 到linux系统 repository 列表中，输入<br>yum localinstall mysql-community-release-el7-5.noarch.rpm</p><p>验证是否添加成功<br>yum repolist enabled | grep “mysql.-community.”</p><p>查看 MySQL 版本，输入<br>yum repolist all | grep mysql</p><p>可以看到 5.5， 5.7 版本是默认禁用的，因为现在最新的稳定版是 5.6<br>yum repolist enabled | grep mysql</p><p>通过 Yum 来安装 MySQL，输入<br>yum install mysql-community-server</p><p>rpm -qi mysql-community-server.x86_64 0:5.6.24-3.el7</p><p>查看 MySQL 的安装目录<br>whereis mysql</p><p>启动 MySQL Server<br>systemctl start mysqld</p><p>查看 MySQL Server 状态<br>systemctl status mysqld</p><p>关闭 MySQL Server<br>systemctl stop mysqld</p><p>测试是否安装成功<br>mysql</p><p>修改mysql密码<br>use mysql;<br>update user set password=password(‘root’) where user=’root’;<br>flush privileges;</p><h3 id="数据导入操作"><a href="#数据导入操作" class="headerlink" title="数据导入操作"></a>数据导入操作</h3><p>load data []local] inpath ‘/root/hsiehchou.txt’ into table hsiehchou; </p><p>load data:加载数据 </p><p>local:可选操作，如果加上local导入是本地linux中的数据，如果去掉local 那么 导入的是hdfs中数据</p><p>inpath:表示的是加载数据的路径 </p><p>into table:表示要加载的对应的表</p><h3 id="hive数据类型"><a href="#hive数据类型" class="headerlink" title="hive数据类型"></a>hive数据类型</h3><table><thead><tr><th align="center">Java数据类型</th><th align="center">Hive数据类型</th><th align="center">长度</th></tr></thead><tbody><tr><td align="center">byte</td><td align="center">TINYINT</td><td align="center">1byte有符号整数</td></tr><tr><td align="center">short</td><td align="center">SMALLINT</td><td align="center">2byte有符号整数</td></tr><tr><td align="center">int</td><td align="center">INT</td><td align="center">4byte有符号整数</td></tr><tr><td align="center">long</td><td align="center">GINT</td><td align="center">8byte有符号整数</td></tr><tr><td align="center">boolean</td><td align="center">BOOLEAN</td><td align="center">false/true</td></tr><tr><td align="center">float</td><td align="center">FLOAT</td><td align="center">单精度浮点</td></tr><tr><td align="center">double</td><td align="center">DOUBLE</td><td align="center">双精度浮点</td></tr><tr><td align="center">string</td><td align="center">STRING</td><td align="center">字符</td></tr><tr><td align="center"></td><td align="center">BINARY</td><td align="center">字节数组</td></tr></tbody></table><h3 id="DDL数据定义"><a href="#DDL数据定义" class="headerlink" title="DDL数据定义"></a>DDL数据定义</h3><h4 id="1）查看数据库"><a href="#1）查看数据库" class="headerlink" title="1）查看数据库"></a>1）查看数据库</h4><p>show databases;</p><h4 id="2）创建库"><a href="#2）创建库" class="headerlink" title="2）创建库"></a>2）创建库</h4><p>create database hive_db;</p><h4 id="3）创建库-标准写法"><a href="#3）创建库-标准写法" class="headerlink" title="3）创建库 标准写法"></a>3）创建库 标准写法</h4><p>create database if not exists hive_db;</p><h4 id="4）创建库指定hdfs路径"><a href="#4）创建库指定hdfs路径" class="headerlink" title="4）创建库指定hdfs路径"></a>4）创建库指定hdfs路径</h4><p>create database hive_db location ‘/hive_db’;</p><h4 id="5）创建表"><a href="#5）创建表" class="headerlink" title="5）创建表"></a>5）创建表</h4><p>如果指定了hdfs路径<br>创建的表存在于这个路径</p><h4 id="6）查看数据库结构"><a href="#6）查看数据库结构" class="headerlink" title="6）查看数据库结构"></a>6）查看数据库结构</h4><p>desc database hive_db;</p><h4 id="7）添加额外的描述信息"><a href="#7）添加额外的描述信息" class="headerlink" title="7）添加额外的描述信息"></a>7）添加额外的描述信息</h4><p>alter database hive_db set dbproperties(‘created’=’hsiehchou’);<br>注意：查询需要使用desc database extended hive_db;</p><h4 id="8）查看指定的通配库-过滤"><a href="#8）查看指定的通配库-过滤" class="headerlink" title="8）查看指定的通配库:过滤"></a>8）查看指定的通配库:过滤</h4><p>show databases like ‘h*’;</p><h4 id="9）删除空库"><a href="#9）删除空库" class="headerlink" title="9）删除空库"></a>9）删除空库</h4><p>drop database hive_db;</p><h4 id="10）删除非空库"><a href="#10）删除非空库" class="headerlink" title="10）删除非空库"></a>10）删除非空库</h4><p>drop database hive_db2 cascade;</p><h4 id="11-删除非空库标准写法"><a href="#11-删除非空库标准写法" class="headerlink" title="11) 删除非空库标准写法"></a>11) 删除非空库标准写法</h4><p>drop database if exists hive_db cascade;</p><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><p>create <code>[external]</code> table <code>[if not exists]</code> table_name(字段信息) <code>[partitioned by(字段信息)][clustered by(字段信息)]</code> [sorted by(字段信息)]row format delimited fields terminated by ‘切割符’;</p><h3 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h3><p>默认不加external创建的就是管理表，也称为内部表。<br>MANAGED_TABLE管理表<br>Table Type:MANAGED_TABLE </p><p>查看表类型：<br>desc formatted hsiehchou2;</p><h3 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h3><p>EXTERNAL_TABLE外部表<br>创建方式：<br>create external table student(id int,name string) </p><p>区别：如果是管理表删除hdfs中数据删除，如果是外部表删除hdfs数据不删除！</p><h3 id="hive命令"><a href="#hive命令" class="headerlink" title="hive命令"></a>hive命令</h3><h4 id="1）不登录hive客户端直接输入命令操作Hive"><a href="#1）不登录hive客户端直接输入命令操作Hive" class="headerlink" title="1）不登录hive客户端直接输入命令操作Hive"></a>1）不登录hive客户端直接输入命令操作Hive</h4><p>[root@hsiehchou121 hive]# bin/hive -e “select * from hsiehchou;”<br>19/02/28 03:09:23 WARN conf.HiveConf: HiveConf of name hive.cli,print.current.db does not exist<br>Logging initialized using configuration in jar:file:/root/hd/hive/lib/hive-common-1.2.2.jar!/hive-log4j.properties<br>OK<br>hsiehchou.id hsiehchou.name<br>1 re<br>2 mi<br>3 zk<br>4 sf<br>5 ls</p><h4 id="2）直接把sql写入到文件中"><a href="#2）直接把sql写入到文件中" class="headerlink" title="2）直接把sql写入到文件中"></a>2）直接把sql写入到文件中</h4><p>bin/hive -f /root/hived.sql</p><h4 id="3）查看hdfs文件"><a href="#3）查看hdfs文件" class="headerlink" title="3）查看hdfs文件"></a>3）查看hdfs文件</h4><p>dfs -ls /;<br>dfs -cat /wc/in/words.txt;</p><h4 id="4）查看历史操作"><a href="#4）查看历史操作" class="headerlink" title="4）查看历史操作"></a>4）查看历史操作</h4><p>[root@hsiehchou121 hive]# cat ~/.hivehistory</p><p>在hive/conf/hive-site.xml中增加</p><pre><code>&lt;!--是否显示当前表头--&gt;&lt;property&gt;        &lt;name&gt;hive.cli.print.header&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!--是否显示当前所在库名--&gt;&lt;property&gt;        &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;        &lt;value&gt;true&lt;/value&gt;&lt;/property</code></pre><p><strong>显示效果</strong><br>hive&gt; select * from hsiehchou;<br>OK<br>hsiehchou.id hsiehchou.name<br>1 re<br>2 mi<br>3 zk<br>4 sf<br>5 ls</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ZooKeeper练习</title>
      <link href="/2019/02/23/zookeeper-lian-xi/"/>
      <url>/2019/02/23/zookeeper-lian-xi/</url>
      
        <content type="html"><![CDATA[<h3 id="命令行操作"><a href="#命令行操作" class="headerlink" title="命令行操作"></a>命令行操作</h3><p>配置环境变量：vi /etc/profile<br>export ZOOKEEPER_HOME=<code>/root/hd/zookeeper-3.4.10</code><br>export PATH=<code>$ZOOKEEPER_HOME/bin:$PATH</code></p><p>声明环境变量：source /etc/profile </p><p>发送到其他机器<br>scp /etc/profile hsiehchou122:/etc/<br>scp /etc/profile hsiehchou123:/etc/<br>scp /etc/profile hsiehchou124:/etc/ </p><p>启动zookeeper<br>zkServer.sh start </p><p>查看zookeeper状态<br>zkServer.sh status</p><h4 id="1）启动客户端"><a href="#1）启动客户端" class="headerlink" title="1）启动客户端"></a>1）启动客户端</h4><p>bin/zkCli.sh</p><h4 id="2）连接其它机器客户端操作"><a href="#2）连接其它机器客户端操作" class="headerlink" title="2）连接其它机器客户端操作"></a>2）连接其它机器客户端操作</h4><p>没有太大必要，每台机器内容都一样<br>connect hsiehchou122:2181<br>connect hsiehchou123:2181<br>connect hsiehchou124:2181</p><h4 id="3）查看历史操作记录"><a href="#3）查看历史操作记录" class="headerlink" title="3）查看历史操作记录"></a>3）查看历史操作记录</h4><p>history</p><h4 id="4）查看当前节点的内容"><a href="#4）查看当前节点的内容" class="headerlink" title="4）查看当前节点的内容"></a>4）查看当前节点的内容</h4><p>ls /</p><h4 id="5）存储：创建节点"><a href="#5）存储：创建节点" class="headerlink" title="5）存储：创建节点"></a>5）存储：创建节点</h4><p>create /hsiehchou 10(存储的数据)</p><h4 id="6）查看节点的值"><a href="#6）查看节点的值" class="headerlink" title="6）查看节点的值"></a>6）查看节点的值</h4><p>get /hsiehchou</p><p>10<br>cZxid = 0x400000004<br>ctime = Sat Feb 23 20:05:58 PST 2019<br>mZxid = 0x400000004<br>mtime = Sat Feb 23 20:05:58 PST 2019<br>pZxid = 0x400000004<br>cversion = 0<br>dataVersion = 0<br>aclVersion = 0<br>ephemeralOwner = 0x0<br>dataLength = 2<br>numChildren = 0</p><h4 id="7）创建节点的可选项"><a href="#7）创建节点的可选项" class="headerlink" title="7）创建节点的可选项"></a>7）创建节点的可选项</h4><p>create <code>[-s] [-e]</code> path data acl</p><p>[-p]永久节点–默认<br>[-e] 短暂节点<br>[-s] 带序号</p><p>create -e /re hm<br>注意：此时-e创建的是临时的短暂节点，退出客户端后消失<br>退出客户端：quit</p><p>create -s /re hm<br>注意：此时-s创建是带序号的节点，可以创建节点名相同的，序号依次累加</p><p>[zk: localhost:2181(CONNECTED) 1] create -s /mm hm<br>Created /mm0000000002<br>[zk: localhost:2181(CONNECTED) 2] create -s /mm hm<br>Created /mm0000000003<br>[zk: localhost:2181(CONNECTED) 3] create -s /mm hm<br>Created /mm0000000004<br>[zk: localhost:2181(CONNECTED) 4] create  /re hm<br>Created /re<br>[zk: localhost:2181(CONNECTED) 5] create  /re hm<br>Node already exists: /re<br>创建短暂带序号节点<br>create -e -s /tt bt</p><h4 id="8）修改节点值"><a href="#8）修改节点值" class="headerlink" title="8）修改节点值"></a>8）修改节点值</h4><p>set path data [version]<br>例如：set /re hm2 1<br>[version] 版本<br>注意：设置版本号 必须从0开始</p><h4 id="9）删除节点"><a href="#9）删除节点" class="headerlink" title="9）删除节点"></a>9）删除节点</h4><p>delete path<br>[zk: localhost:2181(CONNECTED) 12] ls /<br>[mm0000000004, re, zookeeper, mm0000000002, mm0000000003, hsiehchou]<br>[zk: localhost:2181(CONNECTED) 13] delete /mm0000000002<br>[zk: localhost:2181(CONNECTED) 14] ls /<br>[mm0000000004, re, zookeeper, mm0000000003, hsiehchou]</p><h4 id="10）创建子节点"><a href="#10）创建子节点" class="headerlink" title="10）创建子节点"></a>10）创建子节点</h4><p>create /re/pa qi</p><h4 id="11）递归删除"><a href="#11）递归删除" class="headerlink" title="11）递归删除"></a>11）递归删除</h4><p>rmr /re</p><h4 id="12）监听"><a href="#12）监听" class="headerlink" title="12）监听"></a>12）监听</h4><p>获得监听（文件）：get path watch<br>获得当前节点下增减变化（文件夹）：ls path watch</p><h4 id="13）查看当前节点的状态"><a href="#13）查看当前节点的状态" class="headerlink" title="13）查看当前节点的状态"></a>13）查看当前节点的状态</h4><p>stat /hsiehchou</p><h3 id="节点状态信息"><a href="#节点状态信息" class="headerlink" title="节点状态信息"></a>节点状态信息</h3><p>czxid：ZooKeeper事务id<br>ctime：节点创建时间<br>mZxid：最后更新的czxid<br>mtime：最后修改的时间*<br>pZxid：最后更新子节点的czxid<br>cversion：子节点的变化号、子节点修改次数<br>dataVersion：数据变化号<br>aclVersion：访问控制列表的变化号<br>ephemeralOwner：临时节点判断<br>dataLength：节点数据长度<br>numChildren：子节点个数</p><h3 id="JAVA-API-练习"><a href="#JAVA-API-练习" class="headerlink" title="JAVA-API 练习"></a>JAVA-API 练习</h3><p><strong>pom.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;    &lt;artifactId&gt;ZKTest&lt;/artifactId&gt;    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;dependencies&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;            &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;            &lt;version&gt;3.4.10&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;junit&lt;/groupId&gt;            &lt;artifactId&gt;junit&lt;/artifactId&gt;            &lt;version&gt;RELEASE&lt;/version&gt;            &lt;scope&gt;compile&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;</code></pre><h3 id="练习1"><a href="#练习1" class="headerlink" title="练习1"></a>练习1</h3><p><strong>ZkClient类</strong></p><pre><code>package com.hsiehchou.zk;import org.apache.zookeeper.*;import org.apache.zookeeper.data.Stat;import org.junit.Before;import org.junit.Test;import java.io.IOException;import java.util.List;public class ZkClient {    private String conected = &quot;hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181,hsiehchou124:2181&quot;;    //毫秒    private int timeout = 2000;    ZooKeeper zkCli = null;    //连接zookeeper集群    @Before    public void init() throws IOException {        //String:连接集群的IP端口号，Int：超时设置，Watcher：监听        zkCli = new ZooKeeper(conected, timeout, new Watcher() {            //回调方法，显示/节点            public void process(WatchedEvent watchedEvent) {                List&lt;String&gt; children;                //获得节点信息 get                try {                    children = zkCli.getChildren(&quot;/&quot;,true);                } catch (KeeperException e) {                    e.printStackTrace();                } catch (InterruptedException e) {                    e.printStackTrace();                }            }        });    }    //测试 是否连通集群  创建节点    @Test    public void createNode() throws KeeperException, InterruptedException {        String p = zkCli.create(&quot;/bq&quot;, &quot;sk&quot;.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);        System.out.println(p);    }    //查看子节点    @Test    public void getChild() throws KeeperException, InterruptedException {        List&lt;String&gt; children = zkCli.getChildren(&quot;/&quot;, true);        for(String c:children){            System.out.println(c);        }    }    //删除子节点数据:delete path    @Test    public void deleteData() throws KeeperException, InterruptedException {        zkCli.delete(&quot;/da&quot;, -1);    }    //修改数据:set path data    @Test    public void setData() throws KeeperException, InterruptedException {        zkCli.setData(&quot;/hsiehchou&quot;,&quot;nihao&quot;.getBytes(),-1);        //查看/hsiehchou        byte[] data = zkCli.getData(&quot;/hsiehchou&quot;, false, new Stat());        System.out.println(new String(data));    }    //指定节点是否存在    @Test    public void testExist() throws KeeperException, InterruptedException {        Stat exists = zkCli.exists(&quot;/hsiehchou&quot;, false);        System.out.println(exists == null ? &quot;no have&quot;:&quot;have&quot;);    }}</code></pre><h3 id="练习2"><a href="#练习2" class="headerlink" title="练习2"></a>练习2</h3><p><strong>WatchDemo类</strong></p><pre><code>package com.hsiehchou.watch;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import java.io.IOException;public class WatchDemo {    public static void main(String[] args) throws IOException, KeeperException, InterruptedException {        String connected = &quot;hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181,hsiehchou124:2181,&quot;;        //毫秒        int timeout = 2000;        //1.连接zookeeper集群        ZooKeeper zkCli = new ZooKeeper(connected, timeout, new Watcher() {            //监听回调            public void process(WatchedEvent watchedEvent) {                System.out.println(&quot;正在监听中.........&quot;);            }        });        //2.监听： ls / watch    get / watch        zkCli.getChildren(&quot;/&quot;, new Watcher() {            public void process(WatchedEvent watchedEvent) {                System.out.println(&quot;此时监听的路径是：&quot;+watchedEvent.getPath());                System.out.println(&quot;此时监听的类型为：&quot;+watchedEvent.getType());                System.out.println(&quot;有人正在修改数据！！！&quot;);            }        },null);        Thread.sleep(Long.MAX_VALUE);    }}</code></pre><p><strong>WatchDemo1类</strong></p><pre><code>package com.hsiehchou.watch;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import java.io.IOException;public class WatchDemo1 {    public static void main(String[] args) throws IOException, KeeperException, InterruptedException {        ZooKeeper zkCli = new ZooKeeper(&quot;hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181,hsiehchou124:2181&quot;, 2000, new Watcher() {            public void process(WatchedEvent watchedEvent) {            }        });        byte[] data = zkCli.getData(&quot;/re&quot;, new Watcher() {            //具体监听的内容            public void process(WatchedEvent watchedEvent) {                System.out.println(&quot;此时监听的路径是：&quot; + watchedEvent.getPath());                System.out.println(&quot;此时监听的类型为：&quot; + watchedEvent.getType());                System.out.println(&quot;有人正在修改数据！！！&quot;);            }        }, null);        System.out.println(new String(data));        Thread.sleep(Long.MAX_VALUE);    }}</code></pre><h3 id="练习3"><a href="#练习3" class="headerlink" title="练习3"></a>练习3</h3><p><strong>ZkClient类</strong></p><pre><code>package com.hsiehchou.qq;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import java.io.IOException;import java.util.ArrayList;import java.util.List;/** * 实现对zookeeper / 的监听 */public class ZkClient {    public static void main(String[] args) throws IOException, KeeperException, InterruptedException {        //1.获取zookeeper的连接        ZkClient zkCli = new ZkClient();        zkCli.getConnect();        //2.指定监听的节点路径        zkCli.getServers();        //3.写业务逻辑，一直监听        zkCli.getWatch();    }    //1.获得zookeeper连接    private String connected = &quot;hsiehchou121:2181,hsiehchou122:2181,hsiehchou123:2181,hsiehchou124:2181&quot;;    //毫秒    private int timeout = 2000;    ZooKeeper zkCli;    public void getConnect() throws IOException {        zkCli = new ZooKeeper(connected, timeout, new Watcher() {            public void process(WatchedEvent watchedEvent) {                List&lt;String&gt; children;                try {                    children = zkCli.getChildren(&quot;/&quot;, true);                    //服务器列表                    ArrayList&lt;String&gt; serverList = new ArrayList&lt;String&gt;();                    //获取每个节点的数据                    for (String c:children){                        byte[] data = zkCli.getData(&quot;/&quot; + c, true, null);                        serverList.add(new String(data));                    }                    //查看服务器列表                    System.out.println(serverList);                } catch (KeeperException e) {                    e.printStackTrace();                } catch (InterruptedException e) {                    e.printStackTrace();                }            }        });    }    //2.指定监听节点路径    public void getServers() throws KeeperException, InterruptedException {        List&lt;String&gt; children = zkCli.getChildren(&quot;/&quot;, true);        //存储服务器列表        ArrayList&lt;String&gt; serverList = new ArrayList&lt;String&gt;();        for (String c:children){            byte[] data = zkCli.getData(&quot;/&quot; + c, true, null);            //添加集合中            serverList.add(new String(data));        }        //打印服务器列表        System.out.println(serverList);    }    //3.一直监听    public void getWatch() throws InterruptedException {        //循环监听        Thread.sleep(Long.MAX_VALUE);    }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> ZooKeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zookeeper介绍</title>
      <link href="/2019/02/21/zookeeper-jie-shao/"/>
      <url>/2019/02/21/zookeeper-jie-shao/</url>
      
        <content type="html"><![CDATA[<h3 id="ZooKeeper"><a href="#ZooKeeper" class="headerlink" title="ZooKeeper"></a>ZooKeeper</h3><p>官网：<a href="http://zookeeper.apache.org/" target="_blank" rel="noopener">http://zookeeper.apache.org/</a><br>介绍：Apache ZooKeeper致力于开发和维护开源服务器，实现高度可靠的分布式协调</p><p>ZooKeeper是一种集中式服务，用于维护配置信息，命名，提供分布式同步和提供组服务。所有这些类型的服务都以分布式应用程序的某种形式使用。每次实施它们都需要做很多工作来修复不可避免的错误和竞争条件。由于难以实现这些类型的服务，应用程序最初通常会吝啬它们，这使得它们在变化的情况下变得脆弱并且难以管理。即使正确完成，这些服务的不同实现也会在部署应用程序时导致管理复杂性</p><h4 id="1、ZooKeeper工作原理"><a href="#1、ZooKeeper工作原理" class="headerlink" title="1、ZooKeeper工作原理"></a>1、ZooKeeper工作原理</h4><p>ZooKeeper功能：存储+监听</p><h4 id="2、ZooKeeper角色"><a href="#2、ZooKeeper角色" class="headerlink" title="2、ZooKeeper角色"></a>2、ZooKeeper角色</h4><p>主从结构<br>1）Leader领导者-》主<br>2）Follower追随者-》从<br>3）ZooKeeper由一个领导者多个追随者组成<br>ZK集群中只要有半数以上的节点存活，zk集群就能正常工作。所以搭建ZK集群最好搭建<br>奇数台（3,5,11）</p><h4 id="3、ZooKeeper功能"><a href="#3、ZooKeeper功能" class="headerlink" title="3、ZooKeeper功能"></a>3、ZooKeeper功能</h4><p>大数据中使用ZooKeeper业务<br>1）做统一的配置管理<br>2）做统一的命名服务<br>3）做统一的集群管理<br>4）做服务器的动态上下线感知（代码）</p><h4 id="4、单节点安装部署"><a href="#4、单节点安装部署" class="headerlink" title="4、单节点安装部署"></a>4、单节点安装部署</h4><p>1）下载安装包</p><p>2）上传安装到linux<br>alt+p</p><p>3）解压<br>tar -zxvf zookeeper-3.4.10.tar.gz -C hd/</p><p>4）修改配置文件<br>重命名：mv zoo_sample.cfg zoo.cfg</p><p>5）创建文件夹zkData<br>添加到配置文件：zoo.cfg<br>dataDir=/root/hd/zookeeper-3.4.10/zkData</p><p>6）启动ZooKeeper<br>bin/zkServer.sh start</p><p>7）启动ZooKeeper客户端<br>bin/zkCli.sh</p><h4 id="5、ZooKeeper集群安装部署"><a href="#5、ZooKeeper集群安装部署" class="headerlink" title="5、ZooKeeper集群安装部署"></a>5、ZooKeeper集群安装部署</h4><p>1）下载安装包</p><p>2）上传安装到linux<br>alt+p</p><p>3）解压<br>$ tar -zxvf zookeeper-3.4.10.tar.gz -C hd/</p><p>4）修改配置文件名<br>重命名：mv zoo_sample.cfg zoo.cfg<br>或者拷贝：cp zoo_sample.cfg zoo.cfg</p><p>5）修改配置<br>vi zookeeper-3.4.10/conf/zoo.cfg</p><p>dataDir=/root/hd/zookeeper-3.4.10/zkData</p><p>—————-zkconfig————<br>server.1=hsiehchou121:2888:3888<br>server.2=hsiehchou122:2888:3888<br>server.3=hsiehchou123:2888:3888<br>server.4=hsiehchou124:2888:3888</p><p>创建文件<strong>myid</strong> </p><p>添加服务器编号：1<br>[root@hsiehchou121 zookeeper-3.4.10]# cd zkData/<br>[root@hsiehchou121 zkData]# touch myid</p><h4 id="6）拷贝ZooKeeper到其它机器"><a href="#6）拷贝ZooKeeper到其它机器" class="headerlink" title="6）拷贝ZooKeeper到其它机器"></a>6）拷贝ZooKeeper到其它机器</h4><p> scp -r zookeeper-3.4.10/ hsiehchou122:<code>$PWD</code><br> scp -r zookeeper-3.4.10/ hsiehchou123:<code>$PWD</code><br> scp -r zookeeper-3.4.10/ hsiehchou124:<code>$PWD</code></p><h4 id="7）注意需要修改每台机器的myid文件"><a href="#7）注意需要修改每台机器的myid文件" class="headerlink" title="7）注意需要修改每台机器的myid文件"></a>7）注意需要修改每台机器的myid文件</h4><p>设置为当前的机器编号即可</p><h4 id="8）启动ZooKeeper集群"><a href="#8）启动ZooKeeper集群" class="headerlink" title="8）启动ZooKeeper集群"></a>8）启动ZooKeeper集群</h4><p>bin/zkServer.sh start</p><h4 id="9）查看ZooKeeper状态"><a href="#9）查看ZooKeeper状态" class="headerlink" title="9）查看ZooKeeper状态"></a>9）查看ZooKeeper状态</h4><p>bin/zkServer.sh status</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> ZooKeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据常用基本算法</title>
      <link href="/2019/02/18/da-shu-ju-chang-yong-ji-ben-suan-fa/"/>
      <url>/2019/02/18/da-shu-ju-chang-yong-ji-ben-suan-fa/</url>
      
        <content type="html"><![CDATA[<h4 id="1、冒泡排序"><a href="#1、冒泡排序" class="headerlink" title="1、冒泡排序"></a>1、冒泡排序</h4><p>冒泡排序（Bubble Sort），是一种计算机科学领域的较简单的排序算法，它重复地走访过要排序的元素列，依次比较两个相邻的元素，如果他们的顺序（如从大到小、首字母从A到Z）错误就把他们交换过来。走访元素的工作是重复地进行直到没有<br>相邻元素需要交换，也就是说该元素已经排序完成这个算法的名字由来是因为越大的元素会经由交换慢慢“浮”到数列的顶端（升序或降序排列），就如同碳酸饮料中二氧化碳的气泡最终会上浮到顶端一样，故名“<strong>冒泡排序</strong>” </p><p>冒泡排序算法的原理如下：<br>1）比较相邻的元素。如果第一个比第二个大，就交换他们两个 </p><p>2）对每一对相邻元素做同样的工作，从开始第一对到结尾的最后一对。在这一点，最后的元素应该会是最大的数 </p><p>3）针对所有的元素重复以上的步骤，除了最后一个 </p><p>4）持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较 </p><p>列如：<br>数组元素&gt;<br>5 1 7 2 6 4 3 16 </p><p>1）由于第一个元素5比第二个元素大1，交换它们的位置。<br>1 5 7 2 6 4 3 16 </p><p>2）对比每个相邻的元素，此时到第二个元素5与第三个元素7，不交换位置<br>1 5 7 2 6 4 3 16 </p><p>3）对比每个相邻的元素，此时到第三个元素7与第四个元素2，交换位置<br>1 5 2 7 6 4 3 16 </p><p>4）对比每个相邻的元素，此时到第四个元素7与第五个元素6，交换位置<br>1 5 2 6 7 4 3 16 </p><p>5）对比每个相邻的元素，此时到第五个元素7与第六个元素4，交换位置<br>1 5 2 6 4 7 3 16 </p><p>6）对比每个相邻的元素，此时到第六个元素7与第七个元素3，交换位置<br>1 5 2 6 4 3 7 16 </p><p>7）对比每个相邻的元素，此时到第七个元素7与第八个元素16，不换位置<br>1 5 2 6 4 3 7 16</p><h4 id="2、双冒泡排序"><a href="#2、双冒泡排序" class="headerlink" title="2、双冒泡排序"></a>2、双冒泡排序</h4><p>双向冒泡算法，极大的减少了循环排序的次数<br>1）传统冒泡气泡排序的双向进行，先让气泡排序由左向右进行，再来让气泡排序由右往左进行，如此完成一次排序的动作 </p><p>2）使用left与right两个旗标来记录左右两端已排序的元素位置 </p><p>3）当往左递进left &gt;=往右递进的 right时，则排序完成<br>例子如下所示：<br>排序前：45 19 77 81 13 28 18 19 77 11<br>往右排序：19 45 77 13 28 18 19 77 11 [81]<br>向左排序：[11] 19 45 77 13 28 18 19 77 [81]<br>往右排序：[11] 19 45 13 28 18 19 [77 77 81]<br>向左排序：[11 13] 19 45 18 28 19 [77 77 81]<br>往右排序：[11 13] 19 18 28 19 [45 77 77 81]<br>向左排序：[11 13 18] 19 19 28 [45 77 77 81]<br>往右排序：[11 13 18] 19 19 [28 45 77 77 81]<br>向左排序：[11 13 18 19 19] [28 45 77 77 81]<br>此时28&gt;=19条件成立排序完成</p><h4 id="3、快速排序"><a href="#3、快速排序" class="headerlink" title="3、快速排序"></a>3、快速排序</h4><p>快速排序（Quicksort）是对冒泡排序的一种改进快速排序的基本思想：<br>首先选取一个记录作为枢(shu)轴，不失一般性，可选第一个记 录，依它的关键字为基准重排其余记录，将所有关键字比它大的记录都安置在它之后，而将所有关键字比它小的记录都安置在之前，由此完成一趟快速排序；之后，分别对由一趟排序分割成的两个子序列进行快速排序，在大数据情况下要使用快速排序 </p><p>列如：<br>数组元素&gt;<br>5 1 7 2 6 4 3 16 </p><p>思路：<br>取第一个数，把小于它的数往左移动，把大于它的数右移动<br>1）最左侧大于5的为7，最右侧小于5的为3,7与3对调<br>以5为枢轴&gt;<br>5 1 3 2 6 4 7 16 </p><p>2）全部对调完成，此时左侧小于5，右边大于5<br>5 1 3 2 | 6 4 7 16 </p><p>3）5移动到分割位置<br>1 3 2 5 6 4 7 16 </p><p>4）如果把数组元素分为三部分的话 左侧&lt;中间&lt;右侧<br>1 3 2 | 5 | 6 4 7 16<br>此时只需对两侧再重复以上操作就可以了 </p><p>5）重复以上操作<br>1 3 2 &gt;<br>1 2 3<br>此时左侧<br>6 4 7 16 &gt;<br>4 6 7 16<br>简单来说：定义基数，比它小的往左排，比它大的往右排</p><h4 id="4、归并排序"><a href="#4、归并排序" class="headerlink" title="4、归并排序"></a>4、归并排序</h4><p><strong>归并排序（MERGESORT）</strong><br>是建立在归并操作上的一种有效的排序算法,该算法是采用<br>分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为二路归并</p><p>归并操作(merge)，也叫归并算法，指的是将两个顺序序列合并成一个顺序序列的方法<br>如 设有数列1 8 2 9 3 5 6 4 10 </p><p>1）第一次归并后：{1 8},{2 9},{ 3 5},{ 4 6}，{10}此时两两元素排序完的归并 </p><p>2）第二次归并后：{1 2 8 9}，{ 3 4 5 6} ，{10}此时两两元素归并<br>1与2 寻找最小数 1<br>8与2 寻找最小数 2<br>8与9寻找最小数 8<br>{1 2 8 9} </p><p>3）第三次归并后：{1 2 3 4 5 6 8 9} , {10}此时两两元素归并<br>1与3寻找到最小数1 {1}<br>2与3寻找最小数2 {1 2}<br>8与3寻找最小数3 {1 2 3}<br>8与4寻找最小数4 {1 2 3 4}<br>8与5寻找最小数5 {1 2 3 4 5}<br>8与6寻找最小数6 {1 2 3 4 5 6}<br>8 9 落下{1 2 3 4 5 6 8 9}<br>4）第四次归并后：{1 2 3 4 5 6 8 9 10}<br>思路：循环找到最小值落下</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据基础 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据压缩、数据倾斜join操作</title>
      <link href="/2019/02/17/shu-ju-ya-suo-shu-ju-qing-xie-join-cao-zuo/"/>
      <url>/2019/02/17/shu-ju-ya-suo-shu-ju-qing-xie-join-cao-zuo/</url>
      
        <content type="html"><![CDATA[<h3 id="1、数据压缩发生阶段"><a href="#1、数据压缩发生阶段" class="headerlink" title="1、数据压缩发生阶段"></a>1、数据压缩发生阶段</h3><table><thead><tr><th align="center">端</th><th align="center">操作</th><th align="center">Col3</th></tr></thead><tbody><tr><td align="center">数据源</td><td align="center">》数据传输</td><td align="center">数据压缩</td></tr><tr><td align="center">mapper</td><td align="center">map端输出压缩</td><td align="center"></td></tr><tr><td align="center"></td><td align="center">》数据传输</td><td align="center">数据压缩</td></tr><tr><td align="center">reducer</td><td align="center">reduce端输出压缩</td><td align="center"></td></tr><tr><td align="center"></td><td align="center">》数据传输</td><td align="center">数据压缩</td></tr><tr><td align="center">结果数据</td><td align="center"></td><td align="center"></td></tr></tbody></table><p><strong>设置map端输出压缩</strong><br>1）开启压缩<br>conf.setBoolean<br> //开启map端输出压缩<br> conf.setBoolean(“mapreduce.map.output.compress”,true);</p><p>2）设置具体压缩编码<br>conf.setClass<br> //设置压缩方式<br> //conf.setClass(“mapreduce.map.output.compress.codec”, BZip2Codec.class, CompressionCodec.class);</p><p> conf.setClass(“mapreduce.map.output.compress.codec”, DefaultCodec.class, CompressionCodec.class);</p><p><strong>设置reduce端输出压缩</strong><br>1）设置reduce输出压缩<br>FileOutputFormat.setCompressOutput</p><p>//设置reduce端输出压缩<br>FileOutputFormat.setCompressOutput(job,true);</p><p>2）设置具体压缩编码<br>FileOutputFormat.setOutputCompressorClass</p><p>//设置压缩方式<br>//FileOutputFormat.setOutputCompressorClass(job,BZip2Codec.class);</p><p>//FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);</p><p>FileOutputFormat.setOutputCompressorClass(job,DefaultCodec.class);<br>hive数据仓库：mapreduce 用hsql处理大数据</p><h3 id="2、压缩编码使用场景"><a href="#2、压缩编码使用场景" class="headerlink" title="2、压缩编码使用场景"></a>2、压缩编码使用场景</h3><h4 id="1-gt-Gzip压缩方式"><a href="#1-gt-Gzip压缩方式" class="headerlink" title="1-&gt; Gzip压缩方式"></a>1-&gt; Gzip压缩方式</h4><p>压缩率比较高，并且压缩解压缩速度很快<br>hadoop自身支持的压缩方式，用gzip格式处理数据就像直接处理文本数据是完全一样<br>的；<br>在linux系统自带gzip命令，使用很方便简洁<br>不支持split<br>使用每个文件压缩之后大小需要在128M以下（块大小）<br>200M-》设置块大小</p><h4 id="2-gt-LZO压缩方式"><a href="#2-gt-LZO压缩方式" class="headerlink" title="2-&gt;LZO压缩方式"></a>2-&gt;LZO压缩方式</h4><p>压缩解压速度比较快并且，压缩率比较合理<br>支持split<br>在linux系统不可以直接使用，但是可以进行安装<br>压缩率比gzip和bzip2要弱，hadoop本身不支持<br>需要安装</p><h4 id="3-gt-Bzip2压缩方式"><a href="#3-gt-Bzip2压缩方式" class="headerlink" title="3-&gt;Bzip2压缩方式"></a>3-&gt;Bzip2压缩方式</h4><p>支持压缩，具有很强的压缩率。hadoop本身支持<br>linux中可以安装<br>压缩解压缩速度很慢</p><h4 id="4-gt-Snappy压缩方式"><a href="#4-gt-Snappy压缩方式" class="headerlink" title="4-&gt;Snappy压缩方式"></a>4-&gt;Snappy压缩方式</h4><p>压缩解压缩速度很快，而且有合理的压缩率<br>不支持split</p><h3 id="3、数据倾斜"><a href="#3、数据倾斜" class="headerlink" title="3、数据倾斜"></a>3、数据倾斜</h3><p>reduce join<br>数据倾斜就是我们在计算数据的时候，数据的分散度不够，导致大量的数据集中到了一台或者几台机器上计算，这些数据的计算速度远远低于平均计算速度，导致整个计算过程过慢</p><h3 id="4、Hadoop中有哪些组件"><a href="#4、Hadoop中有哪些组件" class="headerlink" title="4、Hadoop中有哪些组件"></a>4、Hadoop中有哪些组件</h3><p>HDFS：数据的分布式存储<br>MapReduce:数据的分布式计算<br>Yarn:资源调度(cpu/内存…)<br>Yarn节点：resourceManager、nodeManager</p><h3 id="5、优化"><a href="#5、优化" class="headerlink" title="5、优化"></a>5、优化</h3><p>MapReduce程序的编写过程中考虑的问题<br>优化目的：提高程序运行的效率<br>优化方案：<br>存储和处理海量数据，如何优化MR<br>影响MR程序的因素<br>1）硬件<br>压缩<br>CPU/磁盘(固态、机械)/内存/网络… </p><p>2）I/O优化<br>传输<br>-》maptask与reducetask合理设置个数<br>-》数据倾斜（reducetask-》merge）<br>避免出现数据倾斜<br>-》大量小文件情况 （combineTextInputFormat）<br>-》combiner优化（不影响业务逻辑）</p><p>具体优化方式：<br>MR（数据接入、Map、Reduce、IO传输、处理倾斜、参数优化）<br>数据接入：小文件的话 进行合并 ，namenode存储元数据信息，sn<br>解决方式：CombineTextInputFormat</p><p>Map:会发生溢写，如果减少溢写次数也能达到优化<br>溢写内存增加这样就减少了溢写次数<br>解决方式：mapred-site.xml<br>属性：<br>mapreduce.task.io.sort.mb<br>100<br>调大</p><p>mapreduce.map.sort.spill.percent<br>0.8<br>调大</p><p>combiner:map后优化</p><p>Reduce:reduceTask设置合理的个数<br>写mr程序可以合理避免写reduce阶段<br>设置map/reduce共存<br>属性：<br>mapred-site.xml<br>mapreduce.job.reduce.slowstart.completedmaps<br>0.05<br>减少</p><p><strong>IO传输：压缩</strong><br>数据倾斜：避免出现数据倾斜，map端合并。手动的对数据进行分段处理，合理的<br>分区</p><p><strong>JVM重用</strong><br>不关JVM<br>一个map运行一个jvm,开启重用，在运行完这个map后JVM继续运行其它map。<br>线程池<br>属性：mapreduce.job.jvm.numtasks<br>20<br>启动40%运行时间</p><h3 id="6、进行两个表的拼接"><a href="#6、进行两个表的拼接" class="headerlink" title="6、进行两个表的拼接"></a>6、进行两个表的拼接</h3><p><strong>DistributedCacheMapper类</strong></p><pre><code>package com.hsiehchou.mapjoin;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.BufferedReader;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.net.URI;import java.util.HashMap;/** * mapjoin * 完成两张表数据的关联操作 */public class DistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; {    HashMap&lt;String, String&gt; pdMap = new HashMap&lt;String, String&gt;();    @Override    protected void setup(Context context) throws IOException, InterruptedException {        //1.加载缓存文件        URI[] cacheFiles = context.getCacheFiles();        BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(cacheFiles[0].getPath()), &quot;UTF-8&quot;));        //这里可以将文件放在当前项目文件下，如果不放就用上面的那两句        //BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(&quot;pd.txt&quot;), &quot;UTF-8&quot;));        String line;        //2.判断缓存文件不为空        while(StringUtils.isNotEmpty(line = br.readLine())){            //切割数据            String[] fields = line.split(&quot;\t&quot;);            //缓冲 到 集合; 商品ID  商品名            pdMap.put(fields[0],fields[1]);        }        br.close();    }    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        //1.获取数据        String line = value.toString();        //2.切分数据        String[] fields = line.split(&quot;\t&quot;);        //3.获取商品的pid,商品名称        String pid = fields[1];        String pName = pdMap.get(pid);        //4.拼接        line = line + &quot;\t&quot; + pName;        //5.输出        context.write(new Text(line),NullWritable.get());    }}</code></pre><p><strong>DistributedCacheDriver类</strong></p><pre><code>package com.hsiehchou.mapjoin;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.net.URI;import java.net.URISyntaxException;public class DistributedCacheDriver {    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException, URISyntaxException {      //创建job任务      Configuration conf = new Configuration();      Job job = Job.getInstance(conf);      //指定jar包位置      job.setJarByClass(DistributedCacheDriver.class);      //关联使用的Mapper      job.setMapperClass(DistributedCacheMapper.class);      //设置最终的输出的数据类型      job.setOutputKeyClass(Text.class);      job.setOutputValueClass(NullWritable.class);      //设置数据输入的路径      FileInputFormat.setInputPaths(job,new Path(&quot;e://test//table//in&quot;));      //设置数据输出的路径      FileOutputFormat.setOutputPath(job,new Path(&quot;e://test//table//out&quot;));      //加载缓存数据      job.addCacheFile(new URI(&quot;file:///e:/test/inputcache/pd.txt&quot;));      //注意：没有跑reducer  需要指定reduceTask为0      job.setNumReduceTasks(0);      //提交任务      boolean rs = job.waitForCompletion(true);      System.exit(rs? 0:1);    }}</code></pre><p><strong>本地模式测试</strong><br>URI[] cacheFiles = context.getCacheFiles();<br>BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(cacheFiles[0].getPath()), “UTF-8”));    </p><p><strong>集群模式时</strong><br>conf.set(“mapreduce.framework.name”, “yarn”);yarn模式<br>job.addCacheFile(new URI(“hdfs:///test2/pd.txt”));//添加hdfs文件做缓存</p>]]></content>
      
      
      <categories>
          
          <category> 大数据实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据基础 </tag>
            
            <tag> combiner </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据之排序、combiner、压缩</title>
      <link href="/2019/02/16/da-shu-ju-zhi-pai-xu-combiner-ya-suo/"/>
      <url>/2019/02/16/da-shu-ju-zhi-pai-xu-combiner-ya-suo/</url>
      
        <content type="html"><![CDATA[<h4 id="1、自定义分区"><a href="#1、自定义分区" class="headerlink" title="1、自定义分区"></a>1、自定义分区</h4><p>需求：统计结果进行分区，根据手机号前三位来进行分区<br>总结：<br>1）自定义类继承partitioner&lt;key,value&gt;<br>2）重写方法getPartition()<br>3）业务逻辑<br>4）在driver类中加入<br>setPartitionerClass<br>5）注意：需要指定setNumReduceTasks(个数=分区数+1) </p><p><strong>新增PhonenumPartitioner类</strong></p><pre><code>package com.hsiehchou.logs1;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;/** * 自定义分区，根据手机号前三位 * 默认分区方式，hash */public class PhonenumPartitioner extends Partitioner&lt;Text, FlowBean&gt; {    @Override    public int getPartition(Text key, FlowBean value, int numPartitions) {        //1.获取手机号的前三位        String phoneNum = key.toString().substring(0, 3);        //2.分区        int partitioner = 4;        if (&quot;135&quot;.equals(phoneNum)){            return 0;        }else if (&quot;137&quot;.equals(phoneNum)){            return 1;        }else if (&quot;138&quot;.equals(phoneNum)){            return 2;        }else if(&quot;139&quot;.equals(phoneNum)){            return 3;        }        return partitioner;    }}</code></pre><p><strong>FlowCountDriver类</strong>中增加</p><pre><code>//加入自定义分区job.setPartitionerClass(PhonenumPartitioner.class);//注意，结果文件几个？job.setNumReduceTasks(5);//7.设置数据输入的路径FileInputFormat.setInputPaths(job, new Path(&quot;E:/test/flow/in&quot;));//8.设置数据输出的路径FileOutputFormat.setOutputPath(job, new Path(&quot;E:/test/flow/out2&quot;));</code></pre><h4 id="2、排序"><a href="#2、排序" class="headerlink" title="2、排序"></a>2、排序</h4><p>需求：每个分区内进行排序？<br>总结：<br>1）实现WritableComparable接口<br>2）重写compareTo方法</p><p>combineTextInputFormat设置切片的大小 maptask</p><p>实现 </p><p><strong>FlowBean类</strong></p><pre><code>package com.hsiehchou.logs2;import org.apache.hadoop.io.WritableComparable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;public class FlowBean implements WritableComparable&lt;FlowBean&gt; {    //定义属性：上行流量 下行流量 总流量总和    private long upFlow;    private long dfFlow;    private long flowsum;    public FlowBean(){}    public FlowBean(long upFlow,long dfFlow){        this.upFlow = upFlow;        this.dfFlow = dfFlow;        this.flowsum = upFlow + dfFlow;    }    public long getUpFlow(){        return upFlow;    }    public void setUpFlow(long upFlow){        this.upFlow = upFlow;    }    public long getDfFlow(){        return dfFlow;    }    public void setDfFlow(long dfFlow){        this.dfFlow = dfFlow;    }    public long getFlowsum(){        return flowsum;    }    public void setFlowsum(long flowsum){        this.flowsum = flowsum;    }    //序列化    public void write(DataOutput out) throws IOException {        out.writeLong(upFlow);        out.writeLong(dfFlow);        out.writeLong(flowsum);    }    //反序列化    public void readFields(DataInput in) throws IOException {        upFlow = in.readLong();        dfFlow = in.readLong();        flowsum = in.readLong();    }    @Override    public String toString() {        return upFlow + &quot;\t&quot; + dfFlow + &quot;\t&quot; + flowsum;    }    public int compareTo(FlowBean o) {        //倒序        return this.flowsum &gt; o.getFlowsum() ? -1:1;    }}</code></pre><p><strong>FlowSortMapper类</strong></p><pre><code>package com.hsiehchou.logs2;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;public class FlowSortMapper extends Mapper&lt;LongWritable,Text,FlowBean,Text&gt; {    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        //1.接入数据        String line = value.toString();        //2.切割 \t        String[] fields = line.split(&quot;\t&quot;);        //3.拿到关键字段:手机号 上行流量 下行流量        String phoneNr = fields[0];        long upFlow = Long.parseLong(fields[1]);        long dfFlow = Long.parseLong(fields[2]);        //4.写出到reducer        context.write(new FlowBean(upFlow,dfFlow),new Text(phoneNr));    }}</code></pre><p><strong>FlowSortReducer类</strong></p><pre><code>package com.hsiehchou.logs2;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;import java.io.IOException;public class FlowSortReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; {    @Override    protected void reduce(FlowBean key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException {        //手机号 流量        context.write(values.iterator().next(),key);    }}</code></pre><p><strong>FlowSortPartitioner类</strong></p><pre><code>package com.hsiehchou.logs2;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;public class FlowSortPartitioner extends Partitioner&lt;FlowBean, Text&gt; {    @Override    public int getPartition(FlowBean key, Text value, int numPartitions) {        //1.获取手机号的前三位        String phoneNum = value.toString().substring(0, 3);        //2.分区        int partitioner = 4;        if (&quot;135&quot;.equals(phoneNum)){            return 0;        }else if (&quot;137&quot;.equals(phoneNum)){            return 1;        }else if (&quot;138&quot;.equals(phoneNum)){            return 2;        }else if(&quot;139&quot;.equals(phoneNum)){            return 3;        }        return partitioner;    }}</code></pre><p><strong>FlowSortDriver类</strong></p><pre><code>package com.hsiehchou.logs2;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;public class FlowSortDriver {    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {        //1.创建job任务        Configuration conf = new Configuration();        Job job = Job.getInstance(conf);        //2.指定kjar包位置        job.setJarByClass(FlowSortDriver.class);        //3.关联使用的Mapper        job.setMapperClass(FlowSortMapper.class);        //4.关联使用的Reducer类        job.setReducerClass(FlowSortReducer.class);        //5.设置mapper阶段输出的数据类型        job.setMapOutputKeyClass(FlowBean.class);        job.setMapOutputValueClass(Text.class);        //6.设置reducer阶段输出的数据类型        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(FlowBean.class);        //加入自定义分区        job.setPartitionerClass(FlowSortPartitioner.class);        //注意，结果文件几个        job.setNumReduceTasks(5);        //7.设置数据输入的路径        FileInputFormat.setInputPaths(job, new Path(&quot;E:/test/flow/out&quot;));        //8.设置数据输出的路径        FileOutputFormat.setOutputPath(job, new Path(&quot;E:/test/flow/out4&quot;));        //9.提交任务        boolean  rs = job.waitForCompletion(true);        System.exit(rs? 0:1);    }}</code></pre><h4 id="3、combiner-合并"><a href="#3、combiner-合并" class="headerlink" title="3、combiner 合并"></a>3、combiner 合并</h4><p>1）combiner是一个组件<br>注意：是Mapper和Reducer之外的一种组件<br>但是这个组件的父类是Reduer</p><p>2）如果想使用combiner继承Reduer即可</p><p>3）通过编写combiner发现与Reducer代码相同<br>只需在Driver端指定<br>setCombinerClass(WordCountReduer.class)<br>注意：前提是不能影响业务逻辑&lt;a,1&gt;&lt;c,1&gt; &lt;a,2&gt;&lt;a,1&gt; = &lt;a,3&gt;<br>数学运算：<br>(3 + 5 + 7)/3 = 5<br>(2 + 6)/2 = 4<br>不进行局部累加：（3 + 5 + 7 + 2 + 6）/5 = 23/5<br>进行了局部累加：（5+4）/2 = 9/2=4.5 不等于 23/5=4.6</p><h4 id="4、数据压缩"><a href="#4、数据压缩" class="headerlink" title="4、数据压缩"></a>4、数据压缩</h4><p>为什么对数据进行压缩？<br>MapReduce操作需要对大量数据进行传输<br>压缩技术有效的减少底层存储系统读写字节数，HDFS<br>压缩提高网络带宽和磁盘空间效率<br>数据压缩节省资源，减少网络I/O</p><p>通过压缩可以影响到MapReduce性能。(小文件优化，combiner)代码角度进行优化</p><p>注意：利用好压缩提高性能，运用不好会降低性能<br>压缩 -》 解压缩 </p><p><strong>mapreduce常用的压缩编码</strong></p><table><thead><tr><th align="center">压缩格式</th><th align="center">是否需要安装</th><th align="center">文件拓展名</th><th align="center">是否可以切分</th></tr></thead><tbody><tr><td align="center">DEFAULT</td><td align="center">直接使用</td><td align="center">.deflate</td><td align="center">否</td></tr><tr><td align="center">bzip2</td><td align="center">直接使用</td><td align="center">.bz2</td><td align="center">是</td></tr><tr><td align="center">Gzip</td><td align="center">直接使用</td><td align="center">.gz</td><td align="center">否</td></tr><tr><td align="center">LZO</td><td align="center">需要安装</td><td align="center">.lzo</td><td align="center">是</td></tr><tr><td align="center">Snappy</td><td align="center">需要安装</td><td align="center">.snappy</td><td align="center">否</td></tr></tbody></table><p><strong>性能测试</strong></p><table><thead><tr><th align="center">压缩格式</th><th align="center">原文件大小</th><th align="center">压缩后大小</th><th align="center">压缩速度</th><th align="center">解压速度</th></tr></thead><tbody><tr><td align="center">gzip</td><td align="center">8.3GB</td><td align="center">1.8GB</td><td align="center">20MB/s</td><td align="center">60MB/s</td></tr><tr><td align="center">LZO</td><td align="center">8.3GB</td><td align="center">3GB</td><td align="center">50MB/s</td><td align="center">70MB/s</td></tr><tr><td align="center">bzip2</td><td align="center">8.3GB</td><td align="center">1.1GB</td><td align="center">3MB/s</td><td align="center">10MB/s</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 大数据实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据基础 </tag>
            
            <tag> HDFS </tag>
            
            <tag> combiner </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据之MapReduce小实战</title>
      <link href="/2019/02/14/da-shu-ju-zhi-mapreduce-xiao-shi-zhan/"/>
      <url>/2019/02/14/da-shu-ju-zhi-mapreduce-xiao-shi-zhan/</url>
      
        <content type="html"><![CDATA[<h3 id="手写wordcount的程序"><a href="#手写wordcount的程序" class="headerlink" title="手写wordcount的程序"></a>手写wordcount的程序</h3><h4 id="1、pom-xml"><a href="#1、pom-xml" class="headerlink" title="1、pom.xml"></a>1、pom.xml</h4><pre><code>  &lt;dependencies&gt;    &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs-client --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;            &lt;version&gt;2.7.3&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;</code></pre><h4 id="2、新建Mapper类"><a href="#2、新建Mapper类" class="headerlink" title="2、新建Mapper类"></a>2、新建Mapper类</h4><pre><code>package com.hsiehchou.wordcount;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;/** * 海量数据 * * hello hsiehchou * nihao * * 数据的输入与输出以Key value进行传输 * keyIN:LongWritable(Long) 数据的起始偏移量 * valuewIN:具体数据 * * mapper需要把数据传递到reducer阶段（&lt;hello,1&gt;） * keyOut:单词 Text * valueOut:出现的次数IntWritable * */public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {    //对数据进行打散 ctrl+o    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        //1、接入数据 hello nihao        String line = value.toString();        //2、对数据进行切分        String[] words = line.split(&quot; &quot;);        //3、写出以&lt;hello,1&gt;        for (String w:words){            //写出reducer端            context.write(new Text(w), new IntWritable(1));        }    }}</code></pre><p><strong>mapper端原理</strong></p><p><img src="../../mapper%E7%AB%AF%E5%8E%9F%E7%90%86.PNG" alt="mapper端原理"></p><h4 id="3、新建Reducer类"><a href="#3、新建Reducer类" class="headerlink" title="3、新建Reducer类"></a>3、新建Reducer类</h4><pre><code>package com.hsiehchou.wordcount;import org.apache.curator.framework.recipes.locks.InterProcessReadWriteLock;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;import java.io.IOException;/** * reducer阶段接收的是Mapper输出的数据 * mapper的输出是reducer输入 * * keyIn:mapper输出的key的类型 * valueIn:mapper输出的value的类型 * * reducer端输出的数据类型，想要一个什么样的结果&lt;hello,1888&gt; * keyOut:Text * valueOut:IntWritalble * */public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {    //key--&gt;单词  value--&gt;次数    @Override    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {        //1、记录出现的次数        int sum = 0;        for (IntWritable v:values){            sum += v.get();        }        //2、l累加求和输出        context.write(key, new IntWritable(sum));    }}</code></pre><h4 id="4、新建驱动类"><a href="#4、新建驱动类" class="headerlink" title="4、新建驱动类"></a>4、新建驱动类</h4><pre><code>package com.hsiehchou.wordcount;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;public class WordCountDriver {    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {        //1、创建job任务        Configuration conf = new Configuration();        Job job = Job.getInstance(conf);        //2、指定jar包位置        job.setJarByClass(WordCountDriver.class);        //3、关联使用的Mapper类        job.setMapperClass(WordCountMapper.class);        //4、关联使用的Reducer类        job.setReducerClass(WordCountReducer.class);        //5、设置mapper阶段输出的数据类型        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(IntWritable.class);        //6、设置reducer阶段输出的数据类型        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(IntWritable.class);        //7、设置数据输入的路径        FileInputFormat.setInputPaths(job, new Path(args[0]));        //8设置数据输出的路径        FileOutputFormat.setOutputPath(job, new Path(args[1]));        //9、提交任务        boolean rs = job.waitForCompletion(true);        System.exit(rs ? 0:1);    }}</code></pre><p>运行结果<br>[root@hsiehchou121 ~]# hadoop jar mapreduce-1.0-SNAPSHOT.jar com.hsiehchou.wordcount.WordCountDriver /wc/in /wc/out<br>[root@hsiehchou121 ~]# hdfs dfs -cat /wc/out/part-r-00000<br>fd  1<br>fdgs    1<br>fdsbv   1<br>gd  1<br>hello   3</p><h4 id="5、IDEA的相关使用"><a href="#5、IDEA的相关使用" class="headerlink" title="5、IDEA的相关使用"></a>5、IDEA的相关使用</h4><p>Ctrl+O导入相关未实现的方法<br>Maven中的Lifecycle的package可以直接打包成jar</p><p>案例分析<br>需求：运营商流量日志<br>10086<br>计算每个用户当前使用的总流量<br>思路？总流量 = 上行流量+下行流量<br>三个字段：手机号 上行流量 下行流量<br>技术选型：PB+<br>数据分析：海量数据(存储hdfs)<br>海量数据计算(分布式计算框架MapReduce)</p><h4 id="4、实现"><a href="#4、实现" class="headerlink" title="4、实现"></a>4、实现</h4><p><strong>FlowBean类</strong></p><pre><code>package com.hsiehchou.logs;import org.apache.hadoop.io.Writable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;/** * 封装数据类型需要怎么做 * hadoop数据类型实现了序列化接口 * 如果自定义需要实现这个序列化接口 */public class FlowBean implements Writable {    //定义属性：上行流量 下行流量 总流量总和    private long upFlow;    private long dfFlow;    private long flowsum;    public FlowBean(){}    public FlowBean(long upFlow, long dfFlow){        this.upFlow = upFlow;        this.dfFlow = dfFlow;        this.flowsum = upFlow + dfFlow;    }    public long getUpFlow(){        return upFlow;    }    public void setUpFlow(long upFlow){        this.upFlow = upFlow;    }    public long getDfFlow(){        return dfFlow;    }    public void setDfFlow(long dfFlow){        this.dfFlow = dfFlow;    }    public long getFlowsum(){        return flowsum;    }    public void setFlowsum(long flowsum){        this.flowsum = flowsum;    }    //序列化    public void write(DataOutput out) throws IOException {        out.writeLong(upFlow);        out.writeLong(dfFlow);        out.writeLong(flowsum);    }    //反序列化    public void readFields(DataInput in) throws IOException {        upFlow = in.readLong();        dfFlow = in.readLong();        flowsum = in.readLong();    }    @Override    public String toString() {        return upFlow + &quot;\t&quot; + dfFlow + &quot;\t&quot; + flowsum;    }}</code></pre><p><strong>FlowCountMapper类</strong></p><pre><code>package com.hsiehchou.logs;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;/** * keyIN: * valueIN: * * 思路：根据想要的结果的kv类型  手机号  流量总和（上行+下行）自定义类 * keyOut: * valueOut: */public class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt; {    @Override    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {        //1、接入数据源        String line = value.toString();        //2、切割   \t        String[] fields = line.split(&quot;\t&quot;);        //3、拿到关键字段        String phoneNr = fields[1];        long upFlow = Long.parseLong(fields[fields.length - 3]);        long dfFlow = Long.parseLong(fields[fields.length - 2]);        //4、写出到reducer        context.write(new Text(phoneNr), new FlowBean(upFlow,dfFlow));    }}</code></pre><p><strong>FlowCountReducer类</strong></p><pre><code>package com.hsiehchou.logs;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;import java.io.IOException;public class FlowCountReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; {    @Override    protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException {        long upFlow_sum = 0;        long dfFlow_sum = 0;        for (FlowBean v:values){            upFlow_sum += v.getUpFlow();            dfFlow_sum += v.getDfFlow();        }        FlowBean rsSum = new FlowBean(upFlow_sum, dfFlow_sum);        //输出结果        context.write(key, rsSum);    }}</code></pre><p><strong>FlowCountDriver类</strong></p><pre><code>package com.hsiehchou.logs;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;public class FlowCountDriver {    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {        //1.创建job任务        Configuration conf = new Configuration();        Job job = Job.getInstance(conf);        //2.指定kjar包位置        job.setJarByClass(FlowCountDriver.class);        //3.关联使用的Mapper        job.setMapperClass(FlowCountMapper.class);        //4.关联使用的Reducer类        job.setReducerClass(FlowCountReducer.class);        //5.设置mapper阶段输出的数据类型        job.setMapOutputKeyClass(Text.class);        job.setMapOutputValueClass(FlowBean.class);        //6.设置reducer阶段输出的数据类型        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(FlowBean.class);        //优化含有大量小文件的数据        //设置读取数据切片的类        job.setInputFormatClass(CombineTextInputFormat.class);        //最大切片大小8M        CombineTextInputFormat.setMaxInputSplitSize(job, 8388608);        //最小切片大小6M        CombineTextInputFormat.setMinInputSplitSize(job, 6291456);        //7.设置数据输入的路径        FileInputFormat.setInputPaths(job, new Path(args[0]));        //8.设置数据输出的路径        FileOutputFormat.setOutputPath(job, new Path(args[1]));        //9.提交任务        boolean  rs = job.waitForCompletion(true);        System.exit(rs? 0:1);    }}</code></pre><p>运行结果<br>[root@hsiehchou121 ~]# hdfs dfs -mkdir -p /flow/in<br>[root@hsiehchou121 ~]# hdfs dfs -put HTTP_20180313143750.dat /flow/in<br>[root@hsiehchou121 ~]# hadoop jar mapreduce-1.0-SNAPSHOT.jar com.hsiehchou.logs.FlowCountDriver /flow/in /flow/out<br>[root@hsiehchou121 ~]# hdfs dfs -cat /flow/out/part-r-00000<br>13480253104    120       1320      1440<br>13502468823    735       11349     12084<br>13510439658    1116      954       2070<br>13560436326    1136      94        1230<br>13560436666    1136      94        1230<br>13560439658    918       4938      5856<br>13602846565    198       910       1108<br>13660577991    660       690       1350<br>13719199419    240       0         240<br>13726130503    299       681       980<br>13726238888    2481      24681     27162<br>13760778710    120       120       240<br>13822544101    264       0         264<br>13884138413    4116      1432      5548<br>13922314466    3008      3720      6728<br>13925057413    11058     4243      15301<br>13926251106    240       0         240<br>13926435656    132       1512      1644<br>15013685858    369       338       707<br>15889002119    938       380       1318<br>15920133257    316       296       612<br>18212575961    1527      2106      3633<br>18320173382    9531      212       9743</p><p><strong>小文件优化</strong></p><p>如果企业中存在海量的小文件数据<br>TextInputFormat按照文件规划切片，文件不管多小都是一个单独的切片，启动mapt<br>ask任务去执行，这样会产生大量的maptask，浪费资源</p><p><strong>优化手段</strong></p><p>小文件合并大文件，如果不动这个小文件内容</p>]]></content>
      
      
      <categories>
          
          <category> 大数据实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据基础 </tag>
            
            <tag> HDFS </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据基础之HDFS3</title>
      <link href="/2019/02/12/da-shu-ju-ji-chu-zhi-hdfs3/"/>
      <url>/2019/02/12/da-shu-ju-ji-chu-zhi-hdfs3/</url>
      
        <content type="html"><![CDATA[<h4 id="1、hdfs的副本的配置"><a href="#1、hdfs的副本的配置" class="headerlink" title="1、hdfs的副本的配置"></a>1、hdfs的副本的配置</h4><p>修改hdfs-site.xml文件</p><pre><code>&lt;!-- 注释配置数据块的冗余度，默认是3 --&gt;&lt;property&gt;    &lt;name&gt;dfs.replication&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt;&lt;/property&gt;&lt;!--注释配置HDFS的权限检查，默认是true--&gt;&lt;property&gt;    &lt;name&gt;dfs.permissions&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; &lt;property&gt;    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;    &lt;value&gt;hsiehchou122:50090&lt;/value&gt;&lt;/property&gt;</code></pre><p>需要同步到其它机器：<br>scp hdfs-site.xml hsiehchou122:<code>$PWD</code><br>scp hdfs-site.xml hsiehchou123:<code>$PWD</code><br>scp hdfs-site.xml hsiehchou124:<code>$PWD</code></p><p>这里我划重点(亲自经历)<br>如果原来的分布式hadoop集群的主节点有Secondary NameNode，需要配置到其他节点，因为如果主节点挂了，其也是挂了，它的作用是在HDFS中提供一个检查点，相当于NameNode的助手节点<br>职责是：合并NameNode的edit logs到fsimage文件中</p><h4 id="2、hadoop启动方式"><a href="#2、hadoop启动方式" class="headerlink" title="2、hadoop启动方式"></a>2、hadoop启动方式</h4><p>1）启动hdfs集群<br>start-dfs.sh </p><p>2）启动yarn集群<br>start-yarn.sh </p><p>3）启动hadoop集群<br>start-all.sh</p><h4 id="3、大数据干什么的"><a href="#3、大数据干什么的" class="headerlink" title="3、大数据干什么的"></a>3、大数据干什么的</h4><p>1）海量数据的存储(mysql/oracle)<br>分布式文件系统hdfs<br>dfs-&gt;Hdfs<br>mapreduce-&gt;mapreduce<br>bigtable-&gt;hbase<br>分而治之！</p><p>2）海量数据的计算<br>分布式计算框架mapreduce<br>配置checkpoint时间</p><pre><code>&lt;property&gt;&lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt;&lt;value&gt;7200&lt;/value&gt;&lt;/property&gt;</code></pre><p>systemctl set-default graphical.target由命令行模式更改为图形界面模式<br>systemctl set-default multi-user.target由图形界面模式更改为命令行模式</p><h4 id="4、hdfs-namenode工作机制"><a href="#4、hdfs-namenode工作机制" class="headerlink" title="4、hdfs-namenode工作机制"></a>4、hdfs-namenode工作机制</h4><p>1）加载编辑日志与镜像文件到内存（NameNode）<br>edits_0001<br>edits_0002<br>fsimage fsimage fsimage </p><p>2）户端发起命令（client）<br>hdfs dfs -ls / </p><p>3）动正在写的edits（NameNode） </p><p>4）录操作日志更新 滚动日志（NameNode） </p><p>5）贝到Secondary NameNode<br>NameNode请求是否需要checkpoint</p><p>Secondary NameNode 触发checkpoint条件：<br>1）定时的时间<br>Secondary NameNode询问NameNode是否需要checkpoint<br>直接带回NameNode是否检查结果</p><p>2）edits中数据已满<br>Secondary NameNode请求执行checkpoint </p><p>3）NameNode滚动正在写的edits日志</p><p>4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode</p><p>5）Secondary NameNode加载编辑日志和镜像文件到内存，并行合并 </p><p>6）生成新的镜像文件fsimage.checkpoint </p><p>7）拷贝fsimage.chkpoint到NameNode </p><p>8）NameNode对fsimage.checkpoint重命名成fsimage</p><h4 id="5、hadoop2-8-4安装部署"><a href="#5、hadoop2-8-4安装部署" class="headerlink" title="5、hadoop2.8.4安装部署"></a>5、hadoop2.8.4安装部署</h4><p>1）准备工作<br>设置主机名：vi /etc/hostname<br>注意：需要重启 reboot<br>设置映射：vi /etc/hosts<br>设置免密登录：ssh-keygen<br>ssh-copy-id hsiehchou121 </p><p>2）安装jdk<br>上传安装包<br>CRT:alt+p </p><p>解压<br>tar -zxvf .tar.gz</p><p>配置环境变量<br>export JAVA_HOME=/root/hd/jdk1.8.0_192<br>export PATH=<code>$JAVA_HOME/bin:$PATH</code></p><p>注意：需要source /etc/profile<br>分发jdk<br>scp jdk hsiehchou122:/root/hd<br>scp /etc/profile hsiehchou122:/etc/<br>source /etc/profile</p><p>3）安装hadoop<br>上传安装包<br>alt + p<br>解压<br>tar -zxvf .tar.gz<br>修改配置文件<br>core-site.xml</p><pre><code>&lt;property&gt;    &lt;name&gt;fs.defaultFS&lt;/name&gt;    &lt;value&gt;hdfs://hsiehchou121:9000&lt;/value&gt;&lt;/property&gt;hdfs-site.xml&lt;property&gt;    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;    &lt;value&gt;/root/hd/dfs/name&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;    &lt;value&gt;/root/hd/dfs/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;    &lt;value&gt;hsiehchou122:50090&lt;/value&gt;&lt;/property&gt;mapred-site.xml&lt;property&gt;    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;    &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;yarn-site.xml&lt;property&gt;    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;    &lt;value&gt;hsiehchou121&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt;        &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;        &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt;        &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;        &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;</code></pre><h4 id="6、配置环境变量"><a href="#6、配置环境变量" class="headerlink" title="6、配置环境变量"></a>6、配置环境变量</h4><p>export HADOOP_HOME=/root/hd/hadoop-2.8.4<br>export PATH=<code>$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH</code><br>修改slaves文件加入从节点<br>格式化namenode<br>hadoop namenode -format<br>启动:start-all.sh</p><h4 id="7、hadoopMapReduce"><a href="#7、hadoopMapReduce" class="headerlink" title="7、hadoopMapReduce"></a>7、hadoopMapReduce</h4><p>官方：Apache™Hadoop®项目开发了用于可靠，可扩展的分布式计算的开源软件<br>Apache Hadoop软件库是一个框架，允许使用简单的编程模型跨计算机集群分布式处理 大型数据集。它旨在从单个服务器扩展到数千台计算机，每台计算机都提供本地计算和 存储。该库本身不是依靠硬件来提供高可用性，而是设计用于检测和处理应用层的故 障，从而在计算机集群之上提供高可用性服务，每个计算机都可能容易出现故障<br>阿里的Flink（9000万欧元） Blink</p><p>MapReduce分布式计算程序的编程框架。基于hadoop的数据分析的应用<br>MR优点：<br>1)框架易于编程<br>2)可靠容错（集群）<br>3)可以处理海量数据（1T+ PB+） 1PB = 1024TB<br>4)拓展性，可以通过动态的增减节点来拓展计算能力</p><h4 id="8、MapReduce的思想"><a href="#8、MapReduce的思想" class="headerlink" title="8、MapReduce的思想"></a>8、MapReduce的思想</h4><p>数据:海量单词<br>hello reba<br>hello mimi<br>hello liya<br>mimi big<br>需求：对每个单词出现的次数统计出来<br>思想：分而治之！<br>解决方式：<br>1）每个单词记录一次(map阶段)<br>&lt;hello,1&gt; &lt;reba,1&gt; &lt;hello,1&gt; &lt;mimi,1&gt; </p><p>2）相同单词的key不变，value累加求和即可（reduce阶段）<br>&lt;hello,1+1+1&gt;<br>对数据进行计算</p><h4 id="9、对wordcount例子程序分析"><a href="#9、对wordcount例子程序分析" class="headerlink" title="9、对wordcount例子程序分析"></a>9、对wordcount例子程序分析</h4><p>1）整个wordcount分为几个阶段？<br>三个 </p><p>2）有哪几个阶段？<br>mapper<br>reducer<br>driver </p><p>3）每个阶段有什么作用<br>mapper:对数据进行打散&lt;hello,1&gt;&lt;mimi,1&gt;<br>reducer:对数据进行聚合&lt;hello,1+1+1&gt;<br>driver:提交任务 </p><p>4）详解</p><p><strong>Mapper阶段</strong></p><p>将数据转换为String<br>对数据进行切分处理<br>把每个单词后加1<br>输出到reducer阶段</p><p>Reducer阶段<br>根据key进行聚合<br>输出key出现总的次数</p><p>Driver阶段<br>创建任务<br>关联使用的Mapper/Reducer类<br>指定mapper输出数据的kv类型<br>指定reducer输出的数据的kv类型<br>指定数据的输入路径与输出路径<br>提交</p><h4 id="10、hadoop数据类型"><a href="#10、hadoop数据类型" class="headerlink" title="10、hadoop数据类型"></a>10、hadoop数据类型</h4><p>我们看到的wordcount程序中的泛型中的数据类型其实是hadoop的序列化的数据类<br>型<br>为什么要进行序列化？用java的类型行不行？（可以）<br>Java的序列化:Serliazable太重<br>hadoop自己开发了一套序列化机制。Writable，精简高效。海量数据<br>hadoop序列化类型与Java数据类型</p><table><thead><tr><th align="center">Java数据类型</th><th align="center">Hadoop序列化类型</th></tr></thead><tbody><tr><td align="center">int</td><td align="center">IntWritable</td></tr><tr><td align="center">long</td><td align="center">LongWritable</td></tr><tr><td align="center">boolean</td><td align="center">BooleanWritable</td></tr><tr><td align="center">byte</td><td align="center">ByteWritable</td></tr><tr><td align="center">float</td><td align="center">FloatWritable</td></tr><tr><td align="center">double</td><td align="center">DoubleWritable</td></tr><tr><td align="center">String</td><td align="center">Text</td></tr></tbody></table><h4 id="11、wordcount测试"><a href="#11、wordcount测试" class="headerlink" title="11、wordcount测试"></a>11、wordcount测试</h4><p>1）本地模式<br>2）集群模式<br>hadoop jar .jar wordcount /wc/in /wc/out</p><p>hadoop jar mapreduce-1.0-SNAPSHOT.jar 全类名 /wc/in /wc/out</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据基础 </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据基础之HDFS2</title>
      <link href="/2019/02/09/da-shu-ju-ji-chu-zhi-hdfs2/"/>
      <url>/2019/02/09/da-shu-ju-ji-chu-zhi-hdfs2/</url>
      
        <content type="html"><![CDATA[<h4 id="1、HDFS下载文件原理"><a href="#1、HDFS下载文件原理" class="headerlink" title="1、HDFS下载文件原理"></a>1、HDFS下载文件原理</h4><p>1、请求<br>2、创建client<br>DFS –&gt;DFSClient<br>3、建立RPC通信<br>4、得到代理对象proxy，通过代理对象请求得到文件元信息<br>5、查找元信息<br>6、返回元信息<br>7、创建输入流<br>8、下载数据块<br>FSDataInputStream<br>9、整合下载文件</p><p>注意：HDFS维护失败列表</p><h4 id="2、安全模式-safe-mode"><a href="#2、安全模式-safe-mode" class="headerlink" title="2、安全模式 safe mode"></a>2、安全模式 safe mode</h4><p>检查副本率是否满足配置要求。副本率不够的时候，会水平复制，当下次那个挂掉的节点如果又活过来的话，副本数就会超过N了，就超了，系统会自动选一个多余的副本删掉<br>（1）冗余度：dfs.replication 3.有几个冗余的副本<br>hdfs-site.xml</p><pre><code>  &lt;!--注释配置数据块的冗余度，默认是3--&gt;  &lt;property&gt;         &lt;name&gt;dfs.replication&lt;/name&gt;         &lt;value&gt;1&lt;/value&gt;  &lt;/property&gt;</code></pre><p>（2）副本率：数据块实际冗余度（M），HDFS配置的数据块应该具有的冗余度（N）<br>M/N*100%;<br>例如：知否知否.avi M=2；HDFS配置的 N=3；<br>2/3=0.667。要求的副本率为 0.99，系统会水平复制数据块到其他节点<br>如果是副本率过高，M=6，N=3，副本率=2；大于0.99.系统会删除多余的数据块<br>在安全模式下 无法操作HDFS，因为正在进行副本率的检查工作<br>进入或查看安全模式的命令：<br>hdfs dfsadmin -safemode get/enter/leave/wait</p><h4 id="3、快照：是一种备份，默认：HDFS快照是关闭"><a href="#3、快照：是一种备份，默认：HDFS快照是关闭" class="headerlink" title="3、快照：是一种备份，默认：HDFS快照是关闭"></a>3、快照：是一种备份，默认：HDFS快照是关闭</h4><p>一般不建议使用<br>快照的本质：将需要备份的数据放到一个隐藏目录下<br>（1）开启和关闭快照<br>hdfs dfsadmin -allowSnapshot <code>&lt;snapshotDir&gt;</code><br>hdfs dfsadmin -disallowSnapshot<br>hdfs lsSnapshottableDir //查看开启快照的所有文件夹  </p><p>（2）创建快照<br>需要创建快照的目录 快照目录的名字<br>hdfs dfs -createSnapshot /test1 backup_test1_20190216<br>快照打出的日志：<br>Created snapshot /test1/.snapshot/backup_test1_20190216 </p><p>（3）删除快照<br>hdfs dfs -deleteSnapshot /test1 backupt1_test1_20190216 </p><p>（4）恢复快照<br>hdfs dfs -cp /test1/.snapshot/backup_test1_20190216/a.txt /test1</p><h4 id="4、回收站：默认HDFS的回收站禁用"><a href="#4、回收站：默认HDFS的回收站禁用" class="headerlink" title="4、回收站：默认HDFS的回收站禁用"></a>4、回收站：默认HDFS的回收站禁用</h4><p>（1）回收站的配置：<br>core-site.xml fs.trash.interval(时间间隔 分钟)<br>关闭集群后才能起作用</p><pre><code>&lt;!--配置回收站，单位是分钟，默认是0--&gt;&lt;property&gt;    &lt;name&gt;fs.trash.interval&lt;/name&gt;    &lt;value&gt;1440&lt;/value&gt;&lt;/property&gt;</code></pre><p>（2）本质是剪切：回收站开启之后，会把删除的文件放到一个/user/root/.Trash/Current<br>（3）回收站恢复也就是粘贴的过程<br>hdfs dfs -cp /user/root/.Trash/Current /</p><h4 id="5、配额：Quota"><a href="#5、配额：Quota" class="headerlink" title="5、配额：Quota"></a>5、配额：Quota</h4><p>（1）名称配额<br>限定HDFS目录下，存放文件（目录）的个数&gt;1，最多存放N-1个<br>setQuota–指定名称配额<br>clrQuota–清除名称配额<br>例如：<br>hdfs dfs -mkdir /myquota1<br>hdfs dfsadmin -setQuota 3 /myquota1<br>hdfs dfs -put ~/a.txt /myquota1—-第1个<br>hdfs dfs -put ~/student01.txt /myquota1–第2个<br>hdfs dfs -put ~/students01.txt /myquota1—第3个 无法放<br>错误：put: The NameSpace quota (directories and files) of directory /myquota1 is exceeded: quota=3 file count=4 </p><p>（2）空间配额 –必须要大于 默认数据块大小<br>setSpaceQuota<br>clrSpaceQuota</p><h4 id="6、HDFS底层原理-RPC"><a href="#6、HDFS底层原理-RPC" class="headerlink" title="6、HDFS底层原理-RPC"></a>6、HDFS底层原理-RPC</h4><p>Remote Procedure Call：远程过程调用，调用代码不在本地执行，实现调用者与被调用者之间的连接和通信 </p><p>基于Client server，相当于 DFSClient 相当于客户端。NameNode集群相当于Server</p><h4 id="7、HDFS底层原理-代理对象Proxy"><a href="#7、HDFS底层原理-代理对象Proxy" class="headerlink" title="7、HDFS底层原理-代理对象Proxy"></a>7、HDFS底层原理-代理对象Proxy</h4><p>（1）代理—明星的经纪人<br>是一种设计模式，提供了对目标对象的另一种访问方式。通过代理对象访问目标对象<br>（2）代理分为静态代理和动态代理<br>a、静态代理：接口的定义 实现接口，被代理对象与对象实现相同的接口<br>b、动态代理：接口的定义 不需要实现接口（匿名内部类+反射 invoke）</p><h4 id="8、RPC与Proxy程序示例"><a href="#8、RPC与Proxy程序示例" class="headerlink" title="8、RPC与Proxy程序示例"></a>8、RPC与Proxy程序示例</h4><p>针对log4j warn<br>log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).<br>log4j:WARN Please initialize the log4j system properly.<br>log4j:WARN See <a href="http://logging.apache.org/log4j/1.2/faq.html#noconfig" target="_blank" rel="noopener">http://logging.apache.org/log4j/1.2/faq.html#noconfig</a> for more info.<br>可以在src/resource/通过 增加 log4j.properties解决</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据基础 </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据基础之HDFS1</title>
      <link href="/2019/02/07/da-shu-ju-ji-chu-zhi-hdfs1/"/>
      <url>/2019/02/07/da-shu-ju-ji-chu-zhi-hdfs1/</url>
      
        <content type="html"><![CDATA[<h4 id="1、免密码登录的原理和配置"><a href="#1、免密码登录的原理和配置" class="headerlink" title="1、免密码登录的原理和配置"></a>1、免密码登录的原理和配置</h4><p>ssh不对称加密算法（加密和解密是两个文件）（对称加密： 加密和解密文件是同一个）<br>（1）公钥–锁：给出去 给其他机器<br>（2）私钥–钥匙：自己留着，解密<br>step1:ssh-keygen -t rsa(3次回车)<br>step2:ssh-copy-id -i ~/.ssh/id_rsa.pub root@hsiehchou121(自己也要拷贝给自己)</p><h4 id="2、Hadoop安装—全分布模式-（重点）"><a href="#2、Hadoop安装—全分布模式-（重点）" class="headerlink" title="2、Hadoop安装—全分布模式 （重点）"></a>2、Hadoop安装—全分布模式 （重点）</h4><p>（1）规划：<br>192.168.116.121 hsiehchou121 ：主节点<br>192.168.116.122 hsiehchou122 ：从节点<br>192.168.116.123 hsiehchou123 ：从节点<br>192.168.116.124 hsiehchou124 ：从节点 </p><p>（2）准备工作:<br>step 1: jdk、防火墙、ssh免密码登录（3次拷贝）、在etc/hosts 添加主机名<br>对于同时操作多台机器可通过 工具-》发送键输入到所有会话 在选项卡排列 实现 水平排列 </p><p>step 2:时间同步（如果能够上网） 使用网络时间（GUI设置）默认的都是一致的<br>不能上网： date -s 2019-01-10(同时操作多台机器) 集群紊乱<br>ntp：在机器里面指定一个服务器 作为时钟服务器 </p><p>step 3: 修改配置文件</p><p>主要在hsiehchou 121操作，其他机器通过scp拷贝</p><h4 id="3、slaves-和自己的从节点机器名字一致"><a href="#3、slaves-和自己的从节点机器名字一致" class="headerlink" title="3、slaves(和自己的从节点机器名字一致)"></a>3、slaves(和自己的从节点机器名字一致)</h4><p>hsiehchou122<br>hsiehchou123<br>hsiehchou124</p><h4 id="4、通过hdfs-namenode-格式化"><a href="#4、通过hdfs-namenode-格式化" class="headerlink" title="4、通过hdfs namenode 格式化"></a>4、通过hdfs namenode 格式化</h4><p>  hdfs namenode -format<br>  成功的标志： Storage directory /opt/module/hadoop-2.7.3/tmp/dfs/name has been successfully formatted</p><h4 id="5、通过scp拷贝"><a href="#5、通过scp拷贝" class="headerlink" title="5、通过scp拷贝"></a>5、通过scp拷贝</h4><p>scp -r /opt/module/hadoop-2.7.3/ root@hsiehchou122:/opt/module/<br>scp -r /opt/module/hadoop-2.7.3/ root@hsiehchou123:/opt/module/<br>scp -r /opt/module/hadoop-2.7.3/ root@hsiehchou124:/opt/module/<br>学会看 vi /opt/module/hadoop-2.7.3/logs/hadoop-root-datanode-hsiehchou123.log<br>Shift+G 看启动日志<br>hdfs体系架构（Yarn资源放在后面）</p><h4 id="6、HDFS-NameNode：名称节点"><a href="#6、HDFS-NameNode：名称节点" class="headerlink" title="6、HDFS-NameNode：名称节点"></a>6、HDFS-NameNode：名称节点</h4><p>（1）职责：对HDFS的节点进行管理，管理员<br>接收客户端（命令行、Java）的请求：创建目录、上传数据、下载数据和删除数据<br>管理和维护hdfs的日志和元信息 </p><p>（2）dfs/name:<br>a、current：主要存放日志和元信息 存贮路径：/opt/module/hadoop-2.7.3/tmp/dfs/name/current<br>edits文件：二进制文件，体现了hdfs的最新状态</p><p>hdfs oev -i edits_inprogress_0000000000000000003 -o ~/a.xml<br>o:表示 offline<br>inprogress:表示最新的</p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;EDITS&gt;  &lt;EDITS_VERSION&gt;-63&lt;/EDITS_VERSION&gt;  &lt;RECORD&gt;    &lt;OPCODE&gt;OP_START_LOG_SEGMENT&lt;/OPCODE&gt;    &lt;DATA&gt;      &lt;TXID&gt;4&lt;/TXID&gt;    &lt;/DATA&gt;  &lt;/RECORD&gt;  &lt;RECORD&gt;    &lt;OPCODE&gt;OP_MKDIR&lt;/OPCODE&gt;    &lt;DATA&gt;      &lt;TXID&gt;5&lt;/TXID&gt;      &lt;LENGTH&gt;0&lt;/LENGTH&gt;      &lt;INODEID&gt;16386&lt;/INODEID&gt;      &lt;PATH&gt;/input&lt;/PATH&gt;      &lt;TIMESTAMP&gt;1550209288319&lt;/TIMESTAMP&gt;      &lt;PERMISSION_STATUS&gt;        &lt;USERNAME&gt;root&lt;/USERNAME&gt;        &lt;GROUPNAME&gt;supergroup&lt;/GROUPNAME&gt;        &lt;MODE&gt;493&lt;/MODE&gt;      &lt;/PERMISSION_STATUS&gt;    &lt;/DATA&gt;  &lt;/RECORD&gt;&lt;/EDITS&gt;</code></pre><p>b、元信息文件 fsimage：记录的数据块的位置信息和数据块冗余信息，没有体现hdfs的最新状态，二进制文件</p><p> hdfs oiv -i fsimage_0000000000000000002 -o ~/b.xml -p XML</p><p>（3）in_use.lock 避免同一文件被多使用，只能启动一个namenode</p><h4 id="7、hdfs-DataNode：数据节点"><a href="#7、hdfs-DataNode：数据节点" class="headerlink" title="7、hdfs-DataNode：数据节点"></a>7、hdfs-DataNode：数据节点</h4><p>（1）主要用来进行数据的存储<br>1.x 64M<br>2.x 128M( hdfs-site.xml 可以修改 blocksize) </p><p>（2）数据块的表现形式就是一个个的blk文件<br>位置：/opt/module/hadoop-2.7.3/tmp/dfs/data/current/BP-298124919-192.168.116.121-1550208140930 ###/current/finalized/subdir0/subdir0<br>尝试上传一个 大于128M的文件（128<em>1024</em>1024）<br>Hadoop 3.x 有 纠删码技术，节约存储空间</p><h4 id="8、上传文件"><a href="#8、上传文件" class="headerlink" title="8、上传文件"></a>8、上传文件</h4><p>首先创建文件夹<br>hdfs dfs -mkdir /software/input<br>上传我本地文件到hdfs上<br>hdfs dfs -put hdfs dfs -put /opt/software/hadoop-2.7.3.tar.gz /software/input<br>就OK了<br>之后可以使用上面的命令查看</p><h4 id="9、hdfs-SecondaryNameNode：第二名称节点"><a href="#9、hdfs-SecondaryNameNode：第二名称节点" class="headerlink" title="9、hdfs-SecondaryNameNode：第二名称节点"></a>9、hdfs-SecondaryNameNode：第二名称节点</h4><p>（1）进行日志信息的合并，根据checkpoint或者时间间隔（3600s）或者edits文件达到64M </p><p>（2）edits文件合并到fsimage里面 edits文件可以清空<br>看日志<br>/opt/moudle/hadoop-2.7.3/logs vi shift+G</p><h4 id="10、hdfs-Web-Console"><a href="#10、hdfs-Web-Console" class="headerlink" title="10、hdfs-Web Console"></a>10、hdfs-Web Console</h4><p>hdfs dfsadmin -report<br><a href="http://192.168.116.125:50070/dfshealth.html#tab-overview" target="_blank" rel="noopener">http://192.168.116.125:50070/dfshealth.html#tab-overview</a> </p><p>（1） Overview–展示hdfs的基本信息<br>Safemode is off.—高级特性 </p><p>（2）DataNodes-数据节点信息<br>增加和删除数据节点（Decomissioning–&gt;Dead） </p><p>（3）Datanode Volume Failures–数据节点 硬件错误 </p><p>（4）Snapshot（快照）—高级特性<br>快照实现数据的备份，防止数据的误操作和丢失。默认是关闭的</p><p>（5）Startup Progress–启动过程 </p><p>（6）Uitlities:<br>Browse 文件 —hdfs -dfs -ls /<br>logs—查看日志</p><h4 id="11、hdfs-普通操作命令–hdfs-dfs-hadoop-dfs"><a href="#11、hdfs-普通操作命令–hdfs-dfs-hadoop-dfs" class="headerlink" title="11、hdfs 普通操作命令–hdfs dfs(hadoop dfs)"></a>11、hdfs 普通操作命令–hdfs dfs(hadoop dfs)</h4><p>（1）创建目录–mkdir<br>hdfs dfs -mkdir / </p><p>（2）查看–ls<br>查看目录和子目录 hdfs dfs -ls -R /<br>hdfs dfs -lsr / </p><p>（3）上传数据<br>hdfs dfs -put hadoop-root-namenode-hsiehchou125.log /test1<br>-put ：<br>-copyFromLocal： 本地路径 hdfs路径<br>hdfs dfs -copyFromLocal ~/temp/a.txt /test0113/<br>-moveFromLocal: 会删除本地文件 剪切 </p><p>（4）下载数据<br>-get:<br>-copyToLocal:从hdfs下载到本地 </p><p>（5）删除数据<br>-rm<br>-rmr: 删除hdfs的目录和子目录<br>删除日志： Deleted /test1<br>回收站—高级特性 默认是关闭</p><p>（6）合并数据–（为hive表数据操作做准备）<br>-getmerge :hdfs 把某个hdfs的目录下的文件进行先合并后下载<br>*：通配符 ？<br>hdfs dfs -getmerge /students /root/students.txt </p><p>（7）计数和文件大小<br>-count 显示 文件夹、文件个数 文件总的大小<br>-du 显示每个文件夹和文件的大小<br>[root@hsiehchou125 ~]# hdfs dfs -count /students<br>1 4 38/students<br>hdfs[root@hsiehchou125 ~]# hdfs dfs -du /students<br>25 /students/students01.txt<br>13 /students/students02.txt </p><p>（8）负载均衡 balancer<br>实现DataNode 数据存储均衡 </p><p><code>##hdfs balancer ##</code></p><h4 id="12、hdfs-管理员命令"><a href="#12、hdfs-管理员命令" class="headerlink" title="12、hdfs 管理员命令"></a>12、hdfs 管理员命令</h4><p>（1）hdfs dfsadmin -report 打印报告 </p><p>（2） -safemode &lt;enter | leave | get | wait&gt;<br>enter:手动进入安全模式<br>leave:手动离开安全模式<br>get:获得当前安全模式的状态<br>hdfs dfsadmin -safemode get<br>[root@hsiehchou125 ~]# hdfs dfsadmin -safemode enter<br>Safe mode is ON </p><p>（3）快照命令<br>[-allowSnapshot <code>&lt;snapshotDir&gt;</code>]<br>[-disallowSnapshot <code>&lt;snapshotDir&gt;</code>]</p><p>（4）Quota 配额<br>a、名称配额–数量<br>[-setQuota <code>&lt;quota&gt; &lt;dirname&gt;...&lt;dirname&gt;</code>]<br>[-clrQuota <code>&lt;dirname&gt;...&lt;dirname&gt;</code>]</p><p>b、空间配额–空间大小</p><p>[-setSpaceQuota <code>&lt;quota&gt;</code> [-storageType <code>&lt;storagetype&gt;</code>] <code>&lt;dirname&gt;...&lt;dirname&gt;</code>]<br>[-clrSpaceQuota [-storageType <code>&lt;storagetype&gt;</code>] <code>&lt;dirname&gt;...&lt;dirname&gt;</code>]</p><h4 id="13、IDEA-Maven工程简介"><a href="#13、IDEA-Maven工程简介" class="headerlink" title="13、IDEA Maven工程简介"></a>13、IDEA Maven工程简介</h4><p>（1）IDEA 下载地址：<br><a href="https://www.jetbrains.com/idea/download/" target="_blank" rel="noopener">https://www.jetbrains.com/idea/download/</a><br>破解方法自行查找 </p><p>（2）File-new Project-&gt;Maven<br>GroupID: 公司名字<br>artifactId：工程名字<br>java程序在：src-》main-&gt;java 右键 新建 java class文件<br>target: 是运行程序生成的class文件 </p><p>（3）管理包<br>/opt/moudle/hadoop-2.7.3/share/hadoop/common/<em>.jar<br>/opt/moudle/hadoop-2.7.3/share/hadoop/common/lib/</em>.jar<br>/opt/moudle/hadoop-2.7.3/share/hadoop/hdfs/<em>.jar<br>/opt/moudle/hadoop-2.7.3/share/hadoop/hdfs/lib/</em>.jar<br>通过maven只需要配置POM文件<br>a、 下载一个maven版本<br><a href="http://maven.apache.org/index.html" target="_blank" rel="noopener">http://maven.apache.org/index.html</a> </p><p>b、通过 File-settings-Maven<br>修改： E:\apache-maven-3.6.0\conf\settings.xml<br>55行：<br><code>&lt;localRepository&gt;</code>E:\Maven\m2\Repository<code>&lt;/localRepository&gt;</code><br> MaveHome：E:\apache-maven-3.6.0<br> User settings:E:\apache-maven-3.6.0\conf\settings.xml</p><p>c、POM中写入包的依赖<br>参考：<a href="https://mvnrepository.com/search?q=hadoop" target="_blank" rel="noopener">https://mvnrepository.com/search?q=hadoop</a></p><pre><code>  &lt;dependencies&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;            &lt;version&gt;2.7.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;            &lt;version&gt;2.7.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;            &lt;version&gt;2.7.3&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;</code></pre><h4 id="14、文件夹的创建"><a href="#14、文件夹的创建" class="headerlink" title="14、文件夹的创建"></a>14、文件夹的创建</h4><pre><code>import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import java.io.IOException;public class hdfsMkDir {public static void main(String[] args) throws IOException {System.setProperty(&quot;HADOOP_USER_NAME&quot;,&quot;root&quot;);//step1 配置参数，指定namenode地址Configuration conf = new Configuration();conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://192.168.116.125:9000&quot;);//step2 创建客户端FileSystem client = FileSystem.get(conf);//step3 创建目录client.mkdirs(new Path(&quot;/test2&quot;));client.close();System.out.println(&quot;Successful&quot;);   }}</code></pre><h4 id="15、hdfs权限问题"><a href="#15、hdfs权限问题" class="headerlink" title="15、hdfs权限问题"></a>15、hdfs权限问题</h4><p>针对用户操作没有权限 permission denied：<br>（1）修改 hdfs-site.xml 去掉权限检查（关闭hdfs服务 stop-all.sh;修改后 重新 Start-all.sh）</p><pre><code>&lt;property&gt;    &lt;name&gt;dfs.permissions&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;</code></pre><p>（2）通过设定用户名字 rootSystem.setProperty(“HADOOP_USER_NAME”,”root”);</p><p>（3）通过java的-D参数传递。 HADOOP_USER_NAME=root （命令行的方式）<br>public static void main(String[] args)</p><p>Java -D命令对应的代码中获取-D后面的参数 和 多个参数时-D命令的使用</p><p>Java代码：</p><pre><code>public class DP {    public static void main(String[] args) {      String fg = System.getProperty(&quot;P&quot;);      System.err.println(fg);    }}</code></pre><p>cmd命令：<br>java -DP=hdfshdfs DP</p><p>执行命令后输出：hdfshdfs<br>注意：-D和Para之间不能有空格</p><p>使用多个参数，如P、P1</p><pre><code>public class DP {    public static void main(String[] args) {        String fg = System.getProperty(&quot;P&quot;);        System.out.println(fg);        String fg1 = System.getProperty(&quot;P1&quot;);        System.out.println(fg1);    }}</code></pre><p>java -DP=hdfshdfs -DP1=1212 DP<br>执行命令后输出：<br>hdfshdfs<br>1212</p><p>（4）hdfs dfs -chmod 777 /input 让所有用户访问</p><p>（5）针对hdfs权限问题，有kerberos认证<br>Kerberos: The Network Authentication Protocol<br><a href="https://www.cnblogs.com/wukenaihe/p/3732141.html" target="_blank" rel="noopener">https://www.cnblogs.com/wukenaihe/p/3732141.html</a></p><h4 id="16、IDEA-Maven工程实现hdfs的文件上传与下载"><a href="#16、IDEA-Maven工程实现hdfs的文件上传与下载" class="headerlink" title="16、IDEA Maven工程实现hdfs的文件上传与下载"></a>16、IDEA Maven工程实现hdfs的文件上传与下载</h4><p>Maven环境中 只有当 POM文件中所有的依赖包全部变成白色<br>pom.xml</p><pre><code>&lt;dependencies&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;            &lt;version&gt;2.7.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;            &lt;version&gt;2.7.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;            &lt;version&gt;2.7.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-core --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;            &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt;            &lt;version&gt;1.2.1&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;</code></pre><p>（1）hdfs文件上传<br>查看源码：crtl+鼠标左键<br><code>## Failed to locate the winutils binary in the hadoop binary path java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries. ##</code><br>step1:<br>下载：hadoop2.7.3 winutils binary<br><a href="https://github.com/rucyang/hadoop.dll-and-winutils.exe-for-hadoop2.7.3-on-windows_X64" target="_blank" rel="noopener">https://github.com/rucyang/hadoop.dll-and-winutils.exe-for-hadoop2.7.3-on-windows_X64</a> </p><p>step2: 配置环境变量 拷贝进入 D:\hadoop-2.7.3\bin文件下<br>hadoop.home.dir —bin/winutils.exe<br>HADOOP_HOME:D:\hadoop-2.7.3,然后再path里面增加 %HADOOP_HOME%\bin<br>或者：System.setProperty(“hadoop.home.dir”, “D:\hadoop-2.7.3”);</p><pre><code>import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import java.io.*;public class hdfsUpload {    public static void main(String[] args) throws IOException {        System.setProperty(&quot;HADOOP_USER_NAME&quot;,&quot;root&quot;);        //System.setProperty(&quot;hadoop.home.dir&quot;,&quot;E:\\hadoop-2.7.3&quot;);        //step1 建立客户端        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://192.168.116.125:9000&quot;);        //使用IP地址  因为没有指定hsiehchou125对应的IP        FileSystem client = FileSystem.get(conf);        //step2 创建本地数据 hdfs dfs -put copyFromLocal        File file1 = new File(&quot;C:\\Users\\hsiehchou\\Desktop\\hadooplibs\\test.txt&quot;);        InputStream input = new FileInputStream(file1);//多态        //step3 创建本地输出流 指向hdfs        OutputStream output = client.create(new Path(&quot;/test8/a.txt&quot;),true);        //step4 开始写入hdfs        /**方法1**///        byte[] buffer = new byte[1024];//        int len = 0;//        //因为read 当读到文件末尾的时候 会返回-1//        while((len=input.read(buffer)) != -1){//            output.write(buffer, 0, len);//        }//循环写入数据//        output.flush();//        input.close();//        output.close();        /**方法2 IOUtils**/        IOUtils.copyBytes(input,output,1024);    }}</code></pre><p>（2）hdfs文件下载<br><code>### 使用IOUtils 输入路径 输出路径###</code><br>IOUtils.copyBytes(input,output,1024);</p><pre><code>import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import java.io.*;public class hdfsDownload {    public static void main(String[] args) throws IOException {        System.setProperty(&quot;HADOOP_USER_NAME&quot;,&quot;root&quot;);        //step1 建立客户端        Configuration conf = new Configuration();        conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://192.168.116.125:9000&quot;);        //使用IP地址  因为没有指定hsiehchou125对应的IP        FileSystem client = FileSystem.get(conf);        //step2 创建数据输入 指向hdfs  从hdfs读取数据  hdfs dfs -get copyToLocal        InputStream input = client.open(new Path(&quot;/test8/a.txt&quot;));        //step3 创建本地输出流 指向hdfs        OutputStream output = new FileOutputStream(&quot;E:\\test\\b.txt&quot;);        //step4 开始写入hdfs        /**IOUtils**/        IOUtils.copyBytes(input,output,1024);    }}</code></pre><p>文件元信息 </p><pre><code>{     文件名: *.txt     路径: /text     大小: 100KB     冗余度: 3     数据块1: DNS1,DNS2,DNS3     (如果文件大切分) }</code></pre><h4 id="17、hdfs上传文件原理"><a href="#17、hdfs上传文件原理" class="headerlink" title="17、hdfs上传文件原理"></a>17、hdfs上传文件原理</h4><p>1、请求上传数据<br>2、创建客户端<br>3、建立RPC通信<br>4、NameNode对象<br>代理对象NameNodeProxies<br>5、请求创建文件元信息<br>6、创建文件元信息<br>7、缓存文件元信息(1000M)<br>8、返回元信息<br>9、根据元信息创建输出流<br>10、上传第一个数据块<br>11、数据块自动复制<br>12、循环上传</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据基础 </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据基础2</title>
      <link href="/2019/02/05/da-shu-ju-ji-chu-2/"/>
      <url>/2019/02/05/da-shu-ju-ji-chu-2/</url>
      
        <content type="html"><![CDATA[<h4 id="1、什么是大数据？"><a href="#1、什么是大数据？" class="headerlink" title="1、什么是大数据？"></a>1、什么是大数据？</h4><p>2002 大数据提出 美国引入。—麦肯锡报告<br>维克托·迈尔-舍恩伯格—大数据之父<br>4V特征：<br>即<br>Volume（数据量大）：PB级<br>Variety（数据多样性）：文本、图像、视频、音频等<br>Velocity（输入和处理速度快）：流式数据<br>Value（价值密度低）： 积累很多的数据才能发掘大数据隐含的意义</p><p>只要能发挥和挖掘数据隐藏的价值，不用纠结与数据量大小<br>大数据核心问题存储、计算和分析—-通过组件（计算框架）解决了</p><h4 id="2、数据仓库和大数据"><a href="#2、数据仓库和大数据" class="headerlink" title="2、数据仓库和大数据"></a>2、数据仓库和大数据</h4><p>（1）传统方式：DW（Data Warehouse），基于传统的关系数据库（Oracle、MySQL等），一般只做 查询分析，TD（Teradata 天睿）–数据仓库一体机</p><p>（2）大数据的方式–分布式<br>GP：greenplum</p><h4 id="3、OLTP和OLAP"><a href="#3、OLTP和OLAP" class="headerlink" title="3、OLTP和OLAP"></a>3、OLTP和OLAP</h4><p>（1）OLTP：Online Transaction Processing 联机事务处理：（insert update、delete）<br>ACID：所有的数据可追溯。——-传统关系型数据库（Oracle Mysql Postgresql等） </p><p>（2）OLAP：Online Analytic Processing 联机分析处理<br>真正生产中是二者的结合：OLTP（后台操作 前台展示 数据设计等）+OLAP（Hive Hbase Spark等）</p><h4 id="4、Google的基本思想：三篇论文重点"><a href="#4、Google的基本思想：三篇论文重点" class="headerlink" title="4、Google的基本思想：三篇论文重点"></a>4、Google的基本思想：三篇论文重点</h4><p>（1）GFS: Google File System—-HDFS —解决存储<br>a、数据库太贵。主要是为了解决 google搜索内容的存储问题。–造价低 易扩展</p><p>b、倒排索引（Reverted Index）：<br>int arry[ ] = {1,2,3,4}<br>索引不一定提高查询速度。—key value </p><p>c、没有公布源码，—-Hadoop之父 Doug Cutting<br>HDFS 默认文件块大小 128M（Hadoop 2.X） 64M（Hadoop 1.x），<br>默认3副本</p><p>（2）MapReduce:分布计算模型<br>PageRank </p><p>（3）BigTable：大表<br>对HDFS进行封装和二次开发，提高查询效率。把所有数据存入一张表中，通过牺牲空间，换取时间</p><h4 id="5、Hadoop的简介"><a href="#5、Hadoop的简介" class="headerlink" title="5、Hadoop的简介"></a>5、Hadoop的简介</h4><p><a href="http://hadoop.apache.org/" target="_blank" rel="noopener">http://hadoop.apache.org/</a><br>Hadoop YARN: A framework for job scheduling and cluster resource management.<br>Apache：HDFS+MapReduce+<br>Yarn<br><a href="https://hbase.apache.org/" target="_blank" rel="noopener">https://hbase.apache.org/</a></p><h4 id="6、HDFS的体系架构"><a href="#6、HDFS的体系架构" class="headerlink" title="6、HDFS的体系架构"></a>6、HDFS的体系架构</h4><p>HDFS 副本数可以再 hdfs-site.xml中修改。不超过机器个数 建议不超过3<br>/opt/module/hadoop-2.7.3/etc/hadoop<br>HDFS=NameNode（主节点 名称节点）+SecondaryNameNode（第二名称节点）+DataNode（数据节点）</p><h4 id="7、MR编程模型"><a href="#7、MR编程模型" class="headerlink" title="7、MR编程模型"></a>7、MR编程模型</h4><p>包含两个阶段 key value 的设计是关键</p><h4 id="8、大数据典型应用场景"><a href="#8、大数据典型应用场景" class="headerlink" title="8、大数据典型应用场景"></a>8、大数据典型应用场景</h4><p>（1）商品推荐–协同过滤<br>（2）画像<br>（3）套牌车</p><h4 id="9、Hadoop的安装准备工作"><a href="#9、Hadoop的安装准备工作" class="headerlink" title="9、Hadoop的安装准备工作"></a>9、Hadoop的安装准备工作</h4><p>Hadoop名字来源–Doug Cutting<br>（1）安装好linux操作系统（IP配置）<br>（2）关闭防火墙<br>systemctl stop（disable） firewalld.service<br>（3）安装Jdk–winscp 上传 opt/software 解压到 opt/module<br>（4）Hadoop安装包—虚拟机的克隆 scp（拷贝）</p><p>a、提前准备好 mkdir /opt/module<br>tar -zxvf hadoop-2.7.3.tar.gz -C /opt/module/ </p><p>b、vi ~/.bash_profile （用于当前用户）或者/etc/profile（所有用户都可以用）增加下面内</p><p>export JAVA_HOME=<code>/opt/module/jdk1.8.0_192</code><br>export PATH=<code>$JAVA_HOME/bin:$PATH</code></p><p>HADOOP_HOME=<code>/opt/module/hadoop-2.7.3</code><br>export HADOOP_HOME</p><p>PATH=<code>$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH</code><br>export PATH</p><p>c、 环境变量生效<br>source ~/.bash_profile<br>通过 输入 start 按两下tab 看是否有内容</p><p>虚拟机克隆<br>（1）保证虚拟机处于关闭状态<br>（2）右键-&gt;管理-&gt;克隆 当前状态 完整克隆–&gt;<br>（3）hostname–修改<br>ip修改 – reboot</p><p>Hadoop（HDFS+Yarn） 本地 伪分布 全分布</p><h4 id="10、Hadoop安装—本地安装"><a href="#10、Hadoop安装—本地安装" class="headerlink" title="10、Hadoop安装—本地安装"></a>10、Hadoop安装—本地安装</h4><p>（1）特点：没有HDFS和Yarn 只能够测试MR程序是否成功， 作为一个普通的java程序。<br>（2）修改文件：<br>vi hadoop-env.sh<br>set number<br>修改25行（行数不一 hadoop版本不一致）</p><p>JAVA_HOME=/opt/module/jdk1.8.0_181<br>cd /root/<br>mkdir temp<br>touch a.txt<br>vi a.txt<br>mapred-site.xml 默认没有，我克隆的文件里面有 这个文件没有被覆盖指定了yarn资源</p><h4 id="11、Hadoop安装—本地安装伪分布模式"><a href="#11、Hadoop安装—本地安装伪分布模式" class="headerlink" title="11、Hadoop安装—本地安装伪分布模式"></a>11、Hadoop安装—本地安装伪分布模式</h4><p>（1）特点：在一台机器上模拟一个分布式环境具备hadoop的所有功能。<br>HDFS：NameNode+DataNode+SecondaryNameNode<br>Yarn：ResourceManager+NodeManager </p><p>（2）修改的文件：<br>step1:hadoop-env.sh<br>vi —– :set number  修改25行<br>JAVA_HOME=<code>/opt/module/jdk1.8.0_192</code><br><code>&lt;!--测试hadoop是否成功--&gt;</code><br>hadoop jar hadoop-mapreduce-examples-2.7.3.jar wordcount ~/temp/a.txt ~/temp/output/wc0107</p><p>step2:hdfs-site.xml</p><pre><code>&lt;!--注释配置数据块的冗余度，默认是3--&gt; &lt;property&gt;    &lt;name&gt;dfs.replication&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;!--注释配置HDFS的权限检查，默认是true--&gt; &lt;!-- &lt;property&gt;    &lt;name&gt;dfs.permissions&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; --&gt;</code></pre><p>step3:core-site.xml</p><pre><code>&lt;!--配置HDFS主节点，namenode的地址,9000是RPC通信端口--&gt;  &lt;property&gt;     &lt;name&gt;fs.defaultFS&lt;/name&gt;     &lt;value&gt;hdfs://hsiehchou121:9000&lt;/value&gt;  &lt;/property&gt; &lt;!--配置HDFS数据块和元数据保存的目录,一定要修改--&gt;   &lt;property&gt;     &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;     &lt;value&gt;/opt/moudle/hadoop-2.7.3/tmp&lt;/value&gt;  &lt;/property&gt;</code></pre><p>step4：mapred-site.xml(默认没有)</p><pre><code>cp mapred-site.xml.template  mapred-site.xml&lt;!--配置MR程序运行的框架--&gt;&lt;property&gt;      &lt;name&gt;mapreduce.framework.name&lt;/name&gt;    &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; </code></pre><p>step5：yarn-site.xml</p><pre><code>&lt;!--配置Yarn的节点--&gt;&lt;property&gt;     &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;   &lt;value&gt;hsiehchou121&lt;/value&gt;&lt;/property&gt; &lt;!--NodeManager执行MR任务的方式是Shuffle洗牌--&gt;&lt;property&gt;     &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;   &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt; </code></pre><p>step 6：通过HDFS namenode 格式化<br>在第4步中，hadoop.tmp.dir–格式化<br> cd /opt/module/hadoop-2.7.3/tmp/<br> [root@hsiehchou121 hadoop]# cd /opt/module/hadoop-2.7.3/tmp/<br> [root@hsiehchou121 tmp]# hdfs namenode -format </p><p>重复格式化:hadoop.tmp.dir   先停止集群，需要删除原来的tmp文件。 rm -rf 重新格式化 启动集群</p><p>命令：hdfs namenode -format<br>验证：是否格式化成功：<br>Storage directory /opt/moudle/hadoop-2.7.3/tmp/dfs/name has been successfully formatted. </p><p>最后启动，通过start-all.sh启动<br>验证：<br>[root@hsiehchou121 tmp]# jps<br>        2336 NameNode<br>        2867 NodeManager<br>        3972 Jps<br>        2629 SecondaryNameNode<br>        2774 ResourceManager<br>        2441 DataNode<br>web访问:<br><a href="http://192.168.116.121:8088" target="_blank" rel="noopener">http://192.168.116.121:8088</a> yarn<br><a href="http://192.168.116.121:50070" target="_blank" rel="noopener">http://192.168.116.121:50070</a> HDFS</p><h4 id="12、免密码登录的原理和配置"><a href="#12、免密码登录的原理和配置" class="headerlink" title="12、免密码登录的原理和配置"></a>12、免密码登录的原理和配置</h4><p>SSH无密码登录<br>1）配置ssh<br>（1）基本语法<br>ssh 另一台电脑的ip地址<br>（2）ssh连接时出现Host key verification failed的解决方法<br>[root@hsiehchou121 opt]# ssh 192.168.116.103<br>The authenticity of host ‘192.168.116.103 (192.168.116.103)’ can’t be established.<br>RSA key fingerprint is cf:1e:de:d7:d0:4c:2d:98:60:b4:fd:ae:b1:2d:ad:06.<br>Are you sure you want to continue connecting (yes/no)?<br>Host key verification failed.<br>（3）解决方案如下：直接输入yes </p><p>2）无密钥配置<br>（1）进入到我的home目录<br>[root@hsiehchou121 opt]$ cd ~/.ssh</p><p>（2）生成公钥和私钥：<br>[root@hsiehchou121 .ssh]$ ssh-keygen -t rsa<br>然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） </p><p>（3）将公钥拷贝到要免密登录的目标机器上<br>[root@hsiehchou121 .ssh]$ <code>ssh-copy-id hsiehchou121</code><br>[root@hsiehchou121 .ssh]$ <code>ssh-copy-id hsiehchou122</code><br>[root@hsiehchou121 .ssh]$ <code>ssh-copy-id hsiehchou123</code><br>[root@hsiehchou121 .ssh]$ <code>ssh-copy-id hsiehchou124</code></p><p>（4）在hsiehchou122、hsiehchou123、hsiehchou124上分别执行所有操作</p><h4 id="13、Hadoop安装—全分布模式"><a href="#13、Hadoop安装—全分布模式" class="headerlink" title="13、Hadoop安装—全分布模式"></a>13、Hadoop安装—全分布模式</h4><p>作业：准备3台机器。完成1 的准备工作<br>加入到 etc/hosts<br>192.168.116.121 hsiehchou121<br>192.168.116.122 hsiehchou122<br>192.168.116.123 hsiehchou123<br>192.168.116.124 hsiehchou124</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git快速上手</title>
      <link href="/2019/02/03/git-kuai-su-shang-shou/"/>
      <url>/2019/02/03/git-kuai-su-shang-shou/</url>
      
        <content type="html"><![CDATA[<h3 id="一、Linux-平台上安装"><a href="#一、Linux-平台上安装" class="headerlink" title="一、Linux 平台上安装"></a>一、Linux 平台上安装</h3><p>Git 的工作需要调用 curl，zlib，openssl，expat，libiconv 等库的代码，所以需要先安装这些依赖工具<br>在有 yum 的系统上（比如 Fedora）或者有 apt-get 的系统上（比如 Debian 体系），可以用下面的命令安装：<br>各 Linux 系统可以很简单多使用其安装包管理工具进行安装：</p><h4 id="1、Debian-Ubuntu"><a href="#1、Debian-Ubuntu" class="headerlink" title="1、Debian/Ubuntu"></a>1、Debian/Ubuntu</h4><p>Debian/Ubuntu Git 安装命令为：</p><pre><code>apt-get install libcurl4-gnutls-dev libexpat1-dev gettext  libz-dev libssl-devapt-get install git-coregit --version</code></pre><h4 id="2、Centos-RedHat"><a href="#2、Centos-RedHat" class="headerlink" title="2、Centos/RedHat"></a>2、Centos/RedHat</h4><p>如果你使用的系统是 Centos/RedHat 安装命令为：</p><pre><code>yum install curl-devel expat-devel gettext-devel openssl-devel zlib-develyum -y install git-coregit --version</code></pre><h4 id="3、Git-配置"><a href="#3、Git-配置" class="headerlink" title="3、Git 配置"></a>3、Git 配置</h4><p><code>--system</code> 针对所有用户<br><code>--global</code> 针对当前用户<br>什么都不加参数，当前项目<br>优先级别：当前项目&gt;global&gt;system</p><h4 id="4、用户信息"><a href="#4、用户信息" class="headerlink" title="4、用户信息"></a>4、用户信息</h4><p>配置个人的用户名称和电子邮件地址：</p><pre><code>git config --global user.name &quot;&quot;git config --global user.email @qq.com</code></pre><h4 id="5、查看配置信息"><a href="#5、查看配置信息" class="headerlink" title="5、查看配置信息"></a>5、查看配置信息</h4><p>要检查已有的配置信息，可以使用 git config <code>--list</code> 命令：<br>git config <code>--list</code><br>也可以直接查阅某个环境变量的设定，只要把特定的名字跟在后面即可，像这样：<br>git config user.name<br>hsiehchou</p><h4 id="6、设置SSH"><a href="#6、设置SSH" class="headerlink" title="6、设置SSH"></a>6、设置SSH</h4><p>[root@test ~]# ssh-keygen -t rsa -C “@qq.com”</p><h4 id="7、Linux环境"><a href="#7、Linux环境" class="headerlink" title="7、Linux环境"></a>7、Linux环境</h4><p>vi /etc/hosts<br>添加一行：13.229.188.59　　github.com<br>linux下</p><p>在~/下， touch创建文件 .git-credentials, 用vim编辑此文件，输入：<br>vi ~/.git-credentials<br>https://{用户名}:@{密码}@github.com<br>注意去掉{}</p><p>在终端下执行 git config –global credential.helper store</p><p>可以看到~/.gitconfig文件，会多了一项：<br>[credential]<br>helper = store</p><h3 id="二、Windows环境"><a href="#二、Windows环境" class="headerlink" title="二、Windows环境"></a>二、Windows环境</h3><p>C:\Windows\System32\drivers\etc\hosts<br>添加一行：13.229.188.59　　github.com</p><h4 id="1、Github端的操作"><a href="#1、Github端的操作" class="headerlink" title="1、Github端的操作"></a>1、Github端的操作</h4><p>在Github的setting里面的SSH and GPG key的SSH keys添加公钥<br>公钥的获取方法是：<br>cat id_rsa_github.pub<br>连接成功<br>[root@test .ssh]# ssh -T <a href="mailto:git@github.com">git@github.com</a></p><p>-i ~/.ssh/id_rsa_github</p><h4 id="2、使用命令"><a href="#2、使用命令" class="headerlink" title="2、使用命令"></a>2、使用命令</h4><p>在git bash 中执行<br>设置记住密码（默认15分钟）：<br>git config <code>--global</code> credential.helper cache</p><p>如果想自己设置时间，可以这样做：<br>git config credential.helper  cache <code>--timeout</code>=7200’<br>这样就设置l两个小时之后失效 </p><p>长期存储密码：<br>git config <code>--global</code> credential.helper store</p><h4 id="3、在git-bash里边输入-git-remote-v"><a href="#3、在git-bash里边输入-git-remote-v" class="headerlink" title="3、在git bash里边输入 git remote -v"></a>3、在git bash里边输入 git remote -v</h4><p>git remote rm origin //删除http<br>git remote add origin <a href="mailto:git@github.com">git@github.com</a>:<del>/</del>.git //添加ssh<br>git push origin //执行更改</p><p>[root@test ~]# mkdir test<br>[root@test ~]# cd test/<br>[root@test test]# git init<br>Initialized empty Git repository in /root/test/.git/<br>[root@test test]# vim readme.md<br>[root@test test]# git status<br>[root@test test]# git add readme.md<br>[root@test test]# git status<br>[root@test test]# vim test1<br>[root@test test]# git add test1<br>[root@test test]# git status<br>[root@test test]# git commit -m “first commit”</p><p>简易提交显示<br>oneline 将 每个提交放在一行显示<br>[root@test test]# git log <code>--pretty</code>=oneline</p><p>图示表示版本提交<br>[root@test test]# git log <code>--graph</code></p><p>commit 8dd0b3d1a2be7064f7bb27e83a6ddc91146d38d0<br>| Author:<br>| Date:<br>|<br>| first commit<br>|</p><p>commit 5c2519af68ea0d0746894122b51a77cd11071ab8</p><p>reset<br>a-&gt;b-&gt;c-&gt;d使用git reset方法回到版本c，有3种方式：<br><code>--hard</code><br>版本库：c<br>暂存区：c，删掉版本d的暂存区<br>工作区：c，删掉版本d的工作区</p><p><code>--sort</code><br>版本库：c<br>暂存区：c，保留版本d的暂存区<br>工作区：c，保留版本d的工作区</p><p><code>--mixed</code>(默认)<br>版本库：c<br>暂存区：c，删除版本d的暂存区<br>工作区：回到版本c，同时保留版本d的工作区</p><p>git diff：暂存区 （比较前的文件）和工作区比较（比较后的文件）<br>git diff <code>--cached</code>：版本库（比较前的文件）和暂存区比较<br>git diff HEAD：版本库（比较前的文件）和工作区比较</p><p>— a/a<br>+++ b/a<br>@ -1 +1 @@<br>-1<br>+123</p><p>工作区：就是电脑上看到的目录，比如目录下test里的文件(.git隐藏目录版本库除外)。或者以后需要再新建的目录文件等等都属于工作区范畴</p><p>版本库(Repository)：工作区有一个隐藏目录.git,这个不属于工作区，这是版本库。其中版本库里面存了很多东西，其中最重要的就是stage(暂存区)，还有Git为我们自动创建了第一个分支master,以及指向master的一个指针HEAD</p><p>git add 把文件添加进去，实际上就是把文件添加到暂存区<br>git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支上</p><p>diff<br>[root@test diff]# git diff<br>diff –git a/a b/a (a（编辑前的版本，暂存区）/a ，b（编辑后的版本，工作区）/a<br>index 14cf074..ac80211 100644（两个文件的哈希值比较）<br>— a/a（—文件变动前的版本，暂存区）<br>+++ b/a（+++文件变动后的版本，工作区）<br>@ -1,2 +1,2 @@（-代表变动前，+代表变动后，1代表第一行，1,2代表连续两行）<br>-123:代表原版本<br>+123 4:代表变动后的版本在前版本上面的增加后的<br>fd</p><p>log<br>git reflog ：查询所有的提交历史<br>git log：看不到commit id的删除记录<br>例如：<br>[root@test log]# git reset <code>--hard</code> 7677a3235b46c<br>HEAD is now at 7677a32 b<br>[root@test log]# git log</p><p>ls ：显示不隐藏的文件与文件夹<br>ls -a：显示当前目录下的所有文件及文件夹包括隐藏的.和..等<br>ls -l ：显示不隐藏的文件与文件夹的详细信息<br>ls -al ：显示当前目录下的所有文件及文件夹包括隐藏的.和..等的详细信息</p><p>mkdir head<br>……<br>项目在创建的时候，git init在.git目录下有个HEAD文件，里面的内容指向了/refs/heads/master，但是没有master文件，说明没有任何提交</p><p>master<br>创建分支：git branch dev<br>切换到分支：git checkout dev</p><p>HEAD告诉我们当前在哪个分支上面，而且是哪一次提交</p><p>合并分支merge<br>[root@test branch]# git log <code>--oneline</code><br>74a5c98 a<br>[root@test branch]# git merge feature<br>Updating 74a5c98..1636fd2<br>Fast-forward<br>b | 1 +<br>c | 1 +<br>d | 1 +<br>3 files changed, 3 insertions(+)<br>create mode 100644 b<br>create mode 100644 c<br>create mode 100644 d<br>[root@test branch]# git log –oneline<br>1636fd2 d<br>8837eb1 b<br>1ef174d c<br>74a5c98 a</p><p>创建并切换新分支<br>[root@test merge]# git checkout -b feature</p><p>在当前分支基础上创建新的分支<br>[root@test merge]# git branch hotfix feature<br>[root@test merge]# git branch -v</p><p>git merge （直接合并到主分支）</p><p>合并并再次提交（合并到主分支，再次提交一次）<br>[root@test merge]# git merge hotfix <code>--no-ff</code></p><p>分支的内容合并后放到主分支里面<br>[root@test merge]# git merge hotfix <code>--squash</code></p><p>查看本地分支<br>git branch -v</p><p>如果新建的项目没有任何提交，是不能创建分支的</p><p>切换分支：git checkout <code>&lt;branchname&gt;</code><br>删除分支：不能再当前分区上删除当前分区</p><p>删除分支：git branch -d <code>&lt;branchname&gt;</code><br>如果当前分支有提交，而没有合并，就只能使用强制删除分支</p><p>强制删除分支：git branch -D <code>&lt;branchname&gt;</code><br>重命名分支：git branch -m <code>&lt;oldbranchname&gt; &lt;newbranchname&gt;</code><br>重命名分支可以在当前分支操作</p><p>创建远程分支：git push -u origin <code>&lt;branchname&gt;</code><br>拉取远程分支：git pull origin <code>&lt;branchname&gt;</code><br>删除远程分支： git push origin <code>--delete</code> <code>&lt;branchname&gt;</code><br>重命名远程分支：<br>1、先删除本地分支<br>2、重命名本地分支<br>3、向远程增加分支</p><p>远程分支覆盖本地分支<br>git pull origin master<br>git reset <code>--hard</code> FETCH_HEAD</p><p>git merge：会有清晰的提交历史<br>git rebase：整洁的提交历史</p><p>git rebase合并中出现冲突情况的解决<br>1、git rebase<br>2、git status<br>3、vim &lt;冲突文件&gt;<br>4、git add &lt;解决完的冲突文件&gt;<br>5、git status<br>6、git rebase <code>--continue</code></p><p>情形1<br>开发新功能问题<br>解决步骤：<br>1、拉取远程仓库代码<br>git pull origin master<br>2、创建新的分支，并在这个分支上写代码，提交<br>3、将自己的代码合并到master分支<br>4、然后将这个master分支推送到远程master分支<br>例如：<br>[root@test newfeature]# git remote add origin <a href="mailto:git@github.com">git@github.com</a>:Hsiehchou/hsiehchou001.git<br>[root@test newfeature]# git pull origin master<br>[root@test newfeature]# vim README.md<br>[root@test newfeature]# git add README.md<br>[root@test newfeature]# git commit -m “new feature develop over”<br>[root@test newfeature]# git checkout master<br>[root@test newfeature]# git push origin master</p><p>情形2<br>自己正在开发新代码，而且暂存区和本地仓库都有代码，在这个时候，老板说线上有个棘手的bug需要修复，自己需要停止手上的新功能开发<br>解决步骤：<br>1、git stash 将现有的暂存区和工作区的代码保留<br>2、然后创建新分支，修复bug<br>3、重新将stash里面的内容拿取出来</p><p>例如：<br>git checkout -b session<br>vim a<br>git add a<br>vim b<br>git status</p><p>git stash<br>git checkout -b hotfix<br>git checkout session<br>git stash pop</p><p>git stash <code>--help</code></p><p>情形3<br>自己在某个分支开发代码，然后别人在 另外的分支开发代码，现在别人已经提交好了代码，然后你想要别人的某几次提交，间隔，或者某一个提交<br>解决步骤：<br>git cherry-pick 合并某一个，或者某几个提交</p><p>git cherry-pick <code>&lt;commit id&gt; &lt;commit id&gt;</code><br>git cherry-pick <code>&lt;commit id&gt;...&lt;commit id&gt;</code><br>合并的时候不包括左边的，但是包括右边的</p><p>[root@test cherry]# git init<br>Initialized empty Git repository in /root/cherry/.git/<br>[root@test cherry]# touch a<br>[root@test cherry]# git add a<br>[root@test cherry]# git commit -m “a”<br>[master (root-commit) 47f4286] a<br>[root@test cherry]# git checkout -b hotfix<br>[root@test cherry]# touch b<br>[root@test cherry]# git add b<br>[root@test cherry]# git commit -m “b”<br>[hotfix 1b01f55] b<br>[root@test cherry]# touch c<br>[root@test cherry]# git add c<br>[root@test cherry]# git commit -m “c”<br>[root@test cherry]# touch d<br>[root@test cherry]# git add d<br>[root@test cherry]# git commit -m “d”<br>[root@test cherry]# git log <code>--oneline</code><br>45e8c64 d<br>cb1f9ef c<br>1b01f55 b<br>47f4286 a<br>[root@test cherry]# git checkout master<br>[root@test cherry]# git cherry-pick cb1f9ef<br>[root@test cherry]# git log <code>--oneline</code><br>c00f63d c<br>47f4286 a</p><p>[root@test cherry]# git log <code>--oneline</code><br>9b2ac83 d<br>8accef7 f<br>ca85346 e<br>c00f63d c<br>47f4286 a<br>[root@test cherry]# git checkout master<br>[root@test cherry]# git log <code>--oneline</code><br>c00f63d c<br>47f4286 a<br>[root@test cherry]# git cherry-pick ca85346 9b2ac83<br>[root@test cherry]# git log <code>--oneline</code><br>413b51b d<br>62fa796 e<br>c00f63d c<br>47f4286 a</p><p>git format-patch生成补丁<br>master a-&gt;b<br>feature a-&gt;b-&gt;c，将bc两次提交的不同生成patch给master，然后master应用补丁</p><p>远程仓库你没有权限，然后你fork远程仓库，代码，然后你clone到本地，接着修改，这是一个很小的修改，然后你又没有权限直接push到别人的仓库，这个时候你就把修改的代码生成一个patch</p><p>git format-patch <code>&lt;commit id&gt;</code>生成patch</p><p>git apply <code>--check</code> *.patch 检查这个patch是否能用</p><p>git am *.patch应用patch，创建git.am这个应用环境</p><p>git apply <code>--reject</code> *.patch会在当前文件夹下生成这个.rej，看这个.rej文件说明，去修改相应冲突的文件，修改完之后，用git add&lt;冲突的文件&gt;</p><p>git am <code>--resolved</code><br>例如：<br>[root@test ~]# mkdir patch<br>[root@test ~]# cd patch/<br>[root@test patch]# git init<br>[root@test patch]# touch a<br>[root@test patch]# git add a<br>[root@test patch]# git commit -m “a”<br>[root@test patch]# git checkout -b feature<br>[root@test patch]# vim b<br>[root@test patch]# git add b<br>[root@test patch]# git commit -m “b”<br>[root@test patch]# vim a<br>[root@test patch]# git add a<br>[root@test patch]# git commit -m “feature a”<br>[root@test patch]# git log <code>--oneline</code><br>01fd4c5 feature a<br>0445b79 b<br>ff994db a<br>[root@test patch]# git checkout master<br>[root@test patch]# vim a<br>[root@test patch]# git add a<br>[root@test patch]# git commit -m “master a”<br>[root@test patch]# git log <code>--oneline</code><br>f8af4f3 master a<br>ff994db a<br>[root@test patch]# git checkout feature<br>[root@test patch]# git log <code>--oneline</code><br>01fd4c5 feature a<br>0445b79 b<br>ff994db a<br>[root@test patch]# git format-patch <code>--help</code><br>[root@test patch]# git format-patch ff994db<br>[root@test patch]# git checkout master<br>[root@test patch]# git apply <code>--check</code> 0001-b.patch<br>[root@test patch]# git apply <code>--check</code> 0002-feature-a.patch<br>[root@test patch]# git am <em>.patch<br>[root@test patch]# git apply 0002-feature-a.patch <code>--reject</code><br>[root@test patch]# cat a.rej<br>diff a/a b/a (rejected hunks)<br>@ -0,0 +1 @@<br>+a<br>[root@test patch]# cat b<br>b<br>[root@test patch]# cat a<br>master a<br>[root@test patch]# vim a<br>[root@test patch]# git add a<br>[root@test patch]# git status<br>[root@test patch]# git am <code>--resolved</code><br>[root@test patch]# cat a<br>master a<br>b<br>[root@test patch]# git status<br>[root@test patch]# cat a.rej<br>diff a/a b/a (rejected hunks)<br>@ -0,0 +1 @@<br>+a<br>[root@test patch]# cat a<br>master a<br>b<br>[root@test patch]# rm -rf a.rej<br>[root@test patch]# rm -rf 000</em></p><p>[root@test patch]# git checkout -b hotfix<br>[root@test patch]# vim a<br>[root@test patch]# git add a<br>[root@test patch]# git commit -m “hotfix”<br>[root@test patch]# git log <code>--oneline</code><br>1cde42d hotfix<br>6b1b69b feature a<br>8301986 b<br>f8af4f3 master a<br>ff994db a<br>[root@test patch]# git format-patch <code>--help</code><br>[root@test patch]# git format-patch 6b1b69b -o /root<br>[root@test patch]# git checkout master<br>[root@test patch]# vim a<br>[root@test patch]# git add a<br>[root@test patch]# git commit -m “master2”<br>[root@test patch]# git apply <code>--check</code> /root/0001-hotfix.patch<br>[root@test patch]# git am /root/0001-hotfix.patch<br>[root@test patch]# git status<br>nothing to commit, working directory clean<br>[root@test patch]# git apply <code>--reject</code> /root/0001-hotfix.patch<br>[root@test patch]# ll<br>[root@test patch]# cat a.rej<br>diff a/a b/a (rejected hunks)<br>@ -1,2 +1,5 @@<br>master a<br>b<br><code>+</code><br>+hotfix<br><code>+</code><br>[root@test patch]# vim a<br>[root@test patch]# git add a<br>[root@test patch]# git am <code>--resolved</code><br>[root@test patch]# git status</p>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git快速上手 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据基础1</title>
      <link href="/2019/02/03/da-shu-ju-ji-chu-1/"/>
      <url>/2019/02/03/da-shu-ju-ji-chu-1/</url>
      
        <content type="html"><![CDATA[<h4 id="1、Linux下命令行中的复制和粘贴"><a href="#1、Linux下命令行中的复制和粘贴" class="headerlink" title="1、Linux下命令行中的复制和粘贴"></a>1、Linux下命令行中的复制和粘贴</h4><p>安装gpm：yum install -y gpm*</p><p>开启gpm服务：systemctl start gpm</p><h4 id="2、打开网卡"><a href="#2、打开网卡" class="headerlink" title="2、打开网卡"></a>2、打开网卡</h4><p>vi /etc/sysconfig/network-scripts/ifcfg-ens33</p><h4 id="3、修改为静态IP"><a href="#3、修改为静态IP" class="headerlink" title="3、修改为静态IP"></a>3、修改为静态IP</h4><p>BOOTPROTO=”dhcp” 这个是动态IP<br>BOOTPROTO=”static”这个是静态IP<br>BOOTPROTO=”none”这个是无</p><h4 id="4、IP地址"><a href="#4、IP地址" class="headerlink" title="4、IP地址"></a>4、IP地址</h4><p>IPADDR=192.168.116.121</p><h4 id="5、网关"><a href="#5、网关" class="headerlink" title="5、网关"></a>5、网关</h4><p>GATEWAY=192.168.116.2</p><h4 id="6、子网掩码"><a href="#6、子网掩码" class="headerlink" title="6、子网掩码"></a>6、子网掩码</h4><p>NETMASK=255.255.255.0</p><h4 id="7、DNS服务器1、2"><a href="#7、DNS服务器1、2" class="headerlink" title="7、DNS服务器1、2"></a>7、DNS服务器1、2</h4><p>DNS1=8.8.8.8<br>DNS2=8.8.4.4</p><h4 id="8、vi-etc-resolve-conf"><a href="#8、vi-etc-resolve-conf" class="headerlink" title="8、vi /etc/resolve.conf"></a>8、vi /etc/resolve.conf</h4><p>nameserver 8.8.8.8<br>nameserver 8.8.4.4</p><h4 id="9、重启网卡"><a href="#9、重启网卡" class="headerlink" title="9、重启网卡"></a>9、重启网卡</h4><p>service network restart</p><h4 id="10、-如何测试可以ping通"><a href="#10、-如何测试可以ping通" class="headerlink" title="10、 如何测试可以ping通"></a>10、 如何测试可以ping通</h4><p>ping 192.168.116.2<br>ping <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a></p><h4 id="11、解压JDK命令"><a href="#11、解压JDK命令" class="headerlink" title="11、解压JDK命令"></a>11、解压JDK命令</h4><p>tar -zxvf jdk-8u192-linux-x64.tar.gz -C /opt/module/</p><h4 id="12、系统JDK环境变量的位置"><a href="#12、系统JDK环境变量的位置" class="headerlink" title="12、系统JDK环境变量的位置"></a>12、系统JDK环境变量的位置</h4><p>vi /etc/profile</p><h4 id="13、写入"><a href="#13、写入" class="headerlink" title="13、写入"></a>13、写入</h4><p>export JAVA_HOME=<code>/opt/module/jdk1.8.0_192</code><br>export PATH=<code>$JAVA_HOME/bin:$PATH</code></p><h4 id="14、环境变量生效"><a href="#14、环境变量生效" class="headerlink" title="14、环境变量生效"></a>14、环境变量生效</h4><p>source /etc/profile</p><h4 id="15、检验JDK是否生效"><a href="#15、检验JDK是否生效" class="headerlink" title="15、检验JDK是否生效"></a>15、检验JDK是否生效</h4><p>输入javac，回车</p><h4 id="16、解压Hadoop"><a href="#16、解压Hadoop" class="headerlink" title="16、解压Hadoop"></a>16、解压Hadoop</h4><p>使用windscp上传<br>tar -zxvf hadoop-2.7.3.tar.gz -C /opt/module/</p><h4 id="17、解压Hadoop"><a href="#17、解压Hadoop" class="headerlink" title="17、解压Hadoop"></a>17、解压Hadoop</h4><p>使用windscp上传<br>tar -zxvf hadoop-2.7.3.tar.gz -C /opt/module/</p><h4 id="18、创建日志logs和临时目录tmp"><a href="#18、创建日志logs和临时目录tmp" class="headerlink" title="18、创建日志logs和临时目录tmp"></a>18、创建日志logs和临时目录tmp</h4><p>mkdir logs tmp</p><p>[root@localhost hadoop-2.7.3]#cd etc/hadoop/</p><h4 id="19、vi-hadoop-env-sh"><a href="#19、vi-hadoop-env-sh" class="headerlink" title="19、vi hadoop-env.sh"></a>19、vi hadoop-env.sh</h4><p>export JAVA_HOME=/opt/module/jdk1.8.0_192</p><h4 id="20、vi-core-site-xml"><a href="#20、vi-core-site-xml" class="headerlink" title="20、vi core-site.xml"></a>20、vi core-site.xml</h4><pre><code>&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt;    &lt;name&gt;fs.defaultFS&lt;/name&gt;    &lt;value&gt;hdfs://hsiehchou121:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt;    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;    &lt;value&gt;/opt/module/hadoop-2.7.3/tmp&lt;/value&gt;&lt;/property&gt;</code></pre><h4 id="21、vi-hdfs-site-xml"><a href="#21、vi-hdfs-site-xml" class="headerlink" title="21、vi hdfs-site.xml"></a>21、vi hdfs-site.xml</h4><pre><code>&lt;!-- 指定HDFS副本的数量 --&gt;&lt;property&gt;    &lt;name&gt;dfs.replication&lt;/name&gt;    &lt;value&gt;1&lt;/value&gt;&lt;/property&gt;</code></pre><h4 id="22、修改主机名"><a href="#22、修改主机名" class="headerlink" title="22、修改主机名"></a>22、修改主机名</h4><p>hostnamectl set-hostname 主机名<br>如：hostnamectl set-hostname hsiehchou121</p><h4 id="23、检查主机名"><a href="#23、检查主机名" class="headerlink" title="23、检查主机名"></a>23、检查主机名</h4><p>hostname</p><h4 id="24、格式化-hdfs"><a href="#24、格式化-hdfs" class="headerlink" title="24、格式化 hdfs"></a>24、格式化 hdfs</h4><p>[root@localhost hadoop-2.7.3]#bin/hdfs namenode -format</p><h4 id="25、启动-hdfs"><a href="#25、启动-hdfs" class="headerlink" title="25、启动 hdfs"></a>25、启动 hdfs</h4><p>[root@localhost hadoop-2.7.3]#sbin/start-dfs.sh</p><h4 id="26、关闭-hdfs"><a href="#26、关闭-hdfs" class="headerlink" title="26、关闭 hdfs"></a>26、关闭 hdfs</h4><p>[root@localhost hadoop-2.7.3]#sbin/stop-dfs.sh</p><h4 id="27、查看进程"><a href="#27、查看进程" class="headerlink" title="27、查看进程"></a>27、查看进程</h4><p>jps</p><p>[root@localhost hadoop-2.7.3]# jps<br>39668 DataNode<br>39547 NameNode<br>39932 Jps<br>39823 SecondaryNameNode</p><h4 id="28、页面"><a href="#28、页面" class="headerlink" title="28、页面"></a>28、页面</h4><p>IP地址:50070<br>如:192.168.116.121:50070<br>显示页面就对了</p><h4 id="29、临时关闭防火墙"><a href="#29、临时关闭防火墙" class="headerlink" title="29、临时关闭防火墙"></a>29、临时关闭防火墙</h4><p>systemctl stop firewalld.service</p><h4 id="30、永久禁用防火墙"><a href="#30、永久禁用防火墙" class="headerlink" title="30、永久禁用防火墙"></a>30、永久禁用防火墙</h4><p>systemctl disable firewalld.service</p><h4 id="31、查看防火墙状态"><a href="#31、查看防火墙状态" class="headerlink" title="31、查看防火墙状态"></a>31、查看防火墙状态</h4><p>systemctl status firewalld.service</p><h4 id="32、SSH无密码登录"><a href="#32、SSH无密码登录" class="headerlink" title="32、SSH无密码登录"></a>32、SSH无密码登录</h4><p>[root@localhost hadoop-2.7.3]$ ssh-keygen -t rsa<br>然后敲（三个回车）</p><p>[root@localhost hadoop-2.7.3]# ssh-copy-id hsiehchou121<br>[root@localhost hadoop-2.7.3]# ssh hsiehchou121<br>[root@hsiehchou121 ~]#<br>[root@hsiehchou121 ~]# exit<br>logout</p><h4 id="33、关闭selinux防火墙"><a href="#33、关闭selinux防火墙" class="headerlink" title="33、关闭selinux防火墙"></a>33、关闭selinux防火墙</h4><p>vi /etc/selinux/config</p><p>SELINUX=enforcing 改成<br>SELINUX=disabled</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker基本使用</title>
      <link href="/2019/02/01/docker-ji-ben-shi-yong/"/>
      <url>/2019/02/01/docker-ji-ben-shi-yong/</url>
      
        <content type="html"><![CDATA[<h2 id="Docker在Linux中的安装"><a href="#Docker在Linux中的安装" class="headerlink" title="Docker在Linux中的安装"></a>Docker在Linux中的安装</h2><h3 id="一、rpm离线安装docker17-12"><a href="#一、rpm离线安装docker17-12" class="headerlink" title="一、rpm离线安装docker17.12"></a>一、rpm离线安装docker17.12</h3><h4 id="1-下载docker安装包"><a href="#1-下载docker安装包" class="headerlink" title="1.下载docker安装包"></a>1.下载docker安装包</h4><p>在<a href="https://download.docker.com/linux/centos/7/x86_64/stable/Packages/下载docker-ce-17.12.0.ce-1.el7.centos.x86_64.rpm" target="_blank" rel="noopener">https://download.docker.com/linux/centos/7/x86_64/stable/Packages/下载docker-ce-17.12.0.ce-1.el7.centos.x86_64.rpm</a></p><h4 id="2-下载9个依赖"><a href="#2-下载9个依赖" class="headerlink" title="2.下载9个依赖"></a>2.下载9个依赖</h4><p>在<a href="http://mirrors.163.com/centos/7/os/x86_64/Packages/下载8个依赖" target="_blank" rel="noopener">http://mirrors.163.com/centos/7/os/x86_64/Packages/下载8个依赖</a><br>audit-libs-python-2.7.6-3.el7.x86_64.rpm<br>checkpolicy-2.5-4.el7.x86_64.rpm<br>libcgroup-0.41-13.el7.x86_64.rpm<br>libseccomp-2.3.1-3.el7.x86_64.rpm<br>libsemanage-python-2.5-8.el7.x86_64.rpm<br>policycoreutils-python-2.5-17.1.el7.x86_64.rpm<br>python-IPy-0.75-6.el7.noarch.rpm<br>setools-libs-3.3.8-1.1.el7.x86_64.rpm</p><p>在<a href="http://rpm.pbone.net/index.php3?stat=3&amp;limit=1&amp;srodzaj=1&amp;dl=40&amp;search=container-selinux&amp;field[]=1&amp;field[]=2下载container-selinux-2.9-4.el7.noarch.rpm" target="_blank" rel="noopener">http://rpm.pbone.net/index.php3?stat=3&amp;limit=1&amp;srodzaj=1&amp;dl=40&amp;search=container-selinux&amp;field[]=1&amp;field[]=2下载container-selinux-2.9-4.el7.noarch.rpm</a></p><p>rpm -ivh /root/docker/*.rpm –nodeps –force</p><p>curl -sSL <a href="http://acs-public-mirror.oss-cn-hangzhou.aliyuncs.com/docker-engine/internet" target="_blank" rel="noopener">http://acs-public-mirror.oss-cn-hangzhou.aliyuncs.com/docker-engine/internet</a> | sh -</p><p>###二、 启动Docker引擎<br>sudo systemctl enable docker<br>sudo systemctl start docker</p><h3 id="三、建立docker用户组"><a href="#三、建立docker用户组" class="headerlink" title="三、建立docker用户组"></a>三、建立docker用户组</h3><p>sudo groupadd docker</p><h3 id="四、将用户加入docker组"><a href="#四、将用户加入docker组" class="headerlink" title="四、将用户加入docker组"></a>四、将用户加入docker组</h3><p>sudo usermod -aG docker $USER</p><p>这里使用阿里云的容器镜像服务，目前公测，免费的。 </p><p>1) 去阿里云官网，登录控制台，在产品与服务里面找到容器镜像服务<br>2）点击开通<br>3) 点击镜像加速器，变有了加速地址（不用镜像加速器的话，镜像都是国外的，因为墙，所有下载是龟速）</p><h3 id="五、镜像加速器"><a href="#五、镜像加速器" class="headerlink" title="五、镜像加速器"></a>五、镜像加速器</h3><p>vi /etc/systemd/system/multi-user.target.wants/docker.service<br>ExecStart=/usr/bin/dockerd –registry-mirror=https://….mi<br>rror.aliyuncs.com</p><p>sudo systemctl daemon-reload<br>sudo systemctl restart docker</p><h3 id="六、检查加速器是否生效"><a href="#六、检查加速器是否生效" class="headerlink" title="六、检查加速器是否生效"></a>六、检查加速器是否生效</h3><p>sudo ps -ef | grep dockerd<br>root 5346 1 0 19:03 ? 00:00:00 /usr/bin/dockerd<br>–registry-mirror=https://*****.mirror.aliyuncs.com</p><p>docker -v</p><p>systemctl start docker</p><p>验证 docker 是否安装成功并在容器中执行一个测试的镜像<br>docker run ubuntu echo hello docker</p><p>docker run nginx</p><p>docker run -p 8080:80 -d nginx<br><a href="http://192.168.116.104:8080/" target="_blank" rel="noopener">http://192.168.116.104:8080/</a></p><p>[root@test3 share]# docker cp index.html<br>[root@test3 share]# docker commit -m ‘fun’ d0e976512485 nginx-fun</p><h3 id="七、删除镜像"><a href="#七、删除镜像" class="headerlink" title="七、删除镜像"></a>七、删除镜像</h3><p>docker rmi IMAGE ID</p><h3 id="八、查看镜像"><a href="#八、查看镜像" class="headerlink" title="八、查看镜像"></a>八、查看镜像</h3><p>docker images<br>docker ps -a</p><h3 id="九、小结"><a href="#九、小结" class="headerlink" title="九、小结"></a>九、小结</h3><table><thead><tr><th align="center">命令</th><th align="center">用途</th></tr></thead><tbody><tr><td align="center">docker pull</td><td align="center">获取image</td></tr><tr><td align="center">docker build</td><td align="center">创建image</td></tr><tr><td align="center">docker images</td><td align="center">列出image</td></tr><tr><td align="center">docker run</td><td align="center">运行container</td></tr><tr><td align="center">docker ps</td><td align="center">列出container</td></tr><tr><td align="center">docker rm</td><td align="center">删除container</td></tr><tr><td align="center">docker rmi</td><td align="center">删除image</td></tr><tr><td align="center">docker cp</td><td align="center">在host 和container之间拷贝文件</td></tr><tr><td align="center">docker commit</td><td align="center">保存改动为新的image</td></tr></tbody></table><h3 id="十、Dockerfile语法"><a href="#十、Dockerfile语法" class="headerlink" title="十、Dockerfile语法"></a>十、Dockerfile语法</h3><p>FROM alpine:latest<br>MAINTAINER hsiehchou<br>CMD echo ‘hello docker’</p><p>touch Dockerfile</p><p>++++++++++++++++++++++++++++++++++++++++<br>FROM ubuntu<br>MAINTAINER hsiehchou<br>RUN sed -i ‘s/archive.ubuntu.com/mirros.ustc.edu.cn/g’ /etc/apt/sources.list<br>RUN apt-get update<br>RUN apt-get install -y nginx<br>COPY index.html /var/www/html<br>ENTRYPOINT [“/usr/sbin/nginx”, “-g”, “daemon off;”]前台运行<br>EXPOSE 80</p><p>docker build -t hsiehchou/hello-nginx .</p><p>docker run -d -p 80:80 hsiehchou/hello-nginx</p><h3 id="十一、小结"><a href="#十一、小结" class="headerlink" title="十一、小结"></a>十一、小结</h3><table><thead><tr><th align="center">命令</th><th align="center">用途</th></tr></thead><tbody><tr><td align="center">FROM</td><td align="center">base image</td></tr><tr><td align="center">RUN</td><td align="center">执行命令</td></tr><tr><td align="center">ADD</td><td align="center">添加文件</td></tr><tr><td align="center">COPY</td><td align="center">拷贝文件</td></tr><tr><td align="center">CMD</td><td align="center">执行命令</td></tr><tr><td align="center">EXPOSE</td><td align="center">暴露端口</td></tr><tr><td align="center">WORKDIR</td><td align="center">指定路径</td></tr><tr><td align="center">MIANTAINER</td><td align="center">维护者</td></tr><tr><td align="center">ENV</td><td align="center">设定环境变量</td></tr><tr><td align="center">ENVRYPOINT</td><td align="center">容器入口</td></tr><tr><td align="center">USER</td><td align="center">指定用户</td></tr><tr><td align="center">VOLUME</td><td align="center">mount point</td></tr></tbody></table><h3 id="十二、镜像分层"><a href="#十二、镜像分层" class="headerlink" title="十二、镜像分层"></a>十二、镜像分层</h3><p>Dockerfile中的每一行都产生一个新层</p><h3 id="十三、Volume"><a href="#十三、Volume" class="headerlink" title="十三、Volume"></a>十三、Volume</h3><p>提供独立于容器之外的持久化存储</p><p>docker run -d –name nginx -v /usr/share/nginx/html nginx</p><p>docker exec -it nginx /bin/bash</p><p>docker run -v $PWD/html:/usr/share/nginx/html nginx</p><p>++++++++++++++++++++++++++++<br>docker create -v $PWD/data:/var /mydata –name data_container ubuntu</p><p>docker run -it –volumes-from data_container ubuntu /bin/bash<br>mount</p><h3 id="十四、Registry"><a href="#十四、Registry" class="headerlink" title="十四、Registry"></a>十四、Registry</h3><p>镜像仓库</p><h3 id="十五、术语"><a href="#十五、术语" class="headerlink" title="十五、术语"></a>十五、术语</h3><table><thead><tr><th align="center">English</th><th align="center">中文</th></tr></thead><tbody><tr><td align="center">host</td><td align="center">宿主机</td></tr><tr><td align="center">image</td><td align="center">镜像</td></tr><tr><td align="center">container</td><td align="center">容器</td></tr><tr><td align="center">registry</td><td align="center">仓库</td></tr><tr><td align="center">daemon</td><td align="center">守护程序</td></tr><tr><td align="center">client</td><td align="center">客户端</td></tr></tbody></table><p>docker search whalesay<br>docker pull docker/whalesay<br>docker push myname/whalesay</p><p>国内的一些仓库<br>daocloud<br>时速云<br>aliyun</p><p>[root@test3 dockerfiler2]# docker run docker/whalesay cowsay Docker你好！</p><p>docker tag docker/whalesay hch/whalesay</p><p>curl -L <a href="https://github.com/docker/compose/releases/download/1.9.0/docker-compose-" target="_blank" rel="noopener">https://github.com/docker/compose/releases/download/1.9.0/docker-compose-</a>(uname -m) &gt; /usr/local/bin/docker-compose</p><p><strong>docker-compose.yml常用命令</strong></p><table><thead><tr><th align="center">命令</th><th align="center">用途</th></tr></thead><tbody><tr><td align="center">build</td><td align="center">本地创建镜像</td></tr><tr><td align="center">command</td><td align="center">覆盖缺省命令</td></tr><tr><td align="center">depends_on</td><td align="center">连接容器</td></tr><tr><td align="center">ports</td><td align="center">暴露端口</td></tr><tr><td align="center">volumes</td><td align="center">卷</td></tr><tr><td align="center">image</td><td align="center">pull镜像</td></tr></tbody></table><p><strong>docker-compose命令</strong></p><table><thead><tr><th align="center">命令</th><th align="center">用途</th></tr></thead><tbody><tr><td align="center">up</td><td align="center">启动服务</td></tr><tr><td align="center">stop</td><td align="center">停止服务</td></tr><tr><td align="center">rm</td><td align="center">删除服务中的各个容器</td></tr><tr><td align="center">logs</td><td align="center">观察各个容器的日志</td></tr><tr><td align="center">ps</td><td align="center">列出服务相关的容器</td></tr></tbody></table><h3 id="十六、docker基本命令"><a href="#十六、docker基本命令" class="headerlink" title="十六、docker基本命令"></a>十六、docker基本命令</h3><p><strong>docker ps</strong>：查看正在运行的容器<br><strong>docker images</strong>：查看现有的镜像<br><strong>docker logs</strong>： 查看某个容器的日志<br><strong>docker run</strong>： 运行某个容器<br><strong>docker inspect</strong>：查看某个容器<br><strong>docker exec</strong>：进入某个容器<br><strong>docker start/stop</strong>：启动或者停止某个容器</p><p>[root@test3 hadoop-docker]# touch Dockerfile<br>[root@test3 hadoop-docker]# ll<br>total 0<br>-rw-r–r– 1 root root 0 Feb 27 19:18 Dockerfile<br>[root@test3 hadoop-docker]# vim Dockerfile</p><pre><code>FROM  ubuntu:14.04MAINTAINER hsiehchouWORKDIR /root# install openssh-server, openjdk and wgetRUN apt-get update &amp;&amp; apt-get install -y openssh-server openjdk-7-jdk wget# install hadoop 2.7.2RUN wget https://github.com/kiwenlau/compile-hadoop/release/download/2.7.2/hadoop-2.7.2.tar.gz &amp;&amp; \        tar -zxvf hadoop-2.7.2.tar.gz &amp;&amp; \        mv hadoop-2.7.2 /usr/local/hadoop &amp;&amp; \        rm hadoop-2.7.2.tar.gz# set environment variableENV JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd-64ENV HADOOP_HOME=/usr/local/hadoopENV PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin# ssh without keyRUN ssh-keygen -t rsa -f ~/.ssh/id_rsa -p &#39;&#39; &amp;&amp; \        cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keysRUN mkdir -p ~/hdfs/namenode &amp;&amp; \    mkdir -p ~/hdfs/datanode &amp;&amp; \    mkdir $HADOOP_HOME/logsCOPY config/* /tmp/RUN mv /tmp/ssh_config ~/.ssh/config &amp;&amp; \    mv /tmp/hadoop-env.sh /usr/local/hadoop/etc/hadoop/hadoop-env.sh &amp;&amp; \    mv /tmp/hdfs-site.xml $HADOOP_HOME/etc/hadoop/hdfs-site.xml &amp;&amp; \    mv /tmp/core-site.xml $HADOOP_HOME/etc/hadoop/core-site.xml &amp;&amp; \    mv /tmp/mapred-site.xml $HADOOP_HOME/etc/hadoop/mapred-site.xml &amp;&amp; \    mv /tmp/yarn-site.xml $HADOOP_HOME/etc/hadoop/yarn-site.xml &amp;&amp; \    mv /tmp/slaves $HADOOP_HOME/etc/hadoop/slaves &amp;&amp; \    mv /tmp/start-hadoop.sh ~/start-hadoop.ssh &amp;&amp; \    mv /tmp/run-wordcount.sh ~/run-wordcount.shRUN chmod +x ~/start-hadoop.sh &amp;&amp; \    chmod +x ~/run-wordcount.sh &amp;&amp; \    chmod +x $HADOOP_HOME/sbin/start-dfs.sh &amp;&amp; \    chmod +x $HADOOP_HOME/sbin/start-yarn.sh# format namenodeRUN /usr/local/hadoop/bin/hdfs namenode -formatCMD [&quot;sh&quot;, &quot;-c&quot;, &quot;service ssh start: bash&quot;]</code></pre>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker基本使用 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java之MySQL的使用</title>
      <link href="/2019/01/30/java-zhi-mysql-de-shi-yong/"/>
      <url>/2019/01/30/java-zhi-mysql-de-shi-yong/</url>
      
        <content type="html"><![CDATA[<h4 id="1、MySQL概要"><a href="#1、MySQL概要" class="headerlink" title="1、MySQL概要"></a>1、MySQL概要</h4><p>关系型数据库。—Access数据库 oracle数据库、Postgresql-<br>非关系型数据库。—-Hbase等<br>库：—package<br>表：–class<br>字段：–属性<br>Oracle旗下产品—-分两种 （GPL协议的 社区版和企业版）<br>CDH HDP–后面大数据给大家讲 Apache–hive hdfs hadoop</p><p>RDBMS：关系数据库管理系统。将数据存储在不同的库表里面<br>支持标准的SQL语句100%<br>体积小 速度快<br><a href="https://www.mysql.com/—》MySQL" target="_blank" rel="noopener">https://www.mysql.com/—》MySQL</a> Community Edition (GPL)-》MySQL OnWindows<br>MySQL Installer-》Windows (x86, 32-bit), MSI Installer 8.0.13 313.8M</p><p>mysql-installer-community-8.0.13.0.msi</p><h4 id="2、MySQL安装"><a href="#2、MySQL安装" class="headerlink" title="2、MySQL安装"></a>2、MySQL安装</h4><p>DBA–数据库管理员<br>黑框框+workbench<br>show databases;–展示所有库<br>show tables;–展示所有表;<br>describe city;–展示表里面的字段信息<br>select * from city limit 10;</p><h4 id="3、Navicat安装与操作MySQL"><a href="#3、Navicat安装与操作MySQL" class="headerlink" title="3、Navicat安装与操作MySQL"></a>3、Navicat安装与操作MySQL</h4><p>Navicat Premium 是一套数据库开发工具，让你从单一应用程序中同时连接 MySQL、MariaDB、MongoDB、SQL Server、Oracle、PostgreSQL 和 SQLite 数据库。它与 Amazon RDS、Amazon Aurora、Amazon Redshift、Microsoft Azure、Oracle Cloud、MongoDB Atlas、<br>阿里云、腾讯云和华为云等云数据库兼容。可以快速轻松地创建、管理和维护数据库</p><p>更新数据库的密码<br>ALTER USER ‘root’@’localhost’ IDENTIFIED WITH mysql_native_password BY ‘密码’;</p><h4 id="4、MySQL数据类型"><a href="#4、MySQL数据类型" class="headerlink" title="4、MySQL数据类型"></a>4、MySQL数据类型</h4><p>（1）数值类型<br>a、整型<br>tinyint 1个字节<br>smallint 2个字节<br>mediumint 3个字节<br>int 4<br>bigint 8<br>b、浮点型（float double）<br>float(M,D) 小数位数部分会四舍五入。M=3 D=2 3.15<br>c、定点数<br>可变长度 decimal(M,D) M:表示总的有效位数，D表示小数的位数<br>3.14<br>3.145 </p><p>（2）字符串类型<br>char:定长字符串 255个<br>varchar：变长字符串 varchar(25) 最大65535字符<br>blob:二进制字符串–文件 图片等<br>text:非二进制字符串–长文本</p><p>（3） 日期数据类型<br>datetime：2018-12-22 21:04:55<br>timestamp：时间戳 2019021600000 ms</p><h4 id="5、MySQL外键、主键、唯一键"><a href="#5、MySQL外键、主键、唯一键" class="headerlink" title="5、MySQL外键、主键、唯一键"></a>5、MySQL外键、主键、唯一键</h4><p>（1）外键 Foreign Key<br>如果换教室 302-303教室 需要对所有的数据进行 更新。30个学生 然后就得跟新30次<br>（2） 主键 Primary Key 唯一不可重复 只能有一个主键，不能为null<br>（3）唯一键 一个表可以有多个 唯一键 unique</p><h4 id="6、SQL语句–增删改查"><a href="#6、SQL语句–增删改查" class="headerlink" title="6、SQL语句–增删改查"></a>6、SQL语句–增删改查</h4><p>SQL：Structure Query Language。—HiveQL Spark SQL<br>查询：<br>select字段(*) from 表明 (limit count) (where); </p><p>插入语句：<br>insert into 表名 [字段名] values(值列表);</p><p>修改语句：<br>update 表名 set 字段=值 where 条件; </p><p>删除语句：<br>delete from 表名 [where 条件];</p><p>例如：<br>SELECT * FROM classroom;<br>insert into classroom VALUES(“001”,”9年级”,”11”,”CC”);<br>update classroom SET classroom.classroomid=”0003” where classroom.classroomid=”001”;<br>DELETE FROM classroom where classroom.classroomid=”0003”;<br>TRUNCATE student;</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java之MySQL的使用 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java反射</title>
      <link href="/2019/01/29/java-fan-she/"/>
      <url>/2019/01/29/java-fan-she/</url>
      
        <content type="html"><![CDATA[<h4 id="1、反射获取Class对象的三种方式"><a href="#1、反射获取Class对象的三种方式" class="headerlink" title="1、反射获取Class对象的三种方式"></a>1、反射获取Class对象的三种方式</h4><p>反编译<br>不是自己写的类，也不知道类里面有哪些方法 变量，让你能够使用程序上线了，修改程序但不终止程序的运行—-反射<br>（1）Object类 getClass 方法<br>getClass 返回此Object的运行时类<br>getName() 返回由 类对象表示的实体（类，接口，数组类，原始类型或空白）的名称，作为 String</p><p>（2）通过Class属性获得<br>都有一个静态的class属性</p><p>（3）通过 forName<br>static 类&lt;?&gt; forName(String className) 返回与给定字符串名称的类或接口相关联的 类对象</p><h4 id="2、反射获得构造方法"><a href="#2、反射获得构造方法" class="headerlink" title="2、反射获得构造方法"></a>2、反射获得构造方法</h4><p>（1）获得构造方法</p><pre><code>Constructor&lt;T&gt; getDeclaredConstructor(类&lt;?&gt;... parameterTypes) 返回一个 Constructor对象，该对象反映 Constructor对象表示的类或接口的指定 类函数Constructor&lt;?&gt;[] getDeclaredConstructors() 返回一个反映 Constructor对象表示的类声明的所有 Constructor对象的数组 类 Constructor&lt;T&gt; getConstructor(类&lt;?&gt;... parameterTypes) 返回一个 Constructor对象，该对象反映 Constructor对象表示的类的指定的公共类函数Constructor&lt;?&gt;[] getConstructors() 返回包含一个数组 Constructor对象反射由此表示的类的所有公共构造 类对象</code></pre><p>（2）使用构造方法<br>public T newInstance() throws InstantiationException, IllegalAccessException<br>访问私有的构造方法。必须通过Accessible设置为true。强行访问</p><p>public void setAccessible(boolean flag) throws SecurityException将此对象的accessible标志设置为指示的布尔值</p><p>true的值表示反射对象应该在使用时抑制Java语言访问检查。 false的值表示反映的对象应该强制执行Java语言访问检查</p><h4 id="3、反射获得成员变量"><a href="#3、反射获得成员变量" class="headerlink" title="3、反射获得成员变量"></a>3、反射获得成员变量</h4><p>（1）获得字段<br>Field[] getDeclaredFields()<br>返回的数组 Field对象反映此表示的类或接口声明的所有字段 类对象</p><p>getField(String name)<br>返回一个 Field对象，它反映此表示的类或接口的指定公共成员字段 类对象</p><p>Field[] getFields()<br>返回包含一个数组 Field对象反射由此表示的类或接口的所有可访问的公共字段 类对象</p><h4 id="4、反射获得成员方法"><a href="#4、反射获得成员方法" class="headerlink" title="4、反射获得成员方法"></a>4、反射获得成员方法</h4><p>Declared–所有的<br>Methods–公共的<br>使用成员方法<br>Object invoke(Object obj, Object… args)<br>在具有指定参数的 方法对象上调用此 方法对象表示的底层方法</p><h4 id="5、泛型"><a href="#5、泛型" class="headerlink" title="5、泛型"></a>5、泛型</h4><p>安全检测机制<br>例如：</p><pre><code> ArrayList&lt;T&gt; arrylist=new ArrayList&lt;T&gt;();</code></pre><p>存在类型错误 类型无法转换成功</p><h4 id="6、泛型方法"><a href="#6、泛型方法" class="headerlink" title="6、泛型方法"></a>6、泛型方法</h4><p>如何写一个方法 实现对 整数 浮点数 字符的输出。–泛型<br>基本原则：</p><p>   a、所有泛型方法的声明都有一个类型参数声明的部分（<code>&lt;T&gt;</code>）–表示所有的类型参数<br>   b、泛型方法只能是引用数据类型，（int double）<br>例如：</p><pre><code>public static&lt;T&gt; void show(){}</code></pre><h4 id="7、泛型类"><a href="#7、泛型类" class="headerlink" title="7、泛型类"></a>7、泛型类</h4><p>泛型类 增加了类型参数声明部分<br>例如：</p><pre><code>class Test&lt;T&gt;{    private T t;    Test(T t){        this.t=t;    }}</code></pre><h4 id="8、泛型擦除"><a href="#8、泛型擦除" class="headerlink" title="8、泛型擦除"></a>8、泛型擦除</h4><p>java本身不存在泛型。增加了泛型机制。—java虚拟机中都是确定的类型 泛型擦除</p><h4 id="9、类型通配符"><a href="#9、类型通配符" class="headerlink" title="9、类型通配符"></a>9、类型通配符</h4><?>–代替具体的类型参数 例如：```  public void print(List<?><p> data){<br>     data.get(0);<br>  }</p><p>```<br>  <code>&lt;T&gt;</code>–指所有的数据类型</p><h4 id="10、反射与泛型"><a href="#10、反射与泛型" class="headerlink" title="10、反射与泛型"></a>10、反射与泛型</h4><p>泛型：允许程序员在编译时检测到非法的数据类型，运行期间Object，泛型擦除<br>通过反射可以添加不同的数据类型</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java反射 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java多线程</title>
      <link href="/2019/01/28/java-duo-xian-cheng/"/>
      <url>/2019/01/28/java-duo-xian-cheng/</url>
      
        <content type="html"><![CDATA[<h4 id="1、ObjectInputStream"><a href="#1、ObjectInputStream" class="headerlink" title="1、ObjectInputStream"></a>1、ObjectInputStream</h4><p>反序列化<br>（1）构造函数<br>ObjectInputStream(InputStream in) 创建从指定的InputStream读取的ObjectInputStream</p><p>（2）主要方法<br>Object readObject() 从ObjectInputStream读取一个对象</p><h4 id="2、POI-实现对word、Excel等文件操作"><a href="#2、POI-实现对word、Excel等文件操作" class="headerlink" title="2、POI 实现对word、Excel等文件操作"></a>2、POI 实现对word、Excel等文件操作</h4><p>Apache–Download 安装包，根据操作系统<br>例如windows .zip<br>导包–build path-&gt;configure build path-&gt;add external jars</p><h4 id="3、多线程简介"><a href="#3、多线程简介" class="headerlink" title="3、多线程简介"></a>3、多线程简介</h4><p>进程：系统资源分配的单位。（cpu 磁盘 内存 网络）<br>线程：独立调度和分配的基本单位，共享进程资源<br>一个进程 包含多个线程，用来完成不同的工作，称之为多线程<br>进程是为了提高系统资源的利用率和系统吞吐量<br>线程是为了减少程序在并发执行时付出的时空开销</p><h4 id="4、线程的使用"><a href="#4、线程的使用" class="headerlink" title="4、线程的使用"></a>4、线程的使用</h4><p>（1）继承 thread 类<br>public class Thread extends Object implements Runnable<br>a、Thread类构造函数<br>Thread()<br>分配一个新的 Thread对象</p><p>Thread(String name)<br>分配一个新的 Thread对象</p><p>主要方法：<br>void run()<br>主要是运行线程所负责的主要任务</p><p>void setName(String name)<br>将此线程的名称更改为等于参数 name</p><p>void setPriority(int newPriority)<br>更改此线程的优先级</p><p>start()<br>导致此线程开始执行; Java虚拟机调用此线程的run方法</p><p>void setDaemon(boolean on)<br>将此线程标记为 daemon线程或用户线程</p><p>static Thread currentThread() 返回对当前正在执行的线程对象的引用</p><p>b、声明方式：</p><pre><code>public class 线程类名 extends Thread{     //重写run方法     public void run(){     } }</code></pre><p>c、调用和开启线程<br>线程类名 初始化<br>线程类名.start();<br>start() 导致此线程开始执行; Java虚拟机调用此线程的run方法</p><p>（2）实现Runnable接口<br>Interface Runnable<br>void run()<br>当实现接口的对象 Runnable被用来创建一个线程<br>启动线程使对象的 run在独立执行的线程中调用的方法</p><pre><code>   public class 线程名 implements Runnable{       //实现 run方法       public void run(){         }   }</code></pre><p>a、建立一个类实现runnable的接口<br>b、使用参数为Runnable对象的Thread构造方法。–Thread(Runnable target) 分配一个新的 Thread对象<br>c、 调用 start方法 开启线程</p><h4 id="5、线程的优先级"><a href="#5、线程的优先级" class="headerlink" title="5、线程的优先级"></a>5、线程的优先级</h4><p>默认的线程优先级：5<br>线程优先级最高为：10<br>最低的优先级为：1<br>优先级指的是一种概率</p><h4 id="6、守护线程"><a href="#6、守护线程" class="headerlink" title="6、守护线程"></a>6、守护线程</h4><p>用户线程：User Thread<br>守护线程：Daemon Thread ：主要提供服务的，为其他线程。比如 gc 垃圾回收线程<br>守护线程主要是在用户线程都执行完的情况下执行，如果没有用户线程执行，守护线程自动退出</p><h4 id="7、窗口卖票小案例"><a href="#7、窗口卖票小案例" class="headerlink" title="7、窗口卖票小案例"></a>7、窗口卖票小案例</h4><p>卖票 是针对同一个票额 同一个票库，两个线程同时访问<br>抢占资源，同一张票 卖给多个人<br>等待<br>保证数据在任何时刻只有一个线程访问，保证数据的完整性</p><h4 id="8、线程的同步"><a href="#8、线程的同步" class="headerlink" title="8、线程的同步"></a>8、线程的同步</h4><p>锁机制<br>（1）同步代码块<br>static{} {}<br>synchronized(){<br>//代码<br>}—-同步代码块</p><p>（2）同步方法<br>synchronized 修饰的方法</p><p>（3）互斥锁<br>lock，保证数据的完整性 一山不容二虎<br>ReentrantLock<br>构造方法–ReentrantLock() 创建一个 ReentrantLock的实例<br>主要方法：<br>void lock() 获得锁<br>void unlock() 尝试释放此锁</p><h4 id="9、线程的wait和notify"><a href="#9、线程的wait和notify" class="headerlink" title="9、线程的wait和notify"></a>9、线程的wait和notify</h4><p>wait：线程等待，直到另一个线程调用该对象的notify()方法或notifyAll()方法<br>notify:唤醒正在等待对象监视器的单个线程<br>notifyall:唤醒正在等待对象监视器的所有线程<br>sleep:long—睡眠时间 Thread类自带的方法—-自然醒<br>wait：继承于object的方法—-被叫醒</p><h4 id="10、线程池概述"><a href="#10、线程池概述" class="headerlink" title="10、线程池概述"></a>10、线程池概述</h4><p>多个线程运行时运行机制，包括排队策略 包括线程存活时间，框架策略<br>管理 创建 释放</p><h4 id="11、线程池使用"><a href="#11、线程池使用" class="headerlink" title="11、线程池使用"></a>11、线程池使用</h4><p>ThreadpoolExecutor<br>（1）构造方法<br>ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue workQueue)<br>corePoolSize：核心池<br>maximumPoolSize：线程池最大线程数<br>keepAliveTime：线程没有任务执行时，最多多长时间会终止&gt;corePoolSize 起作用<br>TimeUnit：keepAliveTime的时间单位<br>workQueue：用于存放待执行的任务<br>ArrayBlockingQueue、LInkedBlockingQueue、SynchronousQueue等<br>10个工人 一个工人同时做一个任务；<br>10个人都在干活，还有任务来了，任务排队<br>活太多，最多招5个人，<br>15个人还是赶不过来。放弃任务<br>额外找来的5个人，3个月后辞去，还是10个人干活<br>corePoolSize：10<br>maximumPoolSize：15<br>keepAliveTime:3个月</p><p>（2）主要方法<br>void execute(Runnable command) 在将来某个时候执行给定的任务<br>submit 提交任务<br>shutdown：关闭线程池<br>shutdownnow：立即关闭</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java多线程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java的IO流</title>
      <link href="/2019/01/26/java-de-io-liu/"/>
      <url>/2019/01/26/java-de-io-liu/</url>
      
        <content type="html"><![CDATA[<h4 id="1、IO概述"><a href="#1、IO概述" class="headerlink" title="1、IO概述"></a>1、IO概述</h4><p>Input ：将磁盘或硬盘、键盘等数据读入到内存的<br>Output：从内存输出到 磁盘、硬盘等<br>主要是以内存为基准</p><h4 id="2、输入输出流分类"><a href="#2、输入输出流分类" class="headerlink" title="2、输入输出流分类"></a>2、输入输出流分类</h4><p>（1）字节流<br>1Byte =8 bit<br>1KB=1024B<br>1MB=1024KB<br>1GB=1024MB<br>1TB=1024GB<br>1PB=1024TB——-商业的存储空间 都是以 1000为单位<br>字节流可以处理所有数据类型的数据，在Java中 以 Stream结尾的</p><p>（2）字符流<br>一个字符=2Byte<br>字符流对于处理文本数据有优势，在Java中 以 Reader 和 Writer结尾</p><p>（3）IO包<br>Java.IO—HDFS 离线计算</p><h4 id="3、File-类"><a href="#3、File-类" class="headerlink" title="3、File 类"></a>3、File 类</h4><p>文件和目录的抽象表示<br>（1）构造方法<br>File(String pathname)<br>pathname:<br>绝对路径：D:\TZ\Java黄埔9期\jdk+api+1.8_google<br>相对路径：day23 课程笔记.txt 相对我们的当前路径来说<br>./test/a.txt<br>File file=new File(pathname);<br>可以写文件目录 也可以写 具体文件</p><p>（2）常用方法<br>boolean exists() 测试此抽象路径名表示的文件或目录是否存在<br>String getAbsolutePath() 返回此抽象路径名的绝对路径名字符串<br>boolean isDirectory() 测试此抽象路径名表示的文件是否为目录<br>boolean isFile() 测试此抽象路径名表示的文件是否为普通文件<br>File[] listFiles() 返回一个抽象路径名数组，表示由该抽象路径名表示的目录中的文件</p><h4 id="4、FileInputStream–输入字节流"><a href="#4、FileInputStream–输入字节流" class="headerlink" title="4、FileInputStream–输入字节流"></a>4、FileInputStream–输入字节流</h4><p>FileInputStream用于读取诸如图像数据的原始字节流<br>public class FileInputStreamextends InputStream<br>（1）构造函数<br>FileInputStream(File file) 通过打开与实际文件的连接创建一个 FileInputStream ，<br>该文件由文件系统中的 File对象 file命名<br>File file=new File(“”);<br>FileInputStream fis=new FileInputStream(file);</p><p>（2）主要方法<br>int read() 从该输入流读取一个字节的数据，，如果达到文件的末尾， -1<br>int read(byte[] b) 从该输入流读取最多 b.length个字节的数据为字节数组。 -1</p><h4 id="5、FileOutputStream–输出字节流"><a href="#5、FileOutputStream–输出字节流" class="headerlink" title="5、FileOutputStream–输出字节流"></a>5、FileOutputStream–输出字节流</h4><p>把内存中的内容 输出到文件中去<br>文件输出流是用于将数据写入到输出流File<br>public class FileOutputStream extends OutputStream</p><p>（1）构造函数<br>FileOutputStream(File file)<br>创建文件输出流以写入由指定的 File对象表示的文件。–默认 append 为 false 覆盖更新文件内容<br>FileOutputStream(File file, boolean append)<br>创建文件输出流以写入由指定的 File对象表示的文件。追加模式可以设置为true</p><p>（2）主要函数<br>void write(int b)<br>将指定的字节写入此文件输出流</p><p>void write(byte[] b)<br>将 b.length个字节从指定的字节数组写入此文件输出流</p><p>void write(byte[] b, int off, int len)<br>将 len字节从位于偏移量 off的指定字节数组写入此文件输出流</p><p>void flush() 刷新此输出流并强制任何缓冲的输出字节被写出</p><h4 id="6、FileReader–输入字符流"><a href="#6、FileReader–输入字符流" class="headerlink" title="6、FileReader–输入字符流"></a>6、FileReader–输入字符流</h4><p>（1）构造函数<br>FileReader(File file) 创建一个新的 FileReader ，给出 File读取</p><p>（2）主要函数<br>public int read() 每次读取一个字符 -1表示到文件结尾；<br>public int read(char[] cbuf, int offset, int length)</p><h4 id="7、FileWriter-输出字符流"><a href="#7、FileWriter-输出字符流" class="headerlink" title="7、FileWriter-输出字符流"></a>7、FileWriter-输出字符流</h4><p>（1）构造函数<br>FileWriter(File file) 给一个File对象构造一个FileWriter对象<br>FileWriter(File file, boolean append) 给一个File对象构造一个FileWriter对象</p><p>（2）主要函数<br>public void write(char)<br>public void write(char[] cbuf,int off,int len)–写入字符数组<br>public void write(String str,int off,int len)–写入字符串</p><h4 id="8、BufferdReader"><a href="#8、BufferdReader" class="headerlink" title="8、BufferdReader"></a>8、BufferdReader</h4><p>(1) InputStreamReader–字符输入流<br>字节流 转为字符流的桥梁<br>编码问题，可以指定编码。–utf-8 GBK<br>InputStreamReader(InputStream in)<br>创建一个使用默认字符集的InputStreamReader</p><p>InputStreamReader(InputStream in, String charsetName)<br>创建一个使用命名字符集的InputStreamReader。</p><p>int read() 读一个字符<br>int read(char[] cbuf, int offset, int length) 将字符读入数组的一部分</p><p>（2）可以把字符流的效率提高，提供缓冲<br>可以使用 FIleReader、InputStreamReader等作为参数<br>实现字节流到字符流的缓冲<br>a、构造函数<br>BufferedReader(Reader in) 创建使用默认大小的输入缓冲区的缓冲字符输入流<br>BufferedReader(Reader in, int sz) 创建使用指定大小的输入缓冲区的缓冲字符输入流</p><p>b、主要函数<br>int read() 读一个字符<br>int read(char[] cbuf, int off, int len) 将字符读入数组的一部分。<br>String readLine() 读一行文字 —特色</p><h4 id="9、BufferedWriter"><a href="#9、BufferedWriter" class="headerlink" title="9、BufferedWriter"></a>9、BufferedWriter</h4><p>（1）OutputStreamWriter–指定字符集<br>将字节流转换为字符流的桥梁<br>OutputStreamWriter(OutputStream out, String charsetName)<br>创建一个使用命名字符集的OutputStreamWriter</p><p>主要方法：<br>void write(char[] cbuf, int off, int len)<br>写入字符数组的一部分</p><p>void write(int c)<br>写一个字符</p><p>void write(String str, int off, int len)<br>写一个字符串的一部分</p><p>（2）BufferedWriter<br>a、构造函数<br>BufferedWriter(Writer out)<br>创建使用默认大小的输出缓冲区的缓冲字符输出流</p><p>BufferedWriter(Writer out, int sz)<br>创建一个新的缓冲字符输出流，使用给定大小的输出缓冲区</p><p>b、主要方法<br>newLine()<br>写一行行分隔符</p><p>void write(char[] cbuf, int off, int len)<br>写入字符数组的一部分</p><p>void write(int c)<br>写一个字符</p><p>void write(String s, int off, int len)<br>写一个字符串的一部分</p><h4 id="10、序列化与反序列化"><a href="#10、序列化与反序列化" class="headerlink" title="10、序列化与反序列化"></a>10、序列化与反序列化</h4><p>序列化 就是把对象转换为字节序列的过程<br>反序列化 就是把字节恢复为对象的过程</p><h4 id="11、ObjectOutputStream–序列化"><a href="#11、ObjectOutputStream–序列化" class="headerlink" title="11、ObjectOutputStream–序列化"></a>11、ObjectOutputStream–序列化</h4><p>（1）构造函数<br>ObjectOutputStream() 为完全重新实现ObjectOutputStream的子类提供一种方法，不必分配刚刚被ObjectOutputStream实现使用的私有数据</p><p>ObjectOutputStream(OutputStream out)<br>创建一个写入指定的OutputStream的ObjectOutputStream</p><p>（2）主要方法<br>void write(byte[] buf) 写入一个字节数组<br>void write(byte[] buf, int off, int len) 写入一个子字节数组<br>void write(int val) 写一个字节<br>void writeObject(Object obj)<br>将指定的对象写入ObjectOutputStream<br>把类进行序列化 需要首先实现 Serializable接口<br>加密传输数据的一种方式</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java的IO流 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java异常</title>
      <link href="/2019/01/24/java-yi-chang/"/>
      <url>/2019/01/24/java-yi-chang/</url>
      
        <content type="html"><![CDATA[<p>Java异常</p><h4 id="1、异常概述"><a href="#1、异常概述" class="headerlink" title="1、异常概述"></a>1、异常概述</h4><p>（1）异常分为：编译时异常 运行时异常<br>（2）编译时异常：Javac IDE（，‘’），一般是指的 语法错误，比较容易修正<br>（3）运行时的异常：运行错误和逻辑错误<br>1/0;<br>（4）不正常的事件<br>异常的类，创建对象<br>NullPointException：空指针异常<br>Student stu;stu—&gt;对象<br>（5）异常处理机制<br>抛出异常—110<br>catch 异常— 依靠自己</p><h4 id="2、异常的分类"><a href="#2、异常的分类" class="headerlink" title="2、异常的分类"></a>2、异常的分类</h4><p>（1）Throwable—异常类的鼻祖。Throwable类是Java语言中所有错误和异常的Throwable类<br>（2）Error：错误<br>（3）Exception：<br>CheckedException：try catch来显示的捕获<br>例如：<br>RuntimeException<br>ArithmeticException：算术异常 例如 除数为0<br>IndexOutOfBoundsException:数组越界<br>NullPointException：空指针异常<br>IOException ：IO异常<br>FileNotFoundException：文件异常<br>ClassNotFoundException：找不到指定类<br>SQLException：SQL执行语句</p><h4 id="3、异常-gt-方法抛出异常-throw-关键字"><a href="#3、异常-gt-方法抛出异常-throw-关键字" class="headerlink" title="3、异常-&gt;方法抛出异常 throw 关键字"></a>3、异常-&gt;方法抛出异常 throw 关键字</h4><p>（1） throw 抛出异常，手动引发异常<br>例如： throw new IOException();<br>（2） throws 抛出异常，会抛出多个异常并不是处理异常 推卸责任<br>谁调用 抛给谁。 多个异常之间可以通过 ，分割</p><h4 id="4、异常-gt-异常的处理方式-try…catch…finally"><a href="#4、异常-gt-异常的处理方式-try…catch…finally" class="headerlink" title="4、异常-&gt;异常的处理方式 try…catch…finally"></a>4、异常-&gt;异常的处理方式 try…catch…finally</h4><pre><code>try{  可能出现异常的代码；}catch(异常处理的类型1 变量){    处理异常的代码}catch(异常处理类型2 变量){}...</code></pre><p>（1） catch 可以有多个<br>（2）异常的捕获必须从小类的异常 到 大类型的异常<br>（3）最多执行1个 catch语句块<br>finally ：一定会执行的代码，一般用来做资源的释放<br>例如：数据库连接的关闭<br>try catch finally 也可以直接与 try 连用<br>try finally<br>try catch finally 不能都单独存在。 catch 与 finally 必须与try 连用</p><h4 id="5、自定义异常"><a href="#5、自定义异常" class="headerlink" title="5、自定义异常"></a>5、自定义异常</h4><p>写一个子类 继承 RuntimeException。主要应对 Exception类内置异常无法解决问题时</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java异常 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java集合</title>
      <link href="/2019/01/20/java-ji-he/"/>
      <url>/2019/01/20/java-ji-he/</url>
      
        <content type="html"><![CDATA[<h4 id="1、集合概念"><a href="#1、集合概念" class="headerlink" title="1、集合概念"></a>1、集合概念</h4><p>回忆数组–数组有固定的长度<br>int[] arry=new int[10];</p><p>针对数据长度可变的情况—》集合<br>Java集合 应对动态增长数据（在编译的时候无法知道具体的数据量）<br>集合类–&gt;可变容器类</p><h4 id="2、集合和数组的区别"><a href="#2、集合和数组的区别" class="headerlink" title="2、集合和数组的区别"></a>2、集合和数组的区别</h4><p>都是容器<br>（1）数组是固定长度，集合的长度是可变的<br>（2）数组放的数据都是基本类型数据（四类8种），但是集合放的数据都是引用数据类型<br>（String、自定义的对象、Integer–int、Long）<br>（3）集合中对于基本数据会转换为引用数据类型再存储</p><h4 id="3、集合包含内容"><a href="#3、集合包含内容" class="headerlink" title="3、集合包含内容"></a>3、集合包含内容</h4><p>（1）Collection–接口 Interface</p><pre><code>Interface Collection&lt;E&gt;---add 方法public abstract class AbstractCollection&lt;E&gt; extends Object implements Collection&lt;E&gt;public abstract class AbstractList&lt;E&gt; extends AbstractCollection&lt;E&gt; implements List&lt;E&gt;</code></pre><p>a、List（接口）集合—特定顺序的元素</p><pre><code>public interface List&lt;E&gt; extends Collection&lt;E&gt;</code></pre><p>add(int index, E element) —指定索引处增加元素的位置<br>iterator() 以正确的顺序返回该列表中的元素的迭代器</p><p>b、Set（接口）集合–不能够有重复的元素</p><p>（2）Map–类似于数据库<br>主要存储”键值对” key-value MapReduce</p><p>（3）Iterable 集合的访问迭代 返回此集合中的元素的迭代器<br>没有关于元素返回顺序的保证（除非这个集合是提供保证的某个类的实例）</p><h4 id="4、集合框架-gt-集合的继承关系图"><a href="#4、集合框架-gt-集合的继承关系图" class="headerlink" title="4、集合框架-&gt;集合的继承关系图"></a>4、集合框架-&gt;集合的继承关系图</h4><p>Collection接口 Map<br>Collection 、Map 、List 、Set 等都是 Interface<br>AbstractCollection、 Abstractlist等 抽象类 实现了 Interface的部分方法<br>ArrayList 、LinkedList等 具体实现类 实现了 所有方法</p><h4 id="5、List集合介绍"><a href="#5、List集合介绍" class="headerlink" title="5、List集合介绍"></a>5、List集合介绍</h4><p>List集合是一个有序（索引有序）、可重复的集合，集合中每个元素都有对应的顺序索引<br>List允许加入重复元素是因为可以通过索引来访问指定位置的元素<br>List集合默认按照元素的添加顺序增加元素的索引</p><h4 id="6、List集合-gt-ArrayList"><a href="#6、List集合-gt-ArrayList" class="headerlink" title="6、List集合-&gt;ArrayList"></a>6、List集合-&gt;ArrayList</h4><p>（1）ArrayList简介<br>ArrayList 是基于数组实现的List类。实现所有可选列表操作，并允许所有元素，包括null </p><p>（2）初始化 ArrayList</p><pre><code>ArrayList&lt;E&gt; arrayList=new ArrayList&lt;E&gt;（）；---初始数据类型为E，容量大小为10的List</code></pre><p>（3）主要方法<br>boolean add(E e) 将指定的元素追加到此列表的末尾<br>void add(int index, E element) 在此列表中的指定位置插入指定的元素<br>boolean addAll(Collection&lt;? extends E&gt; c) 按指定集合的Iterator返回的顺序将指定集合中的所有元素追加到此列表的末尾<br>boolean addAll(int index, Collection&lt;? extends E&gt; c) 将指定集合中的所有元素插入到此列表中，从指定的位置开始<br>boolean contains(Object o) 如果此列表包含指定的元素，则返回 true<br>E get(int index) 返回此列表中指定位置的元素<br>E remove(int index) 删除该列表中指定位置的元素<br>E set(int index, E element) 用指定的元素替换此列表中指定位置的元素<br>Object[] toArray() 以正确的顺序（从第一个到最后一个元素）返回一个包含此列表中所有元素的数组</p><p>（4）List集合遍历的四种方法<br>a、通过 List.size<br>b、通过Iterator<br>boolean hasNext() 如果迭代具有更多元素，则返回 true<br>E next() 返回迭代中的下一个元素</p><h4 id="7、List集合-gt-LinkedList"><a href="#7、List集合-gt-LinkedList" class="headerlink" title="7、List集合-&gt;LinkedList"></a>7、List集合-&gt;LinkedList</h4><p>LinkedList 指的是链表类数据结构<br>与ArrayList的不同<br>（1）链表中的元素可以任意的增加和删除，效率很高，但是 查询效率不如ArrayList（有索引）<br>a-&gt;b-&gt;c….<br>（2）将对象存放在独立的空间中，而且每个空间保存了下一个连接的索引<br>（3）初始化<br>LinkedList linkedlist=new LinkedList();<br>（4）主要的方法<br>void addFirst(E e) 在该列表开头插入指定的元素<br>void addLast(E e) 将指定的元素追加到此列表的末尾<br>E peekFirst() 检索但不删除此列表的第一个元素，如果此列表为空，则返回 null<br>peekLast() 检索但不删除此列表的最后一个元素，如果此列表为空，则返回 null<br>pop() 从此列表表示的堆栈中弹出一个元素</p><h4 id="8、Set接口的介绍"><a href="#8、Set接口的介绍" class="headerlink" title="8、Set接口的介绍"></a>8、Set接口的介绍</h4><p>set集合存放无序不可重复的元素<br>list集合 存放有序可重复的元素。—索引<br>set集合不按照特定方式进行排序，只是放元素放在集合<br>set主要是由 HashSet和TreeSet具体实现类实现</p><h4 id="9、Set集合-gt-HashSet"><a href="#9、Set集合-gt-HashSet" class="headerlink" title="9、Set集合-&gt;HashSet"></a>9、Set集合-&gt;HashSet</h4><p>Hash（哈希算法）—-哈希函数定义的好坏<br>HashCode—哈希值<br>（1）equals（）方法判断两个元素的HashCode值是否相同 </p><p>（2）如果Hashcode值相同，继续与集合的元素作比较，<br>如果还相同则视为同一个对象，不保存在HashSet中<br>如果对象不相同，理论上要存储（比价麻烦）–避免发生 </p><p>（3）如果HashCode值不相同，直接把元素存放在该元素的Hashcode位置<br>public class HashSet extends AbstractSet implements Set, Cloneable, Serializable </p><p>（4）构造函数<br>HashSet hashSet=new HashSet();<br>boolean add(E e) 将指定的元素添加到此集合（如果尚未存在）<br>boolean contains(Object o) 如果此集合包含指定的元素，则返回 true</p><h4 id="10、Set集合-gt-TreeSet"><a href="#10、Set集合-gt-TreeSet" class="headerlink" title="10、Set集合-&gt;TreeSet"></a>10、Set集合-&gt;TreeSet</h4><p>TreeSet 是一个有序集合，默认将元素按照升序排列，Comparable接口<br>equals方法 判断元素是否重复<br>比较器 比较一下大小顺序</p><h4 id="11、Map-集合"><a href="#11、Map-集合" class="headerlink" title="11、Map 集合"></a>11、Map 集合</h4><p>Set 与list 都属于 Collection<br>Map每个元素的值都包含两个对象：key-value 键值对<br>key不能够重复；唯一的key 可以对应多个value<br>map中不存在索引，有key<br>循环访问的方式</p><h4 id="12、Map集合-gt-HashMap"><a href="#12、Map集合-gt-HashMap" class="headerlink" title="12、Map集合-&gt;HashMap"></a>12、Map集合-&gt;HashMap</h4><p>Hash算法<br>public class HashMap&lt;K,V&gt;extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable<br>允许null的值和null键<br>（1）初始化<br>HashMap&lt;key,value&gt; hashMap=new HashMap&lt;key,value&gt; ();</p><p>（2）主要的方法<br>put(K key, V value) 将指定的值与此映射中的指定键相关联<br>get(Object key) 返回到指定键所映射的值，或 null如果此映射包含该键的映射<br>Set keySet() 返回此地图中包含的键的Set视图<br>boolean containsKey(Object key) 如果此映射包含指定键的映射，则返回 true<br>boolean containsValue(Object value) 如果此地图将一个或多个键映射到指定值，则返回 true</p><h4 id="13、Map集合-gt-HashTable"><a href="#13、Map集合-gt-HashTable" class="headerlink" title="13、Map集合-&gt;HashTable"></a>13、Map集合-&gt;HashTable</h4><p>不接受 Null<br>为了成功的在hashtable中存储和获取对象，用作键的对象必须实现 hashcode和equals方法</p><h4 id="14、总结"><a href="#14、总结" class="headerlink" title="14、总结"></a>14、总结</h4><p>集合动态可扩展<br>Set代表无序集合不重复，（TreeSet 有序）<br>List集合有序可重复<br>Map 集合存储键值对- key value<br>自定义对象 要重写 方法（HashCode Comparator equals等）</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java集合 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java常用API</title>
      <link href="/2019/01/18/java-chang-yong-api/"/>
      <url>/2019/01/18/java-chang-yong-api/</url>
      
        <content type="html"><![CDATA[<h4 id="1、Java-API概述"><a href="#1、Java-API概述" class="headerlink" title="1、Java API概述"></a>1、Java API概述</h4><p>Java写好的包 类 方法的使用—API<br>Application Programing Interface：应用程序编程接口。Java提供的一些预定义的函数目的：基于API实现程序的快速编写。只需了解实现的作用，无需关注源代码</p><p>针对一个API首先看 概述了解 类的作用，然后看 构造函数了解如何创建类之后看方法，了解如何调用<br>Java lang–核心包 提供对Java编程语言设计至关重要的类，可以直接使用，不用import</p><h4 id="2、数值运算-Math类"><a href="#2、数值运算-Math类" class="headerlink" title="2、数值运算 Math类"></a>2、数值运算 Math类</h4><p>Math类为Java提供的支持数值运算的类<br>Math类包含执行基本数字运算的方法，如基本指数，对数，平方根和三角函数</p><p>public final class Math—-完美类<br>（1）Math类提供的基本方法：<br>static double abs(double a) 返回值为 double绝对值<br>static double acos(double a) 返回值的反余弦值; 返回的角度在0.0到pi的范围内<br>static double atan(double a)<br>向上取整：<br>static double ceil(double a) 返回大于或等于参数的最小（最接近负无穷大） double值，等于一个数学整数<br>向下取整：<br>static double floor(double a) 返回小于或等于参数的最大（最接近正无穷大） double值，等于一个数学整数<br>四舍五入：<br>static long round(double a) 返回参数中最接近的 long ，其中 long四舍五入为</p><p>static double log(double a) 返回的自然对数（以 e为底） double值<br>static double log10(double a) 返回一个 double的基数10对数值<br>static int max(int a, int b) 返回两个 int值中的较大值<br>static double random() 返回值为 double值为正号，大于等于 0.0 ，小于 1.0<br>public static double sqrt(double a)</p><h4 id="3、字符串运算-String类"><a href="#3、字符串运算-String类" class="headerlink" title="3、字符串运算 String类"></a>3、字符串运算 String类</h4><p>特殊的引用数据类型<br>public final class String—完美类<br>String a;<br>int a;</p><p>类名 对象名=new 类名();<br>String str = “abc”;<br>相当于：<br>char data[] = {‘a’, ‘b’, ‘c’};<br>String str = new String(data);—-不常见</p><p>char charAt(int index) 返回 char指定索引处的值</p><p>boolean contains(CharSequence s) 当且仅当此字符串包含指定的char值序列时才返回true</p><p>boolean equals(Object anObject) 将此字符串与指定对象进行比较</p><p>indexOf(String str) 返回指定子字符串第一次出现的字符串内的索引</p><p>length() 返回此字符串的长度。—循环的中止条件</p><p>boolean matches(String regex) 告诉这个字符串是否匹配给定的 regular expression</p><p>String replace(char oldChar, char newChar) 返回从替换所有出现的导致一个字符串 oldChar在此字符串 newChar</p><p>String[] split(String regex) 将此字符串分割为给定的 regular expression的匹配</p><p>String substring(int beginIndex) 返回一个字符串，该字符串是此字符串的子字符串</p><p>String toLowerCase() 将所有在此字符 String使用默认语言环境的规则，以小写</p><p>String toUpperCase() 将所有在此字符 String使用默认语言环境的规则大写</p><p>String trim() 返回一个字符串，其值为此字符串，并删除任何前导和尾随空格</p><p>/类型转换 将基本数据类型转换为 字符串/<br>static String valueOf(boolean b)<br>返回 boolean参数的字符串 boolean形式</p><p>static String valueOf(char c)<br>返回 char参数的字符串 char形式</p><p>static String valueOf(char[] data)<br>返回 char数组参数的字符串 char形式</p><p>static String valueOf(char[] data, int offset, int count)<br>返回 char数组参数的特定子阵列的字符串 char形式</p><p>static String valueOf(double d)<br>返回 double参数的字符串 double形式</p><p>static String valueOf(float f)<br>返回 float参数的字符串 float形式</p><p>static String valueOf(int i)<br>返回 int参数的字符串 int形式</p><p>static String valueOf(long l)<br>返回 long参数的字符串 long形式</p><p>static String valueOf(Object obj)<br>返回 Object参数的字符串 Object形式</p><p>String == 与equals的区别</p><p>如果声明String 是通过 String str=”” ，可以用 ==和equals<br>声明String 通过 new String(“”),不可以用 ==（调用 Object的equals方法） 只能用 equals</p><h4 id="4、字符串运算-大写字母-小写字母-数字出现的次数"><a href="#4、字符串运算-大写字母-小写字母-数字出现的次数" class="headerlink" title="4、字符串运算-大写字母 小写字母 数字出现的次数"></a>4、字符串运算-大写字母 小写字母 数字出现的次数</h4><p>getCount(string s) AscII码对比 length 遍历</p><h4 id="5、字符串运算-查找父字符串中某一个子字符串出现的次数"><a href="#5、字符串运算-查找父字符串中某一个子字符串出现的次数" class="headerlink" title="5、字符串运算-查找父字符串中某一个子字符串出现的次数"></a>5、字符串运算-查找父字符串中某一个子字符串出现的次数</h4><p>indexof 循环遍历子字符串出现的字数 就需要 截取 substring 把已找到的部分截取遍历后面的<br>边界条件</p><h4 id="6、字符串运算-split方法"><a href="#6、字符串运算-split方法" class="headerlink" title="6、字符串运算-split方法"></a>6、字符串运算-split方法</h4><p>public String[] split(String regex)<br>该方法的工作原理是通过使用给定表达式和限制参数为零调用双参数split方法<br>因此，尾随的空字符串不会包含在结果数组中<br>例如：String s=“1#2#3#4#5#”<br>String[] res=s.split(“#”);<br>res=[“1”,”2”,”3”,”4”,”5”]</p><p>返回的子串的次数 应该是 数组的长度-1</p><p>前后台交互或者进行数据接口会用到</p><h4 id="7、字符串运算-规则匹配"><a href="#7、字符串运算-规则匹配" class="headerlink" title="7、字符串运算-规则匹配"></a>7、字符串运算-规则匹配</h4><p>正则表达式 身份证号 电话号码 邮箱 QQ号等等<br>字符类：</p><table><thead><tr><th align="center">表示</th><th align="center">规则解释</th></tr></thead><tbody><tr><td align="center">[abc]</td><td align="center">a或b或c 都可以</td></tr><tr><td align="center">[a-zA-Z]</td><td align="center">a-z或者A到Z 两头的字母包含在内，所有字母都可以</td></tr><tr><td align="center">[0-9]</td><td align="center">0-9的数字都可以</td></tr><tr><td align="center">\d</td><td align="center">0-9的数字都可以</td></tr><tr><td align="center">\D</td><td align="center">[^0-9] 不是数字</td></tr><tr><td align="center">\w</td><td align="center">表示字母数字 下划线在内的任何字符 [a-zA-Z0-9_]</td></tr><tr><td align="center">X?</td><td align="center">X出现一次或一次也没有</td></tr><tr><td align="center">X*</td><td align="center">X零次或多次</td></tr><tr><td align="center">X+</td><td align="center">X至少出现一次</td></tr><tr><td align="center">X{n}</td><td align="center">恰好只有n次</td></tr><tr><td align="center">X{n,m}</td><td align="center">n=</td></tr></tbody></table><p>规则表达式：<br>^: 表示正则表达式的开头<br>$ : 表示正则表达式的结尾</p><p>例如：验证一个QQ号码<br>要求：<br>（1）QQ号码 5-15位<br>（2）0不能开头<br>规则表达式(字符串)：<br>regex=”[1-9]\d{4,14}”</p><p>public boolean matches(String regex)<br>告诉这个字符串是否匹配给定的regular expression<br>这种形式为str .matches( regex )方法的)产生与表达式完全相同的结果</p><p>Pattern. matches(regex, str)<br>参数<br>regex - 要匹配此字符串的正则表达式<br>结果<br>true如果，并且只有这个字符串与给定的正则表达式匹配<br>异常<br>PatternSyntaxException - 如果正则表达式的语法无效<br>//两个反斜杠 是转义的意思 (.\w{2,3})+ 表示 点出现一次或多次<br>// +号所跟的内容 如果在括号里面 表示 \w{2,3} 可以出现 多次</p><h4 id="8、Date"><a href="#8、Date" class="headerlink" title="8、Date"></a>8、Date</h4><p>（1）概述<br>包含集合框架，旧集合类，事件模型，日期和时间设施<br>国际化和其他实用程序类（字符串tokenizer，随机数生成器和位数组）<br>Java.util.* 工具包<br>在JDK 1.1之前， Date有两个附加功能，它允许将日期解释为年，月，日，小时，分钟和第二个值。 它还允许格式化和解析日期字符串。 不幸的是，这些功能的API不适合国际化。 从JDK 1.1开始， Calendar类应该用于在日期和时间字段之间进行转换，<br>并且DateFormat类应用于格式化和解析日期字符串。 在相应的方法Date被弃用<br>允许JDBC将其标识为 SQlDate值<br>格林尼治标准时间（GMT）定义的，相当于世界时间（UT）<br>（2）构造方法<br>Date()<br>分配一个 Date对象，并初始化它，以便它代表它被分配的时间，测量到最近的毫秒</p><p>Date(long date)<br>分配一个 Date对象，并将其初始化为表示自称为“时代”的标准基准时间以后的指定毫秒数，即1970年1月1日00:00:00 GMT</p><p>（3）常用方法<br>boolean after(Date when)<br>测试此日期是否在指定日期之后</p><p>boolean before(Date when)<br>测试此日期是否在指定日期之前</p><p>Object clone()<br>返回此对象的副本</p><p>int compareTo(Date anotherDate)<br>比较两个日期进行订购</p><p>boolean equals(Object obj)<br>比较两个日期来平等</p><p>static Date from(Instant instant)<br>从 Instant对象获取一个 Date的实例</p><p>getTime()<br>返回自1970年1月1日以来，由此 Date对象表示的00:00:00 GMT的毫秒数</p><h4 id="9、Calendar"><a href="#9、Calendar" class="headerlink" title="9、Calendar"></a>9、Calendar</h4><p>（1）简介<br>相对比较新的日期类，抽象类</p><p>  public abstract class Calendar extends Object<br>    implements Serializable, Cloneable, Comparable<Calendar><br>所述Calendar类是一个抽象类<br>可以为在某一特定时刻和一组之间的转换的方法calendar fields如<br>YEAR ， MONTH ， DAY_OF_MONTH ， HOUR 等等，以及用于操纵该日历字段</p><p>（2）初始化<br>Calendar提供了一种类方法getInstance 用于获取此类型的一般有用的对象<br>Calendar的getInstance方法返回一个Calendar对象，其日历字段已使用当前日期和时间进行初始化<br>Date date=new Date(); —- 对象的初始化<br>Calendar rightNow = Calendar.getInstance(); —抽象类自带方法 获得对象</p><p>（3）常用的方法<br>boolean after(Object when)<br>返回 Calendar是否 Calendar指定时间之后的时间 Object</p><p>boolean before(Object when)<br>返回此 Calendar是否 Calendar指定的时间之前指定的时间 Object</p><p>int getWeekYear()<br>返回这个 Calendar</p><p>set(int year, int month, int date)<br>—Date的时候 需要计算一下 距离 1970 ms数</p><p>public abstract void add(int field,int amount)<br>根据日历的规则，将指定的时间量添加或减去给定的日历字段</p><p>例如，要从当前日历的时间减去5天，可以通过调用以下方法来实现<br>add(Calendar.DAY_OF_MONTH, -5)<br>public abstract void roll(int field,boolean up)<br>在给定时间字段上添加或减少单个时间单位，而不改变较大的字段</p><p>字段可以直接访问 static final ，使用 get set 方法 获得或设置字段值</p><p>public static final int YEAR—直接通过类名可以访问到年<br>public static final int MONTH<br>public static final int WEEK_OF_YEAR<br>public static final int WEEK_OF_MONTH<br>public static final int DAY_OF_YEAR</p><h4 id="10、DateFormat–xxxx年xx月xx日"><a href="#10、DateFormat–xxxx年xx月xx日" class="headerlink" title="10、DateFormat–xxxx年xx月xx日"></a>10、DateFormat–xxxx年xx月xx日</h4><p>格式化日期<br>（1）简介<br>public abstract class DateFormat extends Format<br>DateFormat可帮助格式化和解析任何区域设置的日期</p><p>（2）初始化<br>public static final DateFormat getDateInstance()<br>// 抽象类<br>DateFormat df=DateFormat.getDateInstance;</p><p>（3）主要方法<br>String format(Date date) 将日期格式化成日期/时间字符串<br>Date parse(String source) 从给定字符串的开始解析文本以生成日期</p><h4 id="11、simpleDateFormat–DateFormat子类"><a href="#11、simpleDateFormat–DateFormat子类" class="headerlink" title="11、simpleDateFormat–DateFormat子类"></a>11、simpleDateFormat–DateFormat子类</h4><p>（1）实现了 DateFormat 不是抽象类–优秀的实现类<br>public class SimpleDateFormat extends DateFormat</p><p>例如：转换成 2018/12/5 2018年12月5日<br>SimpleDateFormat允许从选择日期时间格式化的任何用户定义的模式开始。<br>yyyy年MM月dd日</p><p>（2）构造方法<br>SimpleDateFormat() 构造一个 SimpleDateFormat使用默认模式和日期格式符号为默认的 FORMAT区域设置<br>SimpleDateFormat(String pattern) 使用给定模式 SimpleDateFormat并使用默认的 FORMAT语言环境的默认日期格式符号<br>（不需要再使用后面的 applyPattern方法 可以直接赋值）</p><p>（3）主要方法<br>applyPattern(String pattern) 给定的模式字符串应用于此日期格式<br>String format(Date date) 将日期格式化成日期/时间字符串。—进行了重写<br>Date parse(String source) 从给定字符串的开始解析文本以生成日期。–进行了重写</p><p>小案例：我活了多久</p><h4 id="12、StringBuffer"><a href="#12、StringBuffer" class="headerlink" title="12、StringBuffer"></a>12、StringBuffer</h4><p>（1）简介<br>和String一样 final –完美类 可以任意调节数据字符串的长度和内容<br>public final class StringBuffer<br>extends Object<br>implements Serializable, CharSequence</p><p>字符串缓冲区就像一个String ，但可以修改。 在任何时间点，它包含一些特定的字符序列。但可以通过某些方法调用来更改序列的长度和内容。 —-可以变化<br>例如： string s =”abc“;<br>s+=”1”;–abc1<br>Stringbuffer sbuffer =new StringBuffer();</p><p>（2）构造函数<br>StringBuffer() 构造一个没有字符的字符串缓冲区，初始容量为16个字符</p><p>（3）主要方法<br>append(String str) 将指定的字符串附加到此字符序列<br>—相当于 String+insert(int offset, String str) 将字符串插入到此字符序列中<br>toString() 将字符串转为string型</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java常用API </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java面向对象3</title>
      <link href="/2019/01/16/java-mian-xiang-dui-xiang-3/"/>
      <url>/2019/01/16/java-mian-xiang-dui-xiang-3/</url>
      
        <content type="html"><![CDATA[<h4 id="1、访问控制权限（public，private，protected，default）"><a href="#1、访问控制权限（public，private，protected，default）" class="headerlink" title="1、访问控制权限（public，private，protected，default）"></a>1、访问控制权限（public，private，protected，default）</h4><p>public&gt;protected&gt;default&gt;private<br>Java中用来控制类及类的方法和变量访问权限<br>（1）public ：公共的 表示包（package）内及包外的任何类（包括子类和普通类）都可以访问。—最开放<br>（2）protected：受保护的 表示包内的任何类及包外继承了该类的子类才能访问，突出继承<br>（3）default：默认的 表示包内的任何类都可以访问，但是包外的任何类都不能访问<br>（4）private：私有的 只有本类可以访问，包内外的任何类均不能访问。—封装</p><table><thead><tr><th align="center">访问控制修饰符</th><th align="center">同类</th><th align="center">同包</th><th align="center">子类</th><th align="center">不同的包</th></tr></thead><tbody><tr><td align="center">public</td><td align="center">1</td><td align="center">1</td><td align="center">1</td><td align="center">1</td></tr><tr><td align="center">protected</td><td align="center">1</td><td align="center">1</td><td align="center">1</td><td align="center">0</td></tr><tr><td align="center">default</td><td align="center">1</td><td align="center">1</td><td align="center">0</td><td align="center">0</td></tr><tr><td align="center">private</td><td align="center">1</td><td align="center">0</td><td align="center">0</td><td align="center">0</td></tr></tbody></table><h4 id="2、多态定义"><a href="#2、多态定义" class="headerlink" title="2、多态定义"></a>2、多态定义</h4><p>多态分为编译时的多态和运行时多态。其中编译时多态 也可称为静态多态<br>运行时的多态为动态多态，主要通过动态绑定来实现，常说默认的多态<br>多态 为了应对不同的变现形式</p><h4 id="3、静态多态"><a href="#3、静态多态" class="headerlink" title="3、静态多态"></a>3、静态多态</h4><p>其实就是 方法的重载，主要根据参数列表的不同来区分不同的函数<br>静态多态 不需要继承</p><h4 id="4、动态多态"><a href="#4、动态多态" class="headerlink" title="4、动态多态"></a>4、动态多态</h4><p>例如：品酒大师<br>三个杯子 倒了 3杯酒<br>酒 a= 五粮液；<br>酒 b= 茅台酒；<br>酒 c= 二锅头。<br>声明一个 酒的类，三种不同的酒 相当于不同的子类<br>只有在运行时 才能知道 喝的什么酒<br>所谓动态多态就是指 引用在不同的情况下所表现的实际对象<br>（1）继承（实现接口）。在多态中必须存在 父类与子类的关系<br>（2）重写。子类必须对父类的某些方法进行重新定义，在调用这些方法时 就会调用子类的方法<br>（3）向上转型：父类引用指向子类的对象</p><h4 id="5、向上转型"><a href="#5、向上转型" class="headerlink" title="5、向上转型"></a>5、向上转型</h4><p>向上转型：子类的对象转换为父类类型<br>例如：<br>Wine wine=new WLY();—向上转型<br>Wine wine=new Wine();–正常实例化对象<br>子类的单独定义的方法会丢失，能访问子类重写父类的方法</p><h4 id="6、动态多态小案例–动物喂食"><a href="#6、动态多态小案例–动物喂食" class="headerlink" title="6、动态多态小案例–动物喂食"></a>6、动态多态小案例–动物喂食</h4><p>养了一堆宠物 有狗 有猫。宠物喜欢吃什么 也要根据宠物的类型 来选择喂食<br>狗–骨头<br>猫–鱼 </p><pre><code>if(animal is dog ){     food=bone；     eat food; }else if(animal is cat){     food= fish;     eat fish. } </code></pre><p>可否写一个方法 来实现所有宠物的喂食</p><h4 id="7、向下转型"><a href="#7、向下转型" class="headerlink" title="7、向下转型"></a>7、向下转型</h4><p>向下转型是把父类对象转换为子类对象<br>Animal animal=new Animal();<br>Cat cat = （Cat）animal —-不对的<br>把一个动物强制转换为 猫，如果这个动物是只狗，狗是变不成猫的<br>向下转型必须得有向上转型作为前提。因为只有子类相对应的才可以转换<br>代表这个动物是 猫，之后 把动物再变回为猫。—打回原形</p><h4 id="8、-内部类定义"><a href="#8、-内部类定义" class="headerlink" title="8、.内部类定义"></a>8、.内部类定义</h4><p>在Java当中的一个类中在声明一个类 就叫 内部类<br>例如：</p><pre><code> class Outter{    成员变量；    class Inner{    }    成员方法； } </code></pre><h4 id="9、内部类分类"><a href="#9、内部类分类" class="headerlink" title="9、内部类分类"></a>9、内部类分类</h4><p>（1）（普通）成员内部类：与成员level一样，内部类中不能存在 static 关键字，不能够声明静态的方法、属性、静态代码块；<br>最普通的内部类<br>（2）静态（成员）内部类：使用static修饰的成员内部类<br>（3）（普通）局部内部类：局部范围内有效的内部类（例如：方法里面）<br>（4）匿名（局部）内部类：没有名字的局部内部类 </p><p><strong>成员内部类定义</strong><br>（1）定义：与我们的成员变量一样，可以声明类名，在成员内部类中可以声明属性和方法<br>（2）作用：<br>a、成员内部类可以无限制访问外部类的变量和方法（包括private修饰的）<br>b、内部类可以有多个<br>c、成员内部类与外部类如果存在同名的成员变量或方法，优先是内部的。如果访问外部类的<br>需要 Outter.this.(变量或方法名)</p><h4 id="10、成员内部类与外部类的访问"><a href="#10、成员内部类与外部类的访问" class="headerlink" title="10、成员内部类与外部类的访问"></a>10、成员内部类与外部类的访问</h4><p>（1）成员内部类访问外部类 无限制<br>（2）外部类访问内部类的成员，不是无限制的<br>首先要传建一个内部类的对象，然后通过对象来访问</p><h4 id="11、成员内部类的初始化"><a href="#11、成员内部类的初始化" class="headerlink" title="11、成员内部类的初始化"></a>11、成员内部类的初始化</h4><p>不是在类里面操作，如果是其他类要访问时，要访问内部类，首先实现外部类的实例化之后再实例化内部类<br>（1）在外部类对象初始化基础之上初始化内部类，调用内部类的构造函数<br>Outter.Inner inner=outter.new Inner();<br>（2）通过外部类的成员方法获得成员内部类的对象，然后访问其变量和方法</p><h4 id="12、静态内部类"><a href="#12、静态内部类" class="headerlink" title="12、静态内部类"></a>12、静态内部类</h4><p>使用 static修饰的成员内部类叫做静态内部类<br>定义格式如下：</p><pre><code>class Outter{    static  class inner{    }}</code></pre><p>外部类不是静态也可以声明静态内部类<br>静态内部类 要类比 静态成员变量<br>静态内部类可以通过外类直接调用 new Outter.Inner();<br>静态内部类内部可以直接访问外部类中所有的静态变量和方法（包含private）</p><h4 id="13、局部内部类"><a href="#13、局部内部类" class="headerlink" title="13、局部内部类"></a>13、局部内部类</h4><p>定义在代码块、方法体等的类叫局部内部类<br>—局部变量 类比<br>不能够有 public protected private 以及 static 修饰</p><pre><code>class Outter{    public void func(){         class inner{         }     }}</code></pre><p>局部内部类只是在一个方法或区域里起作用</p><h4 id="14、匿名内部类"><a href="#14、匿名内部类" class="headerlink" title="14、匿名内部类"></a>14、匿名内部类</h4><p>没有名字的局部内部类<br>必须要继承一个父类或者实现一个接口<br>定义形式：<br>正常初始化对象：</p><pre><code>类名 对象名=new 类名（）；匿名内部类：new 父类构造方法（）{     //重写一个函数     修饰符 返回参数类型 方法名（参数列表）{      } }；</code></pre><p>局部内部类的区别 局部的位置不同<br>匿名内部类当中不能够有静态属性和静态方法<br>匿名内部类 不需要新建一个类 而是通过匿名的形式吧 实现方法的重写<br>匿名内部类尤其针对 Android开发 例如 监听 鼠标事件 键盘 触屏输入</p><pre><code> Lisenter（）{     @override     MouseMoniter（）{     } }；</code></pre><h4 id="15、总结内部类"><a href="#15、总结内部类" class="headerlink" title="15、总结内部类"></a>15、总结内部类</h4><p>（1）成员内部类<br>（2）静态内部类<br>（3）局部内部类<br>（4）匿名内部类<br>a、每个内部类都可以独立的继承或实现一个接口，而外部类也可以继承一个直接父类。 —多继承的一种表现<br>b、通过内部类可以实现对外界的隐藏。–封装<br>c、内部类可以无限制的使用外部类的成员变量（包括私有），不用生成外部类的对象<br>d、匿名内部类可以简化代码的编写，方便编写事件驱动的程序、线程程序<br>e、成员内部类 静态内部类 可以对比 成员变量和静态变量<br>局部内部类 匿名内部类 可以对比局部变量</p><h4 id="16、面向对象总结"><a href="#16、面向对象总结" class="headerlink" title="16、面向对象总结"></a>16、面向对象总结</h4><p>封装 继承 多态<br>面向对象的思路去设计程序</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java面向对象 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java面向对象2</title>
      <link href="/2019/01/14/java-mian-xiang-dui-xiang-2/"/>
      <url>/2019/01/14/java-mian-xiang-dui-xiang-2/</url>
      
        <content type="html"><![CDATA[<h4 id="1、static关键字"><a href="#1、static关键字" class="headerlink" title="1、static关键字"></a>1、static关键字</h4><p>（1）主要用来修饰类的成员（成员变量、方法）<br>例如：main函数 static 修饰<br>（2）static 特点<br>a、static 修饰的成员在类加载的时候直接运行，优先级要高<br>b、通过类直接访问 类名.成员<br>c、static是针对所有对象的属性值相同时才使用 static 修饰<br>d、被static修饰的方法 无法是有非静态变量；非静态方法 不受限制</p><h4 id="2、静态构造代码块"><a href="#2、静态构造代码块" class="headerlink" title="2、静态构造代码块"></a>2、静态构造代码块</h4><p>形如： </p><pre><code>class 类名{     static{         变量;     } } </code></pre><p>主要是为了 方便变量的统一初始化 执行且只执行一次</p><h4 id="3、构造代码块"><a href="#3、构造代码块" class="headerlink" title="3、构造代码块"></a>3、构造代码块</h4><p>直接在类中定义没有被 static修饰的代码块<br>形如： </p><pre><code>class 类名{     {         变量;     }–构造代码块     func（）{         {        }–普通代码块    } } </code></pre><p>构造代码块可以执行多次，在创建对象的时候使用<br>优先级：先是 静态构造代码块&gt;构造代码块&gt;构造函数</p><h4 id="4、继承的介绍与使用"><a href="#4、继承的介绍与使用" class="headerlink" title="4、继承的介绍与使用"></a>4、继承的介绍与使用</h4><p>（1） extends 多个类中存在相同属性和行为时，将这些内容抽象到单独的一个类中，那么多个类<br>无序再定义这些属性和行为，只需要继承即可<br>父类：又叫基类，超类<br>子类：派生类<br>（2）子类可以访问父类中的非私有的属性和行为<br>（3）子类不能够继承父类的构造方法<br>（4）父类可以被多个子类继承，但是子类只有一个直接父类<br>（5）继承多以存在多级</p><h4 id="5、方法重写"><a href="#5、方法重写" class="headerlink" title="5、方法重写"></a>5、方法重写</h4><p>重载：在同一类中 方法名一样 参数列表不同<br>重写：在继承中出现的，是子类与父类具有相同的方法，子类的这一个方法 叫做重写<br>方法名、返回值、参数列表相同（不同的是函数体） 覆盖</p><h4 id="6、super关键字"><a href="#6、super关键字" class="headerlink" title="6、super关键字"></a>6、super关键字</h4><p>super 作用<br>（1）在子类的构造方法中直接通过super关键字 调用父类的构造方法<br>如果父类有多个构造函数 根据 参数列表来区分 必须放在第一行<br>（2）如果父类与子类中有同名成员变量，此时要访问父类成员变量可以通过super<br>（3）如果子类重写了父类的方法 ，可以通过 super调用父类的方法<br>this–当前对象 子类的方法、属性<br>super–父类对象 父类的方法、属性<br>（4）子类而言 是不是继承了我们父类的所有，自然我们继承了 父类的父类的成员变量和方法<br>所以可以直接通过super调用<br>super.super 多余了<br>（5）破坏了 Java的封装性 只有一个直接父类</p><h4 id="7、final关键字的使用"><a href="#7、final关键字的使用" class="headerlink" title="7、final关键字的使用"></a>7、final关键字的使用</h4><p>final关键字 是一个修饰符，用来修饰 类 方法 变量<br>（1）final修饰一个类，则不能够被继承<br>final类 不想被重新进行重写方法、扩展属性—-直接用 不想被人改变 完美<br>例如：String<br>（2）final 修饰方法，则方法不能够被重写<br>（3）final修饰变量，如果这个值一旦被指定 则 无法改变</p><h4 id="8、static-与-final-关键字"><a href="#8、static-与-final-关键字" class="headerlink" title="8、static 与 final 关键字"></a>8、static 与 final 关键字</h4><p>static：静态变量 只保留一个副本<br>final：用来表示变量不可变<br>被static 修饰以后 只有一个值<br>final 有多个值 因为每次都会赋予一个值 只是保证赋予的这个值不变</p><h4 id="9、Object-类"><a href="#9、Object-类" class="headerlink" title="9、Object 类"></a>9、Object 类</h4><p>顶级父类，是任何类的父类，可以显式的继承 也可以隐式的继承<br>需要重写的方法<br>toString 方法：需要重写 来满足业务需求<br>equals 方法。比较的是地址<br>应用比较广泛</p><h4 id="10、抽象方法"><a href="#10、抽象方法" class="headerlink" title="10、抽象方法"></a>10、抽象方法</h4><p>（1）抽象方法是一种特殊的方法，只有声明没有方法体<br>（2）声明的格式为：<br>abstract 返回值类型 func(参数列表)–抽象方法<br>(public static void main(){方法体})<br>（3）抽象方法存在的意义在于 父类不想或者无法提供方法的方法体（具体实现）<br>只知道有这个方法（针对不同的类 实现方法 不一样）</p><h4 id="11、抽象类"><a href="#11、抽象类" class="headerlink" title="11、抽象类"></a>11、抽象类</h4><p>（1）如果一个类中含有抽象方法，则该类必须被定义为抽象类<br>反过来 抽象类中不一定含有抽象方法<br>（2）声明的格式：<br>abstract class 类名{}—抽象类<br>（3）抽象类特点：<br>a、抽象方法与抽象类均不可以被 final修饰<br>b、如果一个类继承抽象类，则必须完全实现其抽象方法，否则声明为抽象类<br>c、抽象方法必须为 public 或 protected 修饰，不能够用 private 或 static</p><h4 id="12、接口-interface"><a href="#12、接口-interface" class="headerlink" title="12、接口(interface)"></a>12、接口(interface)</h4><p>//数据接口–数据接口协议<br>（1）抽象类的延伸–在一个类中如果所有的方法都是抽象的，则可定义成接口<br>接口比抽象类 更加纯粹。全抽象的<br>（2）接口实现 implements —-对比 继承 extends<br>（3）Java中 子类只能够有一个直接父类，要多继承，必须使用接口<br>接口可以实现多次<br>例如：<br>class A implements I1,I2{</p><p>}<br>//可以组合写 接口可以实现无限个<br>class A extends B implements I1,I2{</p><p>}<br>（4）接口的声明：<br>interface 接口名{}<br>（5）接口不可以实例化，只能够用于实现。<br>（6）接口当中可以含有成员变量和方法，方法都是抽象的<br>变量–public static final<br>一般情况下不要在接口中定义变量<br>（7） 接口中可以定义默认的方法 default 和静态方法</p><h4 id="13、接口与抽象类的区别"><a href="#13、接口与抽象类的区别" class="headerlink" title="13、接口与抽象类的区别"></a>13、接口与抽象类的区别</h4><p>（1）抽象类可以实现接口，接口可以继承接口<br>（2）接口中定义的方法都是抽象的，而抽象类中可以含有普通方法<br>（3）接口中的成员变量 都是public static final的，而抽象类中可以有普通变量<br>（4）接口中一定不含有构造方法，但抽象类中可以有构造方法<br>（5）接口不可以实例化，抽象类可以在子类创建对象的时候自动创建抽象类的对象<br>（6）接口可以多实现，但是抽象类只能够单继承</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java面向对象 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java面向对象1</title>
      <link href="/2019/01/12/java-mian-xiang-dui-xiang-1/"/>
      <url>/2019/01/12/java-mian-xiang-dui-xiang-1/</url>
      
        <content type="html"><![CDATA[<h4 id="1、Eclipse-使用"><a href="#1、Eclipse-使用" class="headerlink" title="1、Eclipse 使用"></a>1、Eclipse 使用</h4><p>IDE：<br>idea myeclipse eclipse NetBeans （visual studio）<br>idea 目前比较流行 有兴趣的可以了解下<br>Git：版本管理工具 从GIt上下载工程<br>JSP：页面 web应用 开发jsp应用<br>点击右上角–》Java<br>（1）选择一个工作空间 —workspace<br>就是电脑上的一个路径，默认的工作空间<br>eclipse-workspace 可以有中文路径<br>（2）project 项目<br>file-&gt;new Java project-&gt;输入工程名-&gt;finish<br>（3）package 包<br>file-&gt;new package-&gt;包的名字（com.ali.entity…）<br>（4）class 类<br>file-&gt;new class-&gt;类的名字（符合规范 字母 数字 下划线 $）<br>（5） run 运行<br>点击 绿色的小三角 run<br>结果在 consonle 去查看<br>WorkSpace-&gt;project-&gt;package-&gt;class run(Javac Java)<br>（6）Eclipse 设置字体大小<br>preference——&gt;font-&gt; Java –设置字体的大小<br>（7）Eclipse 常用快捷键<br>// /<strong>/<br>ctrl+/ 单行注释 //<br>ctrl+shift+/ 多行注释 /</strong>/<br>ctrl+shift+\ 取消多行注释<br>ctrl+s 保存 没事 多按按<br>ctrl+shit+s 工程保存<br>alt+/ 自动补齐<br>ctrl+d 删除<br>ctrl+z 撤销<br>ctrl+shift+f：代码格式化（注意跟 输入法的冲突）<br>ctrl+shift+o：实现包的组织。去除无用的包 实现未导入包的导入</p><h4 id="2、面向对象概述"><a href="#2、面向对象概述" class="headerlink" title="2、面向对象概述"></a>2、面向对象概述</h4><p>Java语言最大的特点<br>面向对象是对现实世界理解和抽象的一种方法<br>核心思想：<br>大象放冰箱里<br>大象：（定义一个类 规定一些属性 身高 体重）<br>冰箱：（定义成一个类 品牌 功率 大小 ）<br>猴子对象<br>冰箱.OpenDoor();<br>冰箱.Save(大象)；<br>冰箱.Close();</p><h4 id="3、面向对象与面向过程"><a href="#3、面向对象与面向过程" class="headerlink" title="3、面向对象与面向过程"></a>3、面向对象与面向过程</h4><p>面向过程：传统程序设计的设计思路。将一个问题看成是一系列函数或者模块的集合<br>自顶向下<br>例如：<br>方法1： 开冰箱门<br>方法2： 放大象<br>方法3：关冰箱门<br>关猴子 重新写方法2<br>最大的区别：面向对象的程序设计具有更高的灵活性，便于程序的扩展和升级<br>面向过程主要是针对特定需求满足某业务条件下的设计<br>面向对象的三大特征：封装 继承 多态</p><h4 id="4、对象"><a href="#4、对象" class="headerlink" title="4、对象"></a>4、对象</h4><p>对象指的是一个具体实例，包含属性和方法<br>例如：<br>夏天属性：身高 体重 年龄 姓名<br>夏天方法：能吃 能睡 工作</p><h4 id="5、类"><a href="#5、类" class="headerlink" title="5、类"></a>5、类</h4><p>具有相同属性和方法的一组对象的集合</p><h4 id="6、类和对象的关系"><a href="#6、类和对象的关系" class="headerlink" title="6、类和对象的关系"></a>6、类和对象的关系</h4><p>对象指的是一个具体的实例<br>类：例如 同学<br>没有指名道姓就不是对象<br>类下面可以有子类 例如： 老鼠是个类 田鼠也是一个类 是老鼠的子类<br>老师、 数学老师、物理老师等都是类</p><h4 id="7、类的创建"><a href="#7、类的创建" class="headerlink" title="7、类的创建"></a>7、类的创建</h4><p>（1）4类8种 基本数据类型<br>（2）引用数据类型：String 数组 接口等<br>自定义的数据类型–用户自己创建的类<br>（3） 修饰符（public等） </p><pre><code>class 类名{     属性：成员变量；     方法：成员函数； } 例如:手机类 public class Phone {}</code></pre><h4 id="8、类和对象的创建与使用"><a href="#8、类和对象的创建与使用" class="headerlink" title="8、类和对象的创建与使用"></a>8、类和对象的创建与使用</h4><p>类名 对象名=new 类名（）;<br>（1）类名 对象名=new 类名（）；–基本形式。<br>可以调用不同参数类型的构造函数 –带参数的形式<br>（2）对象里面的属性（成员变量）、方法<br>通过 对象名.属性<br>对象名.函数<br>实现访问<br>（3）不同的对象 的属性值是不同的 ，而且不交叉<br>相当于一个独立的个体<br>具有独立的地址和存储空间<br>（4）实现对象之间的交互</p><h4 id="9、成员变量与局部变量"><a href="#9、成员变量与局部变量" class="headerlink" title="9、成员变量与局部变量"></a>9、成员变量与局部变量</h4><p>（1）成员变量：对象的属性，放在对象之内<br>（2）局部变量：是在 方法里面 或者 for(int i)<br>成员变量：堆中<br>局部变量 栈中<br>Heap：堆 是临时的 由创建对象时所开辟的一块空间，对象销毁之后，系统回收<br>栈：是方法生成的时候，压栈生成。整个程序结束后才结束。<br>封装 继承 多态 三大特征—-面向对象</p><h4 id="10、封装"><a href="#10、封装" class="headerlink" title="10、封装"></a>10、封装</h4><p>封装：在生活中 包裹。隐私性比较好<br>程序：通过封装成接口，通过方法来调用<br>（1）实现数据的访问权限控制，不是所有人都可以访问<br>（2）实现数据赋值的规范化、标准化的管控<br>例如： person中的性别<br>（3）实现封装的方法是<br>成员变量 加修饰符 private 私有的 无法直接访问 需要生成方法</p><h4 id="11、自动生成-getters-和-setters"><a href="#11、自动生成-getters-和-setters" class="headerlink" title="11、自动生成 getters 和 setters"></a>11、自动生成 getters 和 setters</h4><p>右键-&gt;source-&gt;generate getters and setters-&gt;选中对象的私有属性-&gt;直接生成方法</p><h4 id="12、构造函数"><a href="#12、构造函数" class="headerlink" title="12、构造函数"></a>12、构造函数</h4><p>new 对象的时候 直接初始化 用到构造函数。–&gt;直接赋值<br>例如： int[] arr=new int[]{1,2,3};<br>Person p=new Person(“张三”，20，’男’);<br>构造函数是一种特殊的方法，<br>主要是用来对对象初始化。总是与new 放在一起使用<br>构造函数的函数名是与类名一致<br>构造函数的重载。参数列表不一致的，但是函数名一致的方法<br>按住 ctrl+ 鼠标左键 Open declaration 进入到具体的函数或变量定义的地方</p><h4 id="13、构造函数注意事项"><a href="#13、构造函数注意事项" class="headerlink" title="13、构造函数注意事项"></a>13、构造函数注意事项</h4><p>（1）构造函数 没有返回值<br>（2）构造函数默认存在一个无参的， 自己写一个无参构造函数后 会把默认的冲掉<br>（3）对象在生成的时候调用且只调用一次构造函数<br>（4）如果构造函数调用失败 则无法创建对象<br>（5）对象实例化时 由虚拟机自动调用的</p><h4 id="14、this关键字"><a href="#14、this关键字" class="headerlink" title="14、this关键字"></a>14、this关键字</h4><p>this 表示当前类的对象，哪个对象调用了this所属的方法，this表示哪个对象<br>通过this 可以调用当前对象的 成员变量和方法<br>this(); 表示调用当前对象的无参构造函数</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java面向对象 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础5</title>
      <link href="/2019/01/11/java-ji-chu-5/"/>
      <url>/2019/01/11/java-ji-chu-5/</url>
      
        <content type="html"><![CDATA[<h4 id="1、数组概述"><a href="#1、数组概述" class="headerlink" title="1、数组概述"></a>1、数组概述</h4><p>数组是相同数据类型的一组数据集合。 4类8种基本数据类型<br>数组有索引–代表不同的数值<br>football[7]–&gt;C罗<br>Basketball[23]–&gt;乔丹<br>不同的球队 可以看成不同的数组<br>同一个球队里面 每个球员的编号 唯一<br>数组的长度固定<br>数组的索引从0开始<br>length 数组大小</p><h4 id="2、一维数组"><a href="#2、一维数组" class="headerlink" title="2、一维数组"></a>2、一维数组</h4><p>（1）定义数组<br>dataType 数组名[]<br>dataType[] 数组名—-》<br>例如： int[] array;<br>（2）初始化数组<br>a、首先要确定数组的大小<br>定义时候直接确定：dataType[] array=new dataType[size];<br>dataType[] array;<br>array=new dataType[size];<br>(3) 数组的赋值<br>给数组的元素进行赋值<br>a、 动态赋值<br>b、静态赋值</p><pre><code>dataType[] array=new dataType[]{}; dataType[] array={};</code></pre><h4 id="3、二维数组"><a href="#3、二维数组" class="headerlink" title="3、二维数组"></a>3、二维数组</h4><p>矩阵。 m[i][j] 第i行 j列<br>表示一个 由行列组成的数据，例如 表格<br>10个班级 每个班级有 20 学生 成绩 记录下来<br>用行数 表示 班级<br>用列数表示 每个班级的学生<br>a[10][20]=成绩<br>比如： 小明 第2 班级的底1号学生<br>a[1][0]=90;<br>String[] s;<br>(1)二维数组的声明<br>dataType[][] d_arr=new dataType[row][col];<br>(2)二维数据的初始化<br>a、动态赋值<br>嵌套for循环 遍历二维数组的每个元素<br>b、静态赋值 </p><pre><code>dataType[][] d_arr=new dataType[][]{{},,…,{}}; dataTyep[][] d_arr={{},{},…,{}}; </code></pre><p>二维数组实现 矩阵相乘</p><h4 id="4、方法的概述"><a href="#4、方法的概述" class="headerlink" title="4、方法的概述"></a>4、方法的概述</h4><p>解决某件事情的办法；函数 main<br>计算一个结果<br>处理一段业务逻辑<br>有助于程序的模块化开发和处理<br>方法=函数<br>main函数里面 String[] args 表示的 main函数接受的参数</p><h4 id="5、方法的定义格式"><a href="#5、方法的定义格式" class="headerlink" title="5、方法的定义格式"></a>5、方法的定义格式</h4><p>修饰符 返回值类型 方法的名字（参数列表…）{<br>方法的功能主体<br>return ；// 也可以没有<br>}</p><h4 id="6、方法定义的注意事项"><a href="#6、方法定义的注意事项" class="headerlink" title="6、方法定义的注意事项"></a>6、方法定义的注意事项</h4><p>（1）方法不能定义在其他方法之中 独一性<br>（2）方法如果有返回值类型 一定要返回相应类型的数据<br>例如： double func1（） { return double；不能为 int}<br>（3）调用方法的时候 参数列表一定要对应好<br>例如 func1（int a,b,c）{ (a+b)*c}<br>（4）方法不能重复定义 如果一个方法名字 已经用过了 如果还要用 就需要重载<br>（5） 参数类型与返回值类型无关</p><h4 id="7、方法的重载特性"><a href="#7、方法的重载特性" class="headerlink" title="7、方法的重载特性"></a>7、方法的重载特性</h4><p>同一个类中 允许出现同名的方法，只是方法的参数列表不同，这样的方法称为重载<br>参数列表不同：表示 参数的个数不同 参数数据类型不同<br>（1）重载与参数变量名无关<br>（2）重载与返回值类型无关<br>（3）重载与修饰符无关</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础4</title>
      <link href="/2019/01/09/java-ji-chu-4/"/>
      <url>/2019/01/09/java-ji-chu-4/</url>
      
        <content type="html"><![CDATA[<h4 id="1、位运算符"><a href="#1、位运算符" class="headerlink" title="1、位运算符"></a>1、位运算符</h4><p>主要针对二进制数。 只有 0 1 两种形态。加快运行速度<br>&amp;：位与 两个数同时为1 则为1 否则为0<br>|：位或 两个数中有一个为1 则为1 否则为0<br>^: 异或 相同为0 不同为1</p><p>: 右移运算符 代表位数向右移动<br>&lt;&lt;:左移运算符 代表位数向左移动</p><p>: 无符号右移<br>移动位数很多时，其实按数据的实际有效位数例如 32位，移动位数100%最大位数32 肯定是在32位之内</p><h4 id="2、三元运算符"><a href="#2、三元运算符" class="headerlink" title="2、三元运算符"></a>2、三元运算符</h4><p>布尔表达式？结果1：结果2<br>如果布尔表达式的结果为 true ，进行结果1<br>如果布尔表达式的结果为 false ，进行结果2</p><h4 id="3、转义运算符"><a href="#3、转义运算符" class="headerlink" title="3、转义运算符"></a>3、转义运算符</h4><p>字符并不是你看起来的那个样子，转义了<br>a、八进制转义<br>+用1-3位的8进制数字，范围‘000’-‘377’<br>例如： \0;<br>b、unicode 转义字符<br>\u+ 4位十六进制数字：0-65535<br>\u0000<br>c、特殊字符<br>\”：表示双引号<br>\’:单引号<br>:反斜线<br>d、控制字符<br>\r :回车<br>\n: 换行<br>\t: tab<br>\b:退格</p><p>程序控制语句（顺序 条件 循环）</p><h4 id="4、-if-条件语句"><a href="#4、-if-条件语句" class="headerlink" title="4、 if 条件语句"></a>4、 if 条件语句</h4><p>只要满足某种条件就处理，不完全是 顺序结构，可以跳着执行<br>（1） if （条件语句）{<br>—建议将{ 起始位置写在 if条件之后 便于知道 if语句的范围<br>执行语句；<br>}<br>if else 如果满足条件，我将如何做，否则我该如何做<br>（2） if(条件语句){ </p><pre><code>if(条件语句){     执行语句1； }else{     执行语句2； } </code></pre><p>（3） if..else if（多个）.. else </p><pre><code>if(1){     学习； }else if(2){     运动； }else if(3){     看电视剧;}else{     睡觉}</code></pre><h4 id="5、-switch-条件语句"><a href="#5、-switch-条件语句" class="headerlink" title="5、 switch 条件语句"></a>5、 switch 条件语句</h4><p>形式如下：与 if else if else 很类似 </p><pre><code>switch （条件表达式）{     case 值1：     语句1；     break ；     case 值2：     语句2；     break ；     ….     default :     语句n；     break ； }</code></pre><h4 id="6、-for-循环语句—使用非常广泛"><a href="#6、-for-循环语句—使用非常广泛" class="headerlink" title="6、 for 循环语句—使用非常广泛"></a>6、 for 循环语句—使用非常广泛</h4><p>（1）单层 for 循环语句<br>for(表达式1；表达式2；表达式3){<br>循环体。//就是表示此部分语句需要执行多次。 回旋 跑圈<br>}<br>表达式1：主要是赋一个初始化值， 循环变量的最开始值；<br>表达式2：用来判断 循环变量的值 是否达到 临界值<br>表达式3：主要用来实现 循环变量的增加或减少<br>执行顺序：表达式1 表达式2 循环体 表达式3 表达式2 循环体 表达式3 表达式2 循环体<br>{}–注意 循环体的花括号 可以省略 但是是针对循环体内只有一条语句的情况。<br>(2)嵌套for循环–》在for循环体里面又至少写了一层for循环 </p><pre><code>for(;;){     for(;;){     ….     } }</code></pre><h4 id="7、-while-循环语句"><a href="#7、-while-循环语句" class="headerlink" title="7、 while 循环语句"></a>7、 while 循环语句</h4><p>while(条件表达式){<br>    循环体；<br>}<br>注意 ：条件表达式 一定要注意终止和结束 出现死循环 </p><h4 id="8、-do-while-循环语句"><a href="#8、-do-while-循环语句" class="headerlink" title="8、 do while 循环语句"></a>8、 do while 循环语句</h4><p>do{</p><p>}while(条件表达式)<br>区别： do while 是先执行后判断，至少执行一次<br>while 循环 是先判断后执行</p><h4 id="9、-break-中止语句"><a href="#9、-break-中止语句" class="headerlink" title="9、 break 中止语句"></a>9、 break 中止语句</h4><p>应用：循环体 + 条件语句 switch case<br>（1）针对单层循环结构，表示退出循环<br>（2）针对嵌套循环，表示退出当前的循环<br>（3）switch 条件语句 表示中止 条件语句</p><h4 id="10、-continue-语句"><a href="#10、-continue-语句" class="headerlink" title="10、 continue 语句"></a>10、 continue 语句</h4><p>继续。循环语句里面 使用 continue，并不是中止循环体</p><h4 id="11、-return-语句"><a href="#11、-return-语句" class="headerlink" title="11、 return 语句"></a>11、 return 语句</h4><p>return 的作用主要是<br>（1）用来返回方法的指定类型值<br>（2）结束方法的执行<br>都能中止方法的运行</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础3</title>
      <link href="/2019/01/07/java-ji-chu-3/"/>
      <url>/2019/01/07/java-ji-chu-3/</url>
      
        <content type="html"><![CDATA[<h4 id="1、数据类型的转换"><a href="#1、数据类型的转换" class="headerlink" title="1、数据类型的转换"></a>1、数据类型的转换</h4><p>主要是指的 不同的数据类型之间进行转换<br>（1）自动类型转换<br>范围小的数据类型值，转换为范围大的数据类型的值<br>例如 byte int 自动 byte 转换为 int<br>byte-&gt;short-&gt;int-&gt;long-&gt;float-&gt;double<br>（2）强制数据类型转换<br>数据范围大的转换为数据类型小的<br>强制类型转换不会报错，只是损失了精度<br>例如：喝多了：：： 记不住 精度 就丢失了<br>double 2.134 –&gt; int 2 0.134 没了<br>数据类型之间进行强制转换。比如：<br>int 转换为 String 或者 String转换为 int<br>String与日期类型 转换<br>“2018-11-6 20:37:66:002”–&gt;Date 先记住 后面会在API<br>//Integer.Valueof() ParseInt()</p><h4 id="2、算术运算符"><a href="#2、算术运算符" class="headerlink" title="2、算术运算符"></a>2、算术运算符</h4><p>加减 乘除 求余运算。 + - * / %<br>运算后赋值。赋值运算。<br>+= 相当于 +完之后 赋值 例如 int a=0; a+=10; a=a+10;<br>-=<br>/=<br>关于/，一定要记得 0不能作为除数。异常</p><h4 id="3、自增自减运算符"><a href="#3、自增自减运算符" class="headerlink" title="3、自增自减运算符"></a>3、自增自减运算符</h4><p>++ – int a； 都代表 1次<br>a++:表示自己增加1 表示 先使用变量a 再进行自加运算<br>++a:表示自己增加1 表示 先自加运算 再使用变量a<br>a–:表示自己减少1 表示 先使用变量a 再进行自加运算<br>–a:表示自己减少1 表示 先自减运算 再使用变量a</p><p>一般是在 循环的时候使用–后面讲流程控制时 会详细讲</p><h5 id="4、比较运算符"><a href="#4、比较运算符" class="headerlink" title="4、比较运算符"></a>4、比较运算符</h5><p>&lt; &lt;= &gt;= == !=<br>进行数据的比较，最后的结果为一个 boolean类型的结果<br>条件语句。（if else case while）</p><h4 id="5、逻辑运算符"><a href="#5、逻辑运算符" class="headerlink" title="5、逻辑运算符"></a>5、逻辑运算符</h4><p>逻辑与：<br>&amp;：表示只有表达式两边都是 true 结果才为 true<br>&amp;&amp;：表示只有表达式两边都是 true 结果才为 true<br>区别：短路，提前结束这个判断过程<br>&amp;&amp; 如果第一个条件为 false 则 后面的语句不再运行。 可以加快速度<br>&amp;： 不具有短路功能，从左到右 依次执行<br>逻辑或<br>||：有一个为 true 就为 true<br>|：有一个为 true 就为 true<br>区别： 短路，提前结束这个判断过程<br>||：如果第一个条件为 true 那么后面不再判断，直接输出为 true；<br>|：不具有短路功能，从左到右 依次执行<br>逻辑非<br>！非真即假 非假即真</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础2</title>
      <link href="/2019/01/06/java-ji-chu-2/"/>
      <url>/2019/01/06/java-ji-chu-2/</url>
      
        <content type="html"><![CDATA[<h4 id="1、Java注释"><a href="#1、Java注释" class="headerlink" title="1、Java注释"></a>1、Java注释</h4><p>(1)单行注释 //<br>只能注释一行，而且注释使用实在 // 之后。不会运行<br>例如：<br>(2)多行注释 /<em>*/<br>可以注释多行内容。 主要用来说明一段代码或者一个函数的作用<br>(3)文档注释 /</em>/<br>主要用来说明类的功能，包含的函数、字段以及主要的作者、版本 相关的参数 异常等<br>author：作者<br>version：版本<br>see：参考 一个url链接<br>param：参数<br>return 返回值<br>代码注释很重要 因为不止是对别人看你或理解你的代码，而且对自己也有好处</p><h4 id="2、Java标识符"><a href="#2、Java标识符" class="headerlink" title="2、Java标识符"></a>2、Java标识符</h4><p>所谓标识符就是对Java当中的 变量、类名、对象名、函数等自己的名字，<br>名字必须得符合规范<br>字母、数字、下划线和美元符号组成<br>注意：a、不能以数字开头<br>例如： 1a 错误 但是可以写成 a1<br>b、一般情况下不以美元开头<br>c、见名知意：<br>例如：zhidao licheng juli xingming—不可取<br>Distance Name—推荐用英语 并不是完全的英语照搬<br>xuehao –StudentNumber–&gt;StuNumber、 StdNum StdId<br>d、Java严格区分大小写<br>例如： a 与 A 就是两个变量<br>e、不要使用关键字：<br>例如： public static void if else switch<br>f、驼峰命名法<br>变量： 头一个字母小写 stdName<br>类名：首字母大写 Student StudentInfo<br>具体与单位的要求有关</p><h4 id="3、Java关键字"><a href="#3、Java关键字" class="headerlink" title="3、Java关键字"></a>3、Java关键字</h4><p>（1）所有Java里面被赋予了特殊含义的单词，就叫做关键字<br>（2）Java关键字都是小写<br>a、用于定义数据类型： int class 等<br>b、数据类型值： true false null 等<br>c、流程控制的： for if 等<br>d、访问控制权限的： public 等<br>e、变量、函数等修饰的： static 等<br>f、异常处理的： try catch 等</p><h4 id="4、Java基础数据类型"><a href="#4、Java基础数据类型" class="headerlink" title="4、Java基础数据类型"></a>4、Java基础数据类型</h4><p>计算机里面存储设备的最小单元：位（bit）<br>最小存储单元：1 B=8位 Bit<br>1KB=1024B<br>1MB=1024KB<br>1GB=1024MB<br>1TB=1024GB<br>PB级 的数据量 大数据<br>1PB=1024TB</p><h4 id="5、基本数据类型-4类8种"><a href="#5、基本数据类型-4类8种" class="headerlink" title="5、基本数据类型 4类8种"></a>5、基本数据类型 4类8种</h4><p>（1）整数类型： byte ：1个字节（8位） 范围比较小：-128~127<br>short ：2<br>int:4<br>long :8<br>（2）浮点型（小数）：<br>float ：4 单精度<br>double ：8 双精度 精度高<br>（3）字符型：<br>char ：2 一个字符<br>例如：’笑’、’A’<br>（4）布尔型： boolean ：1个字节 true false</p><h4 id="6、引用数据类型（包含自定义）"><a href="#6、引用数据类型（包含自定义）" class="headerlink" title="6、引用数据类型（包含自定义）"></a>6、引用数据类型（包含自定义）</h4><p>String 型<br>“大家好，欢迎来TZ学习。”<br>数组<br>类等</p><h4 id="7、常量"><a href="#7、常量" class="headerlink" title="7、常量"></a>7、常量</h4><p>常量是一种特殊的变量，只不过是值被设定后，不能改变<br>例如： final(关键字) PI=3.1415926;</p><h4 id="8、变量"><a href="#8、变量" class="headerlink" title="8、变量"></a>8、变量</h4><p>变量是可以随着程序的变化而改变赋值<br>int a=10;<br>a=11;</p><h4 id="9、定义基本数据类型变量"><a href="#9、定义基本数据类型变量" class="headerlink" title="9、定义基本数据类型变量"></a>9、定义基本数据类型变量</h4><p>三要素： 指明类型（整数型、浮点型等）、变量命名、变量赋值（可以没有）</p><h4 id="10、字符串变量的定义"><a href="#10、字符串变量的定义" class="headerlink" title="10、字符串变量的定义"></a>10、字符串变量的定义</h4><p>字符串变量 引用数据类型。应用比较广泛。<br>字符串变量赋值变化时，相当于重新指向了一个对象</p><h4 id="11、数据类型的转换"><a href="#11、数据类型的转换" class="headerlink" title="11、数据类型的转换"></a>11、数据类型的转换</h4><p>主要是指的 不同的数据类型之间进行转换<br>（1）自动类型转换<br>范围小的数据类型值，转换为范围大的数据类型的值<br>例如 byte int 自动 byte 转换为 int<br>（2）强制数据类型转换<br>数据范围大的转换为数据类型小的<br>数据类型之间进行强制转换。比如：<br>int 转换为 String 或者 String转换为 int<br>String与日期类型 转换</p><h4 id="12、算术运算符"><a href="#12、算术运算符" class="headerlink" title="12、算术运算符"></a>12、算术运算符</h4><p>加减 乘除 求余运算<br>+=<br>-=<br>/=<br>自增自减运算符</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础1</title>
      <link href="/2019/01/03/java-ji-chu-1/"/>
      <url>/2019/01/03/java-ji-chu-1/</url>
      
        <content type="html"><![CDATA[<h4 id="1、计算机语言发展史"><a href="#1、计算机语言发展史" class="headerlink" title="1、计算机语言发展史"></a>1、计算机语言发展史</h4><p>Java语言是计算机语言的一种<br>（1）语言：汉语 英语 阿拉伯语 日语—&gt;人与人进行沟通的 一种方式<br>语义。—-》自然语言处理 人工智能中 文本分析 NLP<br>（2）机器语言：人与计算机沟通的语言。—Java 就是其中一种<br>类似于英语在自然语言中的地位 很流行 很主流<br>a、机器语言–初级形态：用二进制编码来表示计算机能够识别和执行的一种机器指令集合<br>例如：0 1 二进制编码 10进制<br>101010110—》启动声卡<br>b、机器语言—中级形态：汇编语言，用一种助记符来机器指令，成为符号语言。<br>例如：mov–表示数据的移动<br>rm-删除<br>add<br>c、机器语言—-高级形态：高级语言。一种接近人们使用习惯高级程序语言<br>例如：c=a+b; 实现数据的加和<br>常见的高级程序语言：Java、C、C++、C#、R、Python、Scala、VB、PHP等等</p><h4 id="2、Java语言概述"><a href="#2、Java语言概述" class="headerlink" title="2、Java语言概述"></a>2、Java语言概述</h4><p>Java语言是一门非常年轻的语言 90后。最早是SUN —–Jamse Gosling（Java之父）<br>Oak–橡树。—Java 看到一个人 拿着爪哇杯 喝咖啡<br>Java语言随着互联网的发展，跨系统、跨平台 能够运行<br>Java语言获得了飞速的发展<br>Java 也形成了自己的一套方法 体系。封装了很多成熟可用的方法可以直接调用<br>API文档 —葵花宝典 Java 字典</p><h4 id="3、Java语言的特性和优点"><a href="#3、Java语言的特性和优点" class="headerlink" title="3、Java语言的特性和优点"></a>3、Java语言的特性和优点</h4><p>（1）跨平台—一次编写 到处运行<br>（2）面向对象—万事万物，皆为对象。 类<br>（3）相对简单—有C语言基础或者其他语言基础，语言之间是有相同性<br>要知道 Java语言的基本语法、基本数据类型、基本程序控制</p><h4 id="4、Java的开发环境"><a href="#4、Java的开发环境" class="headerlink" title="4、Java的开发环境"></a>4、Java的开发环境</h4><p>（1）JDK：Java development Kit： 开发者工具包<br>（2）JRE：Java Runtime Environment：Java 运行环境—只做运行 不做开发时<br>（3）JVM：Java Virtual Machine：Java 虚拟机<br>所有的Java程序都运行在 jvm上<br>JDK或JRE具备后，程序会调用生成 JVM<br>JDk包含JRE</p><h4 id="5、JDK的安装与配置"><a href="#5、JDK的安装与配置" class="headerlink" title="5、JDK的安装与配置"></a>5、JDK的安装与配置</h4><p>jdk 下载官网地址：<a href="https://www.oracle.com/technetwork/Java/Javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/Java/Javase/downloads/jdk8-downloads-2133151.html</a><br>（1）针对从官网上下载的 .exe的安装包<br>a、 选择适合你电脑操作系统和位数的 JDK版本<br>例如Mac linux 还有 32位或64位<br>b、要增加以下<br>1）Java_HOME:jdk 安装的目录。C:\Program Files\Java\jdk1.8.0_171<br>2）在 Path里面 增加 ：<br>英文的字符<br>;%Java_HOME%\bin;%Java_HOME%\jre\bin<br>一定要记得点确定。还要把cmd给关掉，重新打开 cmd 输入Java、Javac等命令验证<br>（2）考一个jdk安装包，之后再Java_home里面进行更改<br>（3）classpath可以不添加</p><h4 id="6、Java程序的概述"><a href="#6、Java程序的概述" class="headerlink" title="6、Java程序的概述"></a>6、Java程序的概述</h4><p>Java程序需要首先完成：<br>（1）Java源文件， .Java 结尾的文件<br>（2）编译生成字节码文件，.class 结尾的文件 很多编码 二进制（16进制）组成的文件<br>（3）将字节码文件 编译器（compiler） JVM能够识别和运行的文件<br>首先编写源文件–》其次通过编译成.class文件–》最后JVM运行</p><h4 id="7、DOS常见的命令"><a href="#7、DOS常见的命令" class="headerlink" title="7、DOS常见的命令"></a>7、DOS常见的命令</h4><p>dir:列出当前目录下的文件及文件夹<br>换盘：直接输入盘符：，例如 切换到D盘 D:<br>cd:换目录 tab键 可以自动补齐。把目录的名字进行补齐<br>md:新建文件目录<br>del:删除文件目录<br>cls:清屏<br>exit：退出<br>上下箭头：可以调用之前输出的命令</p><h4 id="8、第一个Helloworld-Java程序"><a href="#8、第一个Helloworld-Java程序" class="headerlink" title="8、第一个Helloworld Java程序"></a>8、第一个Helloworld Java程序</h4><p>（1）helloworld 的文件名字一定要与 类名（class 后面紧跟的 名字）保持一致<br>（2）设置一下 .Java 源文件编码方式 UTF-8<br>（3）如果设置为 UTF-8 会发现中文输出为乱码，原因是 Java源文件的编码方式与<br>Java编译时的编码方式不一样，造成了乱码</p><h4 id="9、Java注释"><a href="#9、Java注释" class="headerlink" title="9、Java注释"></a>9、Java注释</h4><p>单行注释 //<br>多行注释 /<em>*/<br>文档注释 /</em>/<br>代码注释很重要 因为不止是对别人看你或理解你的代码，而且对你自己也有好处。</p><h4 id="10、Java标识符"><a href="#10、Java标识符" class="headerlink" title="10、Java标识符"></a>10、Java标识符</h4><p>所谓标识符就是对Java当中的 变量、类名、对象名、函数等自己的名字，名字必须得符合规范<br>字母、数字、下划线和美元符号组成</p><h4 id="11、Java关键字"><a href="#11、Java关键字" class="headerlink" title="11、Java关键字"></a>11、Java关键字</h4><p>以Java来编码的 文件 ，蓝色的字符 关键字</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java基础 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
